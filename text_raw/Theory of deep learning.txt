S A N J E E V A R O R A
O T H E R C O N T R I B U T O R S :
R A M A N A R O R A , J O A N B R U N A , N A D AV C O H E N , S I M O N D U , R O N G
G E , S U R I YA G U N A S E K A R , E L A D H A Z A N , C H I J I N , J A S O N L E E , T E N G Y U M A , B E H N A M N E Y S H A B U R ,
Z H A O S O N G
T H E O R Y O F D E E P
L E A R N I N G


Contents
1
Basic Setup and some math notions
17
1.1 List of useful math facts
18
1.1.1
Probability tools
18
1.1.2
Singular Value Decomposition
20
2
Basics of Optimization
21
2.1 Gradient descent (GD)
21
2.1.1
Upperbound on the Taylor Expansion via Smoothness
22
2.1.2
Descent lemma for gradient descent
22
2.2 Stochastic gradient descent (SGD)
23
2.3 Accelerated Gradient Descent
24
2.4 Running time: Learning Rates and Update Directions
25
2.4.1
Pre-conditioners
26
2.5 Convergence rates under smoothness conditions
27
2.5.1
Lower bounds, and the need for smoothness
27
2.5.2
Convergence rates for GD
28
2.5.3
Stochastic gradient descent
29
2.5.4
Adaptive Algorithms and AdaGrad
30
2.5.5
Adagrad Convergence: Diagonal Matrix case
32
2.6 Correspondence of theory with practice
32
3
Note on overparametrized linear regression and kernel regression
35
3.1 Overparametrized least squares linear regression
35
3.1.1
SVD and Matrix pseudo-inverse
36

4
3.2 Kernel least-squares regression
37
4
Note on Backpropagation and its Variants
39
4.1 Problem Setup
39
4.1.1
Multivariate Chain Rule
41
4.1.2
Naive feedforward algorithm (not efficient!)
42
4.2 Backpropagation (Linear Time)
42
4.3 Auto-differentiation
43
4.4 Notable Extensions
44
4.4.1
Hessian-vector product in linear time: Werbos-Pearlmutter trick
45
5
Basics of generalization theory
47
5.1 Occam’s razor formalized for ML
47
5.1.1
Motivation for generalization theory
48
5.1.2
Warmup: Classical polynomial interpolation
49
5.2 Some simple upper bounds on generalization error
49
5.3 Data dependent complexity measures
52
5.3.1
Rademacher Complexity
52
5.3.2
Alternative Interpretation: Ability to correlate with random labels
53
5.4 Understanding limitations of the union-bound approach
53
5.4.1
An illustrative example that mixes optimization and generalization
54
5.5 A Compression-based framework
56
5.5.1
Example 1: Linear classifiers with margin
57
5.5.2
Example 2: Generalization bounds for deep nets using low rank approximations
58
5.6 PAC-Bayes bounds
60
5.7 Exercises
62
6
Tractable Landscapes for Nonconvex Optimization
63
6.1 Preliminaries and challenges in nonconvex landscapes
64
6.2 Cases with a unique global minimum
65
6.2.1
Generalized linear model
66
6.2.2
Alternative objective for generalized linear model
67

5
6.3 Symmetry, saddle points and locally optimizable functions
68
6.4 Case study: top eigenvector of a matrix
70
6.4.1
Characterizing all critical points
70
6.4.2
Finding directions of improvements
72
7
Escaping Saddle Points
75
7.1 Preliminaries
75
7.2 Perturbed Gradient Descent
76
7.3 Saddle Points Escaping Lemma
78
7.3.1
Improve or Localize
79
7.3.2
Bounding the Width of the Stuck Region
79
8
Algorithmic Regularization
83
8.1 Linear models in regression: squared loss
84
8.1.1
Geometry induced by updates of local search algorithms
86
8.1.2
Geometry induced by parameterization of model class
88
8.1.3
Equivalence between geometry inducded by local search algorithms and reparametrization
90
8.1.4
Equivalence Between Commuting Parametrization and Mirror Descent
93
8.2 Matrix factorization
93
8.3 Linear Models in Classification
93
8.3.1
Gradient Descent
94
8.3.2
Steepest Descent
95
8.4 Homogeneous Models with Exponential Tailed Loss
99
8.5 Induced bias in function space
101
9
Ultra-wide Neural Networks and Neural Tangent Kernels
103
9.1 Evolution equation for net parameters
104
9.1.1
Behavior in the infinite limit
105
9.2 NTK: Simple 2-layer example
106
9.3 Explaining Optimization and Generalization of Ultra-wide Neural Networks via NTK
109
9.3.1
Understanding Generalization in 2-layer setting
111

6
9.4 NTK formula for Multilayer Fully-connected Neural Network
113
9.5 NTK in Practice
115
9.6 Exercises
116
10
Interpreting output of Deep Nets: Credit Attribution
117
10.1 Influence Functions
117
10.1.1
Computing Influence Functions
118
10.2 Shapley Values
118
10.2.1
Algorithms to approximate Shapley values
120
10.3 Data Models
121
10.4 Saliency Maps
121
11
Inductive Biases due to Algorithmic Regularization
123
11.1 Matrix Sensing
124
11.1.1
Gaussian Sensing Matrices
126
11.1.2
Matrix Completion
129
11.2 Deep neural networks
131
11.3 Landscape of the Optimization Problem
134
11.3.1
Implicit bias in local optima
136
11.3.2
Landscape properties
138
11.4 Role of Parametrization
144
11.4.1
Related Work
144
12
SDE approximation of SGD and its implications
145
12.1 Understanding gradient noise in SGD
146
12.1.1
Motivating example: Loss with Fixed Gradient
147
12.2 Stochastic processes: Informal Treatment
147
12.2.1
SDEs and SGD
148
12.3 Notion of closeness between stochastic processes
149
12.3.1
Formal Approximation
150
12.3.2
Proof Sketch
152

7
12.4 Stochastic Variance Amplified Gradient (SVAG)
153
13
Effect of Normalization in Deep Learning
155
13.1 Warmup Example: How Normalization Helps Optimization
155
13.2 Normalization schemes and scale invariance
156
13.3 Exponential learning rate schedules
158
13.4 Convergence analysis for GD on Scale-Invariant Loss
158
14
Unsupervised learning: Distribution Learning
163
14.1 Possible goals of unsupervised learning
163
14.2 Training Objective for Learning Distributions: Log Likelihood
165
14.2.1
Notion of goodness for distribution learning
165
14.3 Variational method
167
14.4 Autoencoders and Variational Autoencoder (VAEs)
168
14.4.1
Training VAEs
169
14.5 Normalizing Flows
170
14.6 Stable Diffusion
172
15
Language Models (LMs)
173
15.1 Transformer Architecture
175
15.2 Explanation of Cross-Entropy Loss
175
15.3 Scaling Laws and Emergence
176
15.4 (Mis)understanding, Excess entropy, and Cloze Questions
177
15.5 How to generate text from an LM
178
15.6 Instruction tuning
180
15.7 Aligning LLMs with human preferences
180
15.7.1
Direct Reward Optimization
181
15.8 Mathematical Framework for Skills and Emergence
182
15.8.1
Skills: A Statistical View
184

8
15.9 Analysis of Emergence (uniform cluster)
186
15.9.1
Proof of Theorem 15.9.1.
187
15.9.2
Competence on skills and tuples of skills: Performance Curves
187
15.9.3
The tensorization argument
188
16
Generative Adversarial Nets
191
16.1 Distance between Distributions
191
16.2 Introducing GANs
192
16.2.1
Game-theoretic interpretation and implications for training
194
16.3 "Generalization" for GANs vs Mode Collapse
195
16.3.1
Experimental verification of Mode Collapse: Birthday Paradox Test
196
16.3.2
Other notes on GANs and mode collapse
197
17
Self-supervised Learning
199
18
Adversarial Examples and efforts to combat them
201
18.1 Basic Definitions
201
18.1.1
Attack method: PGD
202
18.1.2
Adversarial Defense
202
18.1.3
Other defense ideas
203
18.2 Provable defense via randomized smoothing
203
19
Examples of Theorems, Proofs, Algorithms, Tables, Figures
207
19.1 Example of Theorems and Lemmas
207
19.2 Example of Long Equation Proofs
207
19.3 Example of Algorithms
209
19.4 Example of Figures
210
19.5 Example of Tables
211
19.6 Exercise
211
Bibliography
213

List of Figures
2.1 Convex and Nonconvex Functions in two variables. For nonconvex
functions GD will reach a stationary point, where gradient is zero.
(Figure from kdnuggets.org)
23
2.2 A difficult “needle in a haystack” case for non-convex optimization.
A function with a hidden valley, with small gradients shown in yel-
low.
28
2.3 How train and test loss behaved during SGD on PreResNet32 on CI-
FAR10 (50k datapoints). The test loss was estimated at various steps
via a fixed held-out dataset. Initial learning rate was 1, and it was
reduced by a factor of 10 at epoch 80 and epoch 300. (An epoch is
a full pass over the training dataset, where the dataset has been ran-
domly partitioned into batches for use in SGDs–in this case the batch-
ses were of size 128.) Note the complicated relationship between train
and test loss, in particular, a slight rise in test loss can happen even
when training loss is flat or going down.
33
2.4 Edge of stability phenomenon. Figure 5 in Cohen et al. 2021 shows
a ResNet trained on 5k examples. When doing GD (as opposed to
SGD) with a small learning rate η, the smoothness is observed to rise
to 2/η and slightly beyond (figure on right). After this point one sees
loss go up and down during iterations, with a long-term downward
trend. No theoretical explanation is known as of now. The authors
use “sharpness”instead of smoothness, which actually makes some
sense because higher L corresponds to a more uneven landscape.
34
4.1 Why it suffices to compute derivatives with respect to nodes.
40
4.2 Multivariate chain rule: derivative with respect to node z can be com-
puted using weighted sum of derivatives with respect to all nodes
that z feeds into.
41
4.3 Vector version of above
44
6.1 Obstacles for nonconvex optimization. From left to right: local min-
imum, saddle point and flat region.
65

10
7.1 Left: Pertubation ball in 3D and “thin pancake” shape stuck region.
Right: Pertubation ball in 2D and “narrow band” stuck region un-
der gradient flow
80
8.1
Steepest descent w.r.t ∥.∥4/3: the global minimum to which steepest descent
converges to depends on η. Here w0 = [0, 0, 0], w∗
∥.∥= arg minR∈G ∥w∥4/3
denotes the minimum norm global minimum, and w∞
η→0 denotes the solu-
tion of infinitesimal SD with η →0. Note that even as η →0, the expected
characterization does not hold, i.e., w∞
η→0 ̸= w∗
∥.∥.
89
9.1 Convergence rate vs. projections onto eigenvectors of the kernel ma-
trix.
111
9.2 Generalization error vs. complexity measure.
112
11.1 Optimization landscape (top) and contour plot (bottom) for a sin-
gle hidden-layer linear autoencoder network with one dimensional
input and output and a hidden layer of width r = 2 with dropout,
for different values of the regularization parameter λ. Left: for λ =
0 the problem reduces to squared loss minimization, which is rota-
tion invariant as suggested by the level sets. Middle: for λ > 0 the
global optima shrink toward the origin. All local minima are global,
and are equalized, i.e. the weights are parallel to the vector (±1, ±1).
Right: as λ increases, global optima shrink further.
137
12.1 Two trajectories generated by two runs of a one-dimensional Wiener
process from the same starting point. They make independent ran-
dom moves and quickly diverge.
148
12.2 Example hybrid trajectories interpolating between SGD and SDE.
152
14.1 Visualization of Pearson’s Crab Data as mixture of two Gaussians.
(Credit: MIX homepage at McMaster University.)
164
14.2 Autoencoder defined using a density distribution p(h, x), where h
is the latent feature vector corresponding to visible vector x. The pro-
cess of computing h given x is called “encoding” and the reverse is
called “decoding.” In general applying the encoder on x followed
by the decoder would not give x again, since the composed trans-
formation is a sample from a distribution.
164
14.3 Faces in the top row were produced by a VAE based method and those
in the second row by RealNVP using normalizing flows. VAE is known
for producing blurry images. RealNVP’s output is much better, but
still has visible artifacts.
171
14.4 Example of noising an image and then denoising, using Diffusion
Model. (Source: Binxu Wang)
172

11
15.1 Perplexity of text from various generation methods. Random and
Greedy are pretty bad. Nucleus Sampling with p = 0.95 gets clos-
est to human. (We did not describe beam search, so please ignore
those rows.)
179
15.2 Performance Curves: Left plot has θ = 0.1 and varies k = 2, 4, 8, 16.
Higher values of k greatly improve performance (for k = 2 valid
α, β did not exist). The right plot has k = 8 and θ = 0.05, 0.1, 0.2.
Section ?? clarifies that it also describes the model’s performance curve
for t-tuples of skills for for θ = 0.05 and t = 1, 2, 4 respectively (e.g.,
blue curve for 4-tuples).
188
18.1 Flying pigs? (A) is image of a pig, and (B) is a slightly perturbed ver-
sion of it. A normally trained ResNet50 classifier labels (B) as "air-
liner." The difference between the two images is tiny; in (C) you see
an image that is 50 times the pixel-wise difference between (A) and
(B). Without the 50x scaling (C) would consist of pixels with values
close to 0 (i.e., blank image). Source: Kolter-Madry Tutorial.
201
18.2 An adversarial 3D object! This stop sign with a few stickers on it re-
liably fooled image recognition classifiers to classify it as a 45mph
speed limit.
201
18.3 PGD attack on input x0. The red arrows correspond to gradient-based
updates. When they produce a point outside Ball(x0, r) a projection
operation (denoted by the green arrows) finds the closest point in
the ball.
202
18.4 Conceptual illustration of adversarial examples for ℓ∞-bounded per-
turbations. In the vanilla classifier most datapoints are close to the
decision boundary, as measured by ℓ∞distance. The red stars are ad-
versarial examples. After adversarial training, a small ℓ∞-ball around
the datapoint no longer intersects the decision boundary. Credit: Madry
et al 2018.
203
18.5 Univariate gaussians centered at x (blue one)and x′ (the red one), and
the point where E(z) switches from 1 to 0.
205
19.1 A chasing sequence
210


List of Tables
19.1 We ignore the O for simplicity. The ℓ∞/ℓ2 is the strongest possible
guarantee, with ℓ2/ℓ2 coming second, ℓ2/ℓ1 third and exactly k-sparse
being the weaker. We also note that all [RV08, CGV13, Bou14, HR16]
obtain improved analyses of the Restricted Isometry property; the
algorithm is suggested and analyzed (modulo the RIP property) in
[BD08]. The work in [HIKP12] does not explicitly state the extension
to the d-dimensional case, but can easily be inferred from the argu-
ments. [HIKP12, IK14, Kap16, KVZ19] work when the universe size
in each dimension are powers of 2.
211


Introduction
Date of this version: March 28 2022
This monograph discusses the emerging theory of deep learning.
It originated from notes by the lecturers at a graduate seminar taught
at Princeton University in Fall 2019 in conjunction with a Special Year
on Optimization, Statistics, and Machine Learning at the Institute for
Advanced Study. Sanjeev Arora has cleaned up and extended the
book during two subsequent offerings of the course in Spring’21 and
Spring’22.
This is closer to lecture notes than to a book. It probably
has many errors and typos.


1
Basic Setup and some math notions
This Chapter introduces the basic nomenclature. Training/test error,
generalization error etc. ≪Tengyu notes: Todos: Illustrate with plots: a typical training
curve and test curve
Mention some popular architectures (feed forward, convolutional, pooling, resnet, densenet) in a
brief para each. ≫
We review the basic notions in statistical learning theory.
• A space of possible data points X .
• A space of possible labels Y.
• A joint probability distribution D on X × Y. We assume that our
training data consist of n data points
(x(1), y(1)), . . . , (x(n), y(n))
i.i.d.
∼D ,
each drawn independently from D.
• Hypothesis space: H is a family of hypotheses, or a family of pre-
dictors. E.g., H could be the set of all neural networks with a fixed
architecture: H = {hθ} where hθ is neural net that is parameter-
ized by parameters θ.
• Loss function: ℓ: (X × Y) × H →R.
– E.g., in binary classification where Y = {−1, +1}, and suppose
we have a hypothesis hθ(x), then the logistic loss function for
the hypothesis hθ on data point (x, y) is
ℓ((x, y), θ) =
1
1 + exp(−yhθ(x)) .
• Expected loss:
L(h) =
E
(x,y)∼D [ℓ((x, y), h)] .
Recall D is the data distribution over X × Y.

18
theory of deep learning
• Training loss (also known as empirical risk):
bL(h) = 1
n
n
∑
i=1
ℓ

x(i), y(i)
, h

,
where

x(1), y(1)
,

x(2), y(2)
, . . . ,

x(n), y(n)
are n training ex-
amples drawn i.i.d. from D.
• Empirical risk minimizer (ERM): bh ∈arg minh∈H bL(h).
• Regularization: Suppose we have a regularizer R(h), then the
regularized loss is
bLλ(h) = bL(h) + λR(h)
.
≪Suriya notes: Misc notations: gradient, hessian, norms≫
1.1
List of useful math facts
Now we list some useful math facts.
1.1.1
Probability tools
In this section we introduce the probability tools we use in the proof.
Lemma 1.1.4, 1.1.5 and 1.1.6 are about tail bounds for random scalar
variables. Lemma 1.1.7 is about cdf of Gaussian distributions. Finally,
Lemma 1.1.8 is a concentration result on random matrices.
Lemma 1.1.1 (Markov’s inequality). If x is a nonnegative random vari-
able and t > 0, then the probability that x is at least t is at most the expecta-
tion of x divided by t:
Pr[x ≥t] ≤E[x]/t.
Lemma 1.1.2 (Chebyshev’s inequality). Let x denote a nonnegative
random variable and t > 0, then
Pr[|x −E[x]| ≥t] ≤Var[x]/t2.
Next we present some concentration bounds regarding sum of
independent random variables. The rule of thumb underlying con-
centration bounds is the Central Limit Theorem.
Theorem 1.1.3 (Central Limit Thm, informal). If X1, X2, . . . , Xn
are independent random variables of mean µ1, µ2, . . . , µn and variances
σ2
1, σ2
2, . . . , σ2
n then as n gets larger, ∑i Xi behaves like the normal distribu-
tion N (∑i µi, ∑i σ2
i ).

basic setup and some math notions
19
Concentration bounds are quantitative versions of this and work
also in settings where pi’s and σi’s could depend on n, the total num-
ber of variables. But in many setting the CLT is a good rule of thumb.
Lemma 1.1.4 (Chernoff bound [Che52]). Let X = ∑n
i=1 Xi, where
Xi = 1 with probability pi and Xi = 0 with probability 1 −pi, and all Xi
are independent. Let µ = E[X] = ∑n
i=1 pi. Then
1. Pr[X ≥(1 + δ)µ] ≤exp(−δ2µ/3), ∀δ > 0 ;
2. Pr[X ≤(1 −δ)µ] ≤exp(−δ2µ/2), ∀0 < δ < 1.
Lemma 1.1.5 (Hoeffding bound [Hoe63]). Let X1, · · · , Xn denote n
independent bounded variables in [ai, bi]. Let X = ∑n
i=1 Xi, then we have
Pr[|X −E[X]| ≥t] ≤2 exp

−
2t2
∑n
i=1(bi −ai)2

.
Lemma 1.1.6 (Bernstein inequality [Ber24]). Let X1, · · · , Xn be indepen-
dent zero-mean random variables. Suppose that |Xi| ≤M almost surely, for
all i. Then, for all positive t,
Pr
"
n
∑
i=1
Xi > t
#
≤exp
 
−
t2/2
∑n
j=1 E[X2
j ] + Mt/3
!
.
Lemma 1.1.7 (Anti-concentration of Gaussian distribution). Let X ∼
N(0, σ2), that is, the probability density function of X is given by ϕ(x) =
1
√
2πσ2 e−x2
2σ2 . Then
Pr[|X| ≤t] ∈
2
3
t
σ, 4
5
t
σ

.
Lemma 1.1.8 (Matrix Bernstein, Theorem 6.1.1 in [Tro15]). Consider a
finite sequence {X1, · · · , Xm} ⊂Rn1×n2 of independent, random matrices
with common dimension n1 × n2. Assume that
E[Xi] = 0, ∀i ∈[m]
and
∥Xi∥≤M, ∀i ∈[m].
Let Z = ∑m
i=1 Xi. Let Var[Z] be the matrix variance statistic of sum:
Var[Z] = max
(
m
∑
i=1
E[XiX⊤
i ]
,

m
∑
i=1
E[X⊤
i Xi]

)
.
Then
E[∥Z∥] ≤(2Var[Z] · log(n1 + n2))1/2 + M · log(n1 + n2)/3.
Furthermore, for all t ≥0,
Pr[∥Z∥≥t] ≤(n1 + n2) · exp

−
t2/2
Var[Z] + Mt/3

.

20
theory of deep learning
explain these in a para
A useful shorthand will be the following: If y1, y2, . . . , ym are in-
dependent random variables each having mean 0 and taking values
in [−1, 1], then their average 1
m ∑i yi behaves like a Gaussian vari-
able with mean zero and variance at most 1/m. In other words, the
probability that this average is at least ϵ in absolute value is at most
exp(−ϵ2m).
1.1.2
Singular Value Decomposition
TBD.

2
Basics of Optimization
This chapter sets up the basic analysis framework for gradient-based
optimization algorithms and discuss how it applies to deep learn-
ing. The algorithms work well in practice; the question for theory
is to analyse them and give recommendations for practice. This has
proved harder, and in recent years it has become clearer that clas-
sical ways of thinking about optimization may not match well with
phenomena encountered in deep learning.
The basic conceptual framework in optimization builds upon
simple Taylor approximation (Equation 2.1) of the loss function and
thus relies upon derivatives (of various orders) of the loss function.
≪Suriya notes: To ground optimization to our case, we can also mention that f is often of the
either the ERM or stochastic optimization form L(w) = ∑l(w; x, y) - it might also be useful to
mention that outside of this chapter, we typically use f as an alternative for h to denote a function
computed≫
2.1
Gradient descent (GD)
Suppose we wish to minimize a continuous function f (w) over Rd.
min
w∈Rd f (w) .
The gradient descent (GD) algorithm is
w0 = initialization
wt+1 = wt −η∇f (wt)
where η is called step size or learning rate. The choice of η is important
and a main subject in the rest of the Chapter.
One motivation or justification of the GD is that the update direc-
tion −∇f (wt) is the steepest descent direction locally. Consider the

22
theory of deep learning
Taylor expansion at a point wt
f (w) = f (wt) + ⟨∇f (wt), w −wt⟩
|
{z
}
linear in w
+ 1
2(w −wt)T∇2 f (wt)(w −wt)
|
{z
}
quadratic in w
+ · · ·
(2.1)
here ∇2( f ) is the matrix of 2nd order derivatives called Hessian. Its
(i, j) entry is ∂2 f /∂wi∂wj. Note that it is a symmetric matrix.
Suppose we drop the higher-order term and only optimize the first
order approximation within a neighborhood of wt
arg min
w∈Rd
f (wt) + ⟨∇f (wt), w −wt⟩
s.t. ∥w −wt∥2 ≤ϵ
Problem 2.1.1. Show that the optimizer of the program above is equal to
w + δ where δ = −α∇f (wt) for some positive scalar α.
In other words, to locally minimize the first order approximation
of f (·) around wt, we should move towards the direction −∇f (wt). 1
1 Gradient descent is not guaranteed
to find optimum solutions for general
loss functions. For instance, complexity
theory shows that given a degree 4
polynomial p(w1, w2, . . . , wn) in n
variables of total degree at most 6,
it is NP-hard to determine whether
or not it is 0 for some assignment
to the variables. This can be proven
easily using NP-completeness of 3SAT
problem.
The classic back-propagation algorithm (Chapter 4) is used to effi-
ciently compute the gradient of the loss. Note that today’s deep nets
often use nonlinear activations, e.g., ReLU, that make the function
computed by the net non-differentiable. However, this differentiabil-
ity is of the mild sort and does not appear to be an issue in practice.
2
2 More recently, replacing ReLU acti-
vations with differentiable ones such
as SWISH or GeLU has been found to
result in no reduction in performance.
2.1.1
Upperbound on the Taylor Expansion via Smoothness
The most basic analysis of training speed of GD involves the smooth-
ness of the loss function.
Definition 2.1.2 (L-smooth). A function f is L-smooth in a domain if for
every w in the domain all eigenvalues of ∇2 f (w) lie in the interval [−L, L].
Problem 2.1.3. Prove that if f is L-smooth then
f (w) ≤f (wt) + ⟨∇f (wt), w −wt⟩+ L
2 ∥w −wt∥2
2
(2.2)
2.1.2
Descent lemma for gradient descent
The following says that with gradient descent and small enough
learning rate, the function value always decreases unless the gradi-
ent at the iterate is zero. (Points where gradient is zero are called
stationary points. )
Lemma 2.1.4 (Descent Lemma). Suppose f is L-smooth. Then, if η <
1/L, we have
f (wt+1) ≤f (wt) −η
2 · ∥∇f (wt)∥2
2

basics of optimization
23
The proof uses the Taylor expansion. The main idea is that even
using the upper bound provided by equation (2.2) suffices. 3
3 The proof also shows that descent
occurs so long as η < 2/L.
Proof. We have that
f (wt+1) = f (wt −η∇f (wt))
≤f (wt) + ⟨∇f (wt), −η∇f (wt)⟩+ L
2 ∥η2∇f (wt)∥2
2
= f (wt) −(η −η2L/2)∥∇f (wt)∥2
2
≤f (wt) −η
2 · ∥∇f (wt)∥2
2,
where the second step follows from Eq. (2.2), and the last step fol-
lows from η ≤1/L.
We’ve shown GD stops making progress when the gradient ∇
becomes zero. Is this good enough?
Figure 2.1: Convex and Non-
convex Functions in two vari-
ables. For nonconvex functions
GD will reach a stationary
point, where gradient is zero.
(Figure from kdnuggets.org)
The loss function for deep learning is non-convex when the net-
work has more than one layer. Thus GD is not guaranteed to produce
a global optimum. Nevertheless, the solutions it finds in practice at-
tain fairly low —even near zero– value of the objective. (Recall that
the loss function is usually non-negative.) Elsewhere in the book this
property of GD is explained in some concrete settings.
Definition 2.1.5. We say w is a stationary point of f if ∇( f (w)) = 0.
If in addition ∇2( f ) is positive-semidefinite at w then w is called a local
minimum.
2.2
Stochastic gradient descent (SGD)
SGD is a very practical variant of gradient descent for large datasets.
Recall that
bL(h) = 1
n
n
∑
i=1
ℓ

(x(i), y(i)), h

.

24
theory of deep learning
Computing the gradient ∇bL(h) scales linearly in n, the size of the
training dataset. Stochastic gradient descent (SGD) estimates the
gradient by sampling a small number of training datapoints and
computing the average. By usual sampling theorems, the gradient
estimate approaches true gradient as the sample size grows.
The updates: We simplify the notations a bit for ease of exposition.
We consider optimizing the function
1
n
n
∑
i=1
fi(w)
So here fi corresponds to ℓ((xi, y(i)), h) in the statistical learning
setting. At each iteration t, the SGD algorithm first samples i1, . . . , iB
uniformly from [n], and then computes the estimated gradient using
the samples:
gS(w) = 1
B
B
∑
k=1
∇fik(wt)
Here S is a shorthand for {i1, . . . , iB}. The SGD algorithm updates the
iterate by
wt+1 = wt −η · gS(wt).
Note that if the learning rate η is very small, then the parameters
will not change much over a sequence of updates, and so SGD would
tend to be similar to (full) GD. However, when η is not too small (and
batch size is small), the gradient estimates from batches are noisy
estimates of the true gradient. One would imagine this makes SGD
worse than GD. In practice SGD tends to be superior to GD. First,
since the stochastic estimation of gradient is so efficient, SGD allows
far more iterations in the same computational budget. Second, SGD
appears to have beneficial effect on generalization, meaning the solu-
tions it finds tend to have better test error than those found by GD.
Later chapters will cover theories that try to account for superiority
of SGD over GD with respect to generalization.
2.3
Accelerated Gradient Descent
The basic version of accelerated gradient descent algorithm is called
heavy-ball algorithm. It has the following update rule:
wt+1 = wt −η∇f (wt) + β(wt −wt−1)
Here β(wt+1 −wt) is the so-called momentum term. The motivation
and the origin of the name of the algorithm comes from that it can be

basics of optimization
25
viewed as a discretization of the second order ODE:
¨w + a ˙w + b∇f (w) = 0
Another equivalent way to write the algorithm is
ut = −∇f (wt) + βut−1
wt+1 = wt + ηut
Exercise: verify the two forms of the algorithm are indeed equivalent.
Another variant of the heavy-ball algorithm is due to Nesterov
ut = −∇f (wt + β · (ut −ut−1)) + β · ut−1,
wt+1 = wt + η · ut.
One can see that ut stores a weighed sum of the all the past gradi-
ents. In effect, the update of wt depends on all past gradients. This is
another interpretation of the accelerate gradient descent algorithm.
Nesterov gradient descent works similarly to the heavy ball al-
gorithm empirically for training deep neural networks. For convex
loss functions it has the advantage of stronger worst case guarantees.
Both versions of accelerated GD can be used with stochastic gradient,
but little is know about the theoretical guarantees about stochastic
accelerated gradient descent.
2.4
Running time: Learning Rates and Update Directions
When the iterations of GD are near a local minimum, the behavior
of gradient descent is clearer because the function can be locally
approximated by a quadratic function. In this section, we assume for
simplicity that we are optimizing a convex quadratic function, and
get some insight on how the curvature of the function influences the
convergence of the algorithm.
We use gradient descent to optimize
min
w
1
2w⊤Aw
where A ∈Rd×d is a positive semidefinite matrix, and w ∈Rd.
Remark: w.l.o.g, we can assume that A is a diagonal matrix. Diago-
nalization is a fundamental idea in linear algebra. Suppose A has
singular vector decomposition A = UΣU⊤where Σ is a diagonal
matrix. We can verify that w⊤Aw = bw⊤Σ bw with bw = U⊤w. In other
words, in a difference coordinate system defined by U, we are deal-
ing with a quadratic form with a diagonal matrix Σ as the coefficient.
Note the diagonalization technique here is only used for analysis.

26
theory of deep learning
Therefore, we assume that A = diag(λ1, . . . , λd) with λ1 ≥· · · ≥
λd. The function can be simplified to
f (w) = 1
2
d
∑
i=1
λiw2
i
The gradient descent update can be written as
x ←w −η∇f (w) = w −ηΣw
Here we omit the subscript t for the time step and use the sub-
script for coordinate. Equivalently, we can write the per-coordinate
update rule
wi ←wi −ηλiwi = (1 −λiηi)wi
Now we see that if η > 2/λi for some i, then the absolute value of
wi will blow up exponentially and lead to an instable behavior. Thus,
we need η ≲
1
max λi . Note that max λi corresponds to the smoothness
parameter of f because λ1 is the largest eigenvalue of ∇2 f = A. This
is consistent with the condition in Lemma 2.1.4 that η needs to be
small.
Suppose for simplicity we set η = 1/(2λ1), then we see that the
convergence for the w1 coordinate is very fast — the coordinate w1 is
halved every iteration. However, the convergence of the coordinate
wd is slower, because it’s only reduced by a factor of (1 −λd/(2λ1))
every iteration. Therefore, it takes O(λ1/λd · log(1/ϵ)) iterations to
converge to an error ϵ. The analysis here can be extended to general
convex function, which also reflects the principle that:
The condition number is defined as κ = σmax(A)/σmin(A) = λ1/λd.
It governs the convergence rate of GD.
≪Tengyu notes: add figure≫
2.4.1
Pre-conditioners
From the toy quadratic example above, we can see that it would be
ideal if we could use a different learning rate for different coordinate.
In other words, if we introduce a learning rate ηi = 1/λi for each
coordinate, then we can achieve faster convergence. In the more gen-
eral setting where A is not diagonal, we don’t know the coordinate
system in advance, and the algorithm corresponds to
w ←w −A−1∇f (w)
In the even more general setting where f is not quadratic, this corre-
sponds to the Newton’s algorithm
w ←w −∇2 f (w)−1∇f (w)

basics of optimization
27
Computing the hessian ∇2 f (w) can be computational difficult
because it scales quadratically in d (which can be more than 1 million
in practice). Therefore, approximation of the hessian and its inverse is
used:
w ←w −ηQ(w)∇f (w)
where Q(w) is supposed to be a good approximation of ∇2 f (w),
and sometimes is referred to as a pre-conditioner. In practice, often
people first approximate ∇2 f (w) by a diagonal matrix and then
take its inverse. E.g., in Adagrad one uses a weighted sum of recent
values of diag(∇f (w)∇f (w)⊤) to approximate the Hessian, and then
use the inverse of the diagonal matrix as the pre-conditioner (see
Section 2.5.4).
2.5
Convergence rates under smoothness conditions
As mentioned in Chapter 2, gradient-based methods cannot in gen-
eral find the optimum value of simple functions such as low-degree
polynomials. But we did note that if the function is differentiable and
smooth, then with a suitably small learning rate, loss does decrease
monotonically so long as the gradient is nonzero. In other words,
the process ends up with a stationary point, where ∇= 0. This chap-
ter establishes upper bounds on how long it takes to get close to a
stationary point. See Chapter 7 for analysis of convergence rate to a
stronger type of solution: local optimum.
As usual the objective/loss function is denoted f (w) where w ∈
ℜd. The procedure has T iterations, and the parameter vectors in
these iterations are denoted w1, ..., wT respectively. We assume
boundedness: i.e., there is a known M such that | f (wt)| ≤M
2 for all
t = 1, . . . , T. We also assume f is β-smooth, i.e.,
f (w) ≤f (w′) + ∇f (w′)(w −w′) + β
2 ∥w −w′∥2.
(2.3)
Throughout the chapter, ∇t is shorthand for ∇f (wt).
This β is the same as L elsewhere in the chapter.
2.5.1
Lower bounds, and the need for smoothness
In constrained non-convex optimization, minimizing the gradient
presents difficult computational challenges. In general, even when
objective functions are bounded, local information may provide no
information about the location of a stationary point.
Consider, for example, the function sketched in Figure 2.2. In
this construction, defined on the hypercube in Rn, the unique point

28
theory of deep learning
with a vanishing gradient is a hidden valley, and gradients outside
this valley are all identical. Clearly, it is hopeless in an information-
theoretic sense to find this point efficiently: the number of value or
gradient evaluations of this function must be exp(Ω(n)) to discover
the valley.
Figure 2.2: A difficult “nee-
dle in a haystack” case for
non-convex optimization. A
function with a hidden valley,
with small gradients shown in
yellow.
To circumvent such inherently difficult and degenerate cases, we
require that the objective function be smooth. As we shall see, this
allows efficient algorithms for finding a point with small gradient.
2.5.2
Convergence rates for GD
This section analyses gradient descent given exact gradient. Next
section analyses stochastic GD.
Algorithm 1 Gradient descent
1: Input: f, T, initial point w1 ∈K, sequence of step sizes {ηt}
2: for t = 1 to T do
3:
Let wt+1 = wt −ηt∇f (wt)
4: end for
5: return wτ, τ ∈[T] s.t. ∇τ is smallest in Euclidean norm.
Theorem 2.5.1. For unconstrained minimization of β-smooth functions
and ηt = 1
β, Algorithm 1 satisfies
∥∇τ∥2 ≤1
T ∑
t
∥∇t∥2 ≤4Mβ
T
.
Proof. Denote ht = f (wt) −f (w∗). The Descent Lemma is given in
the following simple equation,
ht+1 −ht = f (wt+1) −f (wt)
≤∇⊤
t (wt+1 −wt) + β
2 ∥wt+1 −wt∥2
β-smoothness
= −ηt∥∇t∥2 + β
2 η2
t ∥∇t∥2
algorithm defn.
= −1
2β∥∇t∥2
choice of ηt = 1
β

basics of optimization
29
Thus, summing up over T iterations, we have
1
2β
T
∑
t=1
∥∇t∥2 ≤∑
t
(ht −ht+1) = h1 −hT+1 ≤2M
2.5.3
Stochastic gradient descent
In optimization for machine learning, the objective function f takes
the form
f (w) = 1
m ∑
i
ℓ(w, zi),
where zi, i ∈[m] are the training set examples, and ℓis some loss
function that applies to the parameters w and datapoint zi. The key
idea of Stochastic Gradient Descent is that a random variable can
be used in lieu of the gradient, that has the same expectation. This
random variable is simply the average gradient of small batch of
examples from the training set. The analysis below even allows batch
size 1 (see Problem 2.5.3).
We denote by b∇t a random variable such that E[ b∇t] = ∇f (wt) =
∇t (where expectation is over randomness used in gradient estima-
tion) and a bound on the second moment of this random variable
by
E[∥b∇t∥2] = σ2.
(2.4)
Algorithm 2 Stochastic gradient descent
1: Input: f, T, initial point w1 ∈K, sequence of step sizes {ηt}
2: for t = 1 to T do
3:
Let wt+1 = wt −ηt b∇t
4: end for
5: return wτ, τ ∈[T] s.t. ∇τ is smallest in Euclidean norm.
Theorem 2.5.2. For unconstrained minimization of β-smooth functions
and ηt = η =
q
M
βσ2T, Algorithm 2 satisfies
E[∥∇τ∥2] ≤E
"
1
T ∑
t
∥∇t∥2
#
≤2
r
Mβσ2
T
.
Proof. Denote by ∇t the shorthand for ∇f (wt), and ht = f (wt) −
f (w∗). The stochastic descent lemma is given in the following equa-

30
theory of deep learning
tion,
E[ht+1 −ht] = E[ f (wt+1) −f (wt)]
≤E[∇⊤
t (wt+1 −wt) + β
2 ∥wt+1 −wt∥2]
β-smoothness
= −E[η∇⊤
t e∇t] + β
2 η2 E ∥e∇t∥2
algorithm defn.
= −η∥∇t∥2 + β
2 η2σ2
variance bound.
Thus, summing up over T iterations, we have for η =
q
M
βσ2T,
E
"
1
T
T
∑
t=1
∥∇t∥2
#
≤
1
Tη ∑t E [ht −ht+1] + η β
2 σ2 ≤M
Tη + η β
2 σ2
=
q
Mβσ2
T
+ 1
2
q
Mβσ2
T
≤2
q
Mβσ2
T
.
We thus conclude that O( 1
ϵ4 ) iterations are needed to find a point
with ∥∇f (w)∥≤ϵ. However, each iteration only needs a stochastic
estimate of the gradient, so in practice SGD ends up being much
faster.
Problem 2.5.3. Suppose the gradient is estimated using a random sample
of B datapoints. (a) Let e∇(B)
t
be the stochastic gradient at time t when the
batchsize is B. Suppose the variance of e∇(1)
t
(defined as E
 e∇(1)
t
−∇t

2
is bounded by γ2
1. Show that there exists an upper bound γ2
B on the variance
of e∇(B)
t
that scales with 1/B. (b) Compute the asymptotic size of T to find a
point with ∥∇f (w)∥≤ϵ depending on B and ϵ. For simplicity, you only
need to consider the case when η ≤1
β.
2.5.4
Adaptive Algorithms and AdaGrad
Adaptive methods maintain some information from past updates
and use them to modify the basic gradient step. A simple example,
momentum, was briefly discussed in Chapter 2. Adaptive methods re-
quire more space to store their parameters, usually 2 or 3 parameters
for each of the d coordinates in the gradient. But they can have faster
convergence, as well as other myterious properties in deep learning
setting that are not mathematically understood.
tbd: describe rmsprop, adam
Several of these algorithms are not guaranteed to converge even
for convex loss. We analyse AdaGrad 4, which was a precursor to
4 J. Duchi, E. Hazan, and Y. Singer.
Adaptive subgradient methods for
online learning and stochastic opti-
mization. Journal of Machine Learning
Research, 2011
modern adaptive algorithms and does have a proof of convergence.

basics of optimization
31
Algorithm 3 AdaGrad
for t = 1 to T do
Input: Matrices/scalars Pt, as below
Set
wt+1 = wt −Pt b∇t
end for
return wτ, τ ∈[T] s.t. ∇τ is smallest in Euclidean norm.
We start with a simple analysis of an adaptive stepsize first, as per
the following theorem. In this section, in addition to the aforemen-
tioned notation, we also use the shorthand notation ∇1:t = ∑t
i=1 ∇i,
and let G ≥∥∇t∥be an upper bound on the gradient norm.
Theorem 2.5.4. For unconstrained minimization of β-smooth functions
and Pt = ∥b∇2
1:t∥
−1 · I, Algorithm 3 satisfies
E[∥∇τ∥2] ≤E
"
1
T ∑
t
∥∇t∥2
#
≤(β + M log GT) · ∥b∇2
1:t−1∥
T
.
Proof. From the descent lemma:
−M
≤f (wT+1) −f (w1)
= ∑t( f (wt+1) −f (wt))
≤∑t(∇⊤
t (wt+1 −wt) + β
2 ∥wt −wt+1∥2)
smoothness
≤∑t(−∇⊤
t Pt b∇t + β
2 b∇⊤
t P2
t b∇t)
Let Pt = ∥b∇2
1:t∥−1, and let σ2 ≥∥b∇t∥2 be an upper bound on the
second moment of the stochastic gradient. Then notice that by the
Harmonic series,
∑
t
b∇⊤
t P2
t b∇t = ∑
t
∥b∇t∥2
∥b∇2
1:t∥2 = ∑
t
∥b∇t∥2
∑t
i=1 ∥b∇2
i ∥2 ≤log GT
Using this inequality in the previous derivation, we get that
∑
t
∇⊤
t Pt b∇t ≤M + β
2 log GT.
Taking the minimal valued LHS, we get
∇⊤
τ b∇τ · Pτ−1 ≤∇⊤
τ b∇τ · Pτ ≤(β + M log GT)
T
.
Taking expectation over the unbiased gradient estimator, and shifting
sides, we get
∥∇τ∥2 ≤(β + M log GT) · ∥b∇2
1:t−1∥
T
.

32
theory of deep learning
2.5.5
Adagrad Convergence: Diagonal Matrix case
Theorem 2.5.5. For unconstrained minimization of β-smooth functions
and Pt = diag(∑t−1
i=1 b∇i b∇⊤
i + σ2I)−1/2, Algorithm 3 satisfies
E[∥∇τ∥2] ≤E
"
1
T ∑
t
∥∇t∥2
#
≤(M + β log GT) ·
∑j
q
b∇2
1:t(j)
T
.
Proof. From the descent lemma:
M
≥f (w1) −f (wT+1)
= ∑t( f (wt) −f (wt+1))
≥∑t(∇⊤
t (wt −wt+1) −β
2 ∥wt −wt+1∥2)
smoothness
= ∑t(∇⊤
t Pt b∇t −β
2 b∇⊤
t P2
t b∇t).
Taking conditional expectation, and the definition of Pt which is
conditionally independent of b∇t, we get
M
≥∑d
i=1
n
∑t(∇2
t (i)Pt(i) −β
2 b∇2
t (i)P2
t (i))
o
≥∑d
i=1
n
∑t(∇2
t (i)Pt(i) −β
2 log σ2T)
o
≥maxd
i=1 ∑t ∇2
t (i)Pt(i) −β
2 log σ2T,
where the second inequality is due to the Harmonic series,
∑
t
b∇2
t (i)P2
t (i) = ∑
t
b∇2
t (i)
b∇2
1:t−1(i) + σ2 ≤∑
t
b∇2
t (i)
b∇2
1:t(i)
≤log σ2T.
We conclude that any j,
∑
t
∇2
t (j)Pt(j) ≤max
i ∑
t
∇2
t (i)Pt(i) ≤M + β
2 log σ2T.
Let cj be a random variable which is equal to ∇2
t (j) with probabil-
ity 1
T. Then the above implies that
E[cj] ≤M + β
2 log σ2T
TPt(j)
= (M + β log GT) ·
q
b∇2
1:t(j)
T
.
Thus, summing over the coordinates j, we get
E ∥∇2
τ∥2 = E[∑
j
cj] ≤(M + β log GT) ·
∑j
q
b∇2
1:t(j)
T
.
2.6
Correspondence of theory with practice
Now we describe how the above theory compares with reality. Turns
out that the assumption of a fixed and known smoothness (used also
in Chapters 6, 7, and various other places) is problematic in today’s
deep learning settings.

basics of optimization
33
Figure 2.3: How train and test
loss behaved during SGD on
PreResNet32 on CIFAR10 (50k
datapoints). The test loss was
estimated at various steps via
a fixed held-out dataset. Ini-
tial learning rate was 1, and
it was reduced by a factor of
10 at epoch 80 and epoch 300.
(An epoch is a full pass over
the training dataset, where the
dataset has been randomly par-
titioned into batches for use in
SGDs–in this case the batchses
were of size 128.) Note the com-
plicated relationship between
train and test loss, in particular,
a slight rise in test loss can hap-
pen even when training loss is
flat or going down.
Setting learning rate
Various algorithms in this chapter set learning
rate using smoothness parameter. Unfortunately, it is not easy to
estimate the smoothness parameter for the loss function over the
entire space of parameter vectors. For a particular parameter vector
w however it is possible to estimate the maximum eigenvalue of
the Hessian H = ∇2( f ) at w. This uses the power method, which
starts with a gaussian unit vector u and repeatedly computes u ←
Hu/∥Hu∥2. (Each iteration is efficiently implemented thanks to the
Hessian-vector product computation described in Chapter 4.) This
method is called the power method because it effectively amounts
to computing Htx/∥Htx∥2, which can be easily checked to converge
to a vector that is a combination of the eigenvectors corresponding
to the largest eigenvalue (in absolute value) of H. In particular, if
v is the final vector, vTHv would be a good approximation to the
smoothness.
Of course, this only yields the smoothness parameter for a partic-
ular parameter vector w. As w changes, the smoothness can change
too. Recomputing smoothness frequently would be computationally
expensive. In practice the learning rate is set heuristically to some
value. If GD does not lower the loss for a few iterations then learning
rate is reduced by a small factor, like 2 or 5. In later chapters we will
revisit the topic of learning rates.
Edge of stability phenomenon.
The exposition of learning rates given
above is canonical in classical optimization theory—it takes smooth-
ness L as given and describes how learning rate must be set less than
2/L to ensure consistent decrease in the loss. A recent paper 5 gives
5 Jeremy Cohen, Simran Kaur, Yuanzhi
Li, J Zico Kolter, and Ameet Talwalkar.
Gradient descent on neural networks
typically occurs at the edge of stability.
ICLR, 2021
evidence that in deep nets the cart appears before the horse, so to

34
theory of deep learning
speak. In other words, if we set the learning rate to some small η,
the smoothness adjusts quickly to around 2/η. This “edge of stabil-
ity”phase appears to be important for good final performance.
Figure 2.4: Edge of stability
phenomenon. Figure 5 in Co-
hen et al. 2021 shows a ResNet
trained on 5k examples. When
doing GD (as opposed to SGD)
with a small learning rate η,
the smoothness is observed
to rise to 2/η and slightly be-
yond (figure on right). After
this point one sees loss go up
and down during iterations,
with a long-term downward
trend. No theoretical explana-
tion is known as of now. The
authors use “sharpness”instead
of smoothness, which actually
makes some sense because
higher L corresponds to a more
uneven landscape.

3
Note on overparametrized linear regression and kernel
regression
This brief section analyzes gradient descent for a very classic model:
least-squares linear regression. The problem is convex, and optimiza-
tion does work well. We are interested primarily in the underdeter-
mined version, where one has infinitely many zero-loss solution and
the interesting question is: what does gradient descent find? We find
an elegant exact analysis using pseudo-inverse. The analysis also
extends to Kernel least squares regression.
Though classical, these analyses are the starting point for efforts
(described in Chapters 9 and 8)) to understand over-parametrized
deep nets, which also have an abundance of low cost solutions, and
we wish to understand which ones are found by gradient descent
and related algorithms.
3.1
Overparametrized least squares linear regression
As in Chapter 1 we assume our training data consist of n data points,
each drawn independently from a distribution D,
(x(1), y(1)), . . . , (x(n), y(n))
i.i.d.
∼D.
Here x(i) ∈ℜd and y(i) ∈ℜ. It will be convenient to write the ℓ2 loss
bL(w) = 1
2 ∑
i
(x(i) · w −y(i))2,
using matrix notation as bL(w) = 1
2∥Xw −y∥2 where X is the matrix
whose rows are the x(i)’s, w is a column vector and y is the column
vector whose ith entry is y(i). We’re interested in the case where x(i)’s
are independent and d > n. Then the loss has infinitely many min-
imizers1, all of which attain zero training loss. What does gradient
1 You can verify this by noticing that a
feasible solution exists after setting an
arbitrary subset of d −n coordinates in
w to zero.
descent find?

36
theory of deep learning
For simplicity, initialize gradient descent with starting point w0 =
0. The gradient at any w is ∇bL(w) = XT(Xw −y). Thus gradient
descent with learning rate η gives the following trajectory
wt+1 = wt −ηX⊤(Xwt −y)
(3.1)
= (Id×d −ηX⊤X)wt + ηX⊤y
(3.2)
= (
t
∑
j=0
(Id×d −ηX⊤X))jηX⊤y
(3.3)
Assuming η was small enough by trial and error, specifically, η <
1/λmax(XTX), the infinite series as t →∞in the above equation
converges to 2
2 We’re using that ∑i≥0 Ai = (I −A)†
when the largest eigenvalue of positive
semidefinite matrix A is less than 1 and
Z† denotes the pseudo inverse of Z.
Further, the second equality in eq. (3.5)
can be verified by using the SVD of X.
lim
t→∞wt = (X⊤X)†X⊤y
(3.4)
= X⊤(XX⊤)−1y
(3.5)
which of course is the famous pseudo-inverse solution for overdeter-
mined systems of linear equations. This solution is also the minimiz-
ing ℓ2-norm solution that fits the data: arg minw∈ℜd ∥w∥2 s.t. Xw = y.
3.1.1
SVD and Matrix pseudo-inverse
The inverse of a matrix is defined only for the square matrices with
full rank. The above example illustrates that we need notions of in-
verse for non-square matrices as well as for rank-deficient square
matrices. The Moore-Penrose pseudo-inverse was defined in the 20th
century with these motivations. For simplicity we describe this the-
ory for real m × n matrices, which are known to have a singular value
decomposition (SVD) of the form:
M =
k
∑
i=1
σiuivT
i ,
(3.6)
where k is the rank of M, the σi’s are the singular values, and ui ∈
ℜm, vi ∈ℜn are column vectors.
The pseudo-inverse is the n × m matrix M† defined as follows,
where the notation assumes all σi’s are nonzero:
M† =
k
∑
i=1
1
σi
viuT
i
(3.7)
A special case is when M is symmetric m × m, in which case the SVD
has vi = ui and called the spectral decomposition.
Problem 3.1.1. Show that MM†M = M and M†MM† = M†.
Problem 3.1.2. Prove the properties mentioned in (3.5). 3
3 Hint: if M is symmetric, the eigen-
values of Mj are jth powers of the
eigenvalues of M. Also, the eigenvec-
tors constitute an orthonormal basis.

note on overparametrized linear regression and kernel regression
37
3.2
Kernel least-squares regression
Kernel models involve a new representation for the data space X ,
which represents datapoint x ∈X as ϕ(x) where ϕ is a mapping
from ℜd to a suitable Hilbert space 4 known as the “Reproducing
4 Hilbert space is the generalization of
vector spaces to infinite dimensions,
with inner products being well-defined.
Kernel Hilbert Space”or RKHS. This means that the inner product
ϕ(x) · ϕ(y) is well-defined for every x, y ∈X . It is customary to de-
note the inner product as K(x, y), which is called the kernel function.
The function class of interest are linear models over the transformed
features hw(x) = ϕ(x) · w. A plethora of useful kernels are known in
mathematics and data science.
Example 3.2.1. The Kernel K(x, y) = (1 + x · y)2 corresponds to repre-
senting x = (x1, x2, . . . , xd) by
ϕ(x) = (1,
√
2x1, . . . ,
√
2xd,
√
2x1x2, . . . ,
√
2x1xd,
√
2x2x3, . . . ,
√
2x2xd, . . . ,
√
2xd−1xd, x2
1, x2
2, . . . , x2
d).
You can verify that ϕ(x) · ϕ(y) = K(x, y).
In data science the key property needed is that the inner product
K(x1, x2) be efficiently computable. In fact in practice researchers de-
sign the kernel by starting with this property of efficient computabil-
ity and never consider the underlying representation ϕ(·) because
that plays no role in the training.
Problem 3.2.2. Suppose datapoints are unit vectors in ℜd. Find an in-
finite dimensional representation φ() that realizes the following kernels.
(a) (polynomial kernel) K(x1, x2) = (1 + (x · y)d). (b) (Gaussian Ker-
nel) K(x1, x2) = exp(−∥x −y∥2). (c) (Laplace Kernel) K(x1, x2) =
exp(∥x1 −x2∥2) = exp(√1 −2x1 · x2). (Hint for (b) and (c): look at the
Taylor expansion of K().)
Now let’s see how to solve the following kernel regression effi-
ciently.
ℓ(w) = 1
2 ∑
i
(ϕ(x)(i) · w −y(i))2.
While ϕ(x) is infinite dimensional, we quickly realize that the earlier
analysis of over-parametrized regression applies. Computing data
matrix XX⊤requires only inner products, which are well-defined
in RKHS and so the data matrix turns into an n × n gram matrix G
where Gij = ϕ(x(i)) · ϕ(x(j)) = K(x(i), x(j)) In fact computing G
allows performing gradient descent without explicitly computing
ϕ(x(i)).Expression (3.5) shows gradient descent ends with a classifier
h(x) = limt→∞wt.ϕ(x) that maps an input point x ∈X to
h(x) = zTG−1y
(3.8)

38
theory of deep learning
where z is the column vector whose ith coordinate is K(x, xi). Alter-
natively, denoting α = G−1y ∈ℜn, the solution in eq. (3.8) is alter-
natively viewed as a weighted combinations of kernel evaluations at
training points,
h(x) = ∑
i
αiK(x, xi).
(3.9)
Expression in eq. (3.9) is equivalent to minimizing the ℓ2 norm of
w while fitting the kernel regression objective, which viewed in the
function space corresponds to the minimum RKHS norm solution
wrt the kernel K.

4
Note on Backpropagation and its Variants
Throughout the book we rely on computing the gradient of the loss
with respect to model parameters. For deep nets, this computation is
done with Backpropagation, a simple algorithm that uses the chain
rule of calculus. For convenience we describe this more generally as
a way to compute the sensitivity of the output of a neural network to
all of its parameters, namely, ∂f /∂wi, where f is the output and wi
is the ith parameter. Here parameters can be edge weights or biases
associated with nodes or edges of the network. Versions of this basic
algorithm have been apparently independently rediscovered several
times from 1960s to 1980s in several fields. This chapter introduces
this algorithms as well as some advanced variants involving not just
the gradient but also the Hessian.
In most of the book, the quantity of interest is the gradient of
the training loss. But the above phrasing —computing gradient of
the output with respect to the inputs—is fully general since one
can simply add a new output node to the network that computes
the training loss from the old output. Then the quantity of interest
is indeed the gradient of this new output with respect to network
parameters.
The importance of backpropagation derives from its efficiency.
Assuming node operations take unit time, the running time is linear,
specifically, O(Network Size) = O(V + E), where V is the number
of nodes in the network and E is the number of edges. As in many
other settings in computer science —for example, sorting numbers—
the naive algorithm would take quadratic time, and that would be
hugely inefficient or even infeasible for today’s large networks.
4.1
Problem Setup
Backpropagation applies only to acyclic networks with directed
edges. (It can be heuristically applied to networks with cycles, as
sketched later.) Without loss of generality, acyclic networks can be

40
theory of deep learning
visualized as being structured in numbered layers, with nodes in the
t + 1th layer getting all their inputs from the outputs of nodes in lay-
ers t and earlier. We use f ∈R to denote the output of the network.
In all our figures, the input of the network is at the bottom and the
output on the top.
Our exposition uses the notion ∂f /∂u, where f is the output and
u is a node in the net. This means the following: suppose we cut
off all the incoming edges of the node u, and fix/clamp the current
values of all network parameters. Now imagine changing u from
its current value. This change may affect values of nodes at higher
levels that are connected to u, and the final output f is one such
node. Then ∂f /∂u denotes the rate at which f will change as we
vary u. (Aside: Readers familiar with the usual exposition of back-
propagation should note that there f is the training error and this
∂f /∂u turns out to be exactly the "error" propagated back to on the
node u.)
Claim 4.1.1. To compute the desired gradient with respect to the parame-
ters, it suffices to compute ∂f /∂u for every node u.
Proof. Follows from direct application of chain rule and we prove
it by picture, namely Figure 4.1. Suppose node u is a weighted sum
of the nodes z1, . . . , zm (which will be passed through a non-linear
activation σ afterwards). That is, we have u = w1z1 + · · · + wnzn. By
Chain rule, we have
∂f
∂w1
= ∂f
∂u · ∂u
∂w1
= ∂f
∂u · z1.
Figure 4.1: Why it suffices
to compute derivatives with
respect to nodes.
Hence, we see that having computed ∂f /∂u we can compute

note on backpropagation and its variants
41
∂f /∂w1, and moreover this can be done locally by the endpoints
of the edge where w1 resides.
4.1.1
Multivariate Chain Rule
Towards computing the derivatives with respect to the nodes, we
first recall the multivariate Chain rule, which handily describes the
relationships between these partial derivatives (depending on the
graph structure).
Suppose a variable f is a function of variables u1, . . . , un, which
in turn depend on the variable z. Then, multivariate Chain rule says
that
∂f
∂z =
n
∑
j=1
∂f
∂uj
· ∂uj
∂z .
To illustrate, in Figure 4.2 we apply it to the same example as we
used before but with a different focus and numbering of the nodes.
Figure 4.2: Multivariate chain
rule: derivative with respect to
node z can be computed using
weighted sum of derivatives
with respect to all nodes that z
feeds into.
We see that given we’ve computed the derivatives with respect to
all the nodes that is above the node z, we can compute the derivative
with respect to the node z via a weighted sum, where the weights
involve the local derivative ∂uj/∂z that is often easy to compute.
This brings us to the question of how we measure running time. For
book-keeping, we assume that
Basic assumption: If u is a node at level t + 1 and z is any node at level
≤t whose output is an input to u, then computing ∂u
∂z takes unit time
on our computer.

42
theory of deep learning
4.1.2
Naive feedforward algorithm (not efficient!)
It is useful to first point out the naive quadratic time algorithm im-
plied by the chain rule. Most authors skip this trivial version, which
we think is analogous to teaching sorting using only quicksort, and
skipping over the less efficient bubblesort.
The naive algorithm is to compute ∂ui/∂uj for every pair of nodes
where ui is at a higher level than uj. Of course, among these V2 val-
ues (where V is the number of nodes) are also the desired ∂f /∂ui for
all i since f is itself the value of the output node.
This computation can be done in feedforward fashion. If such
value has been obtained for every uj on the level up to and including
level t, then one can express (by inspecting the multivariate chain
rule) the value ∂uℓ/∂uj for some uℓat level t + 1 as a weighted com-
bination of values ∂ui/∂uj for each ui that is a direct input to uℓ.
This description shows that the amount of computation for a fixed
j is proportional to the number of edges E. This amount of work
happens for all j ∈V, letting us conclude that the total work in the
algorithm is O(VE).
4.2
Backpropagation (Linear Time)
The more efficient backpropagation, as the name suggests, computes
the partial derivatives in the reverse direction. Messages are passed
in one wave backwards from higher number layers to lower number
layers. (Some presentations of the algorithm describe it as dynamic
programming.)
Algorithm 4 Backpropagation
The node u receives a message along each outgoing edge from the
node at the other end of that edge. It sums these messages to get a
number S (if u is the output of the entire net, then define S = 1) and
then it sends the following message to any node z adjacent to it at a
lower level:
S · ∂u
∂z
Clearly, the amount of work done by each node is proportional
to its degree, and thus overall work is the sum of the node degrees.
Summing all node degrees ends up double-counting eac edge, and
thus the overall work is O(Network Size).
To prove correctness, we prove the following:
Lemma 4.2.1. At each node z, the value S is exactly ∂f /∂z.
Proof. Follows from simple induction on depth.

note on backpropagation and its variants
43
Base Case: At the output layer this is true, since ∂f /∂f = 1.
Inductive step: Suppose the claim was true for layers t + 1 and higher
and u is at layer t, with outgoing edges go to some nodes u1, u2, . . . , um
at levels t + 1 or higher. By inductive hypothesis, node z indeed re-
ceives ∂f
∂uj ×
∂uj
∂z from each of uj. Thus by Chain rule,
S =
m
∑
i=1
∂f
∂ui
∂ui
∂z = ∂f
∂z .
This completes the induction and proves the Main Claim.
4.3
Auto-differentiation
Since the exposition above used almost no details about the network
and the operations that the node perform, it extends to every com-
putation that can be organized as an acyclic graph whose each node
computes a differentiable function of its incoming neighbors. This
observation underlies many auto-differentiation packages found in
deep learning environments: they allow computing the gradient of
the output of such a computation with respect to the network param-
eters.
We first observe that Claim 4.1.1 continues to hold in this very
general setting. This is without loss of generality because we can
view the parameters associated to the edges as also sitting on the
nodes (actually, leaf nodes). This can be done via a simple transfor-
mation to the network; for a single node it is shown in the picture
below; and one would need to continue to do this transformation in
the rest of the networks feeding into u1, u2, .. etc from below.
Then, we can use the messaging protocol to compute the deriva-
tives with respect to the nodes, as long as the local partial derivative
can be computed efficiently. We note that the algorithm can be imple-
mented in a fairly modular manner: For every node u, it suffices to
specify (a) how it depends on the incoming nodes, say, z1, . . . , zn and
(b) how to compute the partial derivative times S, that is, S · ∂u
∂zj .

44
theory of deep learning
Extension to vector messages
: In fact (b) can be done efficiently in
more general settings where we allow the output of each node in the
network to be a vector (or even matrix/tensor) instead of only a real
number. Here we need to replace ∂u
∂zj · S by ∂u
∂zj [S], which denotes the
result of applying the operator ∂u
∂zj on S. We note that to be consistent
with the convention in the usual exposition of backpropagation,
when y ∈Rp is a funciton of x ∈Rq, we use ∂y
∂x to denote q × p
dimensional matrix with ∂yj/∂xi as the (i, j)-th entry. Readers might
notice that this is the transpose of the usual Jacobian matrix defined
in mathematics. Thus ∂y
∂x is an operator that maps Rp to Rq and we
can verify S has the same dimension as u and ∂u
∂zj [S] has the same
dimension as zj.
For example, as illustrated below, suppose the node U ∈Rd1×d3 is
a product of two matrices W ∈Rd2×d3 and Z ∈Rd1×d2. Then we have
that ∂U/∂Z is a linear operator that maps Rd2×d3 to Rd1×d3, which
naively requires a matrix representation of dimension d2d3 × d1d3.
However, the computation (b) can be done efficiently because
∂U
∂Z [S] = W⊤S.
Such vector operations can also be implemented efficiently using
today’s GPUs.
Figure 4.3: Vector version of
above
4.4
Notable Extensions
Allowing weight tying: In many neural architectures, the designer
wants to force many network units such as edges or nodes to share
the same parameter. For example, in including the ubiquitous
convolutional net, the same filter has to be applied all over the
image, which implies reusing the same parameter for a large set of
edges between two layers of the net.
For simplicity, suppose two parameters a and b are supposed to
share the same value. This is equivalent to adding a new node u
and connecting u to both a and b with the operation a = u and
b = u. Thus, by chain rule,
∂f
∂u = ∂f
∂a · ∂a
∂u + ∂f
∂b · ∂b
∂u = ∂f
∂a + ∂f
∂b .

note on backpropagation and its variants
45
Hence, equivalently, the gradient with respect to a shared pa-
rameter is the sum of the gradients with respect to individual
occurrences.
Backpropagation on networks with loops. The above exposition assumed
the network is acyclic. Many cutting-edge applications such as
machine translation and language understanding use networks
with directed loops (e.g., recurrent neural networks). These archi-
tectures —all examples of the "differentiable computing" paradigm
below—can get complicated and may involve operations on a sep-
arate memory as well as mechanisms to shift attention to different
parts of data and memory.
Networks with loops are trained using gradient descent as well,
using back-propagation through time which consists of expanding
the network through a finite number of time steps into an acyclic
graph, with replicated copies of the same network. These replicas
share the weights (weight tying!) so the gradient can be computed.
In practice an issue may arise with exploding or vanishing gradients,
which impact convergence. Such issues can be carefully addressed
in practice by clipping the gradient or re-parameterization tech-
niques such as long short-term memory. Recent work suggests that
careful initialization of parameters can ameliorate some of the
vanishing gradient problems.
The fact that the gradient can be computed efficiently for such
general networks with loops has motivated neural net models with
memory or even data structures (see for example neural Turing
machines and differentiable neural computer). Using gradient descent,
one can optimize over a family of parameterized networks with
loops to find the best one that solves a certain computational task
(on the training examples). The limits of these ideas are still being
explored.
4.4.1
Hessian-vector product in linear time: Werbos-Pearlmutter trick
It is possible to generalize backpropagation to work with 2nd order
derivatives, specifically with the Hessian H which is the symmet-
ric matrix whose (i, j) entry is ∂2 f /∂wi∂wj. Sometimes H is also
denoted ∇2 f. Just writing down this matrix takes quadratic time
and memory, which is infeasible for today’s deep nets. Surprisingly,
using backpropagation it is possible to compute in linear time the
matrix-vector product Hx for any vector x. The trick is described by
Pearlmutter who attributes it to an earlier work of Werbos 1.
1 P. J. Werbos. Backpropagation: Past
and future. In IEEE InternationalConfer-
ence on Neural Networks, page 343–353,
1988; and Barak Pearlmutter. Fast exact
multiplication by the hessian. Neural
Computation, 1994
Claim 4.4.1. Suppose an acyclic network with V nodes and E edges has
output f and leaves z1, . . . , zm. Then there exists a network of size O(V +

46
theory of deep learning
E) that has z1, . . . , zm as input nodes and ∂f
∂z1 , . . . , ∂f
∂zm as output nodes.
The proof of the Claim follows in straightforward fashion from
implementing the message passing protocol as an acyclic circuit.
Next we show how to compute ∇2 f (z) · v where v is a given fixed
vector. Let g(z) = ⟨∇f (z), v⟩be a function from Rd →R. Then by
the Claim above, g(z) can be computed by a network of size O(V +
E). Now apply the Claim again on g(z), we obtain that ∇g(z) can
also be computed by a network of size O(V + E).
Note that by construction,
∇g(z) = ∇2 f (z) · v.
Hence we have computed the Hessian vector product in network size
time.

5
Basics of generalization theory
Recall from Chapter 1 the language of Empirical Risk Minimization
from Chapter 1. A datapoint x (for classification this is actually a
pairing of a vector and a label) come from a distribution D and S de-
notes the training sample. The loss of a hypothesis h on datapoint x
is ℓ(x, h). (Since hypothesis in deep learning is given by a parameter
vector w we may also represent this as ℓ(x, w).) In generalization the-
ory we are interested in understanding the relationship between the
test loss and the training loss (respectively):
LD(h) = E
x∈D [ℓ(x, h)]
and
bLS(h) = E
x∈S [ℓ(x, h)] .
(5.1)
(Here b· refers to “empirical.” The training is considered a success if
LS(h) is small and the generalization error ∆S(h) = LD(h) −bLS(h) is
small too.
Generalization theory gives estimates of the number of training
samples sufficient to guarantee low generalization error. The classic
ideas described in this chapter give very loose (i.e., trivial) estimates
for deep learning. We survey attempts to provide tighter estimates.
Generalization theory takes inspiration from an old philosophi-
cal principle called Occam’s razor: given a choice between a simpler
theory of science and a more convoluted theory, both of which ex-
plain some empirical observations, we should trust the simpler one.
For instance, Copernicus’s heliocentric theory of the solar system
gained favor in science because it explained known facts more simply
than the ancient Aristotelian theory. While this makes intuitive sense,
Occam’s razor is a bit vague and hand-wavy. What makes a theory
"simpler" or "better"?
5.1
Occam’s razor formalized for ML
The following is the mapping from the above intuitive notions to
notions in ML. (For simplicity we focus only on supervised learning
here, and consider other settings in later chapters.)

48
theory of deep learning
Observations/evidence
↔
Training dataset S
theory
↔
hypothesis h
All possible theories
↔
hypothesis class H
Finding theory to fit observations
↔
Minimize training loss to find h ∈H
Theory is good (good predictions in new settings)
↔
h has low test loss
Simpler theory
↔
h has shorter description
The notion of “shorter description”will be formalized in a variety
of ways using a complexity measure for the class H, denoted C(H), and
use it to upper bound the generalization error.
Let S be a sample of m datapoints. Empirical Risk Minimization
(ERM) paradigm (see Chapter 1) involves finding bh = arg min c
LS(h).
Of course, in deep learning we may not find the absolute optimum
h but in practice the training loss becomes very small and near-zero.
Intuitively, if generalization error is large then the hypothesis’s per-
formance on training sample S does not accurately reflect the perfor-
mance on the full distribution of examples, and we say it overfitted to
the sample S.
The typical upper bound on generalization error 1 shows that
1 This is the format of typical general-
ization bound! In general this chapter
focuses on clear exposition of the basic
ideas, while being a bit sloppy with
constants.
with probability at least 1 −δ over the choice of training data, the
following
∆S(h) ≤
r
C(H) + O(log(1/δ))
m
(5.2)
Thus to drive the generalization error down it suffices to make m
significantly larger than the “Complexity Measure”. Hence classes
with lower complexity require fewer training samples, in line with
Occam’s intuition.
5.1.1
Motivation for generalization theory
Generalization bounds seek to estimate the generalization error using
the trained model h and the training dataset S. Students can wonder
if the bound is any use if the experimenter has already decided on
the architecture, training algorithm etc. Indeed, then the experiment
can proceed with training and use a held out dataset to estimate the
generalization error.
The hope in developing generalization theory is that it provides
insights into how to design architectures and algorithms in the first
place so that they result in “low complexity” in the trained net, caus-
ing it to generalize well. Clearly, more principled understanding
along such lines would be nice.

basics of generalization theory
49
5.1.2
Warmup: Classical polynomial interpolation
Suppose we are given n points (x(1), y(1)), . . . , (x(n), y(n)) chosen
according to the following probabilistic process: x(i) is chosen from
uniform distribution on [0, 1] and y(i) = p(x(i)) + η where p() is
an unknown degree d polynomial and η is a sample from the noise
distribution N (0, σ2). Since E[η] = 0 and E[η2] = σ2 the obvious way
to find p is to minimize the least square fit to find the polynomial’s
coefficients θ0, θ1, . . . , θd:
ℓ(⃗θ) =
n
∑
i=1
(y(i) −∑
j=0
θj(x(i)))2.
This is implicitly doing linear regression using a new data repre-
sentation, whereby the point x ∈ℜis represented using the vector
(1, x, x2, . . . , xd).
But what if we don’t know d and try to fit a degree N polynomial
where N ≫d? Under what conditions would minimizing the above
loss give us roughly the same polynomial as p()? A practical idea
—noting the fact that the above loss is nσ2 even for the ground truth
polynomial p()—is to add for some large-ish λ > 0 the regularizer
λ∥θ∥2
2 to the above loss. This signals to gradient descent that it is
unimportant to reduce the least squares loss all the way to zero, and
instead it should find solutions θ’s of low norm. More generally
one could use other measures of “complexity” than square of the
Euclidean norm.
This example is intuitive and can be analysed more rigorously
but requires the theory of orthonormal polynomials with respect to
natural distributions on [0, 1].
5.2
Some simple upper bounds on generalization error
Recall the union bound in elementary probability: every set of events
A1, A2, . . . satisfy Pr[∪iAi] ≤∑i Pr[Ai]. Many analyses of generaliza-
tion leverage this simple fact.
The first example illustrates this in an almost trivial setting. Later
we shall see the same idea also at the heart of other generalization
bounds2, albeit often hidden inside the proof. The bound shows that
2 The union bound is also referred to
as uniform convergence framework in
many books. Often the hypothesis class
is infinite but the proof discretizes it, as
in Theorem 5.2.7.
if a hypothesis class contains at most N distinct hypotheses, then
log N (i.e., close to the number of bits needed to represent the index
of the hypotheses in this class) is the effective complexity measure in
(5.2).
Theorem 5.2.1 (Finite Hypothesis Class). If the loss function takes
values in [0, 1] and hypothesis class H contains N distinct hypotheses then

50
theory of deep learning
with probability at least 1 −δ, every h ∈H satisfies
∆S(h) ≤2
q
(log N + log(1/δ))/m.
Proof. For any fixed hypothesis g imagine drawing a training sample
of size m. Then c
LS(g) is an average of i.i.d. variables and its expec-
tation is LD(g). Concentration bounds imply that LD(g) −c
LS(g)
has a concentration property at least as strong as univariate Gaus-
sian N (0, 1/m). The previous statement is true for all hypotheses
g in the class, so the union bound implies that the probability is at
most N exp(−ϵ2m/4) that this quantity exceeds ϵ for some hypothesis
in the class. Since h is the solution to ERM, we conclude that when
δ ≤N exp(−ϵ2m/4) then ∆S(h) ≤ϵ. Simplifying and eliminating ϵ,
we obtain the theorem.
At first sight the union bound appears useless for deep nets be-
cause if the net has k real-valued parameters, the set of hypotheses —
even after we have fixed the architecture and number of parameters—
consists of all vectors in Rk, an uncountable set!
Example 5.2.2 (Possible way around?). As we saw in Chapter 2, the end
result of gradient descent on the loss is special. For instance it must be a
stationary point (i.e., where ∇= 0) of the training loss. One can similarly
imagine other conditions on Hessian ∇2() and so forth. One could hope that
the set of points in the loss landscape with such properties could be small
and finite. This line of investigation hasn’t yet worked out because current
nets are so overparametrized (i.e., number of parameters far exceeding the
number of training data points) that the set of such solution points in the
landscape is also in general a continuous set (i.e., uncountable). The next
hope is to take into account the training algorithm, because not all solution
points may be accessible via standard training algorithms. These are some
ideas for restricting attention to a finite set of solutions, though they haven’t
yet worked out.
There is a more obvious way to turn the set of possible deep nets
into a finite set: discretization! Suppose we assume that the ℓ2 norm
of the parameter vectors is at most 1, meaning the set of all deep nets
has been identified with Ball(0, 1). (Here Ball(w, r) refers to set of all
points in Rk within distance r of w.)
Suppose the loss is Lipschitz in the parameters: for every dat-
apoint x and parameter vectors w1, w2 ∥ℓ(x, w1) −ℓ(x, w2)∥≤
C∥w1 −w2∥2 for some explicit constant C. The arguments below
only need local Lipschitz-ness, namely for the condition to hold for
∥w1 −w2∥2 ≤ρ for some explicit constant ρ. Furthermore it only
requires Lipschitz-ness of the average loss on the training set, not loss
on single data points.

basics of generalization theory
51
Suppose ρ > 0 is such that if w1, w2 ∈Rk satisfy ∥w1 −w2∥2 ≤ρ
then the nets with these two parameter vectors have essentially the
same loss on every input, meaning the losses differ by at most γ for
some γ > 0. (NB: This amounts to a local Lipschitz constant, and it
is at most γ/ρ.) It makes intuitive sense such a ρ must exist for every
γ > 0 since as we let ρ →0 the two nets become equal3.
3 The issue we are ignoring is that
ρ, γ may depend upon the size of the
training set. This unfortunately appears
to be the case in real-life deep learning,
which this analysis is ignoring.
Problem 5.2.3. Compute Lipschitz constant of the ℓ2 regression loss: the
loss on datapoint (x, y) is (w · x −y)2.
Problem 5.2.4. Compute Lipschitz constant of ℓ2 loss for a two layer
deep net with ReLU gates (zero bias) on the middle layer. Assume the two
parameter vectors are infinitesimally close.
Definition 5.2.5 (ρ-cover). A set of points w1, w2, . . . ∈Rk is a ρ-cover if
for every w ∈Ball(0, 1) there is some wi such that w ∈Ball(wi, ρ).
Lemma 5.2.6 (Existence of ρ-cover). There exists a ρ-cover of size at most
((2 + ρ)/2ρ)k.
Proof. The proof is simple but clever. Let us pick w1 arbitrarily in
Ball(0, 1). For i = 2, 3, . . . do the following: arbitrarily pick any point
in Ball(0, 1) outside ∪j≤iBall(wj, ρ) and designate it as wi+1.
A priori it is unclear if this process will ever terminate. We now
show it does after at most (2/ρ)k steps. To see this, it suffices to note
that Ball(wi, ρ/2) ∩Ball(wj, ρ/2) = ∅for all i < j. (Because if not,
then wj ∈Ball(wi, ρ), which means that wj could not have been
picked during the above process.) Thus we conclude that the process
must have stopped after at most
volume(Ball(0, 1 + ρ/2))/volume(Ball(0, ρ/2))
iterations4, which is at most ((2 + ρ)/2ρ)k since ball volume in Rk
4 The reason for 1 + ρ/2 in the numer-
ator is that if a wi lies at the surface
of Ball(0, 1) then the ball of radius
ρ/2 around it lies in the ball of radius
1 + ρ/2 around the origin
scales as the kth power of the radius.
Finally, the sequence of wi’s at the end must be a ρ-cover be-
cause the process stops only when no point can be found outside
∪jBall(wj, ρ).
Theorem 5.2.7 (Generalization bound for normed spaces). If (i) hy-
potheses are unit vectors in Rk and (ii) every two hypotheses h1, h2 with
∥h1 −h2∥2 ≤ρ differ in terms of loss on every datapoint by at most γ then
5
5 Even ignoring the dependence on
the Lipschitz constant, this bound
requires the number of datapoints
to grow linearly with the number of
trainable parameters in the net. This
does not begin to explain the surprising
effectiveness of real-life deep learning,
where the number of parameters
greatly exceeds the number of training
datapoints.
∆S(h) ≤γ + 2
q
k log(2/ρ)/m.
Proof. Apply the union bound on the ρ-cover. Every other net can
have loss at most γ higher than nets in the ρ-cover.

52
theory of deep learning
5.3
Data dependent complexity measures
Thus far we considered complexity measures for hypothesis classes
as a way to quantify their “complicatedness.” : the size of the hy-
pothesis class (assuming it is finite) and the size of a γ-cover in it. Of
course, the resulting bounds on sample complexity were still loose.
But these simple bounds hold for every data distribution D. In
practice, it seems clear that deep nets —or any learning method—
works by being able to exploit properties of the input distribution
(e.g., convolutional structure exploits the fact that all subpatches
of images can be processed very similarly). Thus one should try to
prove some measure of complicatedness that depends on the data
distribution.
5.3.1
Rademacher Complexity
Rademacher complexity is a complexity measure that depends on
data distribution. As usual our description assumes loss function
takes values in [0, 1].
The definition concerns the following thought experiment. Recall
that the distribution D is on labeled datapoints (x, y). For simplicity
we denote the labeled datapoint as z.
Now Rademacher Complexity 6 of hypothesis class H on a distribu-
6 Standard accounts of this often con-
fuse students, or at least falsely impress
them with a complicated proof of
Thm 5.3.1 that hides the simple idea
below. Our definition is simplified
a bit: in the standard definition, one
picks a sign ±1 (or Rademacher random
variables) for each of the 2m datapionts
and looks at loss weighted by this sign.
The value yielded by our definition is
within ±O(1/√m) of the one in the
standard definition.
tion D is defined as follows where l(z, h) is loss of hypothesis h on
labeled datapoint z.
Rm,D(H) = E
S1,S2
"
1
2m sup
h∈H
 ∑
z∈S1
l(z, h) −∑
z∈S2
l(z, h)

#
,
(5.3)
where the expectation is over S1, S2 are two iid sample sets (i.e., mul-
tisets) of size m each from the data distribution D. Note that this
definition involves the thought experiment of picking S1, S2 and pick-
ing a classifier where whose training error on these is as different as
possible. The following theorem relates this to generalization error of
the trained hypothesis.
Theorem 5.3.1. If h is the hypothesis trained via ERM using a training set
S2 of size m, then the probability (over S2) is > 1 −δ, that
∆S2(h) ≤2Rm,D(H) + O((log(1/δ))/√
m).
Proof. The generalization error ∆S2(h) = LD(h) −c
LS2(h), and ERM
guarantees an h that maximizes this. Imagine we pick another m iid
samples from distribution D to get another (multi)set S1. Then with
probability at least 1 −δ the loss on these samples closely approxi-
mates LD(h):
∆S2(h) ≤c
LS1(h) −c
LS2(h) + O((log(1/δ))/√
m).

basics of generalization theory
53
Now we notice that S1, S2 thus drawn are exactly like the sets drawn
in the thought experiment of (5.3) 7 (5.3) and the maximizer h for this
7 Here hypothesis h is allowed to
depend on S2 but not S1. In the thought
experiment the supremum is over h that
can depend on both. This only helps
the inequality, since the latter h can
achieve a larger value. Note that the
factor 2 is because of scaling of 2m in
(5.3).
expression defined Rm,D. So the right hand side is at most
2Rm,D(H) + O((log(1/δ))/√
m).
Problem 5.3.2. Show that the Rademacher complexity of the set of linear
classifiers (unit norm vectors U = {w|w ∈Rd, ∥w∥2 = 1}), on a given
sample S = {x1, x2, · · · , xm} (each xi ∈Rd) is ≤maxi∈[m] ∥xi∥2/√m.
Problem 5.3.3. Consider the kernel classifier of the form h(x) = z⊤G−1y
we studied in Section 3.2 where G is the n × n kernel matrix, y is the labels
and z is the column vector whose i-th coordinate is K(x, xi). Show that the
Rademacher complexity upper is
q
2y⊤Gy · Tr(G)/n. (We will use this
result in Chapter 9 to prove certain over-parameterized student nets can
learn simple two-layer teacher nets.)
5.3.2
Alternative Interpretation: Ability to correlate with random la-
bels
Teachers explain Rademacher complexity more intuitively as ability
of classifiers in H to correlate with random labelings of the data. This is
best understood for binary classification (i.e., labels are 0/1), and the
loss function is also binary (loss 0 for correct label and 1 incorrect
label). Now consider the following experiment: Pick S1, S2 as in
the definition of Rademacher Complexity, and imagine flipping the
labels of S1. Now average loss on S2 is 1 −c
LS2(h). Thus selecting h
to maximise the right hand side of (5.3) is like finding an h that has
low loss on S1 ∪S2 where the labels have been flipped on S1. In other
words, h is able to achieve low loss on datasets where labels were
flipped for some randomly chosen set of half of the training points.
When the loss is not binary a similar statement still holds qualita-
tively.
5.4
Understanding limitations of the union-bound approach
The phenomenon captured in the union bound approach and related
approaches is also refered to as uniform convergence. If we have iden-
tified a finite set H of hypotheses and sample S of datapoints is large
enough then with probability is at least 1 −δ over choice of S that
∥LD(h) −bLS(h)∥≤ϵ
∀h ∈H.
(5.4)
Here the important point to note is that a fixed sample set S can be
used for good estimate of generalization error for every classifier h

54
theory of deep learning
in the class8. Of course, using γ-cover this kind of conclusion can be
8 Note that most of these classifiers may
have terrible loss on S; the union bound
only guarantees that the generalization
error is small.
shown also for classes H that are a continuous set, e.g. hypotheses
with bounded ℓ2 norm. Now we describe a nice and simple example
from Nagarajan and Kolter 9 that pinpoints why this framework may
9 V Nagarajan and Zico Kolter. Uni-
form convergence may be unable to
explain generalization in deep learning.
NeurIPS, 2019
be tricky to apply in modern settings, especially deep learning. The
point is that the hypothesis class of interest is implicitly defined via
the optimization algorithm (say, gradient descent), and this class may
not allow a clean analysis via a union bound.
5.4.1
An illustrative example that mixes optimization and generaliza-
tion
Suppose the points are in ℜD+K and the labels are ±1. There is a
fixed vector u ∈ℜk such that labeled datapoints (x, y) come from
the following distribution D: first label y is uniformly picked in
{±1} and then the first K coordinates of x —which we denote x1 for
convenience—are set to the vector y · u. The remaining D coordinates,
denoted x2 consist of a random vector ℜD, whose each coordinate
is drawn independently from N (0, 1/D). Measure concentration
implies x2 is distributed essentially like a random unit vector in ℜD.
The classification can clearly be solved using the linear classifier
x →sgn(w∗· x) where the first K coordinates contain w∗
1 = u/∥u∥2
2,
and the last D coordinates contain w∗
2 = 0.
Let’s consider a simple training objective: find linear classifer
h(x) that maximises y · h(x). Roughly speaking, this ignores the
magnitude of h(x) and tries to align the sign of h(x) and y. Using
learning rate η = 1 and a sample S of m datapoints (xi, yi) for i =
1, . . . , m gradient descent produces a classifier with wS = (w1, w2)
where
wS,1 = m · u,
wS,2 = ∑
i
yixi
2.
(5.5)
Notice that wS,2 is a sum of m random unit vectors, which means
its norm is fairly tightly concentrated around √m. In other words,
unlike our ideal classifier w∗the learnt classifier has a lot of junk in
the last D coordinates that is not relevant to the classification.
Now we describe how to set the various parameters. As usual m
denotes training set size. We set
m
q
(log 1/δ) ≈D
(5.6)
∥u∥2
2 = 1
m
(5.7)
The ℓ2 norm of the learnt classifier wS is around
q
m2∥u∥2
2 + m,
and thus (5.7) implies this norm is
√
2m.

basics of generalization theory
55
Let’s check that the junk coordinates do not interfere with clas-
sification for randomly chosen data points m—in other words, has
good test error. Given a new data point x = (y · u, x2) where x2
is a random unit vector, the learnt classifier produces the answer
wS · x = my∥u∥2
2 + x2 · (∑i yixi
2). Since inner product between a
fixed vector and a random gaussian vector N (0, 1/D) is a univariate
gaussian with standard deviation 1/
√
D times the norm of the fixed
vector, we see that the sign of this is correct i.e. y, with probability
1 −δ so long as
m∥u∥2
2 >
r
m log 1/δ
D
,
which holds from (5.6) and (5.7). Thus the learnt classifier works fine
on random test data points.
But now imagine we try to explain the success of learning via a
union-bound argument. Let’s denote by H0 the set of such classifiers
that could result from GD on training sets of m datapoints. The argu-
ment would have to prove that with high probability, ∆S(h) is small
for all classifiers h ∈H0. The next result shows this is not true.
Claim 5.4.1. For a random sample set S, whp there is a classifier wflip
whose generalization error is large (specifically, whose loss on full distribu-
tion D is small but whose loss on S is large.)
Proof. We let wflip be the classifier trained on the set Sflip, which is
obtained by taking S and flipping the sign of the x2 part. In other
words, datapoint z = (yiu, xi
2), yi) of S turns into zflip = (yiu, −xi
2), yi)
in Sflip. Note that Sflip has exactly the same probability as S. By our
earlier analysis, wS and wflip agree on the first K coordinates, but
have the signs of the last D coordinates flipped. Thus the absolute
value of (wS −wflip) · z is at least 2xi
2 · xi
2 = 2. Thus we have shown
that the signs of wS · z and wflip · z are different.
Let us consider what we have shown. The classifier wS and wflip
both have excellent test error. However, on the training set S used
to produce wS, the classifier wflip has bad generalization error. This
shows a stumbling block on proving good generalization of wS on
training dataset S using the naive union bound.
Note that the limitations shown above do not hold if we are al-
lowed to modify/prune the classifier obtained at the end of training.
One can imagine identifying non-influential coordinates in the learnt
classifier via some simple test and realizing that the last D coordi-
nates can be zero-ed out without greatly affecting accuracy. Then all
learnt classifiers become scalar multiples of the ideal classifier w∗. In
other words, the limitations shown here do not apply to the approach
we describe in the next Section.

56
theory of deep learning
5.5
A Compression-based framework
Now we described a simple compression-based technique10 from
10 Do not confuse this with another
older and unrelated technique in
generalization theory based upon data
compression, which is not applicable to
deep learning.
Arora et al 11 that formalizes a very simple idea. Suppose the train-
11 Sanjeev Arora, Rong Ge, Behnam
Neyshabur, and Yi Zhang. Stronger
generalization bounds for deep nets via
a compression approach. In Proc. ICML
2018, pages 254–263, 2018
ing dataset S contains m samples, and h is a classifier from a com-
plicated class (e.g., deep nets with much more than m parameters)
that incurs very low empirical loss. We are trying to understand
from looking at h and S how well h will generalize. Now suppose we
can compute a classifier g with discrete trainable parameters much
less than m and which incurs similar loss on the training data as h.
We call this an approximator for h. Then if g has sufficiently low de-
scription length, it’s generalization follows by simple union bound
argument. 12
12 This scenario is quite reminiscent
of empirical work in network prun-
ing, whereby trained deep nets are
compressed using one of a long list of
methods that prune away lots of param-
eters and retrain the rest. If network left
after pruning is compact enough, one
can conceivably prove generalization
bounds for the pruned net. See
This framework has the advantage of staying with intuitive pa-
rameter counting and to avoid explicitly dealing with the hypoth-
esis class that includes h (see note after Theorem 5.5.3). Notice, the
mapping from f to g merely needs to exist, not to be efficiently com-
putable. But in all our examples the mapping will be explicit and
fairly efficient. Now we formalize the notions. The proofs are ele-
mentary via concentration bounds and appear in the appendix.
Definition 5.5.1 ((γ,S)-compressible). Let f be a classifier and GA =
{gA|A ∈A} be a class of classifiers. We say f is (γ, S)-compressible via
GA if there exists A ∈A such that for any x ∈S, we have for all y
| f (x)[y] −gA(x)[y]| ≤γ.
We also consider a different setting where the compression al-
gorithm is allowed a“helper string” s, which is arbitrary but fixed
before looking at the training samples. Often s will contain random
numbers. 13
13 A simple example is to let s be the
random initialization used for training
the deep net. Then one could compress
the difference between the final weights
and s; this can give better generalization
bounds.
Definition 5.5.2 ((γ,S)-compressible using helper string s). Suppose
GA,s = {gA,s|A ∈A} is a class of classifiers indexed by trainable param-
eters A and fixed strings s. A classifier f is (γ, S)-compressible with respect
to GA,s using helper string s if there exists A ∈A such that for any x ∈S,
we have for all y
| f (x)[y] −gA,s(x)[y]| ≤γ.
The following theorem is a simple application of the union bound
method above.
Theorem 5.5.3. Suppose GA,s = {gA,s|A ∈A} where A is a set of q
parameters each of which can have at most r discrete values and s is a helper
string. Let S be a training set with m samples. If the trained classifier f is
(γ, S)-compressible via GA,s with helper string s, then there exists A ∈A

basics of generalization theory
57
with high probability over the training set,
L(gA) ≤bLγ( f ) + O
 r
q log r
m
!
,
where L( f ) = E(x,y)∈D[ f (x)[y] ≤maxj̸=y f (x)[j]] is the expected
error and bLγ( f ) is the proportion of data (x, y) satisfying f (x)[y] ≤
maxj̸=y f (x)[j] in the training set S.
Remarks: (1) The framework proves the generalization not of f but
of its compression gA. (An exception is if the two are shown to have
similar loss at every point in the domain, not just the training set.
This is the case in Theorem 5.5.6.)
(2) The previous item highlights the difference from what we called
the union bound earlier (Theorem 5.2.1). There, one needs to fix a
hypothesis class independent of the training set. By contrast we have
no hypothesis class, only a single neural net that has some specific
properties on a single finite training set. But if we can compress this
specific neural net to a simpler neural nets with fewer parameters
then we can use covering number argument on this simpler class to
get the generalization of the compressed net.
(3) Issue (1) exists also in how researchers often apply the standard
PAC-Bayes framework for deep nets (Section 5.6).
5.5.1
Example 1: Linear classifiers with margin
To illustrate the above compression method we use linear classifiers
with high margins. Consider a simple family of linear classifiers,
consisting of unit vectors c ∈Rd whose ±1 output on input x is
given by sgn(c · x) (i.e., sign of the inner product with the datapoint).
Assume that all data points are also unit vectors. Say c has margin γ
if for all training pairs (x, y) we have y(c⊤x) ≥γ.
We show how to compress such a classifier with margin γ to one
that has only O(1/γ2) non-zero entries. First, assume all ci have
absolute value less than γ2/8.
For each coordinate i, toss a coin with Pr[heads] = pi = 8c2
i /γ2 and
if it comes up heads set the coordinate to equal to ci/pi = γ2/8ci.
This yields a vector bc. The expected number of non-zero entries in bc
is ∑d
i=1 pi = 8/γ2. By Chernoff bound we know with high probability
the number of non-zero entries is at most O(1/γ2).
Furthermore, variance of coordinate i of bc is 2pi(1 −pi) c2
i
p2
i ≤2c2
i
pi ≤
γ2/4. Therefore, for any unit vector u that is independent with the
choice of bc, we have E[bc⊤u] = c⊤u . Now we estimate variance of the
random variable bc⊤u. It is ≤γ2/4 · ∥u∥2 ≤γ2/4. By Chebyshev’s
inequality we know Pr[|bc⊤u −c⊤u| ≥γ] ≤1/4, so bc and c will make

58
theory of deep learning
the same prediction for all u satisfying |c⊤u| ≥γ. We can then apply
Theorem 5.5.3 on a discretized version of bc (via trivial rounding)
to show that the sparsified classifier has good generalization with
O(log d/γ2) samples.
Problem 5.5.4. Redo the proof above when some coordinates have absolute
value more than γ2/8.
This compressed classifier works correctly for a fixed input x with
constant probability but not high probability. To fix this, one can
recourse to the “compression with fixed string” model. The fixed
string is a random linear transformation. When applied to unit vector
c, it tends to equalize all coordinates and the guarantee |bc⊤u −c⊤u| <
γ can hold with high probability. This random linear transformation
can be fixed before seeing the training data.
Problem 5.5.5. Prove the above property of random linear transformations.
That is, let M be a random matrix of size O(1/γ2) × d, drawn from a
suitable distribution you choose before seeing the unit vector c and the
training data. Then, show that the following holds for fixed unit vectors c
and u with high probability
∥Mc∥∞= O(1),
|⟨Mc, Mu⟩−⟨c, u⟩| < γ.
This means we can compress a unit vector c to bc = M⊤Mc. Finally, Apply
Theorem 5.5.3 on a discretized version of bc to show a good generalization
bound with eO(1/γ2) samples, where eO can hide polylog factors of d and
1/γ.
5.5.2
Example 2: Generalization bounds for deep nets using low rank
approximations
Some of the early generalization bounds for fully connected nets
used the fact that layer matrices are often found to be low rank. (Or
perhaps the final matrix minus the initialization.) We give a simple
proof of such a result.
Realize that an h × h matrix of rank r has effectively 2hr param-
eters despite having h2 entries. We recall that for a square matrix A
the spectral norm (i.e., largest singular value) is denoted ∥A∥2 and
sum of squares of singular values is denoted denoted ∥A∥2
F where
∥· ∥F is also called Frobenius norm. The ratio ∥A∥2
F/∥A∥2
2 is called
stable rank, and it is clearly upper bounded by the rank. Often the
layers of the trained net have low stable rank even though rank per se
is high.
Theorem 5.5.6. (14) For a depth-d ReLU net with hidden layers of equal
14 Behnam Neyshabur, Srinadh Bhojana-
palli, David McAllester, and Nathan
Srebro. A pac-bayesian approach to
spectrally-normalized margin bounds
for neural networks. ICLR, 2018
width h and single coordinate output, let A1, A2, . . . Ad be weight matrices

basics of generalization theory
59
and γ be the output margin on a training set S of size m. Then the general-
ization error can be bounded by
eO




v
u
u
thd2 maxx∈S ∥x∥∏d
i=1 ∥Ai∥2
2 ∑d
i=1
∥Ai∥2
F
∥Ai∥2
2
γ2m



.
The second part of this expression (∑d
i=1
∥Ai∥2
F
∥Ai∥2
2 ) is sum of stable
ranks of the layers, a natural measure of their true parameter count.
The first part (∏d
i=1 ∥Ai∥2
2) is related to the Lipschitz constant of the
network, namely, the maximum norm of the vector it can produce
if the input is a unit vector. The Lipschitz constant of a matrix op-
erator B is just its spectral norm ∥B∥2. Since the network applies a
sequence of matrix operations interspersed with ReLU, and ReLU is
1-Lipschitz we conclude that the Lipschitz constant of the full net-
work is at most ∏d
i=1 ∥Ai∥2.
To prove Theorem 5.5.6 we use the following lemma to compress
the matrix at each layer to a matrix of smaller rank. Since a matrix
of rank r can be expressed as the product of two matrices of inner
dimension r, it has 2hr parameters (instead of the trivial h2). (Further-
more, the parameters can be discretized via trivial rounding to get a
compression with discrete parameters as needed by Definition 5.5.1.)
Lemma 5.5.7. For any matrix A ∈Rm×n, let bA be the truncated version
of A where singular values that are smaller than δ∥A∥2 are removed. Then
∥bA −A∥2 ≤δ∥A∥2 and bA has rank at most ∥A∥2
F/(δ2∥A∥2
2).
Proof. Let r be the rank of bA. By construction, the maximum singular
value of bA −A is at most δ∥A∥2. Since the remaining singular values
are at least δ∥A∥2, we have ∥A∥F ≥∥bA∥F ≥√rδ∥A∥2.
For each i replace layer i by its compression using the above
lemma, with δ = γ(3d∥x∥∏d
i=1 ∥Ai∥2)−1. How much error does
this introduce at each layer and how much does it affect the output
after passing through the intermediate layers (and getting magni-
fied by their Lipschitz constants)? Since A −bAi has spectral norm
(i.e., Lipschitz constant) at most δ∥Ai∥2, the error at the output due
to changing layer i in isolation is at most ∏d
j=i+1 ∥Aj∥2 · δ∥Ai∥2 ·
∏i−1
j=1 ∥Aj∥2 · ∥x∥≤γ/3d. Rest of the proof is left to the reader and
generalization bound follows immediately from Theorem 5.5.3.
Problem 5.5.8. Complete the above proof using a simple induction (see 15
15 Behnam Neyshabur, Srinadh Bhojana-
palli, David McAllester, and Nathan
Srebro. A pac-bayesian approach to
spectrally-normalized margin bounds
for neural networks. ICLR, 2018
if needed) to show the total error incurred in all layers is strictly bounded by
γ. That is, for an input x, the change in the deep net output is smaller than
γ after replacing every weight matrix Ai with its truncated version bAi.

60
theory of deep learning
5.6
PAC-Bayes bounds
These bounds due to McAllester (1999) [McA99] are in principle the
tightest, meaning previous bounds in this chapter are its subcases.
They are descended from an old philosophical tradition of consider-
ing the logical foundations for belief systems, which often uses Bayes’
Theorem. For example, in the 18th century, Laplace sought to give
meaning to questions like “What is the probability that the sun will rise
tomorrow?” The answer to this question depends upon the person’s
prior beliefs (e.g., degree of scientific knowledge) as well as their em-
pirical observation that the sun has risen every day in their lifetime.
This philosophical connection sometimes helps students improve
their understanding of generalization.
In ML context, PAC-Bayes bounds assume that experimenter (i.e.,
ML expert) has some prior distribution P over the hypothesis H.
If asked to classify without seeing any concrete training data, the
experimenter would pick a hypothesis h according to P (denoted
h ∼P) and classify using it h. After seeing the training data and
running computations, the experimenter’s distribution changes to the
posterior Q, meaning now if asked to classify they would pick h ∼Q
and use that. Thus the expected test loss is
E
h∼Q[LD(h)].
The theory requires Q to be a valid posterior with respect to P,
meaning every hypothesis h that gets zero probability under P also
must have zero probability under Q. The following form of PAC-
Bayes bound is from 16.
16 John Langford. Quantitatively tight
sample complexity bounds. PhD Thesis
CMU, 2002
Theorem 5.6.1 (PAC-Bayes bound). Let D be the data distribution and
P be a prior distribution over hypothesis class H and δ > 0. If S is a set
of i.i.d. samples of size m from D and Q is any valid posterior (possibly
depending arbitrarily on S) then ∆S(Q) = Eh∼Q[LD(h) −bLS(h)] satisfies
the following bound with probability 1 −δ,
∆S(Q) ≤
s
D(Q||P) + ln(2m/δ)
2(m −1)
,
where D(Q||P) = Eh∼Q[ln(Q(h)/P(h))] is the so-called KL-divergence17.
17 This is a measure of distance between
distributions, meaningful when P
dominates Q, in the sense that every h
with nonzero probability in Q also has
nonzero probability in P. Note that in
this definition, 0 ln 0 is interpreted as 0.
In other words, generalization error can be upper bounded using
the (square root of) KL-divergence of the distributions, plus some
terms that arise from concentration bounds.
Example 5.6.2. P could be the standard normal distribution, which assigns
nonzero probability to every vector. For any sample set S, we could let Q be

basics of generalization theory
61
the distribution on parameter vectors obtained by vanilla deep learning us-
ing S: that is, initialize parameters using random Gaaussian, and train with
SGD with a predetermined learning rate schedule. Since SGD is a stochastic
process (due to randomness of batches) it leads to a natural distribution Q
on trained classifiers at the end of training. Notice, Q is a valid posterior of
P because P assigns nonzero probability to every classifier h. As this exam-
ple emphasizes, one can consider various P and Q for the same classification
setup (e.g., by changing some aspect of training) and the generalization
bound will hold for every fixed choice.
Example 5.6.3. Suppose h is any classifier and P, Q are the distribu-
tion that assigns probability 1 to h and zero to all other hypotheses. Then
D(Q||P) = 0, and by Hoeffding bound we have ∆S(Q) = ∆S(h) ≤
q
log(1/δ)
2m
. The inequality in PAC-Bayes bound is satisfied.
Problem 5.6.4. Derive the union bound Theorem 5.2.1 using PAC-Bayes.
Now we’re ready to prove Theorem 5.6.1. In interest of exposition,
we prove a weaker statement that is qualitatively similar but not
quite correct:
∆S(Q) ≤
r
2(D(Q||P) + ln(2/δ))
m
(5.8)
The incorrectness arises due to a simplifying assumption about the
quantity z = √m∆S(h) where h is a fixed classifier and S is a random
subset of m samples. Since ∆S(h) is an average of m iid variables tak-
ing values in [−1, 1] and with mean 0, we assume z behaves exactly
like a normal distribution N (0, 1). Of course, in truth z is dominated
in distribution by N (0, 1) in the limit m →∞. This assumption can
be removed by using a more quantitative argument with Hoeffding
bound. The assumption allows us to assume that expected value of
ez2/(2+ϵ) approaches
√
2 when x is drawn from N (0, 1) and ϵ is an
arbitrarily small constant. For simplicity we will assume ez2/2 =
√
2.
It is possible to fix the proof using concentration bounds.
Proof. (Theorem 5.6.1, weaker version (5.8)) Rearranging the expres-
sion in the theorem statement, we see that it gives an upper bound of
ln(2/δ) on (m/2) Eh∼Q[∆S(h)]2 −D(Q||P). By Jensen’s inequality 18
18 Jensen’s Inequality: For a concave
function f and random variable X,
E[ f (X)] ≤f (E[X]). For convex
function the inequality is reversed.
applied to the square function f (x) = x2, this expression is at most
(m/2) Eh∼Q[∆S(h)2] −D(Q||P). We show this is upper bounded by
ln(2/δ). The steps are:
=
E
h∼Q
h
(m/2)∆S(h)2 −ln(Q(h)/P(h))
i
=
E
h∼Q
h
ln

exp((m/2)∆S(h)2) · P(h)/Q(h)
i
≤ln

E
h∼Q
h
exp((m/2)∆S(h)2) · P(h)/Q(h)
i
,

62
theory of deep learning
where the last inequality uses Jensen’s inequality along with the con-
cavity of ln. Also, since taking expectation over h ∼Q is effectively
like a weighted sum with term for h weighted by Q(h), we have 19
19 Often when you see KL-divergence in
machine learning, you will see this trick
being used to switch the distribution
over which expectation is taken!
ln E
h∼Q
h
exp((m/2)∆(h)2) · P(h)/Q(h)
i
= ln E
h∼P
h
exp((m/2)∆(h)2)
i
.
Recapping, we thus thus shown the following for a fixed dataset S:
(m/2) E
h∼Q[∆S(h)]2 −D(Q||P) ≤ln

E
h∼P
h
e(m/2)∆S(h)2i
(5.9)
Note that the RHS has no dependence on posterior Q. Using the
fact that S is a random sample of size m and that prior belief P was
fixed before seeing S (i.e., is independent of S):
E
S

E
h∼P
h
e(m/2)∆S(h)2i
= E
h∼P

E
S
h
e(m/2)∆S(h)2i
=
√
2 ≤2.
Simple averaging implies that with probability 1 −δ over S,
E
h∼P
h
e(m/2)∆S(h)2i
≤2/δ
(5.10)
and now by taking logarithm of both sides the proof is completed.
5.7
Exercises
1. Assume the loss function ℓis 1-Lipschitz. Consider the kernel
classifier of the form h(x) = z⊤G−1y we studied in Section 3.2
where G is the n × n kernel matrix, y is the labels and z is the
column vector whose i-th coordinate is K(x, xi). Prove that its
Rademacher complexity is upper bounded
q
2y⊤Gy · Tr(G)/n.
(Hint: view kernel classifier as a linear classifier in RKHS)

6
Tractable Landscapes for Nonconvex Optimization
Deep learning relies on optimizing complicated, nonconvex loss
functions. Finding the global minimum of a nonconvex objective is
NP-hard in the worst case. However in deep learning simple algo-
rithms such as stochastic gradient descent often the objective value to
zero or near-zero at the end. This chapter focuses on the optimization
landscape defined by a nonconvex objective and identifies properties
of these landscapes that allow simple optimization algorithms to find
global minima (or near-minima). These properties thus far apply to
simpler nnonconvex problems than deep learning, and it is open how
to analyse deep learning with such landscape analysis.
Warm-up: Convex Optimization
To understand optimization land-
scape, one can first look at optimizing a convex function. If a function
f (w) is convex, then it satisfies many nice properties, including
∀α ∈[0, 1], w, w′,f (αw + (1 −α)w′) ≤α f (w) + (1 −α) f (w′).
(6.1)
∀w, w′,f (w′) ≥f (w) + ⟨∇f (w), w′ −w⟩.
(6.2)
These equations characterize important geometric properties of
the objective function f (w). In particular, Equation (6.1) shows that
all the global minima of f (w) must be connected, because if w, w′
are both globally optimal, anything on the segment αw + (1 −α)w′
must also be optimal. Such properties are important because it gives
a characterization of all the global minima. Equation (6.2) shows that
every point with ∇f (w) = 0 must be a global minimum, because
for every w′ we have f (w′) ≥f (w) + ⟨∇f (w), w′ −w)⟩≥f (w).
Such properties are important because it connects a local property
(gradient being 0) to global optimality.
In general, optimization landscape looks for properties of the ob-
jective function that characterizes its local/global optimal points
(such as Equation (6.1)) or connects local properties with global opti-
mality (such as Equation (6.2)).

64
theory of deep learning
6.1
Preliminaries and challenges in nonconvex landscapes
We have been discussing global/local minimum informally, here we
first give a precise definition:
Definition 6.1.1 (Global/Local minimum). For an objective function
f (w) : Rd →R, a point w∗is a global minimum if for every w we have
f (w∗) ≤f (w). A point w is a local minimum/maximum if there exists
a radius ϵ > 0 such that for every ∥w′ −w∥2 ≤ϵ, we have f (w) ≤f (w′)
(f (w) ≥f (w′) for local maximum). A point w with ∇f (w) = 0 is called a
critical point, for smooth functions all local minimum/maximum are critical
points.
Throughout the chapter, we will always work with functions
whose global minimum exists, and use f (w∗) to denote the opti-
mal value of the function1. For simplicity we focus on optimization
1 Even though there might be multiple
global minima w∗, the value f (w∗) is
unique by definition.
problems that do not have any constraints (w ∈Rd). It is possible to
extend everything in this chapter to optimization with nondegenerate
equality constraints, which would require definitions of gradient and
Hessians with respect to a manifold and is out of the scope for this
book.
Spurious local minimum
The first obstacle in nonconvex optimization
is a spurious local minimum.
Definition 6.1.2 (Spurious local minimum). For an objective function
f (w) : Rd →R, a point w is a spurious local minimum if it is a local
minimum, but f (w) > f (w∗).
Many of the simple optimization algorithms are based on the
idea of local search, thus are not able to escape from a spurious local
minimum. As we will later see, many noncovex objectives do not
have spurious local minima.
Saddle points
The second obstacle in nonconvex optimization is
a saddle point. The simplest example of a saddle point is f (w) =
w2
1 −w2
2 at the point w = (0, 0). In this case, if w moves along direc-
tion (±1, 0), the function value increases; if w moves along direction
(0, ±1), the function value decreases.
Definition 6.1.3 (Saddle point). For an objective function f (w) : Rd →
R, a point w is a saddle point if ∇f (w) = 0, and for every radius ϵ > 0,
there exists w+, w−within distance ϵ of w such that f (w−) < f (w) <
f (w+).
This definition covers all cases but makes it very hard to verify
whether a point is a saddle point. In most cases, it is possible to tell

tractable landscapes for nonconvex optimization
65
whether a point is a saddle point, local minimum or local maximum
based on its Hessian.
Claim 6.1.4. For an objective function f (w) : Rd →R and a critical point
w (∇f (w) = 0), we know
• If ∇2 f (w) ≻0, w is a local minimum.
• If ∇2 f (w) ≺0, w is a local maximum.
• If ∇2 f (w) has both a positive and a negative eigenvalue, w is a saddle
point.
These criteria are known as second order sufficient conditions
in optimization. Intuitively, one can prove this claim by looking at
the second-order Taylor expansion. The three cases in the claim do
not cover all the possible Hessian matrices. The remaining cases are
considered to be degenerate, and can either be a local minimum,
local maximum or a saddle point2.
2 One can consider the w = 0 point of
functions w4, −w4, w3, and it is a local
minimum, maximum and saddle point
respectively.
Flat regions
Even if a function does not have any spurious local min-
ima or saddle point, it can still be nonconvex, see Figure 6.1. In high
dimensions such functions can still be very hard to optimize. The
main difficulty here is that even if the norm ∥∇f (w)∥2 is small, un-
like convex functions one cannot conclude that f (w) is close to f (w∗).
However, often in such cases one can hope the function f (w) to sat-
isfy some relaxed notion of convexity, and design efficient algorithms
accordingly. We discuss one of such cases in Section 6.2.
Figure 6.1: Obstacles for non-
convex optimization. From left
to right: local minimum, saddle
point and flat region.
6.2
Cases with a unique global minimum
We first consider the case that is most similar to convex objectives.
In this section, the objective functions we look at have no spurious
local minima or saddle points. In fact, in our example the objective
is only going to have a unique global minimum. The only obstacle
in optimizing these functions is that points with small gradients may
not be near-optimal.

66
theory of deep learning
The main idea here is to identify properties of the objective and
also a potential function, such that the potential function keeps de-
creasing as we run simple optimization algorithms such as gradient
descent. Many properties were used in previous literature, including
Definition 6.2.1. Let f (w) be an objective function with a unique global
minimum w∗, then
Polyak-Lojasiewicz f satisfies Polyak-Lojasiewicz if there exists a value
µ > 0 such that for every w, ∥∇f (x)∥2
2 ≥µ( f (w) −f (w∗)).
weakly-quasi-convex f is weakly-quasi-convex if there exists a value τ > 0
such that for every w, ⟨∇f (w), w −w∗⟩≥µ( f (w) −f (w∗)).
Restricted Secant Inequality (RSI) f satisfies RSI if there exists a value τ
such that for every w, ⟨∇f (w), w −w∗⟩≥µ∥w −w∗∥2
2.
Any one of these three properties can imply fast convergence
together with some smoothness of f.
Claim 6.2.2. If an objective function f satisfies one of Polyak-Lojasiewicz,
weakly-quasi-convex or RSI, and f is smooth3, then gradient descent con-
3 Polyak-Lojasiewicz and RSI requires
standard smoothness definition as in
Equation (2.2), weakly-quasi-convex
requires a special smoothness property
detailed in [HMR18].
verges to global minimum with a geometric rate4.
4 The potential functions for Polyak-
Lojasiewicz and weakly-quasi-convex
are function value f; potential function
for RSI is the squared distance ∥w −
w∗∥2
2
Intuitively, Polyak-Lojasiewicz condition requires that the gradient
to be nonzero for any point that is not a global minimum, therefore
one can always follow the gradient and further decrease the function
value. This condition can also work in some settings when the global
minimum is not unique. Weakly-quasi-convex and RSI are similar in
the sense that they both require the (negative) gradient to be corre-
lated with the correct direction - direction from the current point w to
the global minimum w∗.
In this section we are going to use generalized linear model as an
example to show how some of these properties can be used.
6.2.1
Generalized linear model
In generalized linear model (also known as isotonic regression)
[KS09, KKSK11], the input consists of samples {x(i), y(i)} that are
drawn from distribution D, where (x, y) ∼D satisfies
y = σ(w⊤
∗x) + ϵ.
(6.3)
Here σ : R →R is a known monotone function, ϵ is a noise that
satisfies E [ϵ|x] = 0, and w∗is the unknown parameter that we are
trying to learn.
In this case, it is natural to consider the following expected loss
L(w) = 1
2
E
(x,y)∼D
h
(y −σ(w⊤x)2i
.
(6.4)

tractable landscapes for nonconvex optimization
67
Of course, in practice one can only access the training loss which is
an average over the observed {x(i), y(i)} pairs. For simplicity we work
with the expected loss here. The difference between the two losses
can be bounded using techniques in Chapter ??.
Generalized linear model can be viewed as learning a single neu-
ron where σ is its nonlinearity.
We will give high level ideas on how to prove properties such as
weakly-quasi-convex or RSI for generalized linear model. First we
rewrite the objective as:
L(w) = 1
2
E
(x,y)∼D
h
(y −σ(w⊤x)2i
= 1
2 E
(x,ϵ)
h
(ϵ + σ(w⊤
∗x) −σ(w⊤x))2i
.
= 1
2 Eϵ
h
ϵ2i
+ 1
2 Ex
h
(σ(w⊤
∗x) −σ(w⊤x)2i
.
Here the second equality uses Definition of the model (Equation
(6.3)), and the third equality uses the fact that E [ϵ|x] = 0 (so there
are no cross terms). This decomposition is helpful as the first term
1
2 Eϵ

ϵ2
is now just a constant.
Now we can take the derivative of the objective:
∇L(w) = Ex
h
(σ(w⊤x) −σ(w⊤
∗x))σ′(w⊤x)x
i
.
Notice that both weakly-quasi-convex and RSI requires that the
objective to be correlated with w −w∗, so we compute
⟨∇L(w), w −w∗⟩= Ex
h
(σ(w⊤x) −σ(w⊤
∗x))σ′(w⊤x)(w⊤x −w⊤
∗x)
i
.
The goal here is to show that the RHS is bigger than 0. A simple
way to see that is to use the intermediate value theorem: σ(w⊤x) −
σ(w⊤∗x) = σ′(ξ)(w⊤x −w⊤∗x), where ξ is a value between w⊤x and
w⊤∗x. Then we have
⟨∇L(w), w −w∗⟩= Ex
h
σ′(ξ)σ′(w⊤x)(w⊤x −w⊤
∗x)2i
.
In the expectation in the RHS, both derivatives (σ′(ξ), σ′(w⊤x))
are positive as σ is monotone, and (w⊤x −w⊤∗x)2 is clearly nonneg-
ative. By making more assumptions on σ and the distribution of x,
it is possible to lowerbound the RHS in the form required by either
weakly-quasi-convex or RSI. We leave this as an exercise.
6.2.2
Alternative objective for generalized linear model
There is another way to find w∗for generalized linear model that is
more specific to this setting. In this method, one estimate a different

68
theory of deep learning
“gradient” for generalized linear model:
∇g(w) = E
x,y
h
(σ(w⊤x) −y)x
i
= Ex
h
(σ(w⊤x) −σ(w⊤
∗x))x
i
.
(6.5)
The first equation gives a way to estimate this “gradient”. The main
difference here is that in the RHS we no longer have a factor σ′(w⊤x)
as in ∇L(w). Of course, it is unclear why this formula is the gradi-
ent of some function g, but we can construct the function g in the
following way:
Let τ(x) be the integral of σ(x): τ(x) = R x
0 σ(x′)dx′. Define
g(w) := Ex

τ(w⊤x) −σ(w⊤∗x)w⊤x

. One can check ∇g(w) is in-
deed the function in Equation (6.5). What’s very surprising is that
g(w) is actually a convex function with ∇g(w∗) = 0! This means that
w∗is a global minimum of g and we only need to follow ∇g(w) to
find it. Nonconvex optimization is unnecessary here.
Of course, this technique is quite special and uses a lot of structure
in generalized linear model. However similar ideas were also used in
5 to learn a single neuron. In general, when one objective is hard to
5
analyze, it might be easier to look for an alternative objective that has
the same global minimum but easier to optimize.
6.3
Symmetry, saddle points and locally optimizable functions
In the previous section, we saw some conditions that allow noncon-
vex objectives to be optimized efficiently. However, such conditions
often do not apply to neural networks, or more generally any func-
tion that has some symmetry properties.
More concretely, consider a two-layer neural network hθ(x) : Rd →
R. The parameters θ is (w1, w2, ..., wk) where wi ∈Rd represents the
weight vector of the i-th neuron. The function can be evaluated as
hθ(x) = ∑k
i=1 σ(⟨wi, x⟩), where σ is a nonlinear activation function.
Given a dataset (x(1), y(1)), . . . , (x(n), y(n))
i.i.d.
∼D , one can define the
training loss and expected loss as in Chapter 1. Now the objective for
this neural network f (θ) = L(hθ) = E(x,y)∼D [ℓ((x, y), hθ)] has permu-
tation symmetry. That is, for any permutation π(θ) that permutes the
weights of the neurons, we know f (θ) = f (π(θ)).
The symmetry has many implications. First, if the global mini-
mum θ∗is a point where not all neurons have the same weight vector
(which is very likely to be true), then there must be equivalent global
minimum f (π(θ∗)) for every permutation π. An objective with this
symmetry must also be nonconvex, because if it were convex, the
point ¯θ =
1
k! ∑π π(θ∗) (where π sums over all the permutations) is
a convex combination of global minima, so it must also be a global
minimum. However, for ¯θ the weight vectors of the neurons are all

tractable landscapes for nonconvex optimization
69
equal to 1
k ∑k
i=1 wi (where wi is the weight of i-th neuron in θ∗), so
h ¯θ(x) = kσ(⟨1
k ∑k
i=1 wi, x⟩) is equivalent to a neural network with
a single neuron. In most cases a single-neuron network should not
achieve the global minimum, so by proof of contradiction we know f
should not be convex.
It’s also possible to show that functions with symmetry must have
saddle points6. Therefore to optimize such a function, the algorithm
6 Except some degenerate cases such as
constant functions.
needs to be able to either avoid or escape from saddle points. More
concretely, one would like to find a second order stationary point.
Definition 6.3.1 (Second order stationary point (SOSP)). For an objec-
tive function f (w) : Rd →R, a point w is a second order stationary point if
∇f (w) = 0 and ∇2 f (w) ⪰0.
The conditions for second order stationary point are known as the
second order necessary conditions for a local minimum. Of course,
generally an optimization algorithm will not be able to find an exact
second order stationary point (just like in Section ?? we only show
gradient descent finds a point with small gradient, but not 0 gradi-
ent). The optimization algorithms can be used to find an approximate
second order stationary point:
Definition 6.3.2 (Approximate second order stationary point). For
an objective function f (w) : Rd →R, a point w is a (ϵ, γ)-second order
stationary point (later abbreviated as (ϵ, γ)-SOSP) if ∥∇f (w)∥2 ≤ϵ and
λmin(∇2 f (w)) ≥−γ.
Later in Chapter ?? we will show that simple variants of gradient
descent can in fact find (ϵ, γ)-SOSPs efficiently.
Now we are ready to define a class of functions that can be opti-
mized efficiently and allow symmetry and saddle points.
Definition 6.3.3 (Locally optimizable functions). An objective function
f (w) is locally optimizable, if for every τ > 0, there exists ϵ, γ = poly(τ)
such that every (ϵ, γ)-SOSP w of f satisfies f (w) ≤f (w∗) + τ.
Roughly speaking, an objective function is locally optimizable
if every local minimum of the function is also a global minimum,
and the Hessian of every saddle point has a negative eigenvalue.
Similar class of functions were called “strict saddle” or “ridable” in
some previous results. Many nonconvex objectives, including matrix
sensing [BNS16a, PKCS17, GJZ17a], matrix completion [GLM16a,
GJZ17a], dictionary learning [SQW16a], phase retrieval [SQW18], ten-
sor decomposition [GHJY15a], synchronization problems [BBV16]
and certain objective for two-layer neural network [GLM18] are
known to be locally optimizable.

70
theory of deep learning
6.4
Case study: top eigenvector of a matrix
In this section we look at a simple example of a locally optimizable
function. Given a symmetric PSD matrix M ∈Rd×d, our goal is to
find its top eigenvector (eigenvector that corresponds to the largest
eigenvalue). More precisely, using SVD we can write M as
M =
d
∑
i=1
λiviv⊤
i .
Here vi’s are orthonormal vectors that are eigenvectors of M, and λi’s
are the eigenvalues. For simplicity we assume λ1 > λ2 ≥λ3 ≥· · · ≥
λd ≥07.
7 Note that the only real assumption
here is λ1 > λ2, so the top eigenvec-
tor is unique. Other inequalities are
without loss of generality.
There are many objective functions whose global optima gives the
top eigenvector. For example, using basic definition of spectral norm,
we know for PSD matrix M the global optima of
max
∥x∥2=1 x⊤Mx
is the top eigenvector of M. However, this formulation requires a
constraint. We instead work with an unconstrained version whose
correctness follows from Eckhart-Young Theorem:
min
x∈Rd f (x) := 1
4∥M −xx⊤∥2
F.
(6.6)
Note that this function does have a symmetry in the sense that
f (x) = f (−x). Under our assumptions, the only global minima of
this function are x = ±√λ1v1. We are going to show that these are
also the only second order stationary points. We will give two proof
strategies that are commonly used to prove the locally optimizable
property.
6.4.1
Characterizing all critical points
The first idea is simple – we will just try to solve the Equation ∇f (x) =
0 to get the position of all critical points; then for the critical points
that are not the desired global minimum, try to prove that they are
local maximum or saddle points.
Computing gradient and Hessian
Before we solve the equation ∇f (x) =
0 for the objective function f (x) defined in Equation (6.6), we first
give a simple way of computing the gradient and Hessian. We will
first expand f (x + δ) (where δ should be thought of as a small pertur-

tractable landscapes for nonconvex optimization
71
bation):
f (x + δ) = 1
4∥M −(x + δ)(x + δ)⊤∥2
F
= 1
4∥M −xx⊤−(xδ⊤+ δx⊤) −δδ⊤∥2
F
= 1
4∥M −xx⊤∥2
F −1
2⟨M −xx⊤, xδ + δx⊤⟩
+
1
4∥xδ⊤+ δx⊤∥2
F −1
2⟨M −xx⊤, δδ⊤⟩

+ o(∥δ∥2
2).
Note that in the last step, we have collected the terms based on the
degree of δ, and ignored all the terms that are smaller than o(∥δ∥2
2).
We can now compare this expression with the Taylor’s expansion of
f (x + δ):
f (x + δ) = f (x) + ⟨∇f (x), δ⟩+ 1
2δ⊤[∇2 f (x)]δ + o(∥δ∥2
2).
By matching terms, we immediately have
⟨∇f (x), δ⟩= −1
2⟨M −xx⊤, xδ⊤+ δx⊤⟩,
δ⊤[∇2 f (x)]δ = 1
2∥xδ⊤+ δx⊤∥2
F −⟨M −xx⊤, δδ⊤⟩.
These can be simplified to give the actual gradient and Hessian8
8 In fact in the next subsection we
will see it is often good enough to
know how to compute ⟨∇f (x), δ⟩and
δ⊤[∇2 f (x)]δ.
∇f (x) = (xx⊤−M)x,
∇2 f (x) = ∥x∥2
2I + 2xx⊤−M.
(6.7)
Characterizing critical points
Now we can execute the original plan.
First set ∇f (x) = 0, we have
Mx = xx⊤x = ∥x∥2
2x.
Luckily, this is a well studied equation because we know the only
solutions to Mx = λx are if λ is an eigenvalue and x is (a scaled
version) of the corresponding eigenvector. Therefore we know x =
±√λivi or x = 0. These are the only critical points of the objective
function f (x).
Among these critical points, x = ±√λ1v1 are our intended so-
lutions. Next we need to show for every other critical point, its
Hessian has a negative eigendirection. We will do this for x =
±√λivi(i > 1). By definition, it suffices to show there exists a δ such
that δ⊤[∇2 f (x)]δ < 0. The main step of the proof involves guessing
what is this direction δ. In this case we will choose δ = v1 (we will
give more intuitions about how to choose such a direction in the next
subsection).

72
theory of deep learning
When x = ±√λivi, and δ = v1, we have
δ⊤[∇2 f (x)]δ = v⊤
1 [∥
p
λivi∥2
2I + 2λiviv⊤
i −M]v1 = λi −λ1 < 0.
Here the last step uses the fact that vi’s are orthonormal vectors and
v⊤
1 Mv1 = λ1. The proof for x = 0 is very similar. Combining all the
steps above, we proved the following claim:
Claim 6.4.1 (Properties of critical points). The only critical points of
f (x) are of the form x = ±√λivi or x = 0. For all critical points except
x = ±√λ1v1, ∇2 f (x) has a negative eigenvalue.
This claim directly implies that the only second order stationary
points are x = ±√λ1v1, so all second order stationary points are also
global minima.
6.4.2
Finding directions of improvements
The approach in Section 6.4.1 is straight-forward. However, in more
complicated problems it is often infeasible to enumerate all the so-
lutions for ∇f (x) = 0. What we proved in Section 6.4.1 is also not
strong enough for showing f (x) is locally optimizable, because we
only proved every exact SOSP is a global minimum, and a locally
optimizable function requires every approximate SOSP to be close to
a global minimum. We will now give an alternative approach that is
often more flexible and robust.
For every point x that is not a global minimum, we define its di-
rection of improvements as below:
Definition 6.4.2 (Direction of improvement). For an objective func-
tion f and a point x, we say δ is a direction of improvement (of f at x) if
|⟨∇f (x), δ⟩| > 0 or δ⊤[∇2 f (x)]δ < 0. We say δ is an (epsilon, γ) direc-
tion of improvement (of f at x) if |⟨∇f (x), δ⟩| > ϵ∥δ∥2 or δ⊤[∇2 f (x)]δ <
−γ∥δ∥2
2.
Intuitively, if δ is a direction of improvement for f at x, then mov-
ing along one of δ or −δ for a small enough step can decrease the
objective function. In fact, if a point x has a direction of improve-
ment, it cannot be a second order stationary point; if a point x has
an (epsilon, γ) direction of improvement, then it cannot be an (ϵ, γ)-
SOSP.
Now we can look at the contrapositive of what we were trying to
prove in the definition of locally optimizable functions: if every point
x with f (x) > f (x∗) + τ has an (ϵ, γ) direction of improvement,
then every (ϵ, γ)-second order stationary point must satisfy f (x) ≤
f (x∗) + δ. Therefore, our goal in this part is to find a direction of
improvement for every point that is not globally optimal.

tractable landscapes for nonconvex optimization
73
For simplicity, we will look at an even simpler version of the
top eigenvector problem. In particular, we consider the case where
M = zz⊤is a rank-1 matrix, and z is a unit vector. In this case, the
objective function we defined in Equation (6.6) becomes
min
x
f (x) = 1
4∥zz⊤−xx⊤∥2
F.
(6.8)
The intended global optimal solutions are x = ±z. This problem is
often called the matrix factorization problem as we are given a matrix
M = zz⊤9 and the goal is to find a decomposition M = xx⊤.
9 Note that we only observe M, not z.
Which direction should we move to decrease the objective func-
tion? In this problem we only have the optimal direction z and the
current direction x, so the natural guesses would be z, x or z −x.
Indeed, these directions are enough:
Lemma 6.4.3. For objective function (6.8), there exists a universal constant
c > 0 such that for any τ < 1, if neither x or z is an (cτ, , 1/4)-direction of
improvement for the point x, then f (x) ≤τ.
The proof of this lemma involves some detailed calculation. To get
some intuition, we can first think about what happens if neither x or
z is a direction of improvement.
Lemma 6.4.4. For objective function (6.8), if neither x or z is a direction of
improvement of f at x, then f (x) = 0.
Proof. We will use the same calculation for gradient and Hessian as
in Equation (6.7), except that M is now zz⊤. First, since x is not a
direction of improvement, we must have
⟨∇f (x), x⟩= 0 =⇒∥x∥4
2 = ⟨x, z⟩2.
(6.9)
If z is not a direction of improvement, we know z⊤[∇2 f (x)]z ≥0,
which means
∥x∥2 + 2⟨x, z⟩2 −1 ≥0 =⇒∥x∥2 ≥1/3.
Here we used the fact that ⟨x, z⟩2 ≤∥x∥2
2∥z∥2
2 = ∥x∥2
2. Together with
Equation (6.9) we know ⟨x, z⟩2 = ∥x∥4
2 ≥1/9.
Finally, since z is not a direction of improvement, we know ⟨∇f (x), z⟩=
0, which implies ⟨x, z⟩(∥x∥2
2 −1) = 0. We have already proved
⟨x, z⟩2 ≥1/9 > 0, thus ∥x∥2
2 = 1. Again combining with Equa-
tion (6.9) we know ⟨x, z⟩2 = ∥x∥4
2 = 1. The only two vectors with
⟨x, z⟩2 = 1 and ∥x∥2
2 = 1 are x = ±z.
The proof of Lemma 6.4.3 is very similar to Lemma 6.4.4, except
we need to allow slacks in every equation and inequality we use. The
additional benefit of having the more robust Lemma 6.4.3 is that the

74
theory of deep learning
proof is also robust if we don’t have access to the exact objective -
in settings where only a subset of coordinates of zz⊤10, one can still
10 This setting is known as matrix
completion and has been widely applied
to recommendation systems.
prove that the objective function is locally optimizable, and hence
find z by nonconvex optimization.
Lemma 6.4.4 and Lemma 6.4.3 both use directions x and z. It is
also possible to use the direction x −z when ⟨x, z⟩≥0 (and x + z
when ⟨x, z⟩< 0). Both ideas can be generalized to handle the case
when M = ZZ⊤where Z ∈Rd×r, so M is a rank-r matrix.

7
Escaping Saddle Points
Gradient descent (GD) and stochastic gradient descent (SGD) are the
workhorses of large-scale machine learning. While classical theory
focused on analyzing the performance of these methods in convex op-
timization problems, the most notable successes in machine learning
have involved nonconvex optimization, and a gap has arisen between
theory and practice.
Indeed, traditional analyses of GD and SGD show that both algo-
rithms converge to stationary points efficiently. But these analyses do
not take into account the possibility of converging to saddle points.
Motivated by the geometric characterizations in the last chapter, the
central difficulty in solving many nonconvex machine learning prob-
lems becomes escaping saddle points.
In this chapter, we will discuss a simple perturbed form of gra-
dient descent, which is capable of escaping saddle points very ef-
ficiently. Particularly, in terms of convergence rate and dimension
dependence, it is almost as if the saddle points are not there!
7.1
Preliminaries
≪Chi notes: Many definitions have appeared in the earlier chapters. Coordinatition may be
required.≫
In this chapter, we are interested in solving general unconstrained
optimization problems of the form:
min
x∈Rd f (x),
where f is a smooth function that can be nonconvex. In particular we
assume that f has Lipschitz gradients and Lipschitz Hessians, which
ensures that the gradient and Hessian can not change too rapidly.
Definition 7.1.1. A differentiable function f is ℓ-gradient Lipschitz if:
∥∇f (x1) −∇f (x2)∥≤ℓ∥x1 −x2∥
∀x1, x2.

76
theory of deep learning
Definition 7.1.2. A twice-differentiable function f is ρ-Hessian Lipschitz
if:
∇2 f (x1) −∇2 f (x2)
 ≤ρ ∥x1 −x2∥
∀x1, x2.
For minimization problems, both saddle points and local maxima
are clearly undesirable. Our focus will be “saddle points,” although
our results also apply directly to local maxima as well. Unfortunately,
distinguishing saddle points from local minima for smooth functions
is still NP-hard in general [Nes00]. To avoid these hardness results,
we focus on a subclass of saddle points.
Definition 7.1.3 (strict saddle point). For a twice-differentiable function
f, x is a strict saddle point if x is a stationary point and λmin(∇2 f (x)) <
0.
A generic saddle point must satisfy that λmin(∇2 f (x)) ≤0. Being
“strict” simply rules out the case where λmin(∇2 f (x)) = 0. We
reformulate our goal as that of finding stationary points that are not
strict saddle points.
Definition 7.1.4 (SOSP). For twice-differentiable function f (·), x is a
second-order stationary point if
∇f (x) = 0,
and
∇2 f (x) ⪰0.
Definition 7.1.5 (ϵ-SOSP). For a ρ-Hessian Lipschitz function f (·), x is
an ϵ-second-order stationary point if:
∥∇f (x)∥≤ϵ
and
∇2 f (x) ⪰−√ρϵ · I.
Definition 7.1.5 characterizes an ϵ-approximate version of SOSP so
that we can discuss rates. The condition on the Hessian in Definition
7.1.5 uses the Hessian Lipschitz parameter ρ to retain a single accu-
racy parameter and to match the units of the gradient and Hessian 1 ,
1 By matching the “units”, we can make
the optimization results invariant to
simple rescaling g(x) = a f (bx) for
scalar a, b > 0. Here, ρ has the scaling
of third-order derivative, ϵ has the
scaling of the first-order derivative, so
√ρϵ has the scaling of the second-order
derivative.
following the convention of [NP06].
7.2
Perturbed Gradient Descent
According to the update equations, Gradient Descent (GD) makes
a non-zero step only when the gradient is non-zero, and thus in the
nonconvex setting it will be stuck at saddle points if initialized there.
We thus consider a simple variant of GD which adds perturbations to
the iterates at each step.
xt+1 ←xt −η(∇f (xt) + ξt),
ξt ∼N (0, (r2/d)I)
At each iteration, Perturbed Gradient Descent (PGD) is almost the
same as gradient descent, except it adds a small isotropic random

escaping saddle points
77
Gaussian perturbation to the gradient. The perturbation ξt is sam-
pled from a zero-mean Gaussian with covariance (r2/d)I so that
E ∥ξt∥2 = r2. Parameter r control the effective radius of the pertur-
bation, which is often chosen to be very small. [JNG+19] proves that
this simple form of PGD is capable of escaping strict saddle points
and finding SOSP efficiently.
In this chapter, to show the insights behind PGD, we turn to an
alternative variant of the algorithm, which has a slightly more com-
plicated form, but a easier analysis. The variant we consider here
performs the following two steps at each iteration:
1. If ∥∇f (xt)∥≤ϵ and no perturbation has been added in the last
T steps, then add small perturbation xt ←xt −ηξt where ξt ∼
Uniform(B0(r))).
2. xt+1 ←xt −η∇f (xt).
where B0(r) is the Euclidean ball centered at 0 with radius r. This
variant of PGD only adds perturbations when gradient is small and
no perturbation has been added in the last T steps, thus adds less
stochasticity to the algorithm compared to the original form of PGD.
In the following theorem, we provide theoretical guarantees for this
variant of PGD as follows:
Theorem 7.2.1. Assume f is ℓ-gradient Lipschitz, and ρ-Hessian Lipschitz.
For any ϵ, δ > 0, if we choose η = 1/ℓ, r = eΘ(ϵ), T = eΘ(ℓ/√ρϵ),
and run PGD for more than eO(ℓ( f (x0) −f ⋆)/ϵ2) iterations, then with
probability at least 1 −δ, at least one of the iterates will be ϵ-SOSP.
Here eO(·), eΘ(·) hides absolute constant and poly-logarithmic
dependence in d, ℓ, ρ, ϵ, δ and f (x0) −f ⋆. Our choice of T is, up to
logarithmic factors, the ratio of the gradient Lipschitz parameter ℓ
and the Hessian accuracy tolerance in ϵ-SOSP — √ρϵ.
We remark that, in classic optimization literature, it is known
that GD finds an ϵ-first-order stationary point (a point x satisfying
∥∇f (x)∥≤ϵ) in O(ℓ( f (x0) −f ⋆)/ϵ2) iterations [Nes98]. Theorem
7.2.1 shows that PGD finds second-order stationary points in almost
the same time as GD finds first-order stationary points, up to only
logarithmic factors. In particular, despite there might be only one
escaping direction within the d-dimensional space at saddle points,
the dimension dependency of PGD is only polylogarithmic. This is
extremely important in high-dimensional settings, such as training
deep neural networks. This provides a compelling explanation why
strict saddle points are computationally benign for first-order gradi-
ent methods.
The overall proof strategy is as follows. According to Definition
7.1.5, if an iterate xt is not an ϵ-second-order stationary point, then xt

78
theory of deep learning
must either have large gradient or be an approximate saddle point.
We prove the following two claims:
1. Large gradient (∥∇f (xt)∥> ϵ), then function value decreases
significantly in one step: f (xt+1) −f (xt) ≤−Ω(ϵ2/ℓ).
2. Approximate saddle point (∥∇f (xt)∥≤ϵ and λmin(∇2 f (xt)) <
−√ρϵ), then, with high probability, function value decreases sig-
nificantly in T steps: f (xt+T ) −f (xt) ≤−eΩ(T · ϵ2/ℓ).
That is, in either case, the function value will decrease by eΩ(ϵ2/ℓ)
on average per step. Since the function value can be no less than the
optimal value f ⋆, we know after eO(ℓ( f (x0) −f ⋆)/ϵ2) steps, at least
one of the iterates must to ϵ-second-order stationary point. The first
claim immediately follows from the descent lemma (Lemma 2.1.4). In
the next section, we will show how to prove the second claim.
7.3
Saddle Points Escaping Lemma
In this section, we formally prove that if the starting point has a
strictly negative eigenvalue of the Hessian, then adding a pertur-
bation and following by gradient descent will yield a significant
decrease in function value in T iterations.
Lemma 7.3.1 (Saddle Points Escaping Lemma). Under the setting of
Theorem 7.2.1, if ex satisfies ∥∇f (ex)∥≤ϵ, and λmin(∇2 f (ex)) ≤−√ρϵ,
then let x0 = ex + ηξ (ξ ∼Uniform(B0(r))), and run gradient descent
starting from x0. With probability at least 1 −δ, we have
f (xT ) −f (ex) ≤−eΩ(T · ϵ2/ℓ)
where xT is the T th gradient descent iterate starting from x0.
Recall that T = eΘ(ℓ/√ρϵ). This implies that both saddle point
escaping time, and the amount of function decrease depend on di-
mension d only polylogarithmically. To prove this lemma, we will
show the followings:
• (Improve or Localize) If gradient descent keeps making little progress
for a certain number of iterations, then all the iterates within those
iterations must be stuck in a small Euclidean ball.
• (Stuck probability is small around saddle points) If the Hessian has a
significant negative eigenvalue, then after a random perturbation,
with high probability, gradient descent will not be stuck in a small
Euclidean ball for a long time.
Combining two statements above, we conclude that GD in the second
statement must make significant progress after a certain number of
iterations, which proves Lemma 7.3.1.

escaping saddle points
79
7.3.1
Improve or Localize
We first prove the following lemma which says that if the function
value does not decrease too much over t iterations, then all iterates
{xτ}t
τ=0 will remain in a small neighborhood of x0.
Lemma 7.3.2 (Improve or Localize). Assume function f is ℓ-gradient
Lipschitz, and run GD with η ≤1/ℓ, then for any t ≥τ > 0, we have:
∥xτ −x0∥≤
q
2ηt( f (x0) −f (xt)).
Proof. Given the gradient update, xt+1 = xt −η∇f (xt), we have that
for any τ ≤t:
∥xτ −x0∥≤
t
∑
τ=1
∥xτ −xτ−1∥
(1)
≤[t
t
∑
τ=1
∥xτ −xτ−1∥2]
1
2
= [η2t
t
∑
τ=1
∥∇f (xτ−1)∥2]
1
2
(2)
≤
q
2ηt( f (x0) −f (xt)),
where step (1) uses Cauchy-Schwarz inequality, and step (2) is due to
the descent lemma (Lemma 2.1.4).
Lemma 7.3.2 immediately implies that if f (xT ) −f (x0) ≥−eO(T ·
ϵ2/ℓ), i.e. GD does not make enough progress in T steps after per-
turbation, then we immediately have that ∥xt −x0∥≤eO(ϵT /ℓ) for
all t ∈[T ].
7.3.2
Bounding the Width of the Stuck Region
Second, we show that the probability for GD sequence to get stuck
is small if initialized with a point around saddle point with random
perturbation. Recall in Lemma 7.3.1 that x0 ∼Uniform(Bex(ηr)).
We refer to Bex(ηr) as the perturbation ball, and define the stuck region
within the perturbation ball to be the set of points starting from
which GD makes little progress in T steps:
Xstuck := {x ∈Bex(ηr) | {xt}T
t=0 is a GD sequence with
x0 = x, and ∀t ∈[T ], ∥xt −x0∥≤eO(ϵT /ℓ)}.
See Figure 7.1 for illustrations. Since x0 sampled uniformly from
this perturbation ball, the probability GD got stuck after perturbation
is equal to the ratio of the volume of the stuck region and the volume
of the perturbation ball. Therefore, we want to show that the stuck
region has small volume.
In general, the shape of the stuck region can be very complicated,
so it is very difficult to directly compute its volume. A crucial obser-
vation here is that, despite we do not know the shape of the stuck

80
theory of deep learning
0
Figure 7.1: Left: Pertubation
ball in 3D and “thin pancake”
shape stuck region. Right: Per-
tubation ball in 2D and “narrow
band” stuck region under gra-
dient flow
region, we can prove the width of Xstuck along the minimum eigen-
value direction of ∇2 f (ex) is small. In fact, if the width is at most ηω,
then we have Vol(Xstuck) ≤Vol(Bd−1
0
(ηr))ηω, and thus,
Pr(x0 ∈Xstuck) = Vol(Xstuck)
Vol(Bd
ex(ηr)) ≤ηω × Vol(Bd−1
0
(ηr))
Vol(Bd
0(ηr))
= ω
r√π
Γ( d
2 + 1)
Γ( d
2 + 1
2)
≤ω
r ·
r
d
π
To achieve failure probability at most δ, we hope ω ≤O(δr/
√
d).
We bound the width of the stuck region Xstuck by the novel technics
of coupling sequences—consider two GD sequences {xt}T
t=0, {x′
t}T
t=0
which satisfy: (1) max{∥x0 −ex∥,
x′
0 −ex
} ≤ηr; and (2) x0 −x′
0 =
ηωe1, where e1 is the minimum eigenvector of ∇2 f (ex), and ω ≥ω0
for some threshold ω0.
Lemma 7.3.3. For any ω0 ∈(0, ϵ], under the setting of Lemma 7.3.1, if
{xt}T
t=0, {x′
t}T
t=0 are coupling sequences as specified above, then for T ≥
Ω(κ · log(ϵκ/ω0)) where κ := ℓ/√ρϵ, we have,
∃t ∈[T ],
max{∥xt −x0∥,
x′
t −x′
0
} ≥eΩ(ϵT /ℓ)
Lemma 7.3.3 claims that for any pair of x0, x′
0 whose difference is
on the e1 direction, with length greater or equal to ηω0, at least one
of x0, x′
0 is outside Xstuck. This directly implies the width of Xstuck in
e1 direction is ηω0. Lemma 7.3.3 further claims that the width ηω0
can be made arbitrarily small by paying only logarithmic factors in
the choice of T .
Proof. We prove by contradiction. Assume the contrary of Lemma
7.3.3 is true—max{∥xt −x0∥,
x′
t −x′
0
} ≤eO(ϵT /ℓ) for all t ∈[T ],
i.e. both GD sequences stuck in a small Euclidean ball for T steps.
We can write out the update equation for the difference of the

escaping saddle points
81
couple sequences bxt := xt −x′
t as:
bxt+1 =bxt −η[∇f (xt) −∇f (x′
t)] = (I −ηH)bxt −η∆tbxt
= (I −ηH)t+1bx0
|
{z
}
p(t+1)
−η
t
∑
τ=0
(I −ηH)t−τ∆τbxτ
|
{z
}
q(t+1)
,
where H = ∇2 f (ex) and ∆t = R 1
0 [∇2 f (x′
t + θ(xt −x′
t) −H]dθ. We note
that term p(t) is the formula of bxt if function f is quadratic around ex,
and q(t) is the approximation error term caused by function f being
non-quadratic.
We will show later that the quadratic term is the dominating term,
in the sense that ∥q(t)∥≤∥p(t)∥/2 for all t ∈[T ]. Given this is true,
since bx0 = ηωe1, we have
∥p(t)∥≥(1 + √ϵρ/ℓ)t · ηω0
This term grows exponentially. Therefore, by choosing T ≥Ω(κ ·
log(ϵκ/ω0)), we have ∥bxt∥≥∥p(t)∥/2 ≥eΩ(ϵT /ℓ), which con-
tradicts the assumption that both GD sequences stuck in a small
Euclidean ball with radius eO(ϵT /ℓ) for T steps (we note
x0 −x′
0
 ≤
2ηr ≪eO(ϵT /ℓ)). This proves Lemma 7.3.3.
For the remaining of the proof, we only need to verify by induc-
tion that ∥q(t)∥≤∥p(t)∥/2 for all t ∈[T ]. The claim is true for the
base case t = 0 as ∥q(0)∥= 0 ≤∥bx0∥/2 = ∥p(0)∥/2. Now suppose
the induction claim is true up to t. Denote λmin(H) = −γ. Note that
bx0 lies in the direction of the minimum eigenvector of H. Thus for
any τ ≤t, we have:
∥bxτ∥≤∥p(τ)∥+ ∥q(τ)∥≤2 ∥p(τ)∥= 2(1 + ηγ)τηω.
By the Hessian Lipschitz property, we further have
∥∆t∥≤ρ max{∥xt −ex∥,
x′
t −ex
} ≤eO(ρϵT /ℓ) = eO(√ρϵ)
therefore:
∥q(t + 1)∥=
η
t
∑
τ=0
(I −ηH)t−τ∆τbxτ

≤η
t
∑
τ=0
∥∆t∥
(I −ηH)t−τ ∥bxτ∥≤eO(η√ρϵ)
t
∑
τ=0
(1 + ηγ)tηω
≤eO(1)(1 + ηγ)tηω ≤eO(1) ∥p(t + 1)∥,
where the second-to-last inequality uses t + 1 ≤T , and eO(ηT √ρϵ) =
eO(1). Finally, with a careful treatment of constant and logarithmic
factors, we can in fact make this eO(1) term less or equal to 1/2 (we
omit the detail here). This finishes the inductive proof.


8
Algorithmic Regularization
Large scale neural networks used in practice are highly over-parameterized
with far more trainable model parameters compared to the number
of training examples. Consequently, the optimization objectives for
learning such high capacity models have many global minima that
fit training data perfectly. However, minimizing the training loss us-
ing specific optimization algorithms take us to not just any global
minima, but some special global minima – in this sense the choice of
optimization algorithms introduce a implicit form of inductive bias in
learning which can aid generalization.
In over-parameterized models, specially deep neural networks,
much, if not most, of the inductive bias of the learned model comes
from this implicit regularization from the optimization algorithm. For
example, early empirical work on this topic (ref. [NTS15a, NSS15,
HS97, KMN+16, ZBH+16a, CCS+16, DPBB17, ADG+16, Ney17,
WRS+17, HHS17, Smi18]) show that deep models often general-
ize well even when trained purely by minimizing the training error
without any explicit regularization, and even when the networks are
highly overparameterized to the extent of being able to fit random
labels. Consequently, there are many zero training error solutions,
all global minima of the training objective, most of which generalize
horribly. Nevertheless, our choice of optimization algorithm, typ-
ically a variant of gradient descent, seems to prefer solutions that
do generalize well. This generalization ability cannot be explained
by the capacity of the explicitly specified model class (namely, the
functions representable in the chosen architecture). Instead, the op-
timization algorithm biasing toward a “simple" model, minimizing
some implicit “regularization measure”, say R(w), is key for gener-
alization. Understanding the implicit inductive bias, e.g. via char-
acterizing R(w), is thus essential for understanding how and what
the model learns. For example, in linear regression it can be shown
that minimizing an under-determined model (with more parameters
than samples) using gradient descent yields the minimum ℓ2 norm

84
theory of deep learning
solution (see Proposition 8.1.1), and for linear logistic regression
trained on linearly separable data, gradient descent converges in the
direction of the hard margin support vector machine solution (Theo-
rem 8.3.2), even though the norm or margin is not explicitly specified
in the optimization problem. In fact, such analysis showing implicit
inductive bias from optimization agorithm leading to generalization
is not new. In the context of boosting algorithms, (author?) [EHJT04]
and (author?) [Tel13] established connections of gradient boosting
algorithm (coordinate descent) to ℓ1 norm minimiziation, and ℓ1 mar-
gin maximization, respectively. minimization was observed. Such
minimum norm or maximum margin solutions are of course very
special among all solutions or separators that fit the training data,
and in particular can ensure generalization [BM03, KST09].
In this chapter, we largely present results on algorithmic regular-
ization of vanilla gradient descent when minimizing unregularized
training loss in regression and classification problem over various
simple and complex model classes. We briefly discuss general algo-
rithmic families like steepest descent and mirror descent.
Meanings of “implicit regularization due to training algorithm.”
Results in the current chapter tend to show that the solution ob-
tained by applying training algorithm A on Objective 1 essentially
to convergence (e.g. to stationary point of gradient descent), also
satisfies KKT local optimality conditions for some other Objective 2.
In many results Objective 2 is simply Objective 1 with a regularizer
term, typically involving some norm of the solution. Hence we can
think of the training algorithm as implicitly regularizing the objective.
While these results give good insight into the effect of the training
a few caveats are in order, especially if we seek takeaways for deep
learning. First, even though the solution found happens to be a KKT
point of Objective 2, it may be never (or almost never) observed if we
actually do standard training on Objective 2. 1 Second, the results in
1 Recall that in a nonconvex landscape
the solution obtained at the end of
training is greatly affected by the
initialization, and in deep learning the
initialization is very special.
this chapter are often stated for training carried out to infinite time,
which may also limit their applicability to real life.
In later chapters we will see a different type of analysis, which
analyses the trajectory followed by the solution as it evolves during
training. This dynamic view of training quickly gets complicated (as
opposed to the more static view taken in understanding stationary
points) and has not been achieved for realistic deep nets yet.
8.1
Linear models in regression: squared loss
suriya: pls see chapter 3 and modify the writeup as needed.
We first demonstrate the algorithmic regularization in a simple

algorithmic regularization
85
linear regression setting where the prediction function is specified by
a linear function of inputs: fw(x) = w⊤x and we have the following
empirical risk minimzation objective.
L(w) =
n
∑
i=1

w⊤x(i) −y(i)2
.
(8.1)
Such simple modes are natural starting points to build analytical
tools for extending to complex models, and such results provide intu-
itions for understaning and improving upon the empirical practices
in neural networks. Although the results in this section are speci-
fied for squared loss, the results and proof technique extend for any
smooth loss a unique finite root: where ℓ(by, y) between a prediction by
and label y is minimized at a unique and finite value of by [GLSS18a].
We are particularly interested in the case where n < d and the ob-
servations are realizable, i.e., minw L(w) = 0. Under these conditions,
the optimization problem in eq. (8.1) is underdetermined and has
multiple global minima denoted by G = {w : ∀i, w⊤x(i) = y(i)}. In
this and all the following problems we consider, the goal is to answer:
Which specific global minima do different optimization algorithms reach
when minimizing L(w)?
The following proposition is the simplest illustration of the algo-
rithmic regularization phenomenon.
Proposition 8.1.1. Consider gradient descent updates wt for the loss in
eq. (8.1) starting with initialization w0. For any step size schedule that
minimizes the loss L(w), the algorithm returns a special global minimizer
that implicitly also minimzes the Euclidean distance to the initialization:
wt →argmin
w∈G
∥w −w0∥2.
Proof. The key idea is in noting that that the gradients of the loss
function have a special structure. For the linear regression loss in eq.
(8.1) ∀w, ∇L(w) = ∑i(w⊤x(i) −y(i))x(i) ∈span({x(i)}) - that is
the gradients are restricted to a n dimentional subspace that is inde-
pendent of w. Thus, the gradient descent updates from iniitalization
wt −w0 = ∑t′<t ηwt′, which linearly accumulate gradients, are again
constrained to the n dimensional subspace. It is now easy to check
that there is a unique global minimizer that both fits the data (w ∈G)
as well as is reacheable by gradient descent (w ∈w0 + span({x(i)})).
By checking the KKT conditions, it can be verified that this unique
minimizer is given by argminw∈G ∥w −w0∥2
2.
In general overparameterized optimization problems, the char-
acterization of the implicit bias or algorithmic regulariztion is of-
ten not this elegant or easy. For the same model class, changing
the algorithm, or changing associated hyperparameter (like step

86
theory of deep learning
size and initialization), or even changing the specific parameteriza-
tion of the model class can change the implicit bias. For example,
(author?) [WRS+17] showed that for some standard deep learning
architectures, variants of SGD algorithm with different choices of
momentum and adaptive gradient updates (AdaGrad and Adam)
exhibit different biases and thus have different generalization per-
formance;(author?) [KMN+16], (author?) [HHS17] and (author?)
[Smi18] study how the size of the mini-batches used in SGD influ-
ences generalization; and (author?) [NSS15] compare the bias of path-
SGD (steepest descent with respect to a scale invariant path-norm) to
standard SGD.
A comprehensive understanding of how all the algorithmic choices
affect the implicit bias is beyond the scope of this chapter (and also
the current state of research). However, in the context of this chapter,
we specifically want to highlight the role of geometry induced by
optimization algorithm and specific parameterization, which are
discussed briefly below.
8.1.1
Geometry induced by updates of local search algorithms
The relation of gradient descent to implicit bias towards minimizing
Euclidean distance to initialization is suggestive of the connection be-
tween algorithmic regularization to the geometry of updates in local
search methods. In particular, gradient descent iterations can be alter-
natively specified by the following equation where the t + 1th iterate
is derived by minimizing the a local (first order Taylor) approxima-
tion of the loss while constraining the step length in Euclidean norm.
wt+1 = argmin
w
⟨w, ∇L(wt)⟩+ 1
2η ∥w −wt∥2
2 .
(8.2)
Motivated by the above connection, we can study other families of
algorithms that work under different and non-Euclidean geometries.
Two convenient families are mirror descent w.r.t. potential R [BT03,
NY83] and steepest descent w.r.t. general norms [BV04].
Mirror descent w.r.t. potential R
Mirror descent updates are de-
fined for any strongly convex and differentiable potential R as
wt+1 = arg min
w
η ⟨w, ∇L(wt)⟩+ DR(w, wt),
=⇒∇R(wt+1) = ∇R(wt) −η∇L(wt)
(8.3)
where DR(w, w′) = R(w) −R(w′) −⟨∇R(w′), w −w′⟩is the Breg-
man divergence [Bre67] w.r.t. R. This family captures updates where
the geometry is specified by the Bregman divergence DR. Exam-
ples of potentials R for mirror descent include the squared ℓ2 norm

algorithmic regularization
87
R(w) = 1/2∥w∥2
2, which leads to gradient descent; the entropy po-
tential R(w) = ∑i w[i] log w[i] −w[i]; the spectral entropy for matrix
valued w, where R(w) is the entropy potential on the singular values
of w; general quadratic potentials R(w) = 1/2∥w∥2
D = 1/2 w⊤Dw
for any positive definite matrix D; and the squared ℓp norms for
p ∈(1, 2].
From eq. (8.3), we see that rather than wt (called primal iterates),
it is the ∇R(wt) (called dual iterates) that are constrained to the low
dimensional data manifold ∇R(w0) + span({x(i)}). The arguments
for gradient descent can now be generalized to get the following
result.
Theorem 8.1.2. For any realizable dataset {x(i), y(i)}N
n=1, and any strongly
convex potential R, consider the mirror descent iterates wt from eq. (8.3) for
minimizing the empirical loss L(w) in eq. (8.1). For any initializations w0
and any step-size schedule , if wt converges to some zero-loss solution w∗,
then it holds that
w∗=
arg min
w:∀i,w⊤x(i)=y(i)
DR(w, w0).
(8.4)
In particular, if we start at w0 = arg minw R(w) (so that ∇R(w0) =
0), then we get to arg minw∈G R(w). 2
2 The analysis of Theorem 8.1.2 and
Proposition 8.1.1 also hold when
instancewise stochastic gradients are
used in place of ∇L(wt).
Proof of Theorem 8.1.2. From Equation (8.3), we have ∇R(wt) =
∇R(w0) + ∑n
i=1 λixi for some λ1, . . . , λn ∈R. Thus w∗is a stationary
point of loss function w 7→DR(w, w0) −∑n
i=1 λixi. Since this function
is convex, w∗is a global minimum, and thus a constrained global
minimum of the loss function among all interpolating solutions.
Steepest descent w.r.t. general norms
Gradient descent is also a
special case of steepest descent (SD) w.r.t a generic norm ∥.∥[BV04]
with updates given by,
wt+1 = wt + ηt∆wt, where ∆wt = arg min
v
⟨∇L(wt), v⟩+ 1
2∥v∥2. (8.5)
Examples of steepest descent include gradient descent, which is
steepest descent w.r.t ℓ2 norm and coordinate descent, which is steep-
est descent w.r.t ℓ1 norm. In general, the update ∆wt in eq. (8.5) is
not uniquely defined and there could be multiple direction ∆wt that
minimize eq. (8.5). In such cases, any minimizer of eq. (8.5) is a valid
steepest descent update.
Generalizing gradient descent and mirror descent, we might ex-
pect the steepest descent iterates to converge to the solution closest
to initialization in corresponding norm, arg minw∈G ∥w −w0∥. This is
indeed the case for quadratic norms ∥v∥D =
√
v⊤Dv when eq. 8.5 is

88
theory of deep learning
equivalent to mirror descent with R(w) = 1/2∥w∥2
D. Unfortunately,
this does not hold for general norms as shown by the following re-
sults.
Example 1.
In the case of coordinate descent, which is a special
case of steepest descent w.r.t. the ℓ1 norm, (author?) [EHJT04] studied
this phenomenon in the context of gradient boosting: obseving that
sometimes but not always the optimization path of coordinate descent
given by ∆wt+1 ∈conv
n
−ηt
∂L(wt)
∂w[jt] ejt : jt = argmaxj
 ∂L(wr)
∂w[j]

o
, coin-
cides with the ℓ1 regularization path given by, bw(λ) = arg minw L(w) +
λ∥w∥1. The specific coordinate descent path where updates aver-
age all the optimal coordinates and the step-sizes are infinitesimal is
equivalent to forward stage-wise selection, a.k.a. ϵ-boosting [Fri01].
When the ℓ1 regularization path bw(λ) is monotone in each of the
coordinates, it is identical to this stage-wise selection path, i.e., to a
coordinate descent optimization path (and also to the related LARS
path) [EHJT04]. In this case, at the limit of λ →0 and t →∞, the
optimization and regularization paths, both converge to the mini-
mum ℓ1 norm solution. However, when the regularization path bw(λ)
is not monotone, which can and does happen, the optimization and
regularization paths diverge, and forward stage-wise selection can
converge to solutions with sub-optimal ℓ1 norm.
Example 2.
The following example shows that even for ℓp norms
where the ∥.∥2
p is smooth and strongly convex, the global minimum
returned by the steepest descent depends on the step-size.
Consider minimizing L(w) with dataset {(x(1) = [1, 1, 1], y(1) =
1), (x(2) = [1, 2, 0], y(2) = 10)} using steepest descent updates w.r.t.
the ℓ4/3 norm. The empirical results for this problem in Figure 8.1
clearly show that steepest descent converges to a global minimum
that depends on the step-size and even in the continuous step-size
limit of η →0, wt does not converge to the expected solution of
arg minw∈G ∥w −w0∥.
In summary, for squared loss, we characterized the implicit bias of
generic mirror descent algorithm in terms of the potential function
and initialization. However, even in simple linear regression, for
steepest descent with general norms, we were unable to get a useful
characterization. In contrast, in Section 8.3.2, we study logistic like
strictly monotonic losses used in classification, where we can get a
characterization for steepest descent.
8.1.2
Geometry induced by parameterization of model class
In many learning problems, the same model class can be parameter-
ized in multiple ways. Given a parameter space Rd and a parametrized
model fw mapping input x to output fw(x), we consider a new pa-

algorithmic regularization
89
0 5
0.0
−0.5
−1.0
0
1
2
3
4
5
6
−4.0
−3.5
−3.0
−2.5
−2.0
−1.5
−1.0
−0.5
0.0
winit=[0,0,0]
w *
||. ||
w∞
η→0
η = 0.01
η = 0.1
η = 0.25
η = 0.3
Figure 8.1: Steepest descent
w.r.t ∥.∥4/3: the global mini-
mum to which steepest de-
scent converges to depends
on η. Here w0
=
[0, 0, 0],
w∗
∥.∥
=
arg minR∈G ∥w∥4/3 de-
notes the minimum norm global
minimum, and w∞
η→0 denotes the
solution of infinitesimal SD with
η →0. Note that even as η →0, the
expected characterization does not
hold, i.e., w∞
η→0 ̸= w∗
∥.∥.
rameter space RD where D ≥d and a parametrization, which is a
surjective map G : θ 7→G(θ) from RD to Rd. The parametrization G
induces a new parametrized model efθ(x) ≜fG(θ)(x), for all input x.
For example, the set of linear functions in Rd can be parameterized in
a canonical way as w ∈Rd with fw(x) = w⊤x, but also equivalently
by θ = (u, v) where u, v ∈Rd with efθ(x) = fu⊙v(x) = (u ⊙v)⊤x or
efθ(x) = fu⊙2−v⊙2(x) = (u⊙2 −v⊙2)⊤x. All such equivalent parameter-
izations lead to equivalent model class, however, in overparemterized
models, using vanilla gradient descent on different parameterizations
lead to different trajectories {G(θt)}t∈N in the original parameter
space Rd, and thus different induced biases in the function space.
The reason behind this phenomenon is that, though vanilla gradi-
ent descent finds the steepest descent direction w.r.t. ℓ2-norm, but
the parametrization G does not necessarily preserve the ℓ2 distance
and thus distort the geometry of local descent. For example, (au-
thor?) [GWB+17, GLSS18b] demonstrated this phenomenon in matrix
factorization and linear convolutional networks, where these param-
eterizations were shown to introduce interesting and unusual biases
towards minimizing nuclear norm, and ℓp (for p = 2/depth) norm
in Fourier domain, respectively. In general, these results are sugges-
tive of role of architecture choice in different neural network models,
and shows how even while using the same gradient descent algo-
rith, different geometries in the function space can be induced by the
different parameterizations.

90
theory of deep learning
8.1.3
Equivalence between geometry inducded by local search algorithms
and reparametrization
In the previous sections, we saw that both the parametrization of the
model class and the local search method of optimization algorithm
can induce a different geometry for the optimization landscape. In
this section we show that the two geometries can be equivalent, that
is, the two optimization trajectories for loss L, wt and G(θt), are the
same for continuous gradient/mirror descents, where xt follows
Mirror Flow Equation (8.6)
d∇R(wt)
dt
= −∇L(wt),
(8.6)
and θt follows Reparametrized Gradient Flow Equation (8.7)
dθt
dt = −∇(L ◦G)(θt)
(8.7)
We stress that though the optimization geometry depends on L,
the main equivalence result that will be presented in this section, is
a property of the parametrization G, the potential R, and potentially
the initialization θ0, w0 = G(θ0). In particular, when the equivalence
holds, it simultaneously holds for all differentiable loss L.
Below we present the intuition behind the equivalence. Mirror
Flow Equation (8.6) can be alternatively written as:
dwt
dt = −(∇2R(wt))−1∇L(wt).
(8.8)
And Reparametrized Gradient Flow Equation (8.7) yields the
following trajectory in the w-space, Rd:
dG(θt)
dt
= −∂G(θt)∂G(θt)⊤∇L(G(θt)).
(8.9)
Thus for the two trajectories G(θt) and wt to be the same, it suf-
fices to require:
∂G(θt)∂G(θt)⊤= (∇2R(G(θt)))−1,
∀t ≥0.
(8.10)
If Equation (8.10) holds, then both G(θt) and wt satisfy the same
differential equation, and thus are the same. A even stronger (but
also more convenient) sufficient condition is the following:
∂G(θ)∂G(θ)⊤= (∇2R(G(θ)))−1,
∀θ.
(8.11)
Equation (8.11) is easier to check because it requires no under-
standing of the optimization trajectories θt. Below are two examples
where equivalenec holds because Equation (8.11) is satisfied:

algorithmic regularization
91
Example 8.1.3 (Quadratic Mirror Map and Linear Reparametrization).
The geometry induced by mirror descent with quadratic potential R(w) =
w⊤Σ−1w/2 is equivalent to the geometry induced by gradient descent with
reparametrization G(θ) =
√
Σθ for any positive definite matrix Σ ∈Rd×d.
Example 8.1.4 (Entropy Mirror Map and Quadratic Reparametriza-
tion). The geometry induced by mirror descent with entropy potential
R(w) = 1/4 ∑i w[i] log w[i] −w[i] is equivalent to the geometry induced by
gradient descent with reparametrization G(θ) = θ⊙2.
However, Equation (8.11) is not necessary for the equivalence to
hold. In fact, there is a large class of examples where Equation (8.11)
does not hold, but the equivalence still holds. SUch examples typi-
cally have a non-invertible parametrization G, which further allows
two different parameters θ1 and θ2 with the value in w-space, i.e.,
G(θ1) = G(θ2). See Example 8.1.5 for an concrete example.
Example 8.1.5. Let D = 2d and θ = (θ+, θ−) ∈RD. Consider the
following parametrization G(θ) = (θ+)⊙2 −(θ−)⊙2.
To see why Equation (8.11) fails, it suffices to take d = 1 and consider
θ1 = (1, 0) and θ2 = (
√
2, 1). We can see G(θ1) = G(θ2) = 1 but
4 = ∂G(θ1)∂G(θ1)⊤̸= ∂G(θ2)∂G(θ2)⊤= 12.
The following theorem Theorem 8.1.6 shows that for the parametriza-
tion defined in Example 8.1.5, for every initialization θ0 of certain
form, there exsits a mirror map depending on the initialization θ0 sat-
isfies Equation (8.10). 3 Thus the equivalence between mirror descent
3 The dependence of mirror map on the
initialization is necessary. Otherwise,
if Equation (8.10) holds for all different
initializations, we can again apply the
construction in Example 8.1.5, which
yields a contradiction.
and gradient descent holds for this parametrization.
Theorem 8.1.6 ((author?) [WGL+20]). Under setting for Example 8.1.5,
for any α > 0 and θ0 = (α1, −α1)4, where 1 ∈Rd is the all-one vector,
4 This theorem can be generalized to
any θ0 ∈R2d with a potentially more
complicated mirror map
the gradient flow trajectory θt of L ◦G (Equation (8.7)) is equivalent to the
mirror flow trajectory wt of Rα(w) ≜α2/4 · ∑n
i=1 q( w[i]
α2 ) (Equation (8.6)),
where
q(z) = 2 −
p
4 + z2 + z · arcsinh
 z
2

.
(8.12)
The high-level proof idea here is that we can write ∂G(θt)∂G(θt)⊤
as a function of G(θt) and θ0, which in turn can be written as the
inverse of the Hessian of mirror map function w.r.t. G(θt). The mirror
map depends on the θ0. A key observation here is a conservation law
over time, Equation (8.13).
Proof of Theorem 8.1.6. First we notice that by chain rule,
d(θ+
t ⊙θ−
t )/dt = 0
(8.13)

92
theory of deep learning
and thus has θ+
t ⊙θ−
t ≡θ+
0 ⊙θ−
0 = α21 is constant over time t. This
further implies that
∂G(θt)∂G(θt)⊤=8 · (θ+
t )⊙2 + (θ−
t )⊙2
2
(8.14)
=8


 
(θ+
t )⊙2 −(θ−
t )⊙2
2
!⊙2
+ 1α4


1/2
(8.15)
=8
  G(θt)
2
⊙2
+ 1α4
!⊙1/2
(8.16)
=4

(G(θt))⊙2 + 4 · 1α4⊙1/2
.
(8.17)
Finally we note that for any i, j ∈[d] and w[i], w[j] ∈R,
∂Rα(w)
∂w[i]
= α2q(w[i]/α2)
dw[i]
= 1
4arcsinh
w[i]
2α2

,
(8.18)
and that,
∂2Rα(w)
∂w[i]∂w[j] = ∂2Rα(w)
∂w[i]∂w[i]1i=j = 1
4
darcsinh
 w[i]
2α2

dw[i]
1i=j
(8.19)
=
1
4
p
w[i]2 + 4α4 1i=j
(8.20)
This completes the proof because now by Equation (8.9), we have
dG(θt)
dt
= −(∇2Rα(G(θt)))−1∇L(G(θt)),
(8.21)
showing G(θt) is equal to wt, as they are both the unique solution of
Equation (8.6).
Combinig the above theorem with Theorem 8.1.2, we have the
following theorem:
Theorem 8.1.7 (Theorem 1 in (author?) [WGL+20]). Under the setting
of Theorem 8.1.6, additionaly assume that
1. L(w) = 1
n(w⊤x(i) −y(i))2 where (x(i), y(i))n
i=1 are training datasets;
2. the reparametrized gradient flow Equation (8.7) with initial point θ0 =
(α1, −α1) converges to a 0-loss solution θ∞.
Then w∞= G(θ∞) satisfies that
w∞= arg min
w∈Rd
Rα(w)
(8.22)
s.t.L(w) = 0.
(8.23)
[zhiyuan:add some basic comments about the mirror map?]

algorithmic regularization
93
8.1.4
Equivalence Between Commuting Parametrization and Mirror De-
scent
In this subsection we are interested in the following two questions:
1. For what mirror map R, there exists a parametrization G such that
the equivalence holds?
2. For what parametrization G, there exists a mirror map R such that
the equivalence holds?
The proof for Theorem 8.1.6 does not give us much insight on how
to decide whether such mirror map R exists for a given parametriza-
tion G. It relies on the conservation law Equation (8.13), which seems
to be a special property of the parametrization G in Example 8.1.5.
In this subsection we will introduce a more general framework
towards attacking the above two questions from (author?) [LWLA22].
It turns out the answer to the first question is always yes. And the
answer to the second question is positive when certain conditions for
parameterization G are met, e.g., the following commuting condition:
Definition 8.1.8 (Commuting parametrization). We say a parametriza-
tion G : RD →Rd is commuting iff [∇Gi, ∇Gj](θ) = ∇2Gi(θ)∇Gj(θ) −
∇2Gj(θ)∇Gi(θ) = 0 for all θ ∈RD, i, j ∈[D].5
5 Here [·, ·] denotes the Lie bracket.
Formally, for any two vector fields
X, Y, the Lie bracket [X, Y] is defined as
[X, Y](θ) = ∂X · Y −∂Y · X for any θ.
One can easily check that parametrizations in both Examples 8.1.3
to 8.1.5 are all commuting parametrizations. (author?) [LWLA22] also
shows that a slightly relaxed notion of commuting parametrizations
is necessary to induce equivalent geometry with some mirror map.
[zhiyuan:TBD]
8.2
Matrix factorization
≪Suriya notes: I would like to include this section here but can also move to a separate chapter.
Ideally, summarize our 2017 paper, Tengyu’s 2018 paper and Nadav’s 2019 paper. May be we
can discuss this after Nadav’s lecture?≫
8.3
Linear Models in Classification
We now turn to studing classification problems with logistic or cross-
entropy type losses. We focus on binary classification problems
where y(i) ∈{−1, 1}. Many continuous surrogate of the 0-1 loss
inlcuding logistic, cross-entropy, and exponential loss are examples of
strictly monotone loss functions ℓwhere the behavior of the implicit
bias is fundamentally different, and as are the situations when the
implicit bias can be characterized.

94
theory of deep learning
We look at classification models that fit the training data {x(i), y(i)}i
with linear decision boundaries f (x) = w⊤x with decision rule given
by by(x) = sign( f (x)). In many instances of the proofs, we also as-
sume without loss of generality that y(i) = 1 for all i, since for linear
models, the sign of y(i) can equivalently be absorbed into x(i). We
again look at unregularized empirical risk minimization objective of
the form in eq. (8.1), but now with strictly monotone losses. When
the training data {x(i), y(i)}n is not linearly separable, the empirical
objective L(w) can have a finite global minimum. However, if the
dataset is linearly separable, i.e., ∃w : ∀i, y(i)w⊤y(i) > 0, the empir-
ical loss L(w) is again ill-posed, and moreover L(w) does not have
any finite minimizer, i.e, L(w) →0 only as ∥w∥→∞. Thus, for any
sequence {wt}∞
t=0, if L(wt) →0, then wt necessarily diverges to infin-
ity rather than converge, and hence we cannot talk about limt→∞wt.
Instead, we look at the limit direction ¯w∞= lim
t→∞
wt
∥wt∥whenever the
limit exists. We refer to existence of this limit as convergence in direc-
tion. Note that, the limit direction fully specifies the decision rule of
the classifier that we care about.
In the remainder of the chapter, we focus on the following expo-
nential loss ℓ(u, y) = exp(−uy). However, our asymptotic results can
be extended to loss functions with tight exponential tails, including
logistic and sigmoid losses, along the lines of (author?) [SHS17] and
(author?) [Tel13].
L(w) =
n
∑
i=1
exp(−y(i)w⊤x(i)).
(8.24)
8.3.1
Gradient Descent
(author?) [SHS17] showed that for almost all linearly separable
datasets, gradient descent with any initialization and any bounded step-
size converges in direction to maximum margin separator with unit ℓ2
norm, i.e., the hard margin support vector machine classifier.
This characterization of the implicit bias is independent of both the
step-size as well as the initialization. We already see a fundamentally
difference from the implicit bias of gradient descent for losses with a
unique finite root (Section ??) where the characterization depended
on the initialization. The above result is rigorously proved as part of
a more general result in Theorem 8.3.2. Below is a simpler statement
and with a heuristic proof sketch intended to convey the intuition for
such results.
Theorem 8.3.1. For almost all dataset which is linearly separable, consider
gradient descent updates with any initialization w0 and any step size that
minimizes the exponential loss in eq. (8.24), i.e., L(wt) →0. The gradient

algorithmic regularization
95
descnet iterates then converge in direction to the ℓ2 max-margin vector, i.e.,
limt→∞
wt
∥wt∥2 =
bw
∥bw∥, where
bw = argmin
w
∥w∥2 s.t. ∀i, w⊤x(i)y(i) ≥1.
(8.25)
Without loss of generality assume that ∀i, y(i) = 1 as the sign for
linear models can be absobed into x(i).
Proof Sketch
We first understand intuitively why an exponential tail
of the loss entail asymptotic convergence to the max margin vector:
examine the asymptotic regime of gradient descent in when the ex-
ponential loss is minimized, as we argued earlier, this required that
∀i : w⊤x(i) →∞. Suppose wt/ ∥wt∥2 converges to some limit w∞, so
we can write wt = g(t)w∞+ ρ(t) such that g(t) →∞, ∀i, w⊤
∞x(i) > 0,
and limt→∞ρ(t)/g(t) = 0. The gradients at wt are given by:
−∇L(w) =
n
∑
i=1
exp

−w⊤x(i)
x(i)
=
n
∑
i=1
exp

−g(t)w⊤
∞x(i)
exp

−ρ(t)⊤x(i)
xn .
(8.26)
As g(t) →∞and the exponents become more negative, only those
samples with the largest (i.e., least negative) exponents will con-
tribute to the gradient. These are precisely the samples with the
smallest margin argminiw⊤
∞x(i), aka the “support vectors”. The accu-
mulation of negative gradient, and hence wt, would then asymptoti-
cally be dominated by a non-negative linear combination of support
vectors. These are precisely the KKT conditions for the SVM problem
(eq. 8.25). Making these intuitions rigorous constitutes the bulk of
the proof in (author?) [SHS17], which uses a proof technique very
different from that in the following section (Section 8.3.2).
8.3.2
Steepest Descent
. Recall that gradient descent is a special case of steepest descent
(SD) w.r.t a generic norm ∥· ∥with updates given by eq. (8.5). The
optimality condition of ∆wt in eq. (8.5) requires
⟨∆wt,−∇L(wt)⟩=∥∆wt∥2 =∥∇L(wt)∥2
⋆.,
(8.27)
where ∥x∥⋆= sup∥y∥≤1 x⊤y is the dual norm of ∥· ∥. Examples of
steepest descent include gradient descent, which is steepest descent
w.r.t ℓ2 norm and greedy coordinate descent (Gauss-Southwell se-
lection rule), which is steepest descent w.r.t ℓ1 norm. In general, the
update ∆wt in eq. (8.5) is not uniquely defined and there could be
multiple direction ∆wt that minimize eq. (8.5). In such cases, any

96
theory of deep learning
minimizer of eq. (8.5) is a valid steepest descent update and satisfies
eq. (8.27).
In the preliminary result in Theorem 8.3.1, we proved the limit di-
rection of gradient flow on the exponential loss is the ℓ2 max-margin
solution. In the following theorem, we show the natural extension of
this to all steepest descent algorithms.
Theorem 8.3.2. For any separable dataset {xi, yi}n
i=1 and any norm ∥·∥,
consider the steepest descent updates from eq. (8.27) for minimizing L(w)
in eq. (8.24) with the exponential loss ℓ(u, y) = exp(−uy). For all initial-
izations w0, and all bounded step-sizes satisfying ηt ≤min{η+,
1
B2L(wt)},
where B := maxn ∥xn∥⋆and η+ < ∞is any finite number, the iterates wt
satisfy the following,
lim
t→∞min
n
yi ⟨wt, yi⟩
∥wt∥
=
max
w:∥w∥≤1 min
n
yi ⟨w,xi⟩=: γ.
In particular, if there is a unique maximum-∥· ∥margin solution w∗=
arg max∥w∥≤1 mini yi ⟨w, xi⟩, then the limit direction satisfies limt→∞
wt
∥wt∥=
w∗.
A special case of Theorem 8.3.2 is for steepest descent w.r.t. the ℓ1
norm, which as we already saw corresponds to greedy coordinate
descent. More specifically, coordinate descent on the exponential
loss with exact line search is equivalent to AdaBoost [SF12], where
each coordinate represents the output of one “weak learner”. In-
deed, initially mysterious generalization properties of boosting
have been understood in terms of implicit ℓ1 regularization [SF12],
and later on AdaBoost with small enough step-size was shown to
converge in direction precisely to the maximum ℓ1 margin solution
[ZY+05, SSS10, Tel13], just as guaranteed by Theorem 8.3.2. In fact,
(author?) [Tel13] generalized the result to a richer variety of exponen-
tial tailed loss functions including logistic loss, and a broad class of
non-constant step-size rules. Interestingly, coordinate descent with
exact line search (AdaBoost) can result in infinite step-sizes, leading
the iterates to converge in a different direction that is not a max-
ℓ1-margin direction [RDS04], hence the bounded step-sizes rule in
Theorem 8.3.2.
Theorem 8.3.2 is a generalization of the result of (author?) to steep-
est descent with respect to other norms, and our proof follows the
same strategy as (author?). We first prove a generalization of the
duality result of (author?) [SSS10]: if there is a unit norm linear sep-
arator that achieves margin γ, then ∥∇L(w)∥⋆≥γL(w) for all w. By
using this lower bound on the dual norm of the gradient, we are able
to show that the loss decreases faster than the increase in the norm
of the iterates, establishing convergence in a margin maximizing
direction.

algorithmic regularization
97
In the rest of this section, we discuss the proof of Theorem 8.3.2.
The proof is divided into three steps:
1. Gradient domination condition: For all norms and any w, ∥∇L(w)∥⋆≥
γL(w)
2. Optimization properties of steepest descent such as decrease of
loss function and convergence of the gradient in dual norm to
zero.
3. Establishing sufficiently fast convergence of L(wt) relative to the
growth of ∥wt∥to prove the Theorem.
Proposition 8.3.3. Gradient domination condition (Lemma 10 of [GLSS18a])
Let γ = max∥w∥≤1 mini yix⊤
i w. For all w,
∥∇L(w)∥⋆≥γL(w).
Next, we establish some optimization properties of the steepest
descent algorithm including convergence of gradient norms and loss
value.
Proposition 8.3.4. (Lemma 11 and 12 of (author?) [GLSS18a]) Consider
the steepest descent iterates wt on (8.24) with stepsize η ≤
1
B2L(w0), where
B = maxi ∥xi∥⋆. The following holds:
1. L(wt+1) ≤L(wt).
2. ∑∞
t=0 ∥∇L(wt)∥2 < ∞and hence ∥∇L(wt)∥⋆→0.
3. L(wt) →0 and hence w⊤
t xi →∞.
4. ∑∞
t=0 ∥∇L(wt)∥⋆= ∞.
Given these two Propositions, the proof proceeds in two steps. We
first establish that the loss converges to zero sufficiently quickly to
lower bound the unnormalized margin mini w⊤
t xi. Next, we upper
bound ∥wt∥. By dividing the lower bound in the first step by the
upper bound in the second step, we can lower bound the normalized
margin, which will complete the proof.
Proof of Theorem 8.3.2. Step 1: Lower bound the unnormalized mar-
gin. First, we establish the loss converges sufficiently quickly. Define

98
theory of deep learning
γt = ∥∇L(wt)∥⋆. From Taylor’s theorem,
L(wt+1) ≤
L(wt) + ηt ⟨∇L(wt), ∆wt⟩+ sup
β∈(0,1)
η2
t
2 ∆wt⊤∇2L (wt + βηt∆wt) ∆wt
(a)
≤L(wt) −ηt ∥∇L(wt)∥2
⋆+ η2
t B2
2
sup
β∈(0,1)
L (wt + βηt∆wt) ∥∆wt∥2
(b)
≤L(wt) −ηt ∥∇L(wt)∥2
⋆+ η2
t B2
2
L(wt) ∥∆wt∥2
(8.28)
where (a) uses v⊤∇2L(w)v ≤L(w)B2∥v∥2 and (b) uses Proposition
8.3.4 part 1 and convexity to show supβ∈(0,1) L (wt + βηt∆wt) ≤
L(wt).
From eq . 8.28, using γt = ∥∇L(wt)∥⋆= ∥∆wt∥, we have that
L(wt+1) ≤L(wt) −ηγ2
t + η2B2L(wt)γ2
t
2
= L(wt)

1 −ηγ2
t
L(wt) + η2B2γ2
t
2

(a)
≤L(wt) exp

−ηγ2
t
L(wt) + η2B2γ2
t
2

(b)
≤L(w0) exp
 
−∑
u≤t
ηuγ2
u
L(wu) + ∑
u≤t
η2B2γ2
u
2
!
,
(8.29)
where we get (a) by using (1 + x) ≤exp(x), and (b) by recursing the
argument.
Next, we lower bound the unnormalized margin. From eq. (8.29),
we have,
max
n∈[N] exp(−⟨wt+1, xn⟩) ≤L(w(t+1))
≤L(w0) exp
 
−∑
u≤t
ηγ2
u
L(wu) + ∑
u≤t
η2B2γ2
u
2
!
(8.30)
By applying −log,
min
n∈[N] ⟨wt+1, xn⟩≥∑
u≤t
ηγ2
u
L(wu) −∑
u≤t
η2B2γ2
u
2
−log L(w0).
(8.31)
Step 2: Upper bound ∥wt+1∥. Using ∥∆wu∥= ∥∇L(wu)∥⋆= γu,
we have,
∥wt+1∥≤∥w0∥+ ∑
u≤t
η ∥∆wu∥≤∥w0∥+ ∑
u≤t
ηγu.
(8.32)

algorithmic regularization
99
To complete the proof, we simply combine Equations (8.31) and
(8.32) to lower bound the normalized margin.
⟨wt+1, xn⟩
∥wt+1∥
≥
∑u≤t
ηγ2u
L(wu)
∑u≤t ηγu + ∥w0∥
−

∑u≤t
η2B2γ2u
2
+ log L(w0)
∥wt+1∥

.
:= (I)
+(II).
(8.33)
For term (I), from Proposition 8.3.3, we have γu = ∥∇L(wu)∥⋆≥
γL(wu). Hence the numerator is lower bounded ∑u≤t
ηγ2u
L(wu) ≥
γ ∑u≤t ηγu. We have
∑u≤t
ηγ2u
L(wu)
∑u≤t ηγu + ∥w0∥≥γ
∑u≤t ηγu
∑u≤t ηγu + ∥w0∥→γ,
(8.34)
using ∑u≤t ηγu →∞and ∥w0∥< ∞from Proposition 8.3.4.
For term (II), log L(w0) < ∞and ∑u≤t
η2B2γ2u
2
< ∞using Proposi-
tion 8.3.3. Thus (II) →0.
Using the above in Equation (8.33), we obtain
lim
t→∞
w⊤
t+1xi
∥wt+1∥≥γ := max
∥w∥≤1 min
i
w⊤xi
∥w∥.
8.4
Homogeneous Models with Exponential Tailed Loss
≪Suriya notes: Jason: I think we should give Kaifengs’ proof here. Its more general and con-
current work.≫In this section, we consider the asymptotic behavior of
gradient descent when the prediction function is homogeneous in the
parameters. Consider the loss
L(w) =
n
∑
i=1
exp(−yi fi(w)),
(8.35)
where fi(cw) = cα fi(w) is α-homogeneous. Typically, fi(w) is the out-
put of the prediction function such as a deep network. Similar to the
linear case in Section 5.5.1, there is a related maximum margin prob-
lem. Define the optimal margin as γ = max∥w∥2=1 mini yi fi(w). The
associated non-linear margin maximization is given by the following
non-convex constrained optimization:
min ∥w∥2 st yi fi(w) ≥γ.
(Max-Margin)
Analogous to Section 5.5.1, we expect that gradient descent on
Equation (8.35) converges to the optimum of the Max-Margin prob-
lem (Max-Margin). However, the max-margin problem itself is a con-
strained non-convex problem, so we cannot expect to attain a global

100
theory of deep learning
optimum. Instead, we show that gradient descent iterates converge to
first-order stationary points of the max-margin problem.
Definition 8.4.1 (First-order Stationary Point). The first-order optimality
conditions of Max-Margin are:
1. ∀i, yi fi(w) ≥γ
2. There exists Lagrange multipliers λ ∈RN+ such that w = ∑n λn∇fn(w)
and λn = 0 for n /∈Sm(w) := {i : yi fi(w) = γ}, where Sm(w) is the
set of support vectors .
We denote by W⋆the set of first-order stationary points.
Let wt be the iterates of gradient flow (gradient descent with step-
size tending to zero). Define ℓit = exp(−fi(wt)) and ℓt be the vector
with entries ℓi(t). The following two assumptions assume that the
limiting direction
wt
∥wt∥exist and the limiting direction of the losses
ℓt
∥ℓt∥1 exist. Such assumptions are natural in the context of max-
margin problems, since we want to argue that wt converges to a
max-margin direction, and also the losses ℓt/∥ℓt∥1 converges to an
indicator vector of the support vectors. We will directly assume these
limits exist, though this is proved in 6.
6
Assumption 8.4.2 (Smoothness). We assume fi(w) is a C2 function.
Assumption 8.4.3 (Asymptotic Formulas). Assume that L(wt) →0,
that is we converge to a global minimizer. Further assume that lim
t→∞
wt
∥wt∥2
and lim
t→∞
ℓt
∥ℓt∥1 exist. Equivalently,
ℓnt = htan + htϵnt
(8.36)
wt = gt ¯w + gtδt,
(8.37)
with ∥a∥1 = 1, ∥¯w∥2 = 1, lim
t→∞h(t) = 0, lim
t→∞ϵnt = 0, and lim
t→∞δtt = 0.
Assumption 8.4.4 (Linear Independence Constraint Qualification).
Let w be a unit vector. LICQ holds at w if the vectors {∇fi(w)}i∈Sm(w) are
linearly independent.
Constraint qualification allow the first-order optimality conditions
of Definition 8.4.1 to be a necessary condition for optimality. Without
constraint qualifications, event he global optimum may not satisfy the
optimality conditions.
For example in linear SVM, LICQ is ensured if the support vectors
xi are linearly independent then LICQ holds. For data sampled from
an absolutely continuous distribution, the linear SVM solution will
always have linearly independent support vectors.

algorithmic regularization
101
Theorem 8.4.5. Define ¯w = limt→∞
wt
∥wt∥. Under Assumptions 8.4.2,
8.4.3, and 8.4.4, ¯w ∈W is a first-order stationary point of (Max-Margin).
Proof. Define S = {i : fi( ¯w) = γ}, where γ is the optimal margin
attainable by a unit norm w.
Lemma 8.4.6. Under the setting of Theorem 8.4.5,
∇fi(wt) = ∇fi(gt ¯w) + O(Bgα−1
t
∥δt∥).
(8.38)
For i ∈S , the second term is asymptotically negligible as a function of t,
∇fi(wt) = ∇fi(gt ¯w) + o(∇fi(gt ¯w)).
Lemma 8.4.7. Under the conditions of Theorem 8.4.5, ai = 0 for i ̸∈S.
From the gradient flow dynamics,
˙w(t) = ∑
i
exp(−fi(wt))∇fi(wt)
= ∑
i
(htai + htϵit)(∇fi(gt ¯w) + ∆it,
where ∆i(t) = R s=1
s=0 ∇2 fi(gt ¯w + sgtδt)gtδtds. By expanding and using
ai = 0 for n ̸∈S (Lemma 8.4.7) ,
˙wt = ∑
i∈S
htai∇fi(gtw)
|
{z
}
I
+ ht ∑
i∈S
ai∆it
|
{z
}
II
+ ht ∑
i
ϵit∇fi(gt ¯w)
|
{z
}
III
+∑
i
htϵit∆it
|
{z
}
IV
Via Assumption 8.4.4, term I = Ω(gα−1
t
ht) and from Lemma 8.4.6
, II = o(I). Using these, the first term I is the largest and so after
normalizing,
˙wt
∥˙wt∥= ∑
i∈S
ai∇fi(gt ¯w) + o(1).
(8.39)
Since limt
wt
∥wt∥= limt
˙wt
∥wt∥[GLSS18a], then
lim
t→∞
wt
∥wt∥= ∑
i∈S
∇fi(gt ¯w).
(8.40)
Thus we have shown w satisfies the first-order optimality condition
of Definition 8.4.1.
8.5
Induced bias in function space
≪Suriya notes: Jason: can you introduce the idea of induced biases and give special results for
linear convnets, any relevant results from yours+tengyu’s margin paper, and infinite width 2 layer
ReLU network?≫


9
Ultra-wide Neural Networks and Neural Tangent Ker-
nels
This chapter concerns a model that seems ridiculous at first sight:
one with infinitely many nodes at inner layers. To understand why
it is actually interesting, let’s recall the mysteries we’re trying to
understand.
Training a neural network is a non-convex optimization problem,
and in the worst case, it is NP-hard [BR89]. On the other hand, em-
pirically, simple gradient algorithms like stochastic gradient descent
can often achieve zero training loss, i.e., the simple algorithm can
find a neural network that fits all training data. Furthermore, one can
still observe this phenomenon for nonsensical data, when the original
labels are replaced with random labels [ZBH+16b].
Key role of overparametrization.
The fact that networks can fit non-
sensical data perfectly is not surprising because the nets are very
over-parameterized. For example, Wide ResNet when trained on
ImageNet has 100x more parameters than the number of training dat-
apoints. Recall from Chapter 3 that under such conditions even linear
regression (solved via gradient descent) can perfectly fit training
data. But this does not explain real-life neural nets because we still
need to prove that: (a) the low loss can be attained by a gradient-based
training starting from a random initialization (b) that the trained net
has good generalization when trained on proper data. Many tradi-
tional generalization bounds yield vacuous guarantees, as mentioned
in Chapter 5.
Teacher/Student Nets.
A difficulty that arises while addressing this
research agenda is that clearly at some point the theory should take
properties of the data into account, and real-life data (e.g., images)
have no good description. One route taken by researchers is to as-
sume that the labels for training data were computed via some

104
theory of deep learning
ground truth net sometimes refered to as teacher net. Thus the net
being trained is thought of as a student net and the goal of good gen-
eralization is to be able to produce labels broadly in agreement with
the teacher net.
Given the importance of overparametrization in real life, it is nat-
ural to allow the student net to be much deeper or wider than the
teacher net. 1
1 Indeed in experiments one finds that if
synthetically labeled data is generated
by passing inputs from a distribution
through a teacher net, then teaching
a new net from scratch to mimic the
teacher is much easier in practice if the
new net is allowed to be significantly
bigger.
Infinite nets and NTKs:
Now we explain the model of infinite neu-
ral networks. The idea is to let the width in the inner layers be very
large, essentially going to infinity. For example, imagine a standard
net like AlexNet being allowed to expand its fully connected layers
to have unlimited width and the convolutional layers have convolu-
tional filters with unlimited number of channels. This hugely inflated
version of AlexNet architecture still takes the same input as before
but its training and generalization behavior could potentially change
a lot from the usual version. Researchers studied these architectures
and quickly realized that at least one way of initialization/training
leads to the net turning into a kernel classifer, called Neural Tangent
Kernel (NTK) 2. This chapter is an introduction to infinitely wide nets
2 Arthur Jacot, Franck Gabriel, and
Clément Hongler. Neural tangent
kernel: Convergence and generalization
in neural networks. In Advances in
neural information processing systems,
pages 8571–8580, 2018
and NTKs. We’ll see that behavior of NTKs can be computed via ef-
ficient algorithms for computing the kernel inner product for NTK.
NTKs do exhibit good optimization (i.e. convergence to low train-
ing loss) and reasonable generalization behavior but are not as good
as their finite counterparts. For example the NTK corresponding to
AlexNet generalizes reasonably OK on image data but with far worse
accuracy than AlexNet. We will discuss some practical uses of NTK
for small-data tasks.
9.1
Evolution equation for net parameters
This section derives evolution of nets during training under least
squares loss. It applies to any net, and the simple expression will
play a key role in NTK theory.
We denote by f (w, x) ∈R the output of a neural network where
w ∈RN is all the parameters in the network and x ∈Rd is the input.
Given a training dataset {(xi, yi)}n
i=1 ⊂Rd × R, consider training the
neural network by minimizing the squared loss over training data:
ℓ(w) = 1
2
n
∑
i=1
( f (w, xi) −yi)2 .
For simplicity, in this chapter, we study gradient flow, a.k.a., gradient
decent with infinitesimally small learning rate. In this case, the dy-

ultra-wide neural networks and neural tangent kernels
105
namics can be described by an ordinary differential equation (ODE):
dw(t)
dt
= −∇ℓ(w(t)).
Using this description of the dynamics of the parameters, the next
lemma describes the dynamics of the predictions on training data
points.
Lemma 9.1.1. Let u(t) = ( f (w(t), xi))i∈[n] ∈Rn be the network outputs
on all xi’s at time t, and y = (yi)i∈[n] be the labels. Then u(t) follows the
following evolution, where H(t) is an n × n positive semidefinite matrix
whose (i, j)-th entry is
D ∂f (w(t),xi)
∂w
,
∂f (w(t),xj)
∂w
E
:
du(t)
dt
= −H(t) · (u(t) −y).
(9.1)
Proof of Lemma 9.1.1. The parameters w evolve according to the differ-
ential equation
dw(t)
dt
= −∇ℓ(w(t)) = −
n
∑
i=1
( f (w(t), xi) −yi) ∂f (w(t), xi)
∂w
,
(9.2)
where t ≥0 is a continuous time index. Under Equation (9.2), the
evolution of the network output f (w(t), xi) can be written as
d f (w(t), xi)
dt
= −
n
∑
j=1
( f (w(t), xj) −yj)
∂f (w(t), xi)
∂w
, ∂f (w(t), xj)
∂w

.
(9.3)
Since u(t) = ( f (w(t), xi))i∈[n] ∈Rn is the network outputs on all xi’s
at time t, and y = (yi)i∈[n] is the desired outputs, Equation (9.3) can
be written more compactly as
du(t)
dt
= −H(t) · (u(t) −y),
(9.4)
where H(t) ∈Rn×n is a kernel matrix defined as [H(t)]i,j =
D ∂f (w(t),xi)
∂w
,
∂f (w(t),xj)
∂w
E
(∀i, j ∈[n]).
9.1.1
Behavior in the infinite limit
Recall that we’re interested in the limit of deep net training when the
training set is fixed, the width goes to infinity, and for a suitable scale
of initialization (which depends on the width). Under fairly general
conditions it can be shown that the matrix H(t) remains rough con-
stant during training i.e., roughly equal to H(0). Furthermore, the
matrix H(0), whose definition involves random weights used at ini-
tialization, converges in probability to the Gram Matrix H∗of the

106
theory of deep learning
training dataset (see Chapter 3) with respect to certain kernel, called
the Neural Tangent Kernel. Then Equation (9.1) becomes
du(t)
dt
= −H∗· (u(t) −y).
(9.5)
In other words, least squares kernel regression as described in Chap-
ter 3, but with infinitesimally small learning rate. Recall that the final
classifier is described as
f ∗(x) = (k(x, x1), . . . , k(x, xn)) · (H∗)−1y.
(9.6)
9.2
NTK: Simple 2-layer example
In this section, we develop the theory in context of a simple two-layer
neural network of the following form:
f (a, W, x) =
1
√m
m
∑
r=1
arσ

w⊤
r x

(9.7)
where σ (·) is the activation function. Here we assume | ˙σ (z)| and
|¨σ (z)| are bounded by 1 for all z ∈R and For example, soft-plus
activation function, σ (z) = log (1 + exp(z)), satisfies this assump-
tion. 3 We also assume input x is a unit vector, i.e., ∥x∥2 = 1. The
3 Note rectified linear unit (ReLU)
activation function does not satisfy this
assumption. However, one can use a
specialized analysis of ReLU to show
H(t) ≈H∗[DZPS18].
scaling 1/√m will play an important role in proving H(t) stays close
to the Gram Matrix H∗for a fixed kernel. Throughout the section, to
measure the closeness of two matrices A and B, we use the operator
norm ∥·∥2. We will use the fact that if corresponding entries are close
then so are their spectral properties A, B. This follows from the fact
that the sum of squared differences across coordinates, ∥A −B∥2
F, is
also an upper bound on ∥A −B∥2
2.
We use random initialization wr(0) ∼N(0, I) and ar ∼Unif [{−1, 1}].
For simplicity, we will only optimize the first layer, i.e., W = [w1, . . . , wm].
Note this is still a non-convex optimization problem.
We can first calculate H(0) and show as m →∞, H(0) converges
to a fixed matrix H∗. Note ∂f (a,W,xi)
∂wr
=
1
√m arxiσ′  w⊤
r xi

. Therefore,
each entry of H(0) admits the formula
[H(0)]ij =
m
∑
r=1
∂f (a, W(0), xi)
∂wr(0)
, ∂f (a, W(0), xj)
∂wr(0)

=
m
∑
r=1
 1
√m arxi ˙σ

wr(0)⊤xi

,
1
√m arxjσ′ 
wr(0)⊤xi

=x⊤
i xj · ∑m
r=1 σ′  wr(0)⊤xi

σ′  wr(0)⊤xj

m
Here the last step we used a2
r = 1 for all r = 1, . . . , m because we
initialize ar ∼Unif [{−1, 1}]. Recall every wr(0) is i.i.d. sampled from

ultra-wide neural networks and neural tangent kernels
107
a standard Gaussian distribution. Therefore, one can view [H(0)]ij
as the average of m i.i.d. random variables. If m is large, then by the law
of large number, we know this average is close to the expectation of
the random variable. Here the expectation is the NTK evaluated on xi
and xj:
H∗
ij ≜x⊤
i xj ·
E
w∼N(0,I)
h
σ′ 
w⊤xi

σ′ 
w⊤xj
i
Problem 9.2.1. If the activation σ is ReLU then (noting that it is differen-
tiable everywhere except at one point) then show that
H∗
ij = Ew∼N (0,I)
h
˙σw⊤x ˙σw⊤x′i
=
π −arccos

x⊤x′
∥x∥2∥x′∥2

2π
.
(9.8)
Using Hoeffding inequality and the union bound, one can easily
obtain the following bound characterizing m and the closeness of
H(0) and H∗.
Lemma 9.2.2 (Perturbation on the Initialization, [DZPS19, SY19]). Fix
some ϵ > 0. If m = Ω
 ϵ−2n2 log (n/δ)

, then with probability at least
1 −δ over w1(0), . . . , wm(0), we have
∥H(0) −H∗∥2 ≤ϵ.
Proof of Lemma 9.2.2. We first fixed an entry (i, j). Note
x⊤
i xjσ′ 
wt(0)⊤xi

σ′ 
wr(0)⊤xj
 ≤1.
Applying Hoeffding inequality, we have with probability 1 −δ
n2 ,
|[H(0)]i,j −H∗
i,j| ≤
 2
m log(2n2/δ)
1/2
≤4(log(n/δ)
m
)1/2 ≤ϵ
n.
Next, applying the union bound over all pairs (i, j) ∈[n] × [n], we
have for all (i, j),
[H(0)]i,j −H∗
i,j
 ≤
ϵ
n2 . To establish the operator
norm bound, we simply use the following chain of inequalities
∥H(0) −H∗∥2 ≤∥H(0) −H∗∥F
=

∑
ij
| [H(0)]i,j −H∗
i,j|21/2
≤(n2 · ϵ2
n2 )1/2 = ϵ.
Now we proceed to show during training, H(t) is close to H(0).
Formally, we prove the following lemma.

108
theory of deep learning
Lemma 9.2.3. Assume yi = O(1) for all i = 1, . . . , n. Given t > 0,
suppose that for all 0 ≤τ ≤t, ui(τ) = O(1) for all i = 1, . . . , n. If
m = Ω

n6t2
ϵ2

, we have
∥H(t) −H(0)∥2 ≤ϵ.
Proof of Lemma 9.2.3. The first key idea is to show that every weight
vector only moves little if m is large. To show this, let us calculate the
movement of a single weight vector wr.
∥wr(t) −wr(0)∥2 =

t
Z
0
dwr(τ)
dτ
dτ

2
=

t
Z
0
1
√m
n
∑
i=1
(ui(τ) −yi) arxi ˙σ

wr(τ)⊤xi

dτ

2
≤1
√m
Z 
n
∑
i=1
(ui(τ) −yi) arxi ˙σ

wr(τ)⊤xi

2
dτ
≤1
√m
n
∑
i=1
t
Z
0
ui(τ) −yiarxi ˙σ

wr(τ)⊤xi

2 dτ
≤1
√m
n
∑
i=1
t
Z
0
O(1)dτ
=O
 tn
√m

.
This calculation shows that at any given time t, wr(t) is close to
wr(0), as long as m is large. Next, we show this implies the kernel
matrix H(t) is close H(0). We calculate the difference on a single

ultra-wide neural networks and neural tangent kernels
109
entry.
[H(t)]ij −[H(0)]ij
=

1
m
m
∑
r=1

˙σ

wr(t)⊤xi

˙σ

wr(t)⊤xj

−˙σ

wr(0)⊤xi

˙σ

wr(0)⊤xj

≤1
m
m
∑
r=1
 ˙σ

wr(t)⊤xi
 
˙σ

wr(t)⊤xj

−˙σ

wr(0)⊤xj

+ 1
m
m
∑
r=1
 ˙σ

wr(0)⊤xj
 
˙σ

wr(t)⊤xj

−˙σ

wr(0)⊤xi

≤1
m
m
∑
r=1
max
r
˙σ

wr(t)⊤xi

∥xi∥2 ∥wr(t) −wr(0)∥2

+ 1
m
m
∑
r=1
max
r
˙σ

wr(t)⊤xi

∥xi∥2 ∥wr(t) −wr(0)∥2

= 1
m
m
∑
r=1
O
 tn
√m

=O
 tn
√m

.
Therefore, using the same argument as in Lemma 9.2.2, we have
∥H(t) −H(0)∥2 ≤∑
i,j
[H(t)]ij −[H(0)]ij
 = O
 tn3
√m

.
Plugging our assumption on m, we finish the proof.
Several remarks are in sequel.
Remark 1: The assumption that yi = O(1) is a mild assumption
because in practice most labels are bounded by an absolute constant.
Remark 2: The assumption on ui(τ) = O(1) for all τ ≤t and
m’s dependency on t can be relaxed. This requires a more refined
analysis. See [DZPS19].
Remark 3: One can generalize the proof for multi-layer neural
network. See [ADH+19b] for more details.
Remark 4: While we only prove the continuous time limit, it is
not hard to show with small learning rate (discrete time) gradient
descent, H(t) is close to H∗. See [DZPS19].
9.3
Explaining Optimization and Generalization of Ultra-wide
Neural Networks via NTK
Now we have established the following approximation
du(t)
dt
≈−H∗· (u(t) −y)
(9.9)

110
theory of deep learning
where H∗is the NTK matrix. Now we use this approximation to
analyze the optimization and generalization behavior of ultra-wide
neural networks.
Understanding Optimization
The dynamics of u(t) that follows
du(t)
dt
= −H∗· (u(t) −y)
is actually linear dynamical system. For this dynamics, there is a
standard analysis. We denote the eigenvalue decomposition of H∗as
H∗=
n
∑
i=1
λiviv⊤
i
where λ1 ≥. . . ≥λn ≥0 are eigenvalues and v1, . . . , vn are eigenvec-
tors. With this decomposition, we consider the dynamics of u(t) on
each eigenvector separately. Formally, fixing an eigenvevector vi and
multiplying both side by vi, we obtain
dv⊤
i u(t)
dt
= −v⊤
i H∗· (u(t) −y)
= −λi

v⊤
i (u(t) −y)

.
Observe that the dynamics of v⊤
i u(t) only depends on itself and
λi, so this is actually a one dimensional ODE. Moreover, this ODE
admits an analytical solution
v⊤
i (u(t) −y) = exp (−λit)

v⊤
i (u(0) −y)

.
(9.10)
Now we use Equation (9.10) to explain why we can find a zero train-
ing error solution. We need to assume λi > 0 for all i = 1, . . . , n,
i.e., all eigenvalues of this kernel matrix are strictly positive. One can
prove this under fairly general conditions. See [DZPS19, DLL+18].
Observe that (u(t) −y) is the difference between predictions and
training labels at time t and the algorithm finds a 0 training error
solutions means as t →∞, we have u(t) −y →0. Equation (9.10)
implies that each component of this difference, i.e., v⊤
i (u(t) −y) is
converging to 0 exponentially fast because of the exp(−λit) term.
Furthermore, notice that {v1, . . . , vn} forms an orthonormal ba-
sis of Rn, so (u(t) −y) = ∑n
i=1 v⊤
i (u(t) −y). Since we know each
v⊤
i (ui(t) −y) →0, we can conclude that (u(t) −y) →0 as well.
Equation (9.10) actually gives us more information about the con-
vergence. Note each component v⊤
i (u(t) −y) converges to 0 at a
different rate. The component that corresponds to larger λi converges
to 0 at a faster rate than the one with a smaller λi. For a set of labels,
in order to have faster convergence, we would like the projections

ultra-wide neural networks and neural tangent kernels
111
Figure 9.1: Convergence rate vs.
projections onto eigenvectors of
the kernel matrix.
of y onto the top eigenvectors to be larger.4 Therefore, we obtain the
4 Here we ignore the effect of u(0) for
simplicity. See [ADH+19a] on how to
mitigate the effect on u(0).
following intuitive rule to compare the convergence rates in a qualita-
tive manner (for fixed ∥y∥2):
• For a set of labels y, if they align with top eigenvectors, i.e., (v⊤
i y)
is large for large λi, then gradient descent converges quickly.
• For a set of labels, if the projections on eigenvectors {(v⊤
i y)}n
i=1
are uniform, or labels align with eigenvectors with respect to small
eigenvalues, then gradient descent converges with a slow rate.
We can verify this phenomenon experimentally. In Figure 9.1,
we compare convergence rates of gradient descent between using
original labels, random labels and the worst case labels (normalized
eigenvector of H∗corresponding to λn. We use the neural network
architecture defined in Equation (9.7) with ReLU activation function
and only train the first layer. In the right figure, we plot the eigen-
values of H∗as well as projections of true, random, and worst case
labels on different eigenvectors of H∗. The experiments use gradient
descent on data from two classes of MNIST. The plots demonstrate
that original labels have much better alignment with top eigenvec-
tors, thus enjoying faster convergence.
9.3.1
Understanding Generalization in 2-layer setting
The approximation in Equation (9.9) implies the final prediction func-
tion of ultra-wide neural network is approximately the kernel predic-
tion function defined in Equation (9.6). Therefore, we can just use the
generalization theory for kernels to analyze the generalization behav-
ior of ultra-wide neural networks. For the kernel prediction function
defined in Equation (9.6), we can use Rademacher complexity bound
to derive the following generalization bound for 1-Lipschitz loss
function (which is an upper bound of classification error):
q
2y⊤(H∗)−1 y · tr (H∗)
n
.
(9.11)

112
theory of deep learning
Figure 9.2: Generalization error
vs. complexity measure.
This is a data-dependent complexity measure that upper bounds the
generalization error.
We can check this complexity measure empirically. In Figure 9.2,
we compare the generalization error (ℓ1 loss and classification error)
with this complexity measure. We vary the portion of random labels
in the dataset to see how the generalization error and the complexity
measure change. We use the neural network architecture defined in
Equation (9.7) with ReLU activation function and only train the first
layer. The left figure uses data from two classes of MNIST and the
right figure uses two classes from CIFAR. This complexity measure
almost matches the trend of generalization error as the portion of
random labels increases.
Learning from simple teacher nets.
Now we explain why NTK can
learn some functions that can be expressed as two-layer nets. Since
we know the optimization error goes to 0 for any label, so it is suffi-
cient to study what teacher nets enables the generalization error to be
small. Concretely, we give some examples of two-layer teach nets that
make generalization bound (9.11) goes to 0 as n →∞.
Linear Function: We begin with a simple example. Assume the label
y = β⊤x for some vector β. Then one can show that (9.11) is upper
bounded by O
 ∥β∥2
√n

. Therefore, NTK can learn linear functions
with bounded coefficients.
Two-layer nets with Polynomial Activation: Consider y = ∑k
j=1 αj

β⊤
j x
p
,
i.e., a two-layer net with the activation function being zp with p be-
ing an even number. Similar to the linear function, one can show
that (9.11) is upper bounded by O

p ∑k
j=1|αj|∥βj∥
p
2
√n

. Therefore, we
can argue NTK can learn two-layer polynomial nets with bounded
coefficients.
Cosine activation Beyond polynomials, one can also show NTK can
learn somewhat bizarre function. For example, if y = ∑k
j=1 αj

cos

β⊤
j x

−1

.
then we can bound (9.11) is by O

p ∑k
j=1|αj|∥βj∥2 sinh(∥β∥2
2)
√n

.
All the these examples can be proved by a general technique based

ultra-wide neural networks and neural tangent kernels
113
on Taylor expansion of NTK. See [ADH+19a].
9.4
NTK formula for Multilayer Fully-connected Neural Network
In this section we show case the NTK formulas of fully-connected
neural network. We first define a fully-connected neural net formally.
Let x ∈Rd be the input, and denote g(0)(x) = x and d0 = d for
notational convenience. We define an L-hidden-layer fully-connected
neural network recursively, for h = 1, 2, . . . , L:
f (h)(x) = W(h)g(h−1)(x) ∈Rdh, g(h)(x) =
r cσ
dh
σ

f (h)(x)

∈Rdh
(9.12)
where W(h) ∈Rdh×dh−1 is the weight matrix in the h-th layer (h ∈
[L]), σ : R →R is a coordinate-wise activation function, and cσ =

Ez∼N (0,1)

σz2−1
. The last layer of the neural network is
f (w, x) = f (L+1)(x) = W(L+1) · g(L)(x)
= W(L+1) ·
r cσ
dL
σW(L) ·
r cσ
dL−1
σW(L−1) · · ·
r cσ
d1
σW(1)x,
where W(L+1) ∈R1×dL is the weights in the final layer, and w =

W(1), . . . , W(L+1)
represents all the parameters in the network.
We initialize all the weights to be i.i.d. N (0, 1) random variables5
5 This scaling is key to the NTK be-
havior. Initializing with much smaller
variance leads to very different behav-
ior.
, and consider the limit of large hidden widths: d1, d2, . . . , dL →∞.
The scaling factor √cσ/dh in Equation (9.12) ensures that the norm
of g(h)(x) for each h ∈[L] is approximately preserved at initial-
ization (see [DLL+18]). In particular, for ReLU activation, we have
E
g(h)(x)

2
2

= ∥x∥2
2 (∀h ∈[L]).
Recall from Lemma 9.1.1 that we need to compute the value that
D ∂f (w,x)
∂w
, ∂f (w,x′)
∂w
E
converges to at random initialization in the infinite
width limit. We can write the partial derivative with respect to a
particular weight matrix W(h) in a compact form:
∂f (w, x)
∂W(h)
= b(h)(x) ·

g(h−1)(x)
⊤
,
h = 1, 2, . . . , L + 1,
where
b(h)(x) =



1 ∈R,
h = L + 1,
q cσ
dh D(h)(x)

W(h+1)⊤
b(h+1)(x) ∈Rdh,
h = 1, . . . , L,
(9.13)
D(h)(x) = diag

˙σ

f (h)(x)

∈Rdh×dh,
h = 1, . . . , L.
(9.14)

114
theory of deep learning
Then, for any two inputs x and x′, and any h ∈[L + 1], we can
compute
∂f (w, x)
∂W(h) , ∂f (w, x′)
∂W(h)

=

b(h)(x) ·

g(h−1)(x)
⊤
, b(h)(x′) ·

g(h−1)(x′)
⊤
=
D
g(h−1)(x), g(h−1)(x′)
E
·
D
b(h)(x), b(h)(x′)
E
.
Note the first term
D
g(h−1)(x), g(h−1)(x′)
E
is the covariance be-
tween x and x′ at the h-th layer. When the width goes to infinity,
D
g(h−1)(x), g(h−1)(x′)
E
will converge to a fix number, which we de-
note as Σ(h−1)(x, x′). This covariance admits a recursive formula, for
h ∈[L],
Σ(0)(x, x′) = x⊤x′,
Λ(h)(x, x′) =
 
Σ(h−1)(x, x)
Σ(h−1)(x, x′)
Σ(h−1)(x′, x)
Σ(h−1)(x′, x′)
!
∈R2×2,
Σ(h)(x, x′) = cσE(u,v)∼N(0,Λ(h)) [σ (u) σ (v)] .
(9.15)
Now we proceed to derive this formula. The intuition is that
h
f (h+1)(x)
i
i = ∑dh
j=1
h
W(h+1)i
i,j
h
g(h)(x)
i
j is a centered Gaussian
process conditioned on f (h) (∀i ∈[dh+1]), with covariance
E
hh
f (h+1)(x)
i
i ·
h
f (h+1)(x′)
i
i
 f (h)i
=
D
g(h)(x), g(h)(x′)
E
= cσ
dh
dh
∑
j=1
σ
h
f (h)(x)
i
j

σ
h
f (h)(x′)
i
j

,
(9.16)
which converges to Σ(h)(x, x′) as dh →∞given that each
h
f (h)i
j is
a centered Gaussian process with covariance Σ(h−1). This yields the
inductive definition in Equation (9.15).
Next we deal with the second term
D
b(h)(x), b(h)(x′)
E
. From Equa-
tion (9.13) we get
D
b(h)(x), b(h)(x′)
E
=
r cσ
dh
D(h)(x)

W(h+1)⊤
b(h+1)(x),
r cσ
dh
D(h)(x′)

W(h+1)⊤
b(h+1)(x′)

.
(9.17)
Although W(h+1) and bh+1(x) are dependent, the Gaussian ini-
tialization of W(h+1) allows us to replace W(h+1) with a fresh new

ultra-wide neural networks and neural tangent kernels
115
sample eW(h+1) without changing its limit: (See [? ] for the precise
proof.)
r cσ
dh
D(h)(x)

W(h+1)⊤
b(h+1)(x),
r cσ
dh
D(h)(x′)

W(h+1)⊤
b(h+1)(x′)

≈
r cσ
dh
D(h)(x)

eW(h+1)⊤
b(h+1)(x),
r cσ
dh
D(h)(x′)

eW(h+1)⊤
b(h+1)(x′)

→cσ
dh
trD(h)(x)D(h)(x′)
D
b(h+1)(x), b(h+1)(x′)
E
→˙Σ(h)  x, x′ D
b(h+1)(x), b(h+1)(x′)
E
.
Applying this approximation inductively in Equation (9.17), we get
D
b(h)(x), b(h)(x′)
E
→
L
∏
h′=h
˙Σ(h′)(x, x′).
Finally, since
D ∂f (w,x)
∂w
, ∂f (w,x′)
∂w
E
= ∑L+1
h=1
D ∂f (w,x)
∂W(h) , ∂f (w,x′)
∂W(h)
E
, we ob-
tain the final NTK expression for the fully-connected neural network:
Θ(L)(x, x′) =
L+1
∑
h=1
 
Σ(h−1)(x, x′) ·
L+1
∏
h′=h
˙Σ(h′)(x, x′)
!
.
9.5
NTK in Practice
Up to now we have shown an ultra-wide neural network with certain
initialization scheme and trained by gradient flow correspond to a
kernel with a particular kernel function. A natural question is: why
don’t we use this kernel classifier directly?
A recent line of work showed that NTKs can be empirically useful,
especially on small to medium scale datasets. Arora et al. [ADL+19]
tested the NTK classifier on 90 small to medium scale datasets from
UCI database. 6 They found NTK can beat neural networks, other
6 https://archive.ics.uci.edu/ml/datasets.php
kernels like Gaussian and the best previous classifier, random forest
under various metrics, including average rank, average accuracy, etc.
This suggests the NTK classifier should belong in any list of off-the-
shelf machine learning methods.
For every neural network architecture, one can derive a corre-
sponding kernel function. Du et al. [DHS+19] derived graph NTK
(GNTK) for graph classification tasks. On various social network
and bioinformatics datasets, GNTK can outperform graph neural
networks.
Similarly, Arora et al. [? ] derived convolutional NTK (CNTK) for-
mula that corresponds to convolutional neural networks. For image
classification task, in small-scale data and low-shot settings, CNTKs
can be quite strong [ADL+19]. However, for large scale data, Arora

116
theory of deep learning
et al. [? ] found there is still a performance gap between CNTK and
CNN. It is an open problem to explain this phenomenon theoretically.
This may need to go beyond the NTK framework.
9.6
Exercises
1. NTK formula for ReLU activation function: prove
Ew∼N (0,I)
h
( ˙σ(w⊤x) ˙σ(w⊤x′)
i
=
π −arccos

x⊤x′
∥x∥2∥x′∥2

2π
.
2. Prove Equation (9.11). (Hint: The generalization bound depends
upon the norm of the difference between the final W matrix and
the initial W, which you can upper bound using a sum/integral
similar to the analysis of kernel regression in Chapter 3.)
3. Why NTK can learn a linear function: in this question, you are
asked to prove that NTK can learn a linear function (the technique
here can be used to show NTK can learn other functions listed in
Section 9.3.1). In this question, we assume we have n data points,
{(xi, yi)}n
i=1 where every input xi has norm 1 and yi = β⊤xi for
some β. Each entry of neural tangent kernel matrix H∗(induced
by the two-layer ReLU neural network) is H∗
ij =
π−arccos

x⊤x′
∥x∥2∥x′∥2

2π
.
(a) Taylor Expansion: use the Taylor expansion of arccos(z) =
π
2 −z −∑∞
ℓ=1
(2ℓ−3)!!z
(2ℓ−2)!! to show that H∗admits the following
form
H∗= K
4 +
∞
∑
ℓ=1
1
2π
(2ℓ−3)!!
(2ℓ−2)!! · K◦2ℓ
2ℓ−1
where Kij = x⊤
i xj and Kℓ
ij =
 x⊤
i xj
ℓ.
(b) Assume H∗and K are invertible. Show (H∗)−1 ⪯4K−1.
(c) Show tr (H∗) ≤n.
(d) Using the assumption yi = x⊤
i β to show
q
2y⊤(H∗)−1y · tr(H∗)
n
≤2
√
2 ∥β∥2
√n
.

10
Interpreting output of Deep Nets: Credit Attribution
This chapter considers methods that try to understand: Why did
the model give the answer it did? The basic notions are quite old, but
proper and efficient application to deep learning settings is fairly
recent. Mathematically, what is needed is to do credit attribution for
the final decision to various components of the system, including
training data.
We consider two types of explanations. Influence functions try to
understand how individual data points affect the model’s answers
on test data points. Saliency methods try to understand the model’s
answer on a test data point in terms of the contents of that same data
point, typically in the form of a heat map (also called saliency maps)
depicting the importance of individual coordinates. An elegant idea
that often arises in these settings is Shapley values.
10.1
Influence Functions
For a fixed training dataset S, the influence function captures how
adding or removing a datapoint x from the training set S affects the
answer (or the loss) on a test datapoint z. Cook and Weisberg’s text 1
1 R D Cook and S Weisberg. Residuals
and influence in regression. 1982
is the standard reference on the topic. The naive way to compute
the influence function is leave-one-out retraining: for every x, recom-
pute the model on S \ {x}. But it is also possible to do a more direct
computation using the model θ∗trained on S, which uses a more
continuous notion of influence.
Let ℓ() be a twice-differentiable loss function with ℓ(x, θ) denoting
the () loss of model parameters θ on datapoint x. For succinctness
we let ℓ(S, θ) denote the average loss on a set S of data points. The
formal definition of influence I(x, z) involves2 the thought experi-
2 Better notation might be IS(x, z), to
clarify dependence on S.
ment of modifying the weighting of a training point x from 1/|S| to
1/|S| + ϵ. For a fixed S let θ∗be a minimizer of ℓ(S, θ) and θ∗
x,ϵ be the
minimizer after the perturbation.

118
theory of deep learning
Definition 10.1.1. I(x, z) = ∂
∂ϵℓ(z, θ∗
x,ϵ)|ϵ=0.
Influence functions were invented for convex models such as least-
squares linear regression, and thus the theory assumes θ∗satisfies
∇θ(S, θ∗) = 0 and ∇2
θ(S, θ∗) is positive semi-definite. 3
3 In a deep learning setting one hopes
to get to a stationary point, i.e., ∇θ(S, θ∗)
is zero (or more-realistically, near-zero),
which in addition is a local optimum,
i.e., ∇2
θ(S, θ∗) is positive semi-definite.
In practice neither holds exactly, but
∇2
θ(S, θ∗) usually does not have large
negative eigenvalues. So the influence
function computation in practice uses
(Hθ∗+ λI)−1 for some small λ > 0.
Theorem 10.1.2. I(x, z) = −∇θ(ℓ(z, θ∗))TH−1
θ∗∇θℓ(x, θ∗).
Proof. By optimality, ℓ(S, θ∗+ ∆θ) ≈ℓ(S, θ∗) + 1
2(∆θ)THθ∗(∆θ) for
small perturbations ∆θ. Changing the weight on datapoint x from
1/{S} to 1/{S} + ϵ and re-optimizing gives θ∗
x,ϵ = θ∗+ ∆θ where ∆θ
is a minimizer of
ϵℓ(x, θ∗+ ∆θ) + 1
2(∆θ)THθ∗(∆θ).
Since ℓ(x, θ∗+ ∆θ) ≈ℓ(x, θ∗) + ∇ℓ(x, θ∗) · ∆θ what we have is a
quadratic expression and it is minimized by ∆θ = −ϵH−1
θ∗∇θ(ℓ(x, θ∗)).
Since a change in parameters from θ∗to θ∗+ ∆θ causes the loss on a
test point z to change by (∆θ) × ∇θ(ℓ(z, θ∗)), the theorem now fol-
lows.
10.1.1
Computing Influence Functions
At first sight, computing influence functions appears difficult due to
the inverse Hessian computation, which naively has cubic complexity
in the number of parameters. Koh and Liang 4 designed much faster
4 P W Koh and P Liang. Understanding
black-box predictions via influence
functions. In Proc. ICML, 2017
methods. The key idea is a simple identify in the following question.
Problem 10.1.3. If A is any positive definite matrix with full rank and
maximum eigenvalue less than 1 then show that5 A−1 = ∑∞
i=0(I −A)i.
5 Hint: Note that A is is diaganalizable.
How do eigenvectors and eigenvalues
of Ai relate to those of A?
Agarwal et al.6 noted how to use this identity for fast (but approx-
6 Agarwal N, Bullins B, and Hazan E.
Second-order stochastic optimization
for machine learning in linear time.
2017
imate) Hessian-vector computations.
Problem 10.1.4. If Sr denotes the truncation of the series to its first r
terms, then show that λmax(A−1 −Sr) ≤??.
Theorem 10.1.2 shows that we need to compute H−1v for some
vector v, but Problem 10.1.4 allows us to approximate it as ∑r
i=0(I −
H)iv for some reasonably small r. Since (I −H)v = v −Hv we
see that it suffices to do r Hessian-vector computations, each which
takes computation time linear in the size of the deep net. (see Sec-
tion 4.4.1).
10.2
Shapley Values
Shapley Values 7 is a concept from cooperative game theory dealing
7 Lloyd Shapley. "Notes on the n-Person
Game – II: The Value of an n-Person
Game". RAND Corporation
with the following setting. There is a population of N players (de-
noted [N] for brevity) who are willing to cooperate towards a certain

interpreting output of deep nets: credit attribution
119
goal. A utility function U stipulates the reward/utility for each sub-
set of players: If a subset S of the players end up cooperating, they
receive utility U(S) receive as a group. If all N of them end up co-
operating, what is the appropriate and fair way to split the utility
U([N]) among them? Under some reasonable conditions, there turn
out to be unique payments s1, s2, . . . , sN, called Shapley values such
that ∑i si = U([N]) (Theorem 10.2.3).
In ML the following two settings are representative of use of Shap-
ley values: (a) Pricing of datapoints: “players”could be individuals
holding data that could be useful for training an ML model, and the
Shapley values can be seen as payments for use of their data. (b)
When we try to understand the output of a deep net on a single (test)
datapoint in terms of the contributions (aka saliency) of individual
coordinates towards the deep net’s decision.
Shapley value of the ith player is defined using the thought exper-
iment of the players adding themselves to the coalition in a random
order, and looking at the expected increase in utility when the ith
player joins the coalition.
Definition 10.2.1. Shapley value of player i, denoted si, is defined as the
following expected value, where π is a random permutation of {1, 2, . . . , N}
and π<i is shorthand for the subset of players that appear before i in the
permutation:
si = Eπ [U(π≤i) −U(π<i)] .
(10.1)
The definition of si’s is sort of natural, though one may quibble
about the slightly artificial assumption of the players joining the
coalition in a random order. Here is an equivalent definition that
avoids the random order.
Problem 10.2.2. Show that the following definition of Shapley value is
equivalent:
si
= ∑S⊆[N]\{i}
{S}!(N−{S}−1)!)
N!
(U(S ∪{i}) −U(S))
= 1
N ∑S⊆[N]\{i}
U(S∪{i})−U(S)
(N−1
|S| )
.
Since (N−1
k ) is the number of subsets of size k in a set of size N −1,
Problem 10.2.2 in effect redefines Shapley value si as the expectation
of the following random process: randomly pick an integer k from
[0, N −1], then a random subset S of size k but not containing i, and
measure the change in utility upon adding i to S.
While this seems more natural, it is still a good idea to understand
whether we are missing out on some radically different definition of
how to distribute credit. Let’s try to formalize natural properties for
any method of credit attribution. The following axioms seem natural
for any system of defining si’s.

120
theory of deep learning
Efficiency: Sum of the players’ values is U([N]).
Symmetry: If U(S ∪{i}) = U(S ∪{j}) for all S not containing i, j then
their payments are the same. 8
8 Sometimes this is described as “pay-
ments should depend upon their
contribution, not on their names.”
Linearity: If U1, U2 are any two utility functions then the payments
for U1 + U2 are the sum of the payments for U1 and the payments
for U2.
Null Player: If U(S ∪{i}) = U(S) for all S not containing i, then the
payment for i is zero.
You can quickly convince yourself that Shapley values satisfy the
axioms.
Theorem 10.2.3. The payment scheme in Definition 10.2.1 is the only one
that satisfies the previous axioms.
Problem 10.2.4. Prove Theorem 10.2.3. 9
9 Hint: Take the difference of the two
payment schemes.
10.2.1
Algorithms to approximate Shapley values
Given a succinct description of the utility function U (e.g., as a cir-
cuit) it is NP-hard in general to compute Shapley values 10 meaning
10 X Deng and C Papadimitriou. On
the complexity of cooperative solution
concepts. Math of Operations Research,
1994
(assuming P ̸= NP) that the running time is going to increase faster
than any polynomial of N and the description of U. However, it is
possible to compute them approximately if the utilities are bounded.
Specifically, we assume an upper bound of R on the absolute value of
U(S ∪{i}) −U(S) for all S, i.
Naive approximation: Pick O(R2N log N/ϵ2) random permutations
and use them to estimate the expectation in (10.1). (Note that the
number of computations of U(·) is O(R2N2 log N/ϵ2). This is the
computational cost.) Then concentration bounds imply that the esti-
mate bsi of the expectation lies within [si −ϵ/
√
N, si + ϵ/
√
N]. Which
implies that the vector of all Shapley values is estimated within ℓ2
norm ϵ, namely, ∥s −bs∥2 ≤ϵ.
Better approximation: We give a method that uses O(R2N log N)
evaluations of U(·). It uses the following fact about Shapley values.
Theorem 10.2.5. Differences of Shapley values satisfy the following:
si −sj =
1
N −1
∑
S⊆[N]\{i,j}
U(S ∪{i}) −U(S ∪{j})
(N−2
|S| )
.
Problem 10.2.6. Prove Theorem 10.2.5 from Problem 10.2.2.
Now we can describe the Method: (1) Use naive approximation
to approximate s1 within additive error ϵ/2
√
N. (2) Use the charac-
terization in Theorem 10.2.5 to sample sets S suitably to estimate all
differences of type s1 −sj within error ϵ/2
√
N.

interpreting output of deep nets: credit attribution
121
Problem 10.2.7. Figure out how to do step (2). (Hint: You pick S with a
certain probability p(|S|). This uses the observation that the same S can be
used to estimate s1 −sj for many j’s.)
10.3
Data Models
This is a method that also assigns credit to individual training dat-
apoints, but uses a linear regression approach 11. By training many
11 Andrew Ilyas, Sung Min Park, Logan
Engstrom, Guillaume Leclerc, and
Aleksander Madry. Datamodels:
Predicting predictions from training
data. arXiv preprint arXiv:2202.00622,
2022
models on subsets of p fraction of datapoints in the training set,
the authors show that some interesting measures of test error (de-
fined using logit values) behave as follows: the measure f (x) is well-
approximable as a (sparse) linear expression θ0 + ∑i θixi, where x is a
binary vector denoting a sample of p fraction of training datapoints,
with xi = 1 indicating presence of i-th training point and xi = −1 de-
noting absence. The coefficients θi are estimated via lasso regression.
The surprise here is that f (x) —which is the result of deep learning
on dataset x—is well-approximated by θ0 + ∑i θixi. The authors note
that the θi’s can be viewed as heuristic estimates for the discrete in-
fluence of the ith datapoint. Note that the estimated θi depends on
the value of p in the above procedure.
Why do data models work? The reason has to do with the fact that
the models are being trained on a random subset of p fraction of the
training set. One can understand it using interesting but elementary
harmonic analysis 12.
12 Mark Braverman Sanjeev Aror
Nikunj Saunshi, Arushi Gupta. Un-
derstanding influence functions and
data models via harmonic analysis.
ICLR, 2023
10.4
Saliency Maps
Saliency methods try to understand the model’s answer on a test data
point in terms of the contents of that same data point, typically in
the form of a heat map (also called saliency maps) depicting the im-
portance of individual coordinates. For example, if the deep net is
labeling images by their labels, then a saliency map for an image that
has been labeled “dog” might involve highlighting the pixels that
were determinative for assigning this label.
to be written. Shapley values can be used to define the "con-
tribution" of each coordinate in the test datapoint to the
final output. There are other methods.
Also gradient-based methods.


11
Inductive Biases due to Algorithmic Regularization
Many successful modern machine learning systems based on deep
neural networks are over-parametrized, i.e., the number of param-
eters is typically much larger than the sample size. In other words,
there exist (infinitely) many (approximate) minimizers of the empir-
ical risk, many of which would not generalize well on the unseen
data. For learning to succeed then, it is crucial to bias the learning
algorithm towards “simpler” hypotheses by trading off empirical loss
with a certain complexity term that ensures that empirical and pop-
ulation risks are close. Several explicit regularization strategies have
been used in practice to help these systems generalize, including ℓ1
and ℓ2 regularization of the parameters [NH92].
Besides explicit regularization techniques, practitioners have used
a spectrum of algorithmic approaches to improve the generalization
ability of over-parametrized models. This includes early stopping
of back-propagation [CLG01], batch normalization [IS15b], dropout
[SHK+14], and more1. While these heuristics have enjoyed tremen-
1 We refer the reader to [KGC17] for an
excellent exposition of over 50 of such
proposals.
dous success in training deep networks, a theoretical understanding
of how these heuristics provide regularization in deep learning re-
mains somewhat limited.
In this chapter, we investigate regularization due to Dropout, an
algorithmic heurisitic recently proposed by [SHK+14]. The basic
idea when training a neural network using dropout, is that during
a forward pass, we randomly drop neurons in the neural network,
independently and identically according to a Bernoulli distribution.
Specifically, at each round of the back-propagation algorithm, for
each neuron, independently, with probability p we “drop” the neu-
ron, so it does not participate in making a prediction for the given
data point, and with probability 1 −p we retain that neuron 2.
2 The parameter p is treated as a hyper-
parameter which we typically tune for
based on a validation set.
Deep learning is a field where key innovations have been driven
by practitioners, with several techniques motivated by drawing in-
sights from other fields. For instance, Dropout was introduced as
a way of breaking up “co-adaptation” among neurons, drawing

124
theory of deep learning
insights from the success of the sexual reproduction model in the
evolution of advanced organisms. Another motivation that was cited
by [SHK+14] was in terms of “balancing networks”. Despite several
theoretical works aimed at explaining Dropout 3, it remains unclear
3
what kind of regularization does Dropout provide or what kinds
of networks does Dropout prefer and how that helps with general-
ization. In this chapter, we work towards that goal by instantiating
explicit forms of regularizers due to Dropout and how they provide
capacity control in various machine learning including linear regres-
sion (Section 11.4), matrix sensing (Section 11.1.1), matrix completion
(Section 11.1.2), and deep learning (Section 11.2).
11.1
Matrix Sensing
We begin with understanding dropout for matrix sensing, a problem
which arguably is an important instance of a matrix learning problem
with lots of applications, and is well understood from a theoretical
perspective. Here is the problem setup.
Let M∗∈Rd2×d0 be a matrix with rank r∗:= Rank(M∗). Let
A(1), . . . , A(n) be a set of measurement matrices of the same size as
M∗. The goal of matrix sensing is to recover the matrix M∗from n
observations of the form yi = ⟨M∗, A(i)⟩such that n ≪d2d0. The
learning algorithm we consider is empirical risk minimization, and
we choose to represent the parameter matrix M ∈Rd2×d0 in terms of
product of its factors U ∈Rd2×d1, V ∈Rd0×d1:
min
U∈Rd2×d1,V∈Rd0×d1
bL(U, V) := 1
n
n
∑
i=1
(yi −⟨UV⊤, A(i)⟩)2.
(11.1)
When d1 ≫r∗, there exist many “bad” empirical minimizers, i.e.,
those with a large true risk. However, recently, [LMZ18] showed that
under restricted isometry property, despite the existence of such poor
ERM solutions, gradient descent with proper initialization is implicitly
biased towards finding solutions with minimum nuclear norm – this
is an important result which was first conjectured and empirically
verified by [GWB+17].
We propose solving the ERM problem (11.1) with algorithmic reg-
ularization due to dropout, where at training time, the corresponding
columns of U and V are dropped independently and identically ac-
cording to a Bernoulli random variable. As opposed to the implicit
effect of gradient descent, this dropout heuristic explicitly regularizes
the empirical objective. It is then natural to ask, in the case of matrix
sensing, if dropout also biases the ERM towards certain low norm
solutions. To answer this question, we begin with the observation
that dropout can be viewed as an instance of SGD on the following

inductive biases due to algorithmic regularization
125
objective:
bLdrop(U, V) = 1
n
n
∑
i=1
EB(yi −⟨UBV⊤, A(i)⟩)2,
(11.2)
where B ∈Rd1×d1 is a diagonal matrix whose diagonal elements are
Bernoulli random variables distributed as Bjj ∼
1
1−pBer(1 −p), for
j ∈[d1]. In this case, we can show that for any p ∈[0, 1):
bLdrop(U, V) = bL(U, V) +
p
1 −p
bR(U, V),
(11.3)
where
bR(U, V) =
d1
∑
j=1
1
n
n
∑
i=1
(u⊤
j A(i)vj)2
(11.4)
is a data-dependent term that captures the explicit regularizer due to
dropout.
Proof. Consider one of the summands in the Dropout objective in
Equation 11.2. Then, we can write
EB[(yi −⟨UBV⊤, A(i)⟩)2]
=

EB[yi −⟨UBV⊤, A(i)⟩]
2
+ Var(yi −⟨UBV⊤, A(i)⟩)(11.5)
For Bernoulli random variable Bjj, we have that E[Bjj] = 1 and
Var(Bjj) =
p
1−p. Thus, the first term on right hand side is equal to
(yi −⟨UV⊤, A(i)⟩)2. For the second term we have
Var(yi −⟨UBV⊤, A(i)⟩) = Var(⟨UBV⊤, A(i)⟩)
= Var(⟨B, U⊤A(i)V⟩)
= Var(
d1
∑
j=1
Bjju⊤
j A(i)vj)
=
d1
∑
j=1
(u⊤
j A(i)vj)2 Var(Bjj)
=
p
1 −p
d1
∑
j=1
(u⊤
j A(i)vj)2
Using the facts above in Equation (11.2), we get
bLdrop(U, V) = 1
n
n
∑
i=1
(yi −⟨UV⊤, A(i)⟩)2 + 1
n
n
∑
i=1
p
1 −p
d1
∑
j=1
(u⊤
j A(i)vj)2
= bL(U, V) +
p
1 −p
bR(U, V).
which completes the proof.

126
theory of deep learning
Provided that the sample size n is large enough, the explicit regu-
larizer on a given sample behaves much like its expected value with
respect to the underlying data distribution 4. Further, given that we
4 Under mild assumptions, we can for-
mally show that the dropout regularizer
is well concentrated around its mean
seek a minimum of bLdrop, it suffices to consider the factors with the
minimal value of the regularizer among all that yield the same em-
pirical loss. This motivates studying the the following distribution-
dependent induced regularizer:
Θ(M) :=
min
UV⊤=M
R(U, V), where R(U, V) := EA[bR(U, V)].
Next, we consider two two important examples of random sensing
matrices.
11.1.1
Gaussian Sensing Matrices
We assume that the entries of the sensing matrices are independently
and identically distributed as standard Gaussian, i.e., A(i)
kℓ∼N (0, 1).
For Gaussian sensing matrices, we show that the induced regularizer
due to Dropout provides nuclear-norm regularization. Formally, we
show that
Θ(M) = 1
d1
∥M∥2
∗.
(11.6)
Proof. We recall the general form for the dropout regularizer for the
matrix sensing problem in Equation 11.4, and take expectation with
respect to the distribution on the sensing matrices. Then, for any pair
of factors (U, V), it holds that the expected regularizer is given as
follows.
R(U, V) =
d1
∑
j=1
E(u⊤
j Avj)2
=
d1
∑
j=1
E(
d2
∑
k=1
d0
∑
ℓ=1
UkjAkℓVℓj)2
=
d1
∑
j=1
d2
∑
k,k′=1
d0
∑
ℓ,ℓ′=1
UkjUk′jVℓjVℓ′j E[AkℓAk′ℓ′]
=
d1
∑
j=1
d2
∑
k=1
d0
∑
ℓ=1
U2
kjV2
ℓj E[A2
kℓ]
=
d1
∑
j=1
d2
∑
k=1
d0
∑
ℓ=1
U2
kjV2
ℓj
=
d1
∑
j=1
∥uj∥2∥vj∥2

inductive biases due to algorithmic regularization
127
Now, using the Cauchy-Schwartz inequality, we can bound the ex-
pected regularizer as
R(U, V) ≥1
d1
 d1
∑
i=1
∥ui∥∥vi∥
!2
= 1
d1
 d1
∑
i=1
∥uiv⊤
i ∥∗
!2
≥1
d1
 
∥
d1
∑
i=1
uiv⊤
i ∥∗
!2
= 1
d1
∥UV⊤∥2
∗
where the equality follows because for any pair of vectors a, b, it
holds that ∥ab⊤∥∗= ∥ab⊤∥F = ∥a∥∥b∥, and the last inequality is due
to triangle inequality.
Next, we need the following key result from [MAV18].
Theorem 11.1.1. For any pair of matrices U ∈Rd2×d1, V ∈Rd0×d1,
there exists a rotation matrix Q ∈SO(d1) such that matrices eU :=
UQ, eV := VQ satisfy ∥eui∥∥evi∥= 1
d1 ∥UV⊤∥∗, for all i ∈[d1].
Using Theorem 11.1.1 on (U, V), the expected dropout regularizer
at UQ, VQ is given as
R(UQ, VQ) =
d1
∑
i=1
∥Uqi∥2∥Vqi∥2
=
d1
∑
i=1
1
d2
1
∥UV⊤∥2
∗
= 1
d1
∥UV⊤∥2
∗
≤Θ(UV⊤)
which completes the proof.
For completeness we provide a proof of Theorem 11.1.1.
Proof. Define M := UV⊤. Let M = WΣY⊤be compact SVD of M.
Define bU := WΣ1/2 and bV := YΣ1/2. Let GU = bU
⊤bU and GV = bV
⊤bV
be respective Gram matrices. Observe that GU = GV = Σ. We will
show that there exists a rotation Q such that for eU = bUQ, eV = bVQ, it
holds that
∥euj∥2 = 1
d1
∥eU∥2
F = 1
d1
Tr(eU
⊤eU) = 1
d1
Tr(Σ) = 1
d1
∥M∥∗
and
∥evj∥2 = 1
d1
∥eV∥2
F = 1
d1
Tr(eV
⊤eV) = 1
d1
Tr(Σ) = 1
d1
∥M∥∗

128
theory of deep learning
Consequently, it holds that ∥eui∥∥evi∥= 1
d1 ∥M∥∗.
All that remains is to give a construction of matrix Q. We note
that a rotation matrix Q satisfies the desired properties above if and
only if all diagonal elements of Q⊤GUQ are equal5, and equal to
5 since (Q⊤GUQ)jj = ∥euj∥2
Tr GU
d1 . The key idea is that for the trace zero matrix G1 := GU −
Tr GU
d1 Id1, if G1 = ∑r
i=1 λieie⊤
i is an eigendecomposition of G1, then
for the average of the eigenvectors, i.e. for w11 =
1
√r ∑r
i=1 ei, it holds
that w⊤
11G1w11 = 0. We use this property recursively to exhibit
an orthogonal transformation Q, such that Q⊤G1Q is zero on its
diagonal.
To verify the claim, first notice that w11 is unit norm
∥w11∥2 = ∥1
√r
r
∑
i=1
ei∥2 = 1
r
r
∑
i=1
∥ei∥2 = 1.
Further, it is easy to see that
w⊤
11Gw11 = 1
r
r
∑
i,j=1
eiGej = 1
r
r
∑
i,j=1
λje⊤
i ej = 1
r
r
∑
i=1
λi = 0.
Complete W1 := [w11, w12, · · · , w1d] be such that W⊤
1 W1 = W1W⊤
1 =
Id. Observe that W⊤
1 G1W1 has zero on its first diagonal elements
W⊤
1 G1W1 =
"
0
b⊤
1
b1
G2
#
The principal submatrix G2 also has a zero trace. With a similar ar-
gument, let w22 ∈Rd−1 be such that ∥w22∥= 1 and w⊤
22G2w22 = 0
and define W2 =
"
1
0
0
· · ·
0
0
w22
w23
· · ·
w2d
#
∈Rd×d such that
W⊤
2 W2 = W2W⊤
2 = Id, and observe that
(W1W2)⊤G1(W1W2) =


0
·
· · ·
·
0
· · ·
...
...
G3

.
This procedure can be applied recursively so that for the matrix
Q = W1W2 · · · Wd we have
Q⊤G1Q =


0
·
· · ·
·
·
0
· · ·
·
...
...
...
...
·
·
·
0


,
so that Tr(eUeU
⊤) = Tr(Q⊤GUQ) = Tr(Σ) = Tr(Q⊤GVQ) = Tr(eV
⊤eV).

inductive biases due to algorithmic regularization
129
11.1.2
Matrix Completion
Next, we consider the problem of matrix completion which can be
formulated as a special case of matrix sensing with sensing matrices
that random indicator matrices. Formally, we assume that for all
j ∈[n], let A(j) be an indicator matrix whose (i, k)-th element is
selected randomly with probability p(i)q(k), where p(i) and q(k)
denote the probability of choosing the i-th row and the j-th column,
respectively.
We will show next that in this setup Dropout induces the weighted
trace-norm studied by [SS10] and [FSSS11]. Formally, we show that
Θ(M) = 1
d1
∥diag(√p)UV⊤diag(√q)∥2
∗.
(11.7)
Proof. For any pair of factors (U, V) it holds that
R(U, V) =
d1
∑
j=1
E(u⊤
j Avj)2
=
d1
∑
j=1
d2
∑
k=1
d0
∑
ℓ=1
p(k)q(ℓ)(u⊤
j eke⊤
ℓvj)2
=
d1
∑
j=1
d2
∑
k=1
d0
∑
ℓ=1
p(k)q(ℓ)U(k, j)2V(ℓ, j)2
=
d1
∑
j=1
∥
q
diag(p)uj∥2∥
q
diag(q)vj∥2
≥1
d1
 d1
∑
j=1
∥
q
diag(p)uj∥∥
q
diag(q)vj∥
!2
= 1
d1
 d1
∑
j=1
∥
q
diag(p)ujv⊤
j
q
diag(q)∥∗
!2
≥1
d1
 
∥
q
diag(p)
d1
∑
j=1
ujv⊤
j
q
diag(q)∥∗
!2
= 1
d1
∥
q
diag(p)UV⊤q
diag(q)∥2
∗
where the first inequality is due to Cauchy-Schwartz and the second
inequality follows from the triangle inequality. The equality right
after the first inequality follows from the fact that for any two vectors
a, b, ∥ab⊤∥∗= ∥ab⊤∥F = ∥a∥∥b∥. Since the inequalities hold for any
U, V, it implies that
Θ(UV⊤) ≥1
d1
∥
q
diag(p)UV⊤q
diag(q)∥2
∗.
Applying Theorem 11.1.1 on (
p
diag(p)U,
p
diag(q)V), there

130
theory of deep learning
exists a rotation matrix Q such that
∥
q
diag(p)Uqj∥∥
q
diag(q)Vqj∥= 1
d1
∥
q
diag(p)UV⊤q
diag(q)∥∗.
We evaluate the expected dropout regularizer at UQ, VQ:
R(UQ, VQ) =
d1
∑
j=1
∥
q
diag(p)Uqj∥2∥
q
diag(q)Vqj∥2
=
d1
∑
j=1
1
d2
1
∥
q
diag(p)UV⊤q
diag(q)∥2
∗
= 1
d1
∥
q
diag(p)UV⊤q
diag(q)∥2
∗
≤Θ(UV⊤)
which completes the proof.
The results above are interesting because they connect Dropout, an
algorithmic heuristic in deep learning, to strong complexity measures
that are empirically effective as well as theoretically well understood.
To illustrate, here we give a generalization bound for matrix comple-
tion with dropout in terms of the value of the explicit regularizer at
the minimum of the empirical problem.
Theorem 11.1.2. Without loss of generality, assume that d2 ≥d0
and ∥M∗∥≤1. Furthermore, assume that mini,j p(i)q(j) ≥
log(d2)
n√d2d0 .
Let (U, V) be a minimizer of the dropout ERM objective in equa-
tion (11.2), and assume that maxi ∥U(i, :)∥2 ≤γ, maxi ∥V(i, :)∥2 ≤γ.
Let α be such that bR(U, V) ≤α/d1. Then, for any δ ∈(0, 1), the fol-
lowing generalization bounds holds with probability at least 1 −2δ
over a sample of size n:
L(U, V) ≤bL(U, V) + C(1 + γ)
r
αd2 log(d2)
n
+ C′(1 + γ2)
r
log(2/δ)
2n
as long as n = Ω
 (d1γ2/α)2 log(2/δ)

, where C, C′ are some absolute
constants.
The proof of Theorem 11.1.2 follows from standard generaliza-
tion bounds for ℓ2 loss [MRT18] based on the Rademacher com-
plexity [BM02] of the class of functions with weighted trace-norm
bounded by √α, i.e. Mα := {M : ∥diag(√p)Mdiag(√q)∥2∗≤α}.
A bound on the Rademacher complexity of this class was established
by [FSSS11]. The technicalities here include showing that the explicit
regularizer is well concentrated around its expected value, as well
as deriving a bound on the supremum of the predictions. A few
remarks are in order.

inductive biases due to algorithmic regularization
131
We require that the sampling distributions be non-degenerate, as
specified by the condition mini,j p(i)q(j) ≥log(d2)
n√d2d0 . This is a natural
requirement for bounding the Rademacher complexity of Mα, as
discussed in [FSSS11].
We note that for large enough sample size, bR(U, V) ≈R(U, V) ≈
Θ(UV⊤) =
1
d1 ∥diag(√p)UV⊤diag(√q)∥2∗, where the second ap-
proximation is due the fact that the pair (U, V) is a minimizer. That
is, compared to the weighted trace-norm, the value of the explicit
regularizer at the minimizer roughly scales as 1/d1. Hence the as-
sumption bR(U, V) ≤α/d1 in the statement of the corollary.
In practice, for models that are trained with dropout, the training
error bL(U, V) is negligible. Moreover, given that the sample size is
large enough, the third term can be made arbitrarily small. Having
said that, the second term, which is eO(γ√αd2/n), dominates the
right hand side of generalization error bound in Theorem 11.1.2.
The assumption maxi ∥U(i, :)∥2 ≤γ, maxi ∥V(i, :)∥2 ≤γ is
motivated by the practice of deep learning; such max-norm constraints
are typically used with dropout, where the norm of the vector of
incoming weights at each hidden unit is constrained to be bound by
a constant [SHK+14]. In this case, if a dropout update violates this
constraint, the weights of the hidden unit are projected back to the
constraint norm ball. In proofs, we need this assumption to give a
concentration bound for the empirical explicit regularizer, as well as
bound the supremum deviation between the predictions and the true
values. We remark that the value of γ also determines the complexity
of the function class. On one hand, the generalization gap explicitly
depends on and increases with γ. However, when γ is large, the
constraints on U, V are milder, so that bL(U, V) can be made smaller.
Finally, the required sample size heavily depends on the value
of the explicit regularizer at the optima (α/d1), and hence, on the
dropout rate p. In particular, increasing the dropout rate increases
the regularization parameter λ :=
p
1−p, thereby intensifies the penalty
due to the explicit regularizer. Intuitively, a larger dropout rate p
results in a smaller α, thereby a tighter generalization gap can be
guaranteed. We show through experiments that that is indeed the
case in practice.
11.2
Deep neural networks
Next, we focus on neural networks with multiple hidden layers.
Let X ⊆Rd0 and Y ⊆Rdk denote the input and output spaces,
respectively. Let D denote the joint probability distribution on X × Y.
Given n examples {(xi, yi)}n
i=1 ∼Dn drawn i.i.d. from the joint
distribution and a loss function ℓ: Y × Y →R, the goal of learning

132
theory of deep learning
is to find a hypothesis fw : X →Y, parameterized by w, that has a
small population risk L(w) := ED[ℓ( fw(x), y)].
We focus on the squared ℓ2 loss, i.e., ℓ(y, y′) = ∥y −y′∥2, and
study the generalization properties of the dropout algorithm for min-
imizing the empirical risk bL(w) := 1
n ∑n
i=1[∥yi −fw(xi)∥2]. We consider
the hypothesis class associated with feed-forward neural networks
with k layers, i.e., functions of the form fw(x) = Wkσ(Wk−1σ(· · · W2σ(W1x) · · · )),
where Wi ∈Rdi×di−1, for i ∈[k], is the weight matrix at i-th layer. The
parameter w is the collection of weight matrices {Wk, Wk−1, . . . , W1}
and σ : R →R is an activation function applied entrywise to an
input vector.
In modern machine learning systems, rather than talk about a cer-
tain network topology, we should think in terms of layer topology
where each layer could have different characteristics – for example,
fully connected, locally connected, or convolutional. In convolutional
neural networks, it is a common practice to apply dropout only to the
fully connected layers and not to the convolutional layers. Further-
more, in deep regression, it has been observed that applying dropout
to only one of the hidden layers is most effective [LMAPH19]. In our
study, dropout is applied on top of the learned representations or
features, i.e. the output of the top hidden layer. In this case, dropout
updates can be viewed as stochastic gradient descent iterates on the
dropout objective:
bLdrop(w) := 1
n
n
∑
i=1
EB ∥yi −WkBσ(Wk−1σ(· · · W2σ(W1xi) · · · ))∥2
(11.8)
where B is a diagonal random matrix with diagonal elements dis-
tributed identically and independently as Bii ∼
1
1−pBern(1 −p), i ∈
[dk−1], for some dropout rate p. We seek to understand the explicit
regularizer due to dropout:
bR(w) := bLdrop(w) −bL(w)
(explicit regularizer)
We denote the output of the i-th hidden node in the j-th hidden
layer on an input vector x by ai,j(x) ∈R; for example, a1,2(x) =
σ(W2(1, :)⊤σ(W1x)). Similarly, the vector aj(x) ∈Rdj denotes the
activation of the j-th layer on input x. Using this notation, we can
conveniently rewrite the Dropout objective (see Equation 11.8) as
bLdrop(w) := 1
n ∑n
i=1 EB ∥yi −WkBak−1(xi)∥2. It is then easy to show
that the explicit regularizer due to dropout is given as follows.
Proposition 11.2.1 (Dropout regularizer in deep regression).
bLdrop(w) = bL(w) + bR(w), where bR(w) = λ
dk−1
∑
j=1
∥Wk(:, j)∥2ba2
j .

inductive biases due to algorithmic regularization
133
where baj =
q
1
n ∑n
i=1 aj,k−1(xi)2 and λ =
p
1−p is the regularization
parameter.
Proof. Recall that E[Bii] = 1 and Var(Bii) =
p
1−p. Conditioned on x, y
in the current mini-batch, we have that
EB[∥y −WkBak−1(x)∥2] =
dk
∑
i=1
EB(yi −Wk(i, :)⊤Bak−1(x))2.
(11.9)
The following holds for each of the summands above:
EB(yi −Wk(i, :)⊤Bak−1(x))2
=

EB[yi −Wk(i, :)⊤Bak−1(x)]
2
+ Var(yi −Wk(i, :)⊤Bak−1(x)).
Since E[B] = I, the first term on right hand side is equal to (yi −Wk(:
, i)⊤ak−1(x))2. For the second term we have
Var(yi −Wk(i, :)⊤Bak−1(x)) = Var(Wk(i, :)⊤Bak−1(x))
= Var(
dk−1
∑
j=1
Wk(i, j)Bjjaj,k−1(x))
=
dk−1
∑
j=1
(Wk(i, j)aj,k−1(x))2 Var(Bjj)
=
p
1 −p
dk−1
∑
j=1
Wk(i, j)2aj,k−1(x)2
Plugging the above into Equation (11.9)
EB[∥y −WkBak−1(x)∥2]
=
∥y −Wkak−1(x)∥2
+
p
1 −p
dk−1
∑
j=1
∥Wk(:, j)∥2aj,k−1(x)2
Now taking the empirical average with respect to x, y, we get
bLdrop(w) = bL(w) +
p
1 −p
dk−1
∑
j=1
∥Wk(:, j)∥2ba2
j = bL(w) + bR(w)
which completes the proof.
The explicit regularizer bR(w) is the summation over hidden nodes,
of the product of the squared norm of the outgoing weights with
the empirical second moment of the output of the corresponding
neuron. For a two layer neural network with ReLU, when the input
distribution is symmetric and isotropic, the expected regularizer is
equal to the squared ℓ2 path-norm of the network [NTS15b]. Such
a connection has been previously established for deep linear net-
works [MAV18, MA19]; here we extend that result to single hidden
layer ReLU networks.

134
theory of deep learning
Proposition 11.2.2. Consider a two layer neural network fw(·) with
ReLU activation functions in the hidden layer. Furthermore, as-
sume that the marginal input distribution PX (x) is symmetric and
isotropic, i.e., PX (x) = PX (−x) and E[xx⊤] = I. Then the expected
explicit regularizer due to dropout is given as
R(w) := E[bR(w)] = λ
2
d0,d1,d2
∑
i0,i1,i2=1
W2(i2, i1)2W1(i1, i0)2,
(11.10)
Proof of Proposition 11.2.2. Using Proposition 11.2.1, we have that:
R(w) = E[bR(w)] = λ
d1
∑
j=1
∥W2(:, j)∥2 E[σ(W1(j, :)⊤x)2]
It remains to calculate the quantity Ex[σ(W1(j, :)⊤x)2]. By symmetry
assumption, we have that PX (x) = PX (−x). As a result, for any
v ∈Rd0, we have that P(v⊤x) = P(−v⊤x) as well. That is, the
random variable zj := W1(j, :)⊤x is also symmetric about the origin.
It is easy to see that Ez[σ(z)2] = 1
2 Ez[z2].
Ez [σ(z)2] =
Z ∞
−∞σ(z)2dµ(z)
=
Z ∞
0
σ(z)2dµ(z) =
Z ∞
0
z2dµ(z)
= 1
2
Z ∞
−∞z2dµ(z) = 1
2 Ez [z2].
Plugging back the above identity in the expression of R(w), we get
that
R(w) = λ
2
d1
∑
j=1
∥W2(:, j)∥2 E[(W1(j, :)⊤x)2] = λ
2
d1
∑
j=1
∥W2(:, j)∥2∥W1(j, :)∥2
where the second equality follows from the assumption that the
distribution is isotropic.
11.3
Landscape of the Optimization Problem
While the focus in Section 11.2 was on understanding the implicit
bias of dropout in terms of the global optima of the resulting regu-
larized learning problem, here we focus on computational aspects
of dropout as an optimization procedure. Since dropout is a first-
order method and the landscape of the Dropout objective (e.g., Prob-
lem 11.11) is highly non-convex, we can perhaps only hope to find a
local minimum, that too provided if the problem has no degenerate
saddle points [LSJR16, GHJY15b]. Therefore, in this section, we pose
the following questions: What is the implicit bias of dropout in terms of

inductive biases due to algorithmic regularization
135
local minima? Do local minima share anything with global minima struc-
turally or in terms of the objective? Can dropout find a local optimum?
For the sake of simplicity of analysis, we focus on the case of sin-
gle hidden layer linear autoencoders with tied weights, i.e. U = V.
We assume that the input distribution is isotropic, i.e. Cx = I. In this
case, the population risk reduces to
E[∥y −UU⊤x∥2]
=
Tr (Cy) −2⟨Cyx, UU⊤⟩+ ∥UU⊤∥2
F
=
∥M −UU⊤∥2
F + Tr (Cy) −∥Cyx∥2
F
where M = Cyx+Cxy
2
. Ignoring the terms that are independent of the
weights matrix U, the goal is to minimize L(U) = ∥M −UU⊤∥2
F.
Using Dropout amounts to solving the following problem:
min
U∈Rd0×d1
Lθ(U) := ∥M −UU⊤∥2
F + λ
d1
∑
i=1
∥ui∥4
|
{z
}
R(U)
(11.11)
We can characterize the global optima of the problem above as fol-
lows.
Theorem 11.3.1. For any j ∈[r], let κj :=
1
j ∑
j
i=1 λi(Cyx). Fur-
thermore, define ρ := max{j ∈[r] :
λj(Cyx) >
λjκj
r+λj}. Then, if
U∗is a global optimum of Problem 11.11, it satisfies that U∗U⊤
∗=
S λρκρ
r+λρ
 Cyx

.
Next, it is easy to see that the gradient of the objective of Prob-
lem 11.11 is given by
∇Lθ(U) = 4(UU⊤−M)U + 4λUdiag(U⊤U).
We also make the following important observation about the critical
points of Problem 11.11. Lemma 11.3.2 allows us to bound different
norms of the critical points, as will be seen later in the proofs.
Lemma 11.3.2. If U is a critical point of Problem 11.11, then it holds
that UU⊤⪯M.
Proof of Lemma 11.3.2. Since ∇Lθ(U) = 0, we have that
(M −UU⊤)U = λUdiag(U⊤U)
multiply both sides from right by U⊤and rearrange to get
MUU⊤= UU⊤UU⊤+ λUdiag(U⊤U)U⊤
(11.12)
Note that the right hand side is symmetric, which implies that the
left hand side must be symmetric as well, i.e.
MUU⊤= (MUU⊤)⊤= UU⊤M,

136
theory of deep learning
so that M and UU⊤commute. Note that in Equation (11.12), Udiag(U⊤U)U⊤⪰
0. Thus, MUU⊤⪰UU⊤UU⊤. Let UU⊤= WΓW⊤be a compact
eigendecomposition of UU⊤. We get
MUU⊤= MWΓW⊤⪰UU⊤UU⊤= WΓ2W⊤.
Multiplying from right and left by WΓ−1 and W⊤respectively, we get
that W⊤MW ⪰Γ which completes the proof.
We show in Section 11.3.1 that (a) local minima of Problem 11.11
inherit the same implicit bias as the global optima, i.e. all local min-
ima are equalized. Then, in Section 11.3.2, we show that for suffi-
ciently small regularization parameter, (b) there are no spurious local
minima, i.e. all local minima are global, and (c) all saddle points are
non-degenerate (see Definition 11.3.4).
11.3.1
Implicit bias in local optima
Recall that the population risk L(U) is rotation invariant, i.e. L(UQ) =
L(U) for any rotation matrix Q. Now, if the weight matrix U were not
equalized, then there exist indices i, j ∈[r] such that ∥ui∥> ∥uj∥.
We show that it is easy to design a rotation matrix (equal to identity
everywhere expect for columns i and j) that moves mass from ui to uj
such that the difference in the norms of the corresponding columns
of UQ decreases strictly while leaving the norms of other columns
invariant. In other words, this rotation strictly reduces the regularizer
and hence the objective. Formally, this implies the following result.
Lemma 11.3.3. All local minima of Problem 11.11 are equalized, i.e. if
U is a local optimum, then ∥ui∥= ∥uj∥∀i, j ∈[r].
Lemma 11.3.3 unveils a fundamental property of dropout. As soon
as we perform dropout in the hidden layer – no matter how small the
dropout rate – all local minima become equalized. We illustrate this
using a toy problem in Figure 11.1.
Proof of Lemma 11.3.3. We show that if U is not equalized, then any
ϵ-neighborhood of U contains a point with dropout objective strictly
smaller than Lθ(U). More formally, for any ϵ > 0, we exhibit a ro-
tation Qϵ such that ∥U −UQϵ∥F ≤ϵ and Lθ(UQϵ) < Lθ(U). Let
U be a critical point of Problem 11.11 that is not equalized, i.e. there
exists two columns of U with different norms. Without loss of gener-
ality, let ∥u1∥> ∥u2∥. We design a rotation matrix Q such that it is
almost an isometry, but it moves mass from u1 to u2. Consequently,
the new factor becomes “less un-equalized” and achieves a smaller

inductive biases due to algorithmic regularization
137
λ = 0
λ = 0.6
λ = 2
Figure 11.1: Optimization land-
scape (top) and contour plot
(bottom) for a single hidden-
layer linear autoencoder net-
work with one dimensional
input and output and a hid-
den layer of width r
=
2 with
dropout, for different values of
the regularization parameter
λ. Left: for λ = 0 the problem
reduces to squared loss min-
imization, which is rotation
invariant as suggested by the
level sets. Middle: for λ > 0 the
global optima shrink toward
the origin. All local minima are
global, and are equalized, i.e.
the weights are parallel to the
vector (±1, ±1). Right: as λ
increases, global optima shrink
further.
regularizer, while preserving the value of the loss. To that end, define
Qδ :=


√
1 −δ2
−δ
0
δ
√
1 −δ2
0
0
0
Ir−2


and let bU := UQδ. It is easy to verify that Qϵ is indeed a rotation.
First, we show that for any ϵ, as long as δ2 ≤
ϵ2
2 Tr(M), we have bU ∈
Bϵ(U):
∥U −bU∥2
F =
r
∑
i=1
∥ui −bui∥2
= ∥u1 −
p
1 −δ2u1 −δu2 ∥2 + ∥u2 −
p
1 −δ2u2 + δu1 ∥2
= 2(1 −
p
1 −δ2)(∥u1∥2 + ∥u2∥2)
≤2δ2 Tr(M) ≤ϵ2
where the second to last inequality follows from Lemma 11.3.2, be-
cause ∥u1∥2 + ∥u2∥2 ≤∥U∥2
F = Tr(UU⊤) ≤Tr(M), and also the fact
that 1 −
√
1 −δ2 =
1−1+δ2
1+
√
1−δ2 ≤δ2.
Next, we show that for small enough δ, the value of Lθ at bU is
strictly smaller than that of U. Observe that
∥bu1∥2 = (1 −δ2)∥u1∥2 + δ2∥u2∥2 + 2δ
p
1 −δ2u⊤
1 u2
∥bu2∥2 = (1 −δ2)∥u2∥2 + δ2∥u1∥2 −2δ
p
1 −δ2u⊤
1 u2
and the remaining columns will not change, i.e. for i = 3, · · · , r,
bui = ui. Together with the fact that Qδ preserves the norms, i.e.
∥U∥F = ∥UQδ∥F, we get
∥bu1∥2 + ∥bu2∥2 = ∥u1∥2 + ∥u2∥2.
(11.13)

138
theory of deep learning
Let δ = −c · sgn(u⊤
1 u2) for a small enough c > 0 such that ∥u2∥<
∥bu2∥≤∥bu1∥< ∥u1∥. Using Equation (11.13), This implies that
∥bu1∥4 + ∥bu2∥4 < ∥u1∥4 + ∥u2∥4, which in turn gives us R(bU) < R(U)
and hence Lθ(bU) < Lθ(U). Therefore, a non-equalized critical point
cannot be local minimum, hence the first claim of the lemma.
11.3.2
Landscape properties
Next, we characterize the solutions to which dropout converges. We
do so by understanding the optimization landscape of Problem 11.11.
Central to our analysis, is the following notion of strict saddle property.
Definition 11.3.4 (Strict saddle point/property). Let f : U →R be
a twice differentiable function and let U ∈U be a critical point of
f. Then, U is a strict saddle point of f if the Hessian of f at U has at
least one negative eigenvalue, i.e. λmin(∇2 f (U)) < 0. Furthermore, f
satisfies strict saddle property if all saddle points of f are strict saddle.
Strict saddle property ensures that for any critical point U that is
not a local optimum, the Hessian has a significant negative eigen-
value which allows first order methods such as gradient descent (GD)
and stochastic gradient descent (SGD) to escape saddle points and
converge to a local minimum [LSJR16, GHJY15b]. Following this
idea, there has been a flurry of works on studying the landscape of
different machine learning problems, including low rank matrix re-
covery [BNS16b], generalized phase retrieval problem [SQW16b],
matrix completion [GLM16b], deep linear networks [Kaw16], matrix
sensing and robust PCA [GJZ17b] and tensor decomposition [GHJY15b],
making a case for global optimality of first order methods.
For the special case of no regularization (i.e. λ = 0; equivalently,
no dropout), Problem 11.11 reduces to standard squared loss min-
imization which has been shown to have no spurious local minima
and satisfy strict saddle property (see, e.g. [BH89, JGN+17]). How-
ever, the regularizer induced by dropout can potentially introduce
new spurious local minima as well as degenerate saddle points. Our
next result establishes that that is not the case, at least when the
dropout rate is sufficiently small.
Theorem 11.3.5. Let r := Rank(M). Assume that d1 ≤d0 and that
the regularization parameter satisfies λ <
rλr(M)
(∑r
i=1 λi(M))−rλr(M). Then it
holds for Problem 11.11 that
1. all local minima are global,
2. all saddle points are strict saddle points.
A few remarks are in order. First, the assumption d1 ≤d0 is by no
means restrictive, since the network map UU⊤∈Rd0×d0 has rank at

inductive biases due to algorithmic regularization
139
most d0, and letting d1 > d0 does not increase the expressivity of the
function class represented by the network. Second, Theorem 11.3.5
guarantees that any critical point U that is not a global optimum is
a strict saddle point, i.e. ∇2L(U, U) has a negative eigenvalue. This
property allows first order methods, such as dropout, to escape such
saddle points. Third, note that the guarantees in Theorem 11.3.5
hold when the regularization parameter λ is sufficiently small. As-
sumptions of this kind are common in the literature (see, for example
[GJZ17b]). While this is a sufficient condition for the result in Theo-
rem 11.3.5, it is not clear if it is necessary.
Proof of Theorem 11.3.5. Here we outline the main steps in the proof of
Theorem 11.3.5.
1. In Lemma 11.3.3, we show that the set of non-equalized crit-
ical points does not include any local optima. Furthermore,
Lemma 11.3.6 shows that all such points are strict saddles.
2. In Lemma 11.3.7, we give a closed-form characterization of all the
equalized critical points in terms of the eigendecompostion of M.
We then show that if λ is chosen appropriately, all such critical
points that are not global optima, are strict saddle points.
3. It follows from Item 1 and Item 2 that if λ is chosen appropri-
ately, then all critical points that are not global optimum, are strict
saddle points.
Lemma 11.3.6. All critical points of Problem 11.11 that are not equalized,
are strict saddle points.
Proof of Lemma 11.3.6. By Lemma 11.3.3, the set of non-equalized crit-
ical points does not include any local optima. We show that all such
points are strict saddles. Let U be a critical point that is not equal-
ized. To show that U is a strict saddle point, it suffices to show that
the Hessian has a negative eigenvalue. In here, we exhibit a curve
along which the second directional derivative is negative. Assume,
without loss of generality that ∥u1∥> ∥u2∥and consider the curve
∆(t):=[(
p
1−t2−1)u1+tu2, (
p
1−t2−1)u2−tu1, 0d,r−2]
It is easy to check that for any t ∈R, L(U + ∆(t)) = L(U) since U +
∆(t) is essentially a rotation on U and L is invariant under rotations.

140
theory of deep learning
Observe that
g(t) := Lθ(U + ∆(t))
= Lθ(U) + ∥
p
1 −t2u1 + tu2∥4 −∥u1∥4 + ∥
p
1 −t2u2 −tu1∥4 −∥u2∥4
= Lθ(U) −2t2(∥u1∥4 + ∥u2∥4) + 8t2(u1u2)2 + 4t2∥u1∥2∥u2∥2
+ 4t
p
1 −t2u⊤
1 u2(∥u1∥2 −∥u2∥2) + O(t3).
The derivative of g then is given as
g′(t) = −4t(∥u1∥4 + ∥u2∥4) + 16t(u1u2)2 + 8t∥u1∥2∥u2∥2
+ 4(
p
1 −t2 −
t2
√
1 −t2 )(u⊤
1 u2)(∥u1∥2 −∥u2∥2) + O(t2).
Since U is a critical point and Lθ is continuously differentiable, it
should hold that
g′(0) = 4(u⊤
1 u2)(∥u1∥2 −∥u2∥2) = 0.
Since by assumption ∥u1∥2 −∥u2∥2 > 0, it should be the case that
u⊤
1 u2 = 0. We now consider the second order directional derivative:
g′′(0) = −4(∥u1∥4 + ∥u2∥4) + 16(u1u2)2 + 8∥u1∥2∥u2∥2
= −4(∥u1∥2 −∥u2∥2)2 < 0
which completes the proof.
We now focus on the critical points that are equalized, i.e. points U
such that ∇Lθ(U) = 0 and diag(U⊤U) = ∥U∥2
F
d1 I.
Lemma 11.3.7. Let r := Rank(M). Assume that d1 ≤d0 and λ <
rλr
∑r
i=1(λi−λr). Then all equalized local minima are global. All other equalized
critical points are strict saddle points.
Proof of Lemma 11.3.7. Let U be a critical point that is equalized. Fur-
thermore, let r′ be the rank of U, and U = WΣV⊤be its rank-r′ SVD,
i.e. W ∈Rd0×r′, V ∈Rd1×r′ are such that U⊤U = V⊤V = Ir′ and
Σ ∈Rr′×r′, is a positive definite diagonal matrix whose diagonal
entries are sorted in descending order. We have:
∇Lθ(U) = 4(UU⊤−M)U + 4λUdiag(U⊤U) = 0
=⇒UU⊤U + λ∥U∥2
F
d1
U = MU
=⇒WΣ3V⊤+ λ∥Σ∥2
F
d1
WΣV⊤= MWΣV⊤
=⇒Σ2 + λ∥Σ∥2
F
d1
I = W⊤MW

inductive biases due to algorithmic regularization
141
Since the left hand side of the above equality is diagonal, it implies
that W ∈Rd0×r′ corresponds to some r′ eigenvectors of M. Let E ⊆
[d0], |E| = r′ denote the set of eigenvectors of M that are present in
W. The above equality is equivalent of the following system of linear
equations:
(I + λ
d1
11⊤)diag(Σ2) = ⃗λ,
where ⃗λ = diag(W⊤MW). The solution to the linear system of
equations above is given by
diag(Σ2) = (I −
λ
d1 + λr′ )⃗λ = ⃗λ −λ ∑r′
i=1 λi
d1 + λr′ 1r′.
(11.14)
Thus, the set E belongs to one of the following categories:
0. E = [r′], r′ > ρ
1. E = [r′], r′ = ρ
2. E = [r′], r′ < ρ
3. E ̸= [r′]
We provide a case by case analysis for the above partition here.
Case 0. [E = [r′], r′ > ρ]. We show that E cannot belong to this class,
i.e. when E = [r′], it should hold that r′ ≤ρ. To see this, consider the
r′-th linear equation in Equation (11.14):
σ2
r′ = λr′ −λ ∑r′
i=1 λi
d1 + λr′ .
Since Rank U = r′, it follows that σr′ > 0, which in turn implies that
λr′ > λ ∑r′
i=1 λi
d1 + λr′ =
λr′κr′
d1 + λr′ .
It follows from maximality of ρ in Theorem 11.3.1 that r′ ≤ρ.
Case 1. [E = [r′], r′ = ρ] When W corresponds to the top-ρ eigenvec-
tors of M, we retrieve a global optimum described by Theorem 11.3.1.
Therefore, all such critical points are global minima.
Case 2. [E = [r′], r′ < ρ] Let Wd0 := [W, W⊥] be a complete eigen-
basis for M corresponding to eigenvalues of M in descending order,
where W⊥∈Rd0×d0−r′ constitutes a basis for the orthogonal sub-
space of W. For rank deficient M, W⊥contains the null-space of M,
and hence eigenvectors corresponding to zero eigenvalues of M. Sim-
ilarly, let V⊥∈Rd1×d1−r′ span the orthogonal subspace of V, such
that Vd1 := [V, V⊥] forms an orthonormal basis for Rd1. Note that
both W⊥and V⊥are well-defined since r′ ≤min{d0, d1}. Define

142
theory of deep learning
U(t) = Wd0Σ′V⊤
d1 where Σ′ ∈Rd0×d1 is diagonal with non-zero
diagonal elements given as σ′
i =
q
σ2
i + t2 for i ≤d1. Observe that
U(t)⊤U(t) = VΣ2V⊤+ t2V⊤
d1Vd1 = U⊤U + t2Id1.
Thus, the parametric curve U(t) is equalized for all t. The population
risk at U(t) equals:
L(U(t)) =
d1
∑
i=1
(λi −σ2
i −t2)2 +
d0
∑
i=d1+1
λ2
i
= L(U) + d1t4 −2t2
d1
∑
i=1
(λi −σ2
i ).
Furthermore, since U(t) is equalized, we obtain the following form
for the regularizer:
R(U(t)) = λ
d1
∥U(t)∥4
F = λ
d1

∥U∥2
F + d1t22
= R(U) + λd1t4 + 2λt2∥U∥2
F.
Define g(t) := L(U(t)) + R(U(t)). We have that
g(t) = L(U) + R(U) + d1t4 −2t2
d1
∑
i=1
(λi −σ2
i ) + λd1t4 + 2λt2∥U∥2
F.
It is easy to verify that g′(0) = 0. Moreover, the second derivative of g
at t = 0 is given as:
g′′(0) = −4
d1
∑
i=1
(λi −σ2
i ) + 4λ∥U∥2
F = −4
d1
∑
i=1
λi + 4(1 + λ)∥U∥2
F
(11.15)
We use ∥U∥2
F = ∑r′
i=1 σ2
i and Equation (11.14) to arrive at
∥U∥2
F = trΣ2 =
r′
∑
i=1
(λi −
λ ∑r′
j=1 λj
d1 + λr′ ) = (
r′
∑
i=1
λi)(1−
λr′
d1 + λr′ ) = d1 ∑r′
i=1 λi
d1 + λr′
Plugging back the above equality in Equation (11.15), we get
g′′(0) = −4
d1
∑
i=1
λi + 4d1 + d1λ
d1 + λr′
r′
∑
i=1
λi = −4
d1
∑
i=r′+1
λi + 4(d1 −r′)λ
d1 + λr′
r′
∑
i=1
λi
To get a sufficient condition for U to be a strict saddle point, it suf-

inductive biases due to algorithmic regularization
143
fices that g′′(t) be negative at t = 0, i.e.
g′′(0) < 0 =⇒(d1 −r′)λ
d1 + λr′
r′
∑
i=1
λi <
d1
∑
i=r′+1
λi
=⇒λ < (d1 + λr′) ∑r
i=r′+1 λi
(d1 −r′) ∑r′
i=1 λi
=⇒λ(1 −
r′ ∑d1
i=r′+1 λi
(d1 −r′) ∑r′
i=1 λi
) <
d1 ∑d1
i=r′+1 λi
(d1 −r′) ∑r′
i=1 λi
=⇒λ <
d1 ∑d1
i=r′+1 λi
(d1 −r′) ∑r′
i=1 λi −r′ ∑d1
i=r′+1 λi
=⇒λ <
d1h(r′)
∑r′
i=1 (λi −h(r′))
where h(r′) :=
∑
d1
i=r′+1 λi
d1−r′
is the average of the tail eigenvalues λr′+1, . . . , λd1.
It is easy to see that the right hand side is monotonically decreasing
with r′, since h(r′) monotonically decreases with r′. Hence, it suffices
to make sure that λ is smaller than the right hand side for the choice
of r′ = r −1, where r := Rank(M). That is, λ <
rλr
∑r
i=1(λi−λr).
Case 3. [E ̸= [r′]] We show that all such critical points are strict sad-
dle points. Let w′ be one of the top r′ eigenvectors that are missing in
W. Let j ∈E be such that wj is not among the top r′ eigenvectors of
M. For any t ∈[0, 1], let W(t) be identical to W in all the columns but
the jth one, where wj(t) =
√
1 −t2wj + tw′. Note that W(t) is still an
orthogonal matrix for all values of t. Define the parametrized curve
U(t) := W(t)ΣV⊤for t ∈[0, 1] and observe that:
∥U −U(t)∥2
F = σ2
j ∥wj −wj(t)∥2
= 2σ2
j (1 −
p
1 −t2) ≤t2 Tr M
That is, for any ϵ > 0, there exist a t > 0 such that U(t) belongs to
the ϵ-ball around U. We show that Lθ(U(t)) is strictly smaller than
Lθ(U), which means U cannot be a local minimum. Note that this
construction of U(t) guarantees that R(U′) = R(U). In particular, it
is easy to see that U(t)⊤U(t) = U⊤U, so that U(t) remains equalized
for all values of t. Moreover, we have that
Lθ(U(t)) −Lθ(U) = ∥M −U(t)U(t)⊤∥2
F −∥M −UU⊤∥2
F
= −2 Tr(Σ2W(t)⊤MW(t)) + 2 Tr(Σ2W⊤MW)
= −2σ2
j t2(wj(t)⊤Mwj(t) −w⊤
j Mwj) < 0,
where the last inequality follows because by construction wj(t)⊤Mwj(t) >
w⊤
j Mwj. Define g(t) := Lθ(U(t)) = L(U(t)) + R(U(t)). To see that
such saddle points are non-degenerate, it suffices to show g′′(0) < 0.

144
theory of deep learning
It is easy to check that the second directional derivative at the origin
is given by
g′′(0) = −4σ2
j (wj(t)⊤Mwj(t) −w⊤
j Mwj) < 0,
which completes the proof.
11.4
Role of Parametrization
For least squares linear regression (i.e., for k = 1 and u = W⊤
1 ∈Rd0
in Problem 11.8), we can show that using dropout amounts to solving
the following regularized problem:
min
u∈Rd0
1
n
n
∑
i=1
(yi −u⊤xi)2 + λu⊤bCu.
All the minimizers of the above problem are solutions to the fol-
lowing system of linear equations (1 + λ)X⊤Xu = X⊤y, where
X = [x1, · · · , xn]⊤∈Rn×d0, y = [y1, · · · , yn]⊤∈Rn×1 are the de-
sign matrix and the response vector, respectively. Unlike Tikhonov
regularization which yields solutions to the system of linear equa-
tions (X⊤X + λI)u = X⊤y (a useful prior, discards the directions that
account for small variance in data even when they exhibit good dis-
criminability), the dropout regularizer manifests merely as a scaling
of the parameters. This suggests that parametrization plays an impor-
tant role in determining the nature of the resulting regularizer. How-
ever, a similar result was shown for deep linear networks [MA19]
that the data dependent regularization due to dropout results in
merely scaling of the parameters. At the same time, in the case of
matrix sensing we see a richer class of regularizers. One potential
explanation is that in the case of linear networks, we require a con-
volutional structure in the network to yield rich inductive biases. For
instance, matrix sensing can be written as a two layer network in the
following convolutional form:
⟨UV⊤, A⟩= ⟨U⊤, V⊤A⊤⟩= ⟨U⊤, (I ⊗V⊤)A⊤⟩.
11.4.1
Related Work

12
SDE approximation of SGD and its implications
In Chapter 2 the analysis of convergence of gradient descent assumed
that learning rate η is set small enough that the loss decreases at each
iteration. Thus if η is taken to zero, the optimization still works just
as well. When η →0 the trajectory becomes continuous and the
process is called gradient flow (GF), given by the differential equation
dx
dt = −∇f (x)
(Gradient Flow).
If we are trying to understand how the model parameters evolve
during training, gradient flow is the most natural process to analyse,
as we did for instance in Chapter 8. In Chapter 2 we also studied
SGD, which can be more efficient because it can use noisy estimates
of the gradient from random minibatches. But if η →0 in SGD
then the updates again are infinitesimally small and so the stochastic
gradient averages out to true gradient. So as η →0, GD and SGD
both reduce to GF.
But as we have discussed in subsequent chapters, the three al-
gorithms can have very different behavior with respect to general-
ization. In other words, the trajectory matters. And yet in the limit
η →0 they become identical! This chapter presents a way to model
SGD as an process with infinitesimal steps: stochastic differential equa-
tions, or SDEs. These augment differential equations using noise from
a stochastic process. We shall see that they yield so-called scaling
rules for large-batch training; understanding how quickly SGD may
escape saddle points1 and the norm dynamics of normalized net-
1 Wenqing Hu, Chris Junchi Li, Lei Li,
and Jian-Guo Liu. On the diffusion
approximation of nonconvex stochastic
gradient descent. Annals of Mathematical
Sciences and Applications, 4(1), 2019
works2. More recently, these techniques have been applied to study
2 Zhiyuan Li, Kaifeng Lyu, and Sanjeev
Arora. Reconciling modern deep
learning with traditional optimization
analyses: The intrinsic learning rate.
In Hugo Larochelle, Marc’Aurelio
Ranzato, Raia Hadsell, Maria-Florina
Balcan, and Hsuan-Tien Lin, editors,
Advances in Neural Information Processing
Systems 33: Annual Conference on Neural
Information Processing Systems 2020,
NeurIPS 2020, December 6-12, 2020,
virtual, 2020
adaptive optimization algorithms, such as RMSprop and Adam, and
derive scaling rules for them as well.
In this section we use x for the vector of parameters instead of our
usual w, because it is more standard in SDE literature.

146
theory of deep learning
12.1
Understanding gradient noise in SGD
Suppose there are m training points and let fi(x) be the loss func-
tion on example i when x denotes the model parameters. In SGD, we
sample a batch of size B to compute f1, ..., fB and use the batch gradi-
ent ∇f (B)(x) = 1
B ∑B
i=1 ∇fi(x) to update the parameters. 3 Hence, the
3 In practice the training data is ran-
domly partitioned into batches for each
epoch, instead of the statistically correct
method of drawing a fresh random
batch for each gradient estimation.
noise in this estimate is z(B) = ∇f (B)(x) −∇f (x) = 1
B ∑B
i=1 zi. Clearly,
it has mean zero, implying the expectation of the stochastic gradient
is the true gradient ∇= 1
m∇fi(x).
The convergence analysis for SGD in Section 2.5.3 assumed also
that there is a known upper bound on the magnitude of variance. But
as mentioned, in addition to the magnitude of the noise, the nature of
noise appears to be important for good generalization4 and we now
4 For instance, computing the full
gradient and adding uniform Gaussian
noise to it does not have the same
beneficial effect as the noise in SGD.
Bin Shi, Weijie J Su, and Michael I
Jordan.
On learning rates and
schrödinger operators. arXiv preprint
arXiv:2004.06977, 2020; and Stephan
Mandt, Matthew D Hoffman, and
David M Blei. Stochastic gradient
descent as approximate bayesian infer-
ence. The Journal of Machine Learning
Research, 18(1):4873–4907, 2017
try to better understand it. If a vector random variable z has mean
zero, its covariance matrix is the expectation E[zzT]. This is a measure
of the “shape“ of its distribution.
Problem 12.1.1. Show that the covariance matrix of z(B)(x) is
Σ(B)(x) = 1
B E
i (∇fi(x) −∇)(∇fi(x) −∇)T.
The noise z(B) is zero-mean, but its covariance Σ(B)(x) depends
upon the current parameters and scales inversely with B. To high-
light this we use Σ(x) for the covariance with B = 1, and thus
Σ(B)(x) =
1
BΣ(x). We abstract this formula for gradient which im-
plicitly assumes a loss function, and highlights the importance of
both the shape and the scale of the noise.
Definition 12.1.2 (Noisy Gradient Oracle with Scale). A noisy gradient
oracle with scale parameter σ > 0 (NGOS) takes a parameter x and returns
a stochastic gradient g = ∇f (x) + σz, where z is drawn from a mean-zero
distribution Zσ(x) with covariance Σ(x). Zσ(x) can change with σ, but
Σ(x) must remain fixed.
Improvements in multi-GPU parallelism have encouraged prac-
titioners try large batch sizes: if the architecture can handle batch
size B, then training time (keeping number of epochs constant) scales
as 1/B. This is an attractive idea but runs into the trouble that gen-
eralization error rises with B. Clearly, the magnitude of the noise
covariance plays an important role in generalization just as shape
does. The following was used in 5 to raise the effective scale of the
5 Priya Goyal, Piotr Dollár, Ross Gir-
shick, Pieter Noordhuis, Lukasz
Wesolowski, Aapo Kyrola, Andrew
Tulloch, Yangqing Jia, and Kaiming He.
Accurate, large minibatch sgd: Train-
ing imagenet in 1 hour. arXiv preprint
arXiv:1706.02677, 2017
noise 6 and it turned out to preserve generalization error for fairly
6 The linear scaling rule seems specific
to SGD and does not apply to, say,
adaptive gradient methods. It also fails
for extremely large batch sizes, which
has also been studied mathematically.
large batch sizes (although it also fails when the batch size gets too
large). Later we will mathematically justify it.
Definition 12.1.3 (Linear Scaling Rule). When running SGD with batch
size B′ = κB, use learning rate η′ = κη.

sde approximation of sgd and its implications
147
12.1.1
Motivating example: Loss with Fixed Gradient
We now consider a simple case where the gradient is fixed at ¯g for
every time step and the NGOS has isotropic noise: gk ∼N ( ¯g, σ2I).
Then, after k steps of SGD, xk ∼N (−η ¯gk, η2σ2k). If there is a con-
tinuous approximation for the SGD trajectory, then xk must be able
to be approximated by the value of a continuous trajectory at a fixed
time t independent of σ (i.e., regardless of batch size). To prevent the
distribution of xk from changing with σ, we must adjust η and k so
that η ∼1/σ2 and k ∼1/η. The first observation yields the linear
scaling rule, which can be seen by noting that σ ∼1/
√
B. The latter
observation motivates a continuous time-scaling of t ∼kη. We note,
however, that the scaling rule and the continuous time scale can only
be made rigorous by formally showing the quality of a corresponding
SDE formulation, as in Theorem 12.3.4. In order to proceed in a more
rigorous fashion, we must now precisely describe what it means for a
continuous trajectory to approximate a discrete one.
12.2
Stochastic processes: Informal Treatment
Stochastic processes can be thought of as the continuous-time version
of a random walk.
Definition 12.2.1 (Lévy process). A stochastic process W = {Wt : t ≥0}
is a Lévy process if it satisfies the following properties. 7
7 Think of t as time.
1. W0 = 0 almost surely
2. Continuity: For any ϵ > 0 and t ≥0, limh→0 Pr[|Wt+h −Wt| > ϵ] =
0.
3. Stationary increments: For s < t, the distribution of Wt −Ws is equal to
the distribution of Wt−s.
4. Independent increments: for every t > 0, future increments Wt+δ −Wt
for δ ≥0 are independent of past values Ws for s ≤t.
5. Wt is continuous in t.8
8 i.e., trajectory of Wt has no discontinu-
ities.
Definition 12.2.2 (Wiener process). 9 A Wiener process in ℜd is a Lévy
9 Wiener process is also called Brownian
motion, used by Einstein to explain the
observed movement of tiny particles
suspended in a fluid. The overall trajec-
tory results from very tiny movements
in random directions due to collisions
with molecules.
process that has Gaussian increments: Wt+δ −Wt ∼N (0, δId×d) for δ ≥0.
Note that the trajectory defined by a single run of these processes
is not differentiable in the normal sense. Ito Calculus gives a way to
define a derivative of sorts, denoted, dWt, whose integral from time
0 through T is WT. We omit details, since those will not be needed
below.

148
theory of deep learning
Figure 12.1: Two trajectories
generated by two runs of a
one-dimensional Wiener pro-
cess from the same starting
point. They make independent
random moves and quickly
diverge.
A Stochastic Differential Equation has the form
dxt = µ(xt, t)dt + σ(xt, t)dWt,
(SDE)
(12.1)
where dWt denotes a Weiner process and µ() and σ() depend upon
current time as well as location xt. The heuristic interpretation is as
follows:
In a small time interval [t, t + δ] the process moves by a random
amount according to he Gaussian of mean µ(xt, t)δ and covariance
matrix σ(xt, t)2δI.
Example 12.2.3 (Time change). Suppose we change the scale of time, so
that the new time τ is t/a. How does this change the equation? Using the
above intuition, it becomes
dxτ = aµ(xτ, τ)dτ + √
aσ(xτ, τ)dWτ.
The fundamental reason is that the Weiner process (being a geometric ran-
dom walk) only goes a distance proportional to
√
δ in time δ.
12.2.1
SDEs and SGD
The simplest SDE to model for SGD on loss f () is
dxt = −η∇f (xt) + ησdWt
(12.2)
where dWt is the standard Weiner process and η, σ are fixed con-
stants. This is modeling noise in batch gradients as a uniform Gaus-
sian10. A more realistic SDE for the NGOS in Definition 12.1.2 is
10 Bin Shi, Weijie J Su, and Michael I
Jordan.
On learning rates and
schrödinger operators. arXiv preprint
arXiv:2004.06977, 2020; and Stephan
Mandt, Matthew D Hoffman, and
David M Blei. Stochastic gradient
descent as approximate bayesian infer-
ence. The Journal of Machine Learning
Research, 18(1):4873–4907, 2017
dxt = −η∇f (x) + ησΣ(x)1/2dWt
(12.3)
By the reasoning of Example 12.2.3 this is equivalent to

sde approximation of sgd and its implications
149
dxt = −∇f (x) + √ησΣ(x)1/2dWt
(Canonical SDE for SGD)
(12.4)
which is a bit nicer (albeit a bit disconcerting at first sight) because
the learning rate η does not appear before the gradient; only in the
noise term.
Problem 12.2.4. Define the loss function f (x) = x2 + 9, and let f1(x) =
(x −3)2, f2(x) = (x + 3)2, and f3(x) = x2 + 9. Each iteration of SGD
will uniformly sample f1, f2, or f3 and use the corresponding gradient to
compute the next iterate with learning rate η: xk+1 = xk −η∇fi(x),
where i ∼U({1, 2, 3}). Write down the precise SDE approximation for the
trajectory SGD takes in this setting.
Note that we have defined the canonical SDE approximation
heuristically; we have not shown that this continuous approximation
actually tracks the corresponding SGD. Nevertheless, we can now
give an informal justification for LSR, which we later try to make a
bit more rigorous in Theorem 12.3.5.
Informal justification for Linear Scaling Rule:
11Canonical SDE
11 S Jastrz˛ebski, Z Kenton, D Arpit,
N Ballas, A Fischer, Y Bengio, and
A Storkey. Three Factors Influencing
Minima in SGD. ICANN, 2018
captures generalization properties. If we simultaneously scale the batch size
B and learning rate η in SGD by a factor κ then the noise scale σ changes
by 1/√κ and thus the Canonical SDE does not change.
12.3
Notion of closeness between stochastic processes
SGD and the corresponding SDE in (12.4) are both stochastic pro-
cesses, one discrete and the other continuous. Each induces a dis-
tribution over trajectories in the parameter space. We will use k for
discrete time steps and t for continuous time. By our above discus-
sion, if the SDE trajectory evolves for time T then this corresponds to
K = ⌊T/ηe⌋discrete steps in the SGD. 12 Corresponding trajectories
12 Although ηe = η for SGD, we use
ηe instead of η because a different
optimization algorithm may require a
different continuous time scaling (e.g.,
Section 12.4).
involve sequences of parameter vectors, denote {xηe
k }K
k=0 for SGD and
by {X}T for SDE13.
13 The set notation captures that we
are discussing the family of stochastic
trajectories driven by random seeds for
a fixed choice of ηe, but we interchange-
ably discuss the family and a single
trajectory without loss of generality.
What does it mean to say that two stochastic processes are close?
We need formulations of a distance between distributions over the
parameter vectors that they induce (as in Chapters 14 and 16). A
common way to measure closeness is to compare difference in ex-
pectations of certain test functions on the two distributions14. For ex-
14 The discriminator net in Chapter 16 is
an example of a test function, and we
saw there that classes of test functions
can define transportations metrics on
the space of distributions.
ample a natural test function in our context is test error of the trained
net.
Definition 12.3.1 (Distance between Distributions). For a function class
F, define the distance between distributions { eXk} and {xk} as
dF({xk}, { eXk}) = sup
f ∈F
|
E
X∼{ eXk}
f (X) −
E
x∼{xk} f (x)|

150
theory of deep learning
Completely general test functions do not make sense in such con-
texts, because they can magnify a negligible difference in distribu-
tion to a large difference in expectation. To prevent this we restrict
the function class to have at most polynomial growth15. Class G of
15 Test functions relevant to deep learn-
ing, such as generalization error, are
probably not polynomial growth on
the entire space of parameter vectors.
But when we apply the definition to
measure closeness of SDE and SGD, we
are only interested in parameter vectors
occuring on training trajectories, where
the generalization error may be better
behaved.
continuous functions Rd →R has polynomial growth if ∀g ∈G
there exist positive integers κ1, κ2 > 0 such that for all x ∈Rd,
|g(x)| ≤κ1(1 + |x|2κ2). For α ∈N+, we denote by Gα the set of α-
times continuously differentiable functions g where all partial deriva-
tives of the form
∂αg
∂xα1
1 ···∂x
αd
d
s.t. ∑d
i=1 αi = α ≤α, are also in G. We can
extend the standard definition of a distance between the positional
distributions to the distributions of the entire trajectory by taking the
maximum over the positional distances.
Definition 12.3.2 (Trajectory Distance). The trajectory distance over a
finite number of steps K > 0 between a discrete trajectory {x}K and the
corresponding rescaled continuous trajectory { eX}K under a class of test
functions G is
DG({x}K, { eX}K, K) = max
k=0,...,K dG({xk}, { eXk})
We can thus define a notion of weak approximation, which guaran-
tees an upper bound on the maximum distinguishing expectation for
a class of test functions with at most polynomial growth.
Definition 12.3.3 (Order-α Weak Approximation). We say {X}t and
{x}k are order-α weak approximations16 of each other if for every test func-
16 Qianxiao Li, Cheng Tai, and
E Weinan. Stochastic modified
equations and dynamics of stochas-
tic gradient algorithms i: Mathematical
foundations. J. Mach. Learn. Res., 20:40–
1, 2019
tion g ∈G2, there exists a constant C > 0 independent of ηe such that
DG({x}k, { eX}k, T) ≤Cηα
e
12.3.1
Formal Approximation
Now we explain in what sense
Theorem 12.3.4 (SDE is an order-1 weak approximation of SGD).
Assume the NGOS and loss function f satisfy
1. ∇f (x) is Lipschitz and C∞-smooth
2. All partial derivatives of ∇f and Σ1/2 up to and including the 4th order
have polynomial growth
3. Low skewness: there exists a function K(x) of polynomial growth inde-
pendent of σ such that | Ez∼Zσ(x)[z⊗3]| ≤K(x)/σ for all x ∈Rd and
all noise scales σ.

sde approximation of sgd and its implications
151
4. Bounded moments: for all integers m ≥1 and all noise scales σ, there
exists a constant C2m independent of σ such that Ez∼Zσ(x)[∥z∥2m
2 ]
1
2m ≤
C2m(1 + ∥x∥2) for all x ∈Rd.
Let {x}K be a family of discrete SGD trajectories with learning rate η
and {X}T be the corresponding family of SDE trajectories given by Equa-
tion (12.4). Then, {x}K and {X}T are order-1 weak approximations (Defi-
nition 12.3.3) of each other for any T > 0 and with ηe = η.17
17 Qianxiao Li, Cheng Tai, and
E Weinan. Stochastic modified
equations and dynamics of stochas-
tic gradient algorithms i: Mathematical
foundations. J. Mach. Learn. Res., 20:40–
1, 2019
Normalized networks can violate the Lipschitzness condition on
the gradient because the derivatives are unbounded, but if the trajec-
tory is bounded away from the origin and infinity, then the condition
is still satisfied. The low skewness condition requires the NGOS to
have a small third-order moment, and the bounded moments condi-
tion ensures the NGOS is not heavy-tailed. Together, these two con-
ditions allow the stochastic noise to be modeled by a Wiener process.
The above theorem can be extended to show the validity of LSR.
Theorem 12.3.5 (Validity of Linear Scaling Rule). Let {x}(B)
K
be a
family of discrete SGD trajectories with batch size B and learning rate
η and {x}(κB)
⌊K/κ⌋be the family with batch size κB and learning rate κη.
Furthermore, define the time-rescaled discrete trajectory {ex}(κB)
K
where
{exk}(κB) = {x⌊k/κ⌋}(κB). Then, if {x}(B)
K
and {ex}(κB)
K
have the same initial
condition, for any g with at most polynomial growth and any number of
time steps K > 0,
Mg({x}(B)
K , {ex}(κB)
K
, T) = C(1 + κ)η
Proof. The linearity of covariance implies that scaling the batch size
by κ only modifies the NGOS by scaling σ by 1/√κ. Therefore, the
SDE is unchanged when modifying the hyperparameters according
to LSR. The weak approximation of the SDE to SGD is in terms of η,
and since η is scaled by κ in LSR, the same method gives an upper
bound of Cκη. We also account for the case when κ < 1 and hence
get a bound of C(1 + κ)η.
Through the proof mechanism, we see that the linear scaling rule
holds when the SDE approximation does. It is also possible for LSR
to hold when the Itô SDE approximation fails (e.g., when the noise
distribution violates the Gaussian-like assumption18). If (1 + κ) is
18 Zhiyuan Li, Sadhika Malladi, and
Sanjeev Arora. On the validity of mod-
eling sgd with stochastic differential
equations (sdes). Advances in Neural
Information Processing Systems, 34, 2021
treated as a constant, then we recover the same weak approximation
as before. When κ becomes large, the bound becomes loose, and
indeed, in practice, we observe that the linear scaling rule breaks for
very large batches.19
19 Priya Goyal, Piotr Dollár, Ross
Girshick, Pieter Noordhuis, Lukasz
Wesolowski, Aapo Kyrola, Andrew
Tulloch, Yangqing Jia, and Kaiming He.
Accurate, large minibatch sgd: Train-
ing imagenet in 1 hour. arXiv preprint
arXiv:1706.02677, 2017; and Zhiyuan Li,
Sadhika Malladi, and Sanjeev Arora.
On the validity of modeling sgd with
stochastic differential equations (sdes).
Advances in Neural Information Processing
Systems 34 2021

152
theory of deep learning
12.3.2
Proof Sketch
For ease of notation, we drop the set notation used to describe the
positional distributions of the two trajectories and instead use xk and
eXk. We follow two broad steps to show that the weak approximation
in Theorem 12.3.4 holds. First, we divide a T-length time interval
into a series of one-step intervals. We define xk(x, k0) as the value
of the discrete trajectory at time k with initial condition xk0 = x
and do the same for Xt and eXk. Under this notation, eXk(xk, k) = xk
(i.e., SGD after k steps), and eXk(x0, 0) = eXk = Xkηe (i.e., the SDE
after kηe continuous time). We start from the SGD trajectory and
sequentially replace each interval with the SDE trajectory starting
from the corresponding initial condition, as shown in Figure 12.2.
Figure 12.2: Example hybrid
trajectories interpolating be-
tween SGD and SDE.
These hybrid trajectories yield the following error decomposition
for any 1 ≤k ≤K.
| E[g(xk)] −E[g( eXk)]| ≤
k−1
∑
j=0
E[g( eXk(xj+1, j + 1))] −E[g( eXk(xj, j))]

Problem 12.3.6. Show that the above error decomposition holds.
As such, the error over the entire time interval is related to the
sum of the single-step errors.
The second step is to show the single-step error of the approxima-
tion is sufficiently small through Taylor expansion. Define the single-
step movements with initial condition x for the discrete trajectory as
∆(x) = x1 −x and for the continuous trajectory e∆(x) = eX1 −x. The
Taylor expansion of the discrete trajectory is straightforward:
E[g(x + ∆)] = g(x) + ⟨E[∆], ∇g(x)⟩+ E
"*
∆∆⊤
2
, ∇2g(x)
+#
+ . . .

sde approximation of sgd and its implications
153
To Taylor expand g(x + e∆), we introduce a key technical tool from
stochastic calculus, Itô’s Lemma, which is also known as the stochastic
counterpart to the standard chain rule.
Definition 12.3.7 (Itô’s Lemma). For a general Itô SDE dXt = b(Xt)dt +
σ(Xt)dWt, where Wt is a Wiener process, and a twice differentiable function
h : Rd →R,
dh(Xt) = ⟨∇h(Xt), b(Xt)⟩dt + ⟨∇h(Xt), σ(Xt)dWt⟩+ 1
2 Tr[∇2h(Xt)σ⊤(Xt)σ(Xt)]dt
The first two terms are the standard calculus chain rule, and the
last term is a correction term to account for stochasticity. We omit a
complete calculation of the stochastic Taylor expansion, but we note
that the NGOS conditions are exploited in this step to show that the
higher order terms in both Taylor expansions are small.
Now, we can compare the Taylor expansions to show that the
single-step approximation error is O(η2
e ), and since there are T/ηe
intervals, the approximation error over a finite interval of time T is
O(ηe), as desired.
12.4
Stochastic Variance Amplified Gradient (SVAG)
The approximation error bound in Definition 12.3.3 relies on a small
learning rate. However, real-life deep networks are often trained with
larger learning rates, especially when following LSR and using a
large batch size, so it is unclear if the SDE approximation is valid for
practical settings. Directly simulating the SDE (e.g., through a stan-
dard discretization method, like Euler-Maruyama) is computationally
intractable, because it requires computing the gradient covariance
Σ(Xt) and the full gradient ∇f (Xt) repeatedly for fine-grained inter-
vals. In this section, we discuss a computationally efficient simulation
of the SDE: SVAG.20
20 Zhiyuan Li, Sadhika Malladi, and
Sanjeev Arora. On the validity of mod-
eling sgd with stochastic differential
equations (sdes). Advances in Neural
Information Processing Systems, 34, 2021
Definition 12.4.1 (SVAG Algorithm). For a given NGOS g, learning rate
η, and a chosen hyperparameter ℓ> 0, the SVAG algorithm computes the
stochastic gradient as
bg = 1 +
√
2ℓ−1
2
gγ1 + 1 −
√
2ℓ−1
2
gγ2
where gγ indicates a stochastic gradient sampled with γ as the random seed,
and uses learning rate η/ℓwith the same update rule as SGD.
The SVAG algorithm can be implemented by sampling two batches
and computing a weighted average of the losses as above. Because
the learning rate is scaled by 1/ℓ, we must run SVAG for ℓsteps in
order to approximate the SDE value at η continuous time (i.e., the

154
theory of deep learning
SDE value corresponding to a single discrete step of SGD). We can
formally show that the SVAG trajectory is a weak approximation of
the SDE for SGD with ηe = η/ℓ.
Theorem 12.4.2 (SVAG algorithm approximates SDE). Let {X}T be
the SDE for SGD with hyperparameter η, and let {x}K be the analogous
SVAG trajectory with hyperparameter ℓ. Furthermore, define { eX}K such
that { eXk} = {Xkη/ℓ} and set ηe = η/ℓ. Assume conditions 1, 2, and 4
hold from Theorem 12.3.4. Then, for any test function g ∈G4 and finite
time interval T > 0:
DG({x}K, { eX}K, T) ≤Cη/ℓ
Proof. The proof relies on showing that if conditions 1, 2, and 4 hold
from Theorem 12.3.4, then applying the SVAG algorithm results in
an NGOS that satisfies condition 3. With all the conditions satisfied,
one can regard the SVAG algorithm as SGD with a smaller learning
rate and the guarantee that the NGOS satisfies the Gaussian-like and
non-heavy-tailed assumptions on the noise distribution. Hence, we
can directly apply the standard approximation theorem between SGD
and the corresponding SDE (i.e., Theorem 12.3.4) to conclude that
SVAG is an order-1 weak approximation of the SDE for SGD.
We note here that the bound is η/ℓ, so it can be made small for a
fixed η by increasing ℓ. Increasing ℓrequires taking more gradient
steps to simulate the SDE, but it was found that the SVAG trajectory
seems to converge and often match the SDE for computationally
tractable values of ℓ.21
21 Zhiyuan Li, Sadhika Malladi, and
Sanjeev Arora. On the validity of mod-
eling sgd with stochastic differential
equations (sdes). Advances in Neural
Information Processing Systems, 34, 2021
Problem 12.4.3. Let b
Zℓσ(x) be the distribution of
bz = 1
ℓ
 
1 +
√
2ℓ−1
2
z1 + 1 −
√
2ℓ−1
2
z2
!
Let bg be defined as in Definition 12.4.1. Show that bg has the same distribu-
tion as ∇f (x) + ℓσbz.

13
Effect of Normalization in Deep Learning
Around 2014, efforts to build upon new deep learning successes like
AlexNet were stymied by the inability to make nets deeper. Train-
ing was too finicky, often failing to lower the loss very much. Ioffe
and Szegedy 1 introduced Batch Normalization, a form of normalizing
1 S Ioffe and C Szegedy. Batch nor-
malization: Accelerating deep network
training by reducing internal covariate
shift. ICML, 2015
the layer parameters, which they found to make training 10x faster,
and improved generalization as well. Since then other related meth-
ods have been invented, including Layer Normalization 2 and Group
2 J Ba, J R Kiros, and G E Hinton. Layer
normalization. NeurIPS, 2016
Normalization 3. Today most deep architectures utilize some form of
3 Y Wu and K He. Group Normaliza-
tion. ECCV, 2017
normalization.
In this chapter you will quickly note –e.g., Theorem 13.3.1— that
normalization causes modern training to be fairly incompatible with
traditional analyses of optimization that were surveyed in Chapter 2
and other chapters. The chapter introduces new analyses that take
normalization into account. The key new mathematical notion is scale
invariance.
13.1
Warmup Example: How Normalization Helps Optimization
Since deep nets are believed to be very over-parametrized for the
tasks they are being used for, the net can in principle implement
the desired input-output behavior in multiple ways. Let’s con-
sider a simple scenario with 1-dimensional non-separable dataset
{(xi, yi) : i = 1, . . . , n} where xi ∈ℜ, yi ∈{+1, −1}. The standard
logistic loss bℓ(W) would be ∑i log(1 + exp(−Wxiyi)). Suppose we
overparametrize it, allowing k variables (w1, . . . , wk) and
ℓ(w1, . . . , wk) = bl
 
k
∏
i=1
wi
!
= ∑
i
log(1 + exp(−xiyi
k
∏
i=1
wi)).
This is logically equivalent to the standard loss, but not equivalent
with respect to behavior of gradient descent. Whereas GD quickly
optimizes the original loss using a fixed learning rate, here the fol-
lowing happens.

156
theory of deep learning
Problem 13.1.1. Suppose at initialization, all wi’s are the same, ∏i wi =
W0 and k is even. Show that if learning rate η >
2
|∇bl(W0)||W0|1/k−1 and
W0 > W∗where W∗> 0 is the minimizer of bl, then ∏i wi will monotoni-
cally increase (i.e., explode).
This example (also see the less trivial Example 1.2 in 4) illustrates
4 Z Li, S Bhojanapalli, M Zaheer, Reddi
S, and Kumar S. Robust training of
neural networks using scale invariant
architectures. arxiv, 2022
that making nets deep can have the effect of producing big numbers
in the gradient, which can complicate training.
Definition 13.1.2 (Degree of homogeneity). A function f : ℜd →ℜd′
has degree of homogeneity k if for all c > 0 and all x, f (c · x) = ck f (x).
We also call such functions k-homogeneous.
Problem 13.1.3. Show that if any mapping from parameters to outputs
with inputs fixed, f : ℜd →ℜd′, is computed by a feed-forward net with
depth k and only ReLU gates with zero bias 5 then it has degree of homo-
5 In other words, ReLU(z) = max{0, z}.
geneity k.
In a k-homogeneous network for a high k, small changes in param-
eters can lead to large swings in gradient and Hessian. Normaliza-
tion schemes reduce this effect.
13.2
Normalization schemes and scale invariance
Normalization can be done in many ways, and the following variant
is possibly the easiest to understand.
Layer normalization: Let ai denote the ith coordinate of the input
of some layer in a usual feed-forward deep net (possibly with con-
volutions) with a fixed training datapoint. Layer normalization
will change this architecture by first computing µ =
1
H ∑i ai and
σ2 =
1
H ∑i(ai −µ)2. The ith coordinate of the output of Layer Nor-
malization layer is defined as
LayerNorm(a)i = γi · ai −µ
σ
+ βi,
where γi, βi are learnable parameters associated with this Layer Nor-
malization layer.
Group normalization is a generalization of Layer Normalization
where the statistics µ and σ are allowed to be computed only for
subgroups of the layer. Batch normalization is similar to Layer Normal-
ization, except the average µ and variance σ2 is computed at the node
with respect to all datapoints in the current training batch.
In general it is not clear how to analyse optimization once the net
incorporates normalization. The paper of Arora et al 6 suggests a
6 S Arora, Z Li, and K Lyu. Theoretical
analysis of auto rate-tuning by batch
normalization. ICLR, 2019
way forward by identifying a property called scale invariance.

effect of normalization in deep learning
157
Definition 13.2.1 (Scale Invariance). A function f : ℜd →ℜd′ is scale
invariant if f (c · x) = f (x) for all c > 0. 7
7 This means its degree of homogeneity
is zero.
Note that if h1, h2 are k-homogeneous then so are h1 + h2 and
ReLU(h1), whereas h1/h2 is scale-invariant.
Lemma 13.2.2. In the above description of layer normalization, if w denotes
the parameter vector for the entire network, then for each layer l ≥1,
the output of ReLU, x(l) has degree of homogeneity 1 with respect to w,
where L is the total number of layers, x(0) is the input of the network and
x(l) := ReLU(LayerNorm(W(l−1)x(l−1))) for 1 ≤l ≤L.
If the parameters after the last normalization, γ(L−1)
i
, β(L−1)
i
, W(L) are
fixed during training then the function computed is scale-invariant.
Proof. The proof is by induction on the height of the layer, l. Re-
call that the layer starts by computing a linear functions of the out-
put of the previous layer. Thus in the above description, the func-
tion represented by each x(l)
i
has degree of homogeneity 1 (except
for x(0)
i
, which is 0-homogeneous), as do µ(l) and σ(l). The nor-
malized value (x(l)
i
−µ(l))/σ(l) is then scale-invariant. However,
γ(l)
i (x(l)
i
−µ(l))/σ(l) + β(l)
i
is again 1-homogeneous, and it remains
1-homogeneous after passing through the ReLU. This completes the
induction. We conclude that if the output of the previous layer is
1-homogeneous then so is the output of the next layer.
A simple fix makes the network scale-invariant: randomly fix the
parameters after the last normalization, γ(L−1)
i
, β(L−1)
i
, W(L) at the
start of training. Then train as usual. By the above lemma, the train-
ing loss becomes scale-invariant with respect to network parameters.
Experiments in 8 show that fixing the top layer does not hurt classifi-
8 S Arora, Z Li, and K Lyu. Theoretical
analysis of auto rate-tuning by batch
normalization. ICLR, 2019
cation accuracy etc. While above we focused on a simple architecture,
the basic idea can be modified to show scale invariance for most
known deep architectures with normalization including ResNets and
language models; see 9, 10.
9 Z Li and S Arora. An exponential
learning rate schedule for deep learn-
ing. ICLR, 2019
10 Z Li, S Bhojanapalli, M Zaheer, Reddi
S, and Kumar S. Robust training of
neural networks using scale invariant
architectures. arxiv, 2022
In the rest of the chapter, we assume scale invariance while prov-
ing convergence rates for optimization.
Lemma 13.2.3 (Properties of scale-invariant functions). If L is scale-
invariant then the following hold:
1. ⟨w, ∇L(w)⟩= 0.
2. ∇L(c · w) = 1
c ∇L(w).
3. ∇2L(c · w) = 1
c2 ∇2L(w).
Proof. 1) follows by differentiating L(c · w) = L(w) with respect to
c and then setting c = 1. 2) follows by taking gradient of L(c · w) =
L(w) with respect to w, and (3) follows by differentiating twice.

158
theory of deep learning
13.3
Exponential learning rate schedules
Usually training deep nets involves careful learning rate adjustments,
with the rate being reduced over the course of training. However, in
past few years several exotic learning rate schedules such as cosine
have been successfully used. This appears to be a mystery at first.
However, it can be shown provably that certain learning schedules
that are nonsensical in a classical viewpoint become effective in nor-
malized nets. We describe a result of 11 that even raising the learning
11 Z Li and S Arora. An exponential
learning rate schedule for deep learn-
ing. ICLR, 2019
rate at an exponential rate (i.e., multiply η by (1 + c) for some c > 0
at each iteration) is at least as powerful as usual training.
This happens because in practice normalization is used together
with weight decay (WD) and momentum. For simplicity we ignore mo-
mentum (see the above-mentioned paper for a full analysis). The
basic update with LR (Learning Rate) η and WD parameter λ is as
follows, where ∇Lt(·) denotes gradient computed using the mini-
batch in the t-th iteration:
wt+1 ←(1 −ηλ)wt −η∇Lt(wt).
(GD + WD)
(13.1)
Now we show that there is an alternative GD-based algorithm
that can achieve the same effect but whose LR increases by a multi-
plicative factor of (1 + α) in each iteration. This shows exponentially
increasing LR is at least as effective as the standard GD+WD training.
Theorem 13.3.1. If training loss is scale-invariant, the effect of (13.1) for T
steps can also be obtained by the following alternative protocol:
bwt+1 ←bwt −ηt∇Lt( bwt).
(13.2)
with learning rate at step t being ηt = (1 −ηλ)−(2t+1).
Proof. Let wt denote the parameter vector after t steps of GD + WD,
and bwt the parameter vector after t steps of our alternative protocol.
We show by induction that bwt = wt/(1 −ηλ)t. 12 This holds for
12 Recall that the loss is invariant to
scalings of parameter vector.
t = 0 by design. Assuming it held for t, (13.2) gives
bwt+1 ←
wt
(1 −ηλ)t −
η
(1 −ηλ)2t+1 ∇Lt(
wt
(1 −ηλ)t ).
which by Lemma 13.2.3 part 2 simplifies to (1 −ηλ)−(t+1)wt+1←(1 −
ηλ)−(t+1) ((1 −ηλ)wt −η∇Lt(wt)), thus completing the induction.
13.4
Convergence analysis for GD on Scale-Invariant Loss
Now we analyze convergence rate for GD +WD (13.1) on scale-
invariant loss L(·). The basic iteration is
wt+1 = (1 −ηλ)wt −η∇L(wt).
(13.3)

effect of normalization in deep learning
159
Here we denote the unit-norm vector w/∥w∥2 as w.
The standard convergence analysis as in Theorem 2.5.1 of Sec-
tion 2.5.2 doesn’t work for a couple of reasons. First, Lemma 13.2.3
part 2 shows that making the gradient norm smaller need not imply
low loss or even approach to a local optimum: increasing the scale
of the parameter vector reduces the gradient but does not affect loss.
Second, LR cannot be set using the reciprocal of the smoothness (i.e.,
largest eigenvalue of the Hessian) since the smoothness becomes
unbounded as the parameter vector moves toward the origin. We
present the first convergence analysis for fixed LR (taken from in 13)
13 Z Li, S Bhojanapalli, M Zaheer, Reddi
S, and Kumar S. Robust training of
neural networks using scale invariant
architectures. arxiv, 2022
in this setting, which has the added benefit of showing that the scale
of initialization doesn’t much matter —as one would intuitively ex-
pect in the scale-invariant setting.
Definition 13.4.1. ρ = maxw:∥w∥2=1 ∥∇2L(w)∥2 = maxw ∥∇2L(w)∥2.
Theorem 13.4.2 (Main). For ηλ < 1
2, there is t ≤
1
2λη
ln ∥w0∥2
2
ρπ2η
 + 3

such that ∥∇L(wt)∥2
2 ≤8π4ρ2λη. 14
14 The number of iterations has only
logarithmic dependence on ∥w0∥,
highlighting how normalization makes
optimization fairly robust to the scale of
initialization.
To understand whether the norm upper bound guaranteed by the
above theorem is meaningful, we try to understand the scale of the
various quantities.
Lemma 13.4.3. 1. ∥∇L(w)∥≤πρ for all w of unit ℓ2 norm.
2. L(w) −minw L(w) ≤π2ρ/2 for all w ̸= 0.
Proof. Part 1: Let w∗be any local minimizer of L on the unit sphere.
Let γ: [0, 1] →ℜd be the geodesic curve on the unit sphere with
γ(0) = w∗and γ(1) = w. We know the length of s is at most π and
hence and
∥∇L(γ(1))∥2 = ∥
Z 1
t=0 ∇2(L(γ(t)))dγ
dt dt∥2 ≤
Z 1
t=0 ∥∇2(L(γ(t))∥2∥dγ
dt ∥2dt ≤πρ.
Part 2 follows similarly and is left as exercise.
Problem 13.4.4. Prove part 2 of Lemma 13.4.3.
Theorem 13.4.2 guarantees that the algorithm quickly finds a so-
lution where ∥∇L(w)∥2 is at most a O(
p
λη) factor of the maximum
possible value on the unit sphere. This is meaningful since in practice
λη is tiny, like 10−4 ∼10−6.
Lemma 13.4.5. A twice-differential scale-invariant function L : ℜd →ℜ
with ρ = max∥x∥2=1 ∥∇2L(x)∥2 satisfies for every pair of orthogonal
vectors x, v
L(x + v) −L(x) ≤⟨v, ∇L(x)⟩+ ρ∥v∥2
2
2∥x∥2
2
.

160
theory of deep learning
Proof. Define a function γ: [0, 1] →ℜd as γ(s) = x + s · v and F(s) :=
L(γ(s)). Then L(γ(0)) = L(x) and L(γ(1)) = L(x + v). By Taylor
expansion and intermediate value theorem F(1) = F(0) + F′(0) +
F”(s∗)/2 for some s∗∈[0, 1]. Furthermore, F′(0) = ⟨∇L(x), v⟩and
scale invariance implies:
F”(s∗) = γ′(s∗)∇2(γ(s∗))γ′(s∗) ≤
ρ
∥γ(s∗)∥2
2
∥γ′(s∗)∥2
2.
The lemma now follows by noting that γ′(s∗) = v and ∥γ(s∗)∥2 ≥
∥x∥2
2 thanks to the orthogonality.
The next theorem lower bounds the change in loss using the norm
squared of the gradient, and is analogous to similar bounds in the
simpler setting of Section 2.5.2.
Theorem 13.4.6. If wt+1, wt are as in (13.3) and ηλ ≤1/2 then
L(wt) −L(wt+1) ≥η(1 −
2ρη
∥wt∥2
2
)∥∇L(wt)∥2
2.
Problem 13.4.7. Prove Theorem 13.4.6 from Lemma 13.4.5. (Hint: Use
(1 −ηλ)wt) as x and −η∇L(wt) as v.)
As pointed out earlier, ∇2L(w) can blow up as w approaches
the zero vector. Accordingly, the analysis has to separate out two
cases depending on ∥w0∥2. First we show if the initial norm is too
small then it quickly becomes large enough so the argument in
Lemma 13.4.9 will apply.
Lemma 13.4.8. In any sequence of
1
6λη successive iterations there must
exist some step T where ∥wT∥2
2 ≥π2ρη or ∥∇L(wT)∥2
2 ≤8π4ρ2λη.
Furthermore, ∥wT∥2
2 ≤2(π2ρη)2
∥w0∥2
2 .
Proof. So long as ∥wt∥2
2 ≤π2ρη and ∥∇L(wt)∥2
2 ≥8π4ρ2λη then
using Pythagoras theorem and the fact that ∇L(wt) is perpendicular
to wt one can conclude
∥wt+1∥2
2 −(1 −ηλ)2∥wt∥2
2 = η2∥∇L(wt)∥2
2.
(13.4)
which yields
∥wt+1∥2
2 −∥wt∥2
2 ≥η2∥∇L(wt)∥2
2/∥wt∥2
2 −2ηλ∥wt∥2
2 ≥6π2ρλη2.
Summing up these inequalities over t shows that the left hand side is
∥wt∥2 −∥w0∥2, which is at most π2ρη. On the other hand, the right
hand side scales linearly with t, namely 6tπ2ρλη2. We conclude t
cannot be more than 1/(6λη). So there must be a first T before this

effect of normalization in deep learning
161
point where ∥wT∥2
2 > π2ρη ≥∥wT−1∥2
2. Applying Pythagoras
theorem again, we have
∥wT∥2
2 ≤∥wT−1∥2
2 + η2∥∇L(wT−1)∥2
2/∥wT−1∥2
2
≤π2ρη + η2∥∇L(wT−1)∥2
2/∥w0∥2
2
≤2(π2ρη)2
∥w0∥2
2
,
which yields the desired upper bound on ∥wT∥2
2.
Leveraging the previous lemma we can focus on the case where
initial norm large enough.
Lemma 13.4.9. For ηλ < 1
2, if ∥w0∥2
2 > π2ρη and T0 =
1
2ηλ ln 2∥w0∥2
2
ρπ2η
then some t < T0 must satisfy
∥∇L(wt)∥2
2 ≤8π4ρ2λη.
(13.5)
Proof. First we show there exists some T ≤T0 that ∥wT∥2
2 ≤π2ρη.
Otherwise, a simple induction using (13.4) gives
∥wT0∥2
2 −(1 −ηλ)2T0∥w0∥2
2 =
T0−1
∑
t=0
η2(1 −ηλ)2(T0−t)∥∇L(wt)∥2
2
≤
T0−1
∑
t=0
η2
2 ∥∇L(wt)∥2
2.
Summing the basic inequality proved in Theorem 13.4.6 over t = 0
to T0 −1 and the assumption that ∥wt∥≥π2ρη together show that
the right hand side is upper bounded by η(L(w0) −L(wT0)) which is
at most π2ηρ/2. Finally, by choice of T0 we have (1 −ηλ)2T0∥w0∥2
2 <
π2ηρ/2. Substituting in the expression of T0, we conclude ∥wT0∥2
2 ≤
π2ηρ. Contradiction! So there must be a first step T ≤T0 where
∥wT∥2
2 ≤π2ηρ < ∥wT−1∥2
2. (Note: T ≥0 since the norm exceeds π2ηρ
at initialization.) Since ∥wT∥2 ≥(1 −ηλ)∥wT−1∥2, using (13.4) we
conclude that
∥∇L(wT−1)∥2
2 ≤η−2 
∥wT∥2
2 −(1 −ηλ)2∥wT−1∥2
2

∥wT−1∥2
2
≤η−2 · 2λη∥wT∥2
2 ·
∥wT∥2
2
(1 −ηλ)2
≤8π4ρ2λη.
which implies the desired upper bound on ∥∇L(wT−1)∥2.
The main theorem, Theorem 13.4.2 is proved by a straightforward
combination of Lemmas 13.4.8 and 13.4.9.


14
Unsupervised learning: Distribution Learning
Much of the book so far concerned supervised learning —i.e., where
training dataset consists of datapoints and a label indicating which
class they belong to, and the model has to learn to produce the right
label given an input. This chapter is an introduction to unsupervised
learning, where one has randomly sampled datapoints but no labels
or classes. We survey possible goals for this form of learning, and
then focus on distribution learning, which addresses many of these
goals.
14.1
Possible goals of unsupervised learning
Learn hidden/latent structure of data. An example would be Principal
Component Analysis (PCA), concerned with finding the most impor-
tant directions in the data. Other examples of structure learning
can include sparse coding (aka dictionary learning) or nonnegative
matrix factorization (NMF).
Learn the distribution of the data. A classic example is Pearson’s 1893
contribution to theory of evolution by studying data about the crab
population on Malta island. Biologists had sampled a thousand
crabs in the wild, and measured 23 attributes (e.g., length, weight,
etc.) for each. The presumption was that these datapoints should
exhibit Gaussian distribution, but Pearson could not find a good fit
to a Gaussian. He was able to show however that the distribution
was actually mixture of two Gaussians. Thus the population con-
sisted of two distinct species, which had diverged not too long ago
in evolutionary terms.
In general, in density estimation the hypothesis is that the un-
labeled dataset consists of iid samples from a fixed distribution,
and model θ learns representation of some distribution pθ(·) that
assigns a probability pθ(x) to datapoint x. This is the general prob-
lem of density estimation.

164
theory of deep learning
Figure 14.1: Visualization of
Pearson’s Crab Data as mix-
ture of two Gaussians. (Credit:
MIX homepage at McMaster
University.)
One form of density estimation is to learn a generative model, where
the learnt distribution has the form pθ(h, x) where x is the observ-
able (i.e., datapoint) and h consists of a vector of hidden variables,
often called latent variables. Then the density distribution of x is
R
pθ(h, x)dh. In the crab example, the distribution a mixture of
Gaussians N (µ1, Σ1), N (µ2, Σ2) where the first contributes ρ1 frac-
tion of samples and the other contributes 1 −ρ1 fraction. Then θ
vector consists of parameters of the two Gaussians as well as ρ1.
The visible part x consists of attribute vector for a crab. Hidden
vector h consists of a bit, indicating which of the two Gaussians
this x was generated from, as well as the value of the gaussian
random variable that generated x.
Learning good representation/featurization of data For example, the pixel
representation of images may not be very useful in other tasks
and one may desire a more “high level” representation that allows
downstream tasks to be solved in a data-efficient way. One would
hope to learn such featurization using unlabeled data.
In some settings, featurization is learnt via generative models: one
assumes a data distribution pθ(h, x) as above and the featurization
of the visible samplepoint x is assumed to be the hidden variable
h that was used to generate it. More precisely, the hidden variable
is a sample from the conditional distribution p(h|x). This view of
representation learning is used in the autoencoders described later.
Figure 14.2: Autoencoder de-
fined using a density distri-
bution p(h, x), where h is the
latent feature vector corre-
sponding to visible vector x.
The process of computing h
given x is called “encoding”
and the reverse is called “de-
coding.” In general applying
the encoder on x followed by
the decoder would not give
x again, since the composed
transformation is a sample from
a distribution.
For example, topic models are a simple probabilistic model of text

unsupervised learning: distribution learning
165
generation, where x is some piece of text, and h is the proportion
of specific topics (“sports,” “politics” etc.). Then one could imag-
ine that h is some short and more high-level descriptor of x.
Many techniques for density estimation —such as variational
methods, described later —also give a notion of a representation: the
method for learning the distribution often also come with a candi-
date distribution for p(h|x). This why students sometimes conflate
representation learning with density estimation. But many of today’s
approaches to representation learning do not boil down to
14.2
Training Objective for Learning Distributions: Log Likeli-
hood
We wish to infer the best θ given the set S of i.i.d. samples (“evi-
dence”) from the distribution. One standard way to quantify “best”
is pick θ is according to the maximum likelihood principle, which says
that the best model is one that assigns the highest probability to the
training dataset.1
1 The maximum likelihood principle is a
philosophical stance, not a consequence
of some mathematical analysis.
max
θ
∏
x(i)∈S
pθ(x(i))
(14.1)
Because log is monotone, this is also equivalent to minimizing the
log likelihood, which is a sum over training samples and thus similar
in form to the training objectives seen so far in the book:
max
θ
∑
x(i)∈S
log pθ(x(i))
(log likelihood)
(14.2)
Often one uses average log likelihood per datapoint, which means
dividing (14.2) by ∥S∥.
As in supervised learning, one has to keep track of training log-
likelihood in addition to generalization, and choose among models
that maximize it. In general such an optimization is computationally
intractable for even fairly simple settings, and variants of gradient
descent are used in practice.
Of course, the more important question is how well does the
trained model learn the data distribution. Clearly, we need a no-
tion of “goodness” for unsupervised learning that is analogous to
generalization in supervised learning.
14.2.1
Notion of goodness for distribution learning
The most obvious notion of generalization follows from the log like-
lihood objective. The notion of generalization most analogous to the

166
theory of deep learning
one in supervised learning is to evaluate the log likelihood objective
on held-out data: reserve some of the data for testing and compare the
average log likelihood of the model on training data with that on test
data.
Example 14.2.1. The log likelihood objective makes sense for fitting any
parametric model to the training data. For example, it is always possible to
fit a simple Gaussian distribution N (µ, σ2I) to the training data in ℜd. The
log-likehood objective is
∑
i
|xi −µ|2
σ2
,
which is minimized by setting µ to 1
m ∑i xi and σ2 to ∑i
1
n|xi −µ|2.
Suppose we carry this out for the distribution of real-life images. What do
we learn? The mean µ will be the vector of average pixel values, and σ2 will
correspond to the average variance per pixel. Thus a random sample from
the learn distribution will look like some noisy version of the average pixel.
This example also shows that matching average log-likelihood for training
and held-out data is insufficient for actually learning the distribution. The
gaussian model only has d + 1 parameters and simple ϵ-cover arguments
as in Chapter 5 show under fairly general conditions (such as coordinates
of xi’s being bounded) that if the number of training samples is moderately
high then the log-likelihood on the average test sample is similar to that
on the average training sample. However, the learned distribution may be
nothing like the true distribution.
This is reminiscent of the situation in supervised learning whereby a
nonsensical model —e.g., one that outputs random labels—has excellent
generalization as well because it has similar loss on training as well as test
data.
But how can we know that log likelihood objective is in principle
capable of learning the distribution? The following theorem shows
so.
Theorem 14.2.2. Given enough training data, the θ maximizing (14.2)
minimizes the KL divergence KL(P||Q) where P is the true distribution and
Q is the learnt distribution.
Proof. This follows from
KL(P||Q) = E
x~P[log P(x)
Q(x)]
= E
x~P[log P(x)] −E
x~P[log Q(x)].
Notice that Ex~P[log P(x)] is a constant that depends only upon
the data distribution, and that computing log-likelihood using iid
samples from P is like estimating the second term. We conclude that

unsupervised learning: distribution learning
167
given enough samples, minimizing KL(P||Q) amounts to maximising
log likelihood up to an additive constant.
Note that except for low-dimensional settings, the previous Theo-
rem does not give any meaningful bounds on the number of training
datapoints needed for proper learning.
14.3
Variational method
As sketched above, we are assuming a ground truth generative model
p(x, h) and we are assuming we have samples of x obtained by gener-
ating pairs of (x, h) according to the ground truth and discarding the
h part. The variational method tries to learn p(x) from such samples,
where “variational”in the title refers to calculus of variations. It lever-
ages duality, a widespread principle in math. The idea is to maintain
a distribution q(h|x) as an attempt to model p(h|x) and improve a
certain lower bound on p(x). The key fact is the following. 2
2 See the blog post on offconvex.org by
Arora and Risteski on how algorithms
try to use some form of gradient de-
scent or local improvement to improve
q(h|x).
Lemma 14.3.1 (ELBO Bound). For any distribution q(h|x)
log p(x) ≥Eq(h|x)[log(p(x, h))] + H[q(h|x)],
(14.3)
where H is the Shannon Entropy. (Note: equality is attained when q(h|x) =
p(h|x).)
Proof. Since
KL[q(h|x) || p(h|x)] = Eq(h|x)

log q(h|x)
p(h|x)

(14.4)
and p(x)p(h|x) = p(x, h) (Bayes’ Rule) we have:
KL[q(h|x)|p(h|x)] = Eq(h|x)[log q(h|x)
p(x, h) · p(x)]
(14.5)
= Eq(h|x)[log(q(h|x))]
|
{z
}
−H(q(h|x))
−Eq(h|x)[log(p(x, h))] + Eq(h|x)[log p(x)]
(14.6)
But since KL divergence is always nonnegative, so we get:
Eq(h|x)[log(p(x))] −Eq(h|x)[log(p(x, h))] −H(q(h|x)) ≥0
(14.7)
which leads to the desired inequality since log(p(x)) is constant over
q(h|x) and thus Eq(h|x)[log(p(x))] = p(x).

168
theory of deep learning
14.4
Autoencoders and Variational Autoencoder (VAEs)
Autoencoders find a compressed latent representation h of the dat-
apoint x such that x can be approximately recovered from h. They
can be defined in multiple ways by chaging the formalization of what
“approximate recovery” means.
In this section we formalize them using latent variable generative
models. A popular instantiation of this in deep learning is Variational
Autoencoder (VAE) 3. As its name suggests two core classical ideas rest
3
behind the design of VAEs: autoencoders – the original data x ∈Rn
is mapped into a high-level descriptor z ∈Rd on a low dimensional
(hopefully) meaningful manifold; variational inference – the objective
to maximize is a lower bound on log-likelihood instead of the log-
likelihood itself.
Recall that in density estimation we are given a data sample
x1, . . . , xm and a parametric model pθ(x), and our goal is to maximize
the log-likelihood of the data: maxθ ∑m
i=1 log pθ(xi). As a variational
method, VAEs use the evidence lower bound (ELBO) as a training ob-
jective instead. For any distributions p on (x, z) and q on z|x, ELBO is
derived from the fact that KL(q(z|x) || p(z|x)) ≥0
log p(x) ≥Eq(z|x)[log p(x, z)] −Eq(z|x)[log q(z|x)] = ELBO
(14.8)
where equality holds if and only if q(z|x) ≡p(z|x). In the VAE set-
ting, the distribution q(z|x) acts as the encoder, mapping a given data
point x to a distribution of high-level descriptors, while p(x, z) =
p(z)p(x|z) acts as the decoder, reconstructing a distribution on data x
given a random seed z ∼p(z). Deep learning comes in play for VAEs
when constructing the aforementioned encoder q and decoder p. In
particular,
q(z|x) = N (z; µx, σ2
x Id),
µx, σx = Eϕ(x)
(14.9)
p(x|z) = N (x; µz, σ2
z In),
µz, σz = Dθ(z),
p(z) = N (z; 0, Id)
(14.10)
where Eϕ and Dθ are the encoder and decoder neural networks pa-
rameterized by ϕ and θ respectively, µx, µz are vectors of correspond-
ing dimensions, and σx, σz are (nonnegative) scalars. The particular
choice of Gaussians is not a necessity in itself for the model and can
be replaced with any other relevant distribution. However, Gaussians
provide, as is often the case, computational ease and intuitive back-
ing. The intuitive argument behind the use of Gaussian distributions
is that under mild regularity conditions every distribution can be ap-
proximated (in distribution) by a mixture of Gaussians. This follows
from the fact that by approximating the CDF of a distribution by step
functions one obtains an approximation in distribution by a mixture

unsupervised learning: distribution learning
169
of constants, i.e. mixture of Gaussians with ≈0 variance. The compu-
tational ease, on the other hand, is more clearly seen in the training
process of VAEs.
14.4.1
Training VAEs
As previously mentioned, the training of variational autoencoders
involves maximizing the RHS of (14.8), the ELBO, over the parame-
ters ϕ, θ under the model described by (14.9), (14.10). Given that the
parametric model is based on two neural networks Eϕ, Dθ, the ob-
jective optimization is done via gradient-based methods. Since the
objective involves expectation over q(z|x), computing an exact esti-
mate of it, and consequently its gradient, is intractable so we resort
to (unbiased) gradient estimators and eventually use a stochastic
gradient-based optimization method (e.g. SGD).
In this section, use the notation µϕ(x), σϕ(x) = Eϕ(x) and µθ(z), σθ(z) =
Dθ(z) to emphasize the dependence on the parameters ϕ, θ. Given
training data x1, . . . , xm ∈Rn, consider an arbitrary data point
xi, i ∈[m] and pass it through the encoder neural network Eϕ to
obtain µϕ(xi), σϕ(xi). Next, sample s points zi1, . . . , zis, where s is the
batch size, from the distribution q(z|x = xi) = N (z; µϕ(xi), σϕ(xi)2Id)
via the reparameterization trick 4 by sampling ϵ1, . . . , ϵs ∼N (0, Id)
4
from the standard Gaussian and using the transformation zij =
µϕ(xi) + σϕ(xi) · ϵj. The reason behind the reparameterization trick
is that the gradient w.r.t. parameter ϕ of an unbiased estimate of ex-
pectation over a general distribution qϕ is not necessarily an unbiased
estimate of the gradient of expectation. This is the case, however,
when the distribution qϕ can separate the parameter ϕ from the ran-
domness in the distribution, i.e. it’s a deterministic transformation
that depends on ϕ of a parameter-less distribution. With the s i.i.d.
samples from q(z|x = xi) we obtain an unbiased estimate of the
objective ELBO
s
∑
j=1
log p(xi, zij) −
s
∑
j=1
log q(zij|xi) =
s
∑
j=1
[log p(xi|zij) + log p(zij) −log q(zij|xi)]
(14.11)
Here the batch size s indicates the fundamental tradeoff between
computational efficiency and accuracy in estimation. Since each
of the terms in the sum in (14.11) is a Gaussian distribution, we
can write the ELBO estimate explicitly in terms of the parameter-
dependent µϕ(xi), σϕ(xi), µθ(zij), σθ(zij) (while skipping some con-
stants). A single term for j ∈[s] is given by
−1
2
"
||xi −µθ(zij)||2
σθ(zij)2
+ n log σθ(zij)2 + ||zij||2 −||zij −µϕ(xi)||2
σϕ(xi)2
−d log σϕ(xi)2
#
(14.12)

170
theory of deep learning
Notice that (14.12) is differentiable with respect to all the compo-
nents µϕ(xi), σϕ(xi), µθ(zij), σθ(zij) while each of these components,
being an output of a neural network with parameters ϕ or θ, is dif-
ferentiable with respect to the parameters ϕ or θ. Thus, the tractable
gradient of the batch sum (14.11) w.r.t. ϕ (or θ) is, due to the reparame-
terization trick, an unbiased estimate of ∇ϕELBO (or ∇θELBO) which
can be used in any stochastic gradient-based optimization algorithm
to maximize the objective ELBO and train the VAE.
14.5
Normalizing Flows
The limitation of VAE is that instead of direct log likelihood, it op-
timizes a lower bound to it. Ideally we would want to get around
this limitation while staying with a deep model with sophisticated
representation capability. (The simple Gaussian fit as described at
the start of the chapter also optimizes log likelihood directly but it
cannot represent complicated distributions.) Normalizing flows can do
this,
The idea in Normalizing Flows (Rezende and Mohamed 2015) is
to make the deep net invertible. Specifically, it computes a function
fθ : ℜd →ℜd that is parametrized by trainable parameter vector θ
and maps image x to its representation h = fθ(x) (note: both have the
same dimension). Importantly, f is an invertible map (i.e., one-to-one
and onto) and differentiable (or almost everywhere differentiable).
The advantage of such a transformation is that it gives a clear con-
nection between the probability densities of x and h. In generative
models h is assumed to have some prescribed probability density
µ(h), usually uniform gaussian. Via the invertible map, this translates
to a density ρ(·) on x given by
ρ(x) = µ( f (x))|det(Jf )|
(14.13)
where Jf is the Jacobian of f namely, whose (i, j) entry is ∂f (x)i/∂xj
and det(·) denotes determinant of the matrix. This exact expression
for likelihood of the training datapoints allows usual gradient-based
training.
Which raises the question: how does one constrain nets to be in-
vertible? Note that it suffices to constrain individual layers to be
invertible, because the overall Jacobian is the composition of layer
Jacobians. 5 To make layers invertible one often uses a variant of the
5 Since det(AB) = det(A)det(B) the
determinant of the net Jacobian is the
product of the determinants of the
layers.
following trick from the models NICE 6 and Real NVP 7. If zl is the
6 Laurent Dinh, David Krueger, and
Yoshua Bengio. NICE: Nonlinear
Independent Component Analysis.
Proc. ICLR, 2015
7 Laurent Dinh, Jascha Sohl-Dickstein,
and Samy Bengio. Density Eestimation
using Real NVP. Proc. ICLR, 2017
input to layer l and zl+1 its output, then identify a special set of coor-
dinates A in zl and zl+1 and impose the restriction (where zA denotes

unsupervised learning: distribution learning
171
Figure 14.3: Faces in the top
row were produced by a VAE
based method and those in
the second row by RealNVP
using normalizing flows. VAE
is known for producing blurry
images. RealNVP’s output is
much better, but still has visible
artifacts.
portion of z in the coordinates given by A, and B is shorthand for A).
zl+1
A
= zl
A
(14.14)
zl+1
B
= zl
B ⊙hθ(zl
A) + sθ(zl
A)
(14.15)
where ⊙denotes component-wise product and hθ() is a function
whose each output is nonnegative, with a convenient choice being to
make it exp(rθ(zl
A)) for some other function rθ().
This layer is invertible because given zl+1 one can recover zl as
follows:
zl
A = zl+1
A
(14.16)
zl
B = (zl+1
B
−sθ(zl
A)) ⊙hθ(zl
A)
(14.17)
Note that the choice of A, B can change from layer to layer, so all
coordinates may get updated as they go through multiple layers.
Furthermore, denoting by z|A the portion of the layer vector on co-
ordinates A, the Jabobian for the layer mapping is lower triangular.
Hence the determinant is the product of the diagonal entries.
∂
∂zl zl+1 =
 
I|A|×|A|
0
∂
zl|A zl+1|B
diag(hθ(zl
A))
!
Normalizing flows can be extended to convolutional nets by re-
stricting the convolutions are 1 × 1. Then convolutional filters just
involve scalings of channel values, and the corresponding Jacobian
is a diagonal nonzero matrix. Also the split of coordinates into A
and B split can be done within channels as well. This is one of the
ideas in GLOW model 8, which can generate better images than its
8 Diederik P. Kingma and Prafulla
Dhariwal. GLOW: Generative Flow
with Invertible 1 × 1 convolutions. Proc.
Neurips, 2019
predecessors.
More recent auto-regressive models such as PixelCNN are capable
of producing very realistic-looking images from random seeds. How-
ever, they do not fit into the distribution learning paradigm described
above so we do not discuss them here. They involve generating the

172
theory of deep learning
image pixel by pixel (roughly speaking) and thus do not parallelize
well.
Problem 14.5.1. Let (z1, z2, z3, z4) be distributed as a standard Gaussian
N (0, I) in R4. Let f : R4 →R4 be an invertible function which maps
(z1, z2, z3, z4) to (z1, z2, ea0z3 + a1z2
1 + a2z2
2, eb0z4 + b1z2
1 + b2z2
2) for
some coefficients a0, a1, a2, b0, b1, b2 ∈R. Compute the probability density
function of f (z1, z2, z3, z4).
14.6
Stable Diffusion
You may have seen AI models that generate artificial imagery given a
text prompt such as “Pope Francis walking in a puffer jacket.” These
are made by diffusion models 9, which we describe in this section,
9
albeit without the text prompts.
Diffusion models are reminiscent of normalizing flows and au-
toencoders, in that they define a mapping f that transforms the set
of all images to the set N (0, I), as well as an inverse mapping f −1
that maps gaussian vectors to images. The difference is that f is very
trivial; just a series of noising steps. Furthermore, f −1 is just net
custom-trained on denoising the output of f.
Figure 14.4: Example of noising
an image and then denoising,
using Diffusion Model. (Source:
Binxu Wang)
The noising layers noise an an image x0 to xT in T steps as follows
where zt ∼N (0, I) and each αt ∈(0, 1)
xt+1 = √αtxt +
p
1 −αtzt,
t = 0, . . . , T −1
(14.18)
A simple induction shows this is equivalent to the following where
αt = ∏t
i=1 αi:
xt+1 =
√
αtxt +
p
1 −αtz.
(14.19)
The above calculation is using the following.
Problem 14.6.1. If z1, z2 are independent samples from N (µ1, σ2
1 I) and
N (µ2, σ2
2 I) respectively then z1 + z2 is distributed as
N (µ1 + µ2, σ2
1 + σ2
2 I).

15
Language Models (LMs)
Starting around 2020, language models (LMs) have suddenly become
the most visible face of AI research. This chapter introduces the basic
notions and results. The heart of it
A key concept underlying LMs is that text produced by humans is
assumed to have be sampled from a probabilistic distribution, with
Pr(w1 w2 . . . wi) the probability associated with a sequence of words
w1 w2 . . . wi. Then Bayes’ rule implies a factorization:
Pr[w1 w2 . . . wi+1] = Pr[w1 w2 . . . wi] Pr[wi+1 |w1w2 . . . wi]
(15.1)
This implies that to generate a sequence of words, we only need abil-
ity to compute Pr[w1] and then a way to generate the i + 1’th word
given the previous i words. This, in a nutshell, is what a language
model does.1 The simplest models date back to 1950s, when com-
1 This exposition ignores other types
of language models that are used to
compute semantic embeddings for
text-pieces. Famous ones include BERT,
ERNIE, RoBERTA etc.
puting pioneer Claude Shannon proposed various simple approx-
imations to (15.1). The simplest, called unigram model, computes
estimates of the probability Pr[w] by measuring empirical frequency
of word w in a sufficiently large corpus, and uses the approxima-
tion Pr[wi+1 |w1w2 . . . wi] = Pr[wi+1], which is equivalent to saying
Pr[w1 w2 . . . wi+1] = ∏i Pr[wi]. The bigram model does something
similar but assumes that Pr[wi+1 |w1w2 . . . wi] = Pr[wi+1 | wi]. That
is to say, the probability of the next word depends upon the previous
word, but not on any earlier words. Then one only needs to empiri-
cally estimate Pr[w | w′] for all word pairs w, w′ which can be done
using a modest corpus size.
In recent decades as neural nets were applied to language model-
ing a key idea emerged: semantic vector of the context. The idea is
that to predict the next word wi+1 using the sequence w1w2 . . . wi of
preceding words, you compute an embedding ci ∈ℜD of the “mean-
ing” of w1w2 . . . wi. You also have a semantic embedding vw ∈ℜD of
every word w. Then the distribution of the next word is described as:
Pr[wi+1 |w1w2 . . . wi] ∝exp(vwi+1 · ci).
(15.2)

174
theory of deep learning
Since this idea has become ubiquitous in language modeling, let’s
understand something called softmax. 2 Describing the distribution
2 Recall that softmax is a mapping
from ℜk to the simplex, namely,
{(p1, p2, . . . , pk) : pi ≥0, ∑i pi = 1}. For
(s1, s2, . . . , sk) ∈ℜk define its softmax
as the vector whose i’th coordinate is
exp(si)/(∑k
j=1 exp(sj)).
via (15.2) amounts to saying that the distribution on the next word
is softmax of the vector in ℜV whose i + 1th coordinate is vwi+1 · ci.
Here V denotes the vocabulary size (i.e., number of distinct words).
Semantic embeddings and softmax are used for defining the “next-
word” distribution in today’s LMs as well. The key change with
modern LMs is the transformer architecture for computing the em-
beddings.
The goodness of a language model is computed via its cross entropy
loss, which for a sequence of words w1w2 . . . wt is:
ℓ(M) = ∑
i
log
1
PrM[wi+1 | w1w2 . . . wi]
(Cross Entropy)
(15.3)
Models are trained by minimizing (via gradient descent on model
parameters) this training loss on a text corpus, and their goodness
is computed by their test loss—evaluating the same loss expression
on a held-out text from the same corpus. Often the training corpus
is so large that the model trains only once (or a few times) on each
piece of text, and by the end, the test loss on held-out text is almost
the same as the training loss. Thus generalization error is fairly small.
Since minimizing the above loss amounts to maximising
∏
i
Pr
M[wi+1 | w1w2 . . . wi],
sometimes the goal of language modeling is described as trying to
create a model that assigns the largest possible probability to the
training corpus. This is actually a bit misleading to students. Sec-
tion 15.2 explains that the real goal is to make the model learn the
human distribution.
Language tasks: Decades of research in NLP has identified thou-
sands of tasks. Some examples: Implication: given two pieces of text
decide whether or not the first implies the other. Sentiment given a
piece of text decide whether it has positive, negative or neutral sen-
timent. Question-answering: given a piece of text, answer questions
related to it. Translation: Given a piece of text in one language, trans-
late it into another.
For many years such tasks were difficult. Today as you know
these as well as much more difficult tasks are routinely solved using
large LMs. This chapter will not discuss the architecture and training
details since good writeups exist on the internet. Instead it discusses
the conceptual underpinnings of cross-entropy loss (Section 15.2), as
well as how to train LMs to generate text that is useful or meaningful
in human interactions (Sections 15.6, ??. Then we focus on

language models (lms)
175
15.1
Transformer Architecture
For about 15 years leading up to 2017, there was a lot of innovation
in LMs with many neural architectures designed along the way. The
goal of the architecture is to compute the word embeddings and
context embeddings to allow prediction of the next word.
Starting 2017 these architectures have been effectively replaced
in most settings with the transformer architecture, which is also
used now for images, sound, genomes, and other types of data. We
recommend reading an excellent introduction to transformers on
Lilian Weng’s blog. 3 The following question invites you to test your
3 “Tranformer Family: Version 2.
https://lilianweng.github.io/.
2023
understanding.
Problem 15.1.1. Consider a N-layer single-head transformer with input
section length L and hidden state dimension d. For each layer l, let the input
for the layer be Xl and the output be Xl+1, we have
Xl+1 = Vl · softmax
 
Q⊤
l Kl
√
d
!
,
where Ql = Wq
l Xl, Kl = Wk
l Xl, Vl = Wv
l Xl, and Xl ∈Rd×L, Wq
l , Wk
l , Wv
l ∈
Rd×d. If the size of dataset is M (total number of tokens), find the asymp-
totic training time of each epoch in terms of M, N, L, d.
Note that here we consider the input data Xl as a collection of column vec-
tors (each column is a data point), so the parameter matrices W’s are mul-
tipled on the left of X’s. In some other literatures such as the Lilian Weng’s
blog, Xl is a collection of row vectors (each row is a datapoint). The 2 defini-
tions are sometimes used interchangeably.
15.2
Explanation of Cross-Entropy Loss
Now we try to understand the conceptual framework underlying
cross-entropy loss (15.3). As mentioned, there is a ground-truth (i.e.,
humans’) distribution for generating the next word, which assigns
probability pi(w | w1w2 . . . wi) to the event that the (i + 1)th word
is w given that the previous words were w1w2 . . . wi. In interest of
compact notation we shorten pi(w | w1w2 . . . wi) to pi(w), Thus the
entropy of the (i + 1)th word is
∑
w
pi(w) log
1
pi(w)
(entropy)
(15.4)
This entropy is an inherent property of language, arising from many
choices human writers make for the next word. Given sequence
w1w2 . . . wi the model has a probability distribution q(w|w1w2 . . . wi)
for the next word w. Extending our compact notation, we use qi(w)

176
theory of deep learning
as a shorthand for this. The cross-entropy loss of the model on the
(i + 1)th word is log
1
q(wi+1), which should be seen as an empirical
estimate of
Ew∼pi()[log
1
q(w)] (expected c-e loss)
(15.5)
KL divergence, also sometimes called excess entropy, is non-negative
and defined as
KL(pi||qi) = Ew∼pi()[log pi(w)
qi(w) ]
excess entropy
(15.6)
Thus on a per-word basis we have:
expected c-e loss = entropy + excess entropy
(15.7)
Summing over the entire held out corpus, one obtains a similar es-
timate for the entire corpus. One can make mild assumptions to the
effect that the conditional probabilities pi(), qi() only depend only on
(say) the previous 103 words, whereas the held out corpus size M is
much bigger, e.g., M ≫108. So the corpus consists of a random walk
of sorts, where every 104 words or so it switches to a different por-
tion of the language distribution. Under such assumptions the above
relationship, which holds in expectation at the word level, should
hold fairly precisely at the corpus level.
To summarize, since entropy of text is a constant, we can interpret
((15.7)) as follows.
Goal of Language Modeling: The goal of language modeling is to min-
imize KL-divergence of the human’s next-word distribution to the model’s
distribution.
Note that this applies to vanilla modeling as in this chapter. In
practice, training methods and even the loss function, deviate from
the simple picture presented above.
15.3
Scaling Laws and Emergence
An old ambition in deep learning was to be able to simply scale
up the network and train it with more data and continue to solve
problems better. While this hope had worked out nicely prior to
2017, thereafter thanks to transformers it got turbocharged. The key
discovery was so-called scaling laws.
These are empirically-derived expressions describe how test cross
entropy loss on held-out data scales (in experiments) with number
of model parameters (N) and size of the dataset (D) 4 5, 6. For Chin-
4
5
6 Yasaman Bahri, Ethan Dyer, Jared
Kaplan, Jaehoon Lee, and Utkarsh
Sharma. Explaining neural scaling laws.
arXiv preprint arXiv:2102.06701, 2021
chilla models 7 the law is as follows:
7 Jordan Hoffmann, Sebastian Borgeaud,
Arthur Mensch, Elena Buchatskaya,
Trevor Cai, Eliza Rutherford, Diego
de Las Casas, Lisa Anne Hendricks, Jo-
hannes Welbl, Aidan Clark, et al. Train-
ing compute-optimal large language
models. arXiv preprint arXiv:2203.15556,
2022
L(N, D) = A +
B
N0.34 +
C
D0.28
A = 1.61
B = 406.4
C = 410.7.
(15.8)

language models (lms)
177
Here the constants A, B, C in 15.8 hold only for the specific architec-
ture and training strategy —even the constant A depends upon the
tokenization. This description of macro behavior using two basic pa-
rameters —reminiscent of 2nd Law of Thermodynamics— will help
us circumvent the need for mechanistic understanding of training.
Our theory will only rely upon the general form of the equations,
specifically, that the dependence is inverse polynomial in N, D. So it
applies to other frameworks of training (e.g., overtrained models 8)
8 Niklas Muennighoff, Alexander M.
Rush, Boaz Barak, Teven Le Scao,
Aleksandra Piktus, Nouamane Tazi,
Sampo Pyysalo, Thomas Wolf, and
Colin Raffel. Scaling data-constrained
language models, 2023
where scaling laws have also been found.
Emergence:
Emergence refers to an interesting empirical phenomenon
that as D, N are increased together then the model’s performance
(zero shot or few-shot) on a broad range of language tasks improves
in a correlated way. The improvement can appear as a quick transi-
tion when D, N are plotted on a log scale (which is often the case)
but it is now generally accepted that for most tasks the performance
improves gradually when D, N are scaled up. Thus the term slow
emergence is more correct. Furthermore, it is known that emergence
happens at different rates for different tasks, and is often quickest for
tasks where the text is plausibly close to text found in training data 9.
9 Jason Wei, Yi Tay, Rishi Bommasani,
Colin Raffel, Barret Zoph, Sebastian
Borgeaud, Dani Yogatama, Maarten
Bosma, Denny Zhou, Donald Metzler,
et al. Emergent abilities of large
language models. arXiv preprint
arXiv:2206.07682, 2022
Plenty of tasks are known that stump current models, and they usu-
ally tend to be very different from what one would find in usual text
corpora. See [WTB+22? , SMK23] for experimental results on emer-
gence rates of the broad range of language tasks. One might thus
posit, with some justification from the above-mentioned studies, that
the emergence of skills arises from training on related tasks that were
implicitly solved while solving next-word prediction in the training
dataset. This is indeed our starting point.
15.4
(Mis)understanding, Excess entropy, and Cloze Questions
Thinking about emergence and Scaling Laws, it is possible to get con-
fused as follows: “When we increase D from 1011 to 1012 then according
to (15.8) this changes cross-entropy by a tiny amount. Why does it lead to
big changes in macroscopic behavior?” This section explains the flaw in
this reasoning.
The flaw is that most of the loss captures merely the inherent
entropy of language (the A term in (15.8)). We argue now that the
model’s mistakes on downstream tasks (i.e., its misunderstandings)
are captured by the excess entropy, which as noted in Section 15.2
reduces by a constant factor each time the model is scaled up by an
order of magnitude.
We illustrate using a classic example from 10, which later inspired
10 Terry Winograd. Procedures as
a representation for data in a com-
puter program for understanding
natural language. Technical report,
MASSACHUSETTS INST OF TECH
CAMBRIDGE PROJECT MAC, 1971
the Winograd Schema Challenge(WSC) 11:
11 Hector Levesque, Ernest Davis, and
Leora Morgenstern. The winograd
schema challenge. In Thirteenth inter-
national conference on the principles of
knowledge representation and reasoning,
2012

178
theory of deep learning
The city councilmen refused the demonstrators a permit because
they feared violence.
Here the pronoun they is ambiguous— grammar rules allow it to
refer to either demonstrators or city councilmen. Winograd pointed
out that disambiguating it (i.e., anaphora resolution) requires world
knowledge that is unavailable in the text itself, namely that demon-
strations can get violent, and city councilmen don’t like violence.
A key idea in designing test-beds for language understanding
such as WSC is the Cloze Procedure 12,13, popular also for testing
12
13 Cloze questions are multiple choice,
which allows testing most language
skills. They are not a good match
for skills such as understanding of
irony since one of the multiple choices
already explains the joke.
language development in children. To test the model’s understand-
ing of they in this sentence, we can append a prompt: Q. Who feared
violence?. This is followed by either a blank, or a choice of multi-
ple answers: A. city councilmen.
B. demonstrators. For WSC
examples, even though a human would be hundred percent sure of
the answer, language models circa 2016 were roughly 50/50 confused
between the two options.
In the above example, the human is 100% certain of the answer,
which implies their entropy here is log 1, namely 0. However if the
model is split 50-50 between the two options this implies it has cross-
entropy log 2, all of which is excess entropy! Given the frequency of
ambiguous pronouns in usual English, one concludes that a model
that has not learned pronoun disambiguation will display huge ex-
cess entropy at many places in surrounding text. Thus reductions
in excess entropy (which happen naturally due to scaling) will tend
to squeeze out such errors. The ensuing analysis tries to make this
intuition mathematically precise.
Of course, text corpora do not normally contain such artificial
cloze questions. But one could imagine that the model’s basic mis-
understanding of the above type could, often, lead to prediction mis-
takes in neighboring text. Our theory in Section 15.8 will assume that
cloze questions can closely capture the model’s misunderstanding.
15.5
How to generate text from an LM
The title of this section may feel ridiculous, since the very definition
of LM’s in (15.1) involves ability to predict the next word given the
preceding words. This appears to give an obvious way to generate
goodtext: use the model distribution for the first word to sample a
particular w1, then sample from the distribution Pr[w2|w1] to gen-
erate the second word, and carry on like that. The procedure just
described is called random generation or simply sampling and it
actually does not produce good text14.
14 The quality of sampling gets better as
models scale up and thus closer to the
language distribution.
The next natural idea is to greedy: Having generated w1w2 . . . wi,
generate wi+1 using the word that has the highest probability in the

language models (lms)
179
next position. At first sight this seems appealing since the training
objective for LMs implicitly trains them to maximise the probability
they assign to the training text given to them. So when generating
text, why not try to generate pieces of text that are given the highest
possible probability by the model? 15 The reason is that, as explained
15 Actually greedy doesn’t quite max-
imise the probability but is an attempt
in that direction.
in Section 15.2, the true goal of language modeling is to minimize KL
distance to the human distribution.
Greedy text looks flat and unexciting. For instance the greedy
continuations of Thanks for the dessert, it was . . . is probably great, but
there could be a variety of more interesting ones with lower prob-
ability, such as exquisite, life-changing etc. Human communication
often veers into low-probability words. Indeed, the perplexity of text
produced by the greedy method is far from that of human-generated
text! A good discussion of this issue appears in 16, from where Fig-
16 A Holtzman, J Buys, L Du, M Forbes,
and Y Choi. The curious case of neural
text degeneration. ICLR, 2020
ure 15.1 was taken.
Method
Perplexity
Human
12.38
Greedy
1.50
Beam, b=16
1.48
Stochastic Beam, b=16
19.20
Pure Sampling
22.73
Sampling, t=0.9
10.25
Top-k=40
6.88
Top-k=640
13.82
Top-k=40, t=0.7
3.48
Nucleus p=0.95
13.13
Figure 15.1: Perplexity of text
from various generation meth-
ods. Random and Greedy are
pretty bad. Nucleus Sampling
with p
=
0.95 gets closest to
human. (We did not describe
beam search, so please ignore
those rows.)
The best methods actually reshape the distribution via some greedy-
ish selections.
• Top-k: Identify the top k choices for first word, and restrict your-
self to pick the first word among them (i.e., disallow picking any
other word in this position). If their combined probability is pk
you do this by rescaling the probabilities of these k words by 1/pk
and zero-ing probabilities of all other words, and then pick from
this distribution. Having picked the first word, continue similarly
to pick the rest. You set k by trial and error to best match perplex-
ity to human text.
• Nucleus sampling (aka “top p”): This is a softer variant of the
above. Instead of making a hard decision about the number of
possible choices for the first word (i.e., k), decide that you will
allow k to vary but then impose hard constraint that the total
probability of all the choices you will allow in the first position is
p. Continue similarly for rest of the word positions, with the same

180
theory of deep learning
“probability budget.” You set p by trial and error to best match
perplexity to human text.
15.6
Instruction tuning
As mentioned, LLMs may have a good idea about language but may
not have a good understanding of participate in human conversation.
For instance, if the human asks Can you write a haiku that fits in a
tweet? The LLM may just response “yes” since it may not understand
that it is expected to provide such haiku. Instruction-tuning consists
of training on a dataset of (x, y) pairs where x is an instruction from
a human and y is a model answer. The machine is fed such (x, y)
pairs and is trained to minimize cross-entropy on the tokens in y.
At the end of this it is able to provide good answers to human
questions. Let’s call this distribution πSFT(y|x), where SFT stands for
‘supervised fine tuning.’
15.7
Aligning LLMs with human preferences
Although instruction tuning gives LLMs the ability to answer ques-
tions, in settings where the prompts are problematic, its answers may
not align with our notions of correctness, morality etc. For instance,
when asked to help plot a crime, instruction-tuned LLMs will easily
provide detailed instructions. Or when asked about an event that it
has not seen a data, it may hallucinate facts about this event because a
language model always has nonzero probability for all kinds of texts.
Let D be a distribution on such problematic prompts. Think of this as
giving a weighting to prompts according to how“tricky” they are.
The training dataset consists of human preference data consisting
of pairs17 (x, y1) ≻(x, y2) where x is a problematic prompt drawn
17 Humans tend to find it much easier
to give a preference between two
alternatives than to produce a ranking
among a larger set of alternatives.
according to , y1, y2 are two different answers to it, and y1 is prefered
to y2.
The famous Bradley-Terry model 18 for human preferences sug-
18 Ralph A. Bradley and Milton E.
Terry. Rank analysis of incomplete
block designs: I. the method of Paired
Comparisons. Biometrika, 1952
gests that such pairwise rankings correspond to a probabilistic sam-
pling from the following distribution:
p∗(y1 ≻y2 | x) = σ(r∗(y1|x) −r∗(y2|x))
(15.9)
where sig is the sigmoid function 19 and r∗is a so-called reward func-
19 The sigmoid function σ(t) =
1
1+e−t
=
et
1+et maps (−∞, ∞) to (0, 1).
tion that maps a (prompt, response) pair (x, y) to ℜ. The assumption
is that humans have such a reward function in their heads. If you
give a person two responses y1, y2 for prompt x then they prefer y1
over y2 according to the function above. 20 We express the reward
20 For instance, if the prompt is a ques-
tion asking for help committing a
horrendous crime, then most hu-
mans would strongly disapprove of
a response y|x that gives advice for
committing the crime. This can be in-
terpreted as as this response having
extremely negative value of r∗.
r∗(y1 | x) using conditional notation because the reward function is
only valid for comparing responses to the same prompt x. It can’t be

language models (lms)
181
used to compare rewards for a responses to two different prompts x
and x′. How can we learn this (unknown) reward function r ∗() in
human heads? The trick is to
Problem 15.7.1 (Learning reward model). Suppose we have a dataset D
consisting of (x, y1, y2) triples where x is a question, y1, y2 are two answers
and human raters have indicated (x, y1) ≻(x, y2). Show that the max-
likelihood fit for a reward function rϕ is
L(rϕ, D) = −E(x,y1,y2)∼D

log σ(rϕ(x, y1) −rϕ(x, y2))

(15.10)
We want to train a parametric model πθ(y|x) that acts according to
the learnt reward function r∗(). We can approach this via the follow-
ing thought experiment: Imagine that a prompt x is sampled from
D and the model generates a response according to πθ(y | x). Then
the machine gets a reward r∗(y | x). Thus a good model ought to be
optimizing
max
E
x∼D,y∼π(y|x)[r∗(y | x)].
(15.11)
However, this objective only involves the distribution D that favors
“tricky” prompts. To continue to be a good language model, πθ()
must not wander away far from πSFT(). Thus we must add a regular-
izer term to keep the distribution from deviating unnecessarily from
πSFT().
max
E
x∼D,y∼πθ(y|x)[r∗(y | x)] −β · KL(πθ(y | x)||πSFT(y | x)||). (15.12)
This is the essence of Reinforcement Learning with Human Feedback
(RLHF)21, which was for several years the prefered method to align
21 P. Christiano, J. Leike, T. Brown,
M. Martic, S. Legg, and D. Amodei.
Deep reinforcement learning from
human preferences. NeurIPS, 2017
language models with human values. We will not discuss in detail
the exact optimization technique and instead move on to a better
alternative below.
Problem 15.7.2. Show that if the parameterization θ for the policy has
unlimited size then the optimum solution satisfies
πθ(y | x) =
1
Z(x)πSFT(y | x) exp
 1
βr∗(y | x)

,
(15.13)
where Z(x) = ∑y πSFT(y | x) exp

1
βr∗(y | x)

is the partition function.
15.7.1
Direct Reward Optimization
The RLHF method above was often difficult to implement due to
optimization issues. Just recently a method called Direct Preference
Optimization (DPO) 22 has gained adherents because it leverages
22 Rafael Rafailov, Archit Sharma, Eric
Mitchell, Stefano Ermon, Christopher D.
Manning, and Chelsea Finn. Direct
preference optimization: Your language
model is secretly a reward model. arxiv,
2023
the same Bradley-Terry framework of preferences with a simpler
optimization.

182
theory of deep learning
The main idea is to skip the reward function and instead use
(15.7.2) to express the reward function directly using the model
πθ(y | x), and directly optimizing the human preference pairs
(x, y1) ≻(x, y2). Specifically, (15.7.2) implies that the optimum model
p∗(y | x) satisfies for all x
Pr[(y1 | x) ≻(y2 | x)] =
1
1 + exp

β log
π∗(y2|x)
πSFT(y2|x) −β log
π∗(y1|x)
πSFT(y1|x)

(15.14)
Problem 15.7.3. (DPO) Use the above to show that the following objective
captures the search for the best parametric model πθ(y|x) given preferences
(x, y1, y2)
max
θ
E(x,y1,y2)

log σ

β log
πθ(y1 | x)
πSFT(y1 | x) −β log
πθ(y2 | x)
πSFT(y2 | x)

.
(15.15)
DPO actually attaches a KL penalty to this objective just like
RLHF, and optimizes that. See the paper for implementation details
and experiments.
15.8
Mathematical Framework for Skills and Emergence
We now give the main theory component of this chapter, which is
a new mathematical framework for thinking about skills and how
they might relate to language comprehension tasks, and also the
emergence phenomenon. This is from a recent paper 23.
23 Sanjeev Arora and Anirudh Goyal.
A theory for emergence of complex
skills in language models. arXiv preprint
arXiv:2307.15936, 2023
First, it is assumed that language comprehension involves a set of
skills, though the theory will not need to know a precise list. (Schol-
ars have discovered and named thousands of skills. Well-trained
transformers have undoubtedly discovered many more that remain
unnamed.) Next, the theory will assume scaling laws such as (15.8)
and thus not need to reason about training and generalization. In-
stead, it can reason directly about the model’s behavior on the test
distribution, i.e., the distribution from which the training data was
drawn. We assume this test distribution is structured as a long un-
ordered list of text-pieces, each with an associated measure24 Tradi-
24 Text-pieces should be thought of as
having a size between a paragraph to a
few pages, drawn from a longer corpus.
To allow good prediction for the model,
the text-piece could include ancillary
text that preceded it the longer corpus.
The model need not do predictions
for the words in this ancillary text but
can use it to make predictions on the
text-piece.
tional cross-entropy loss is averaged using this associated measure.
Definition 15.8.1 (Text piece). The test corpus for the model is viewed
as being divided into text-pieces, each consisting of Ctest tokens. There is
also a measure µ2() on these text-pieces, with µ2(t) denoting the measure
of text-piece t. The usual cross-entropy loss is computed by weighting text-
pieces with respect to this measure.
Now we make some assumptions. We assume that the model’s
“comprehension” of a text piece is testable via suitable cloze ques-

language models (lms)
183
tions analogous to the Winograd example in Section 15.4. Specifically,
we assume that an (unknown) process cloze has been used to add
such cloze questions to the text pieces at test time. These are clearly-
marked multiple-choice questions in simple English that the model
has to answer. Note that the training corpus did not contain such
cloze questions, so this is a simple form of distribution shift at test
time. The prediction loss on cloze questions does not require predict-
ing the location or contents of the cloze question —it only requires
selecting the correct answer to the multiple-choice cloze question.
We allow the process cloze to tailor the questions to the model
being tested. Thus the next assumption is reasonable.
Assumption 15.8.2. [Cloze Sufficiency Assumption:] The pre-trained
model’s average (multiclass) prediction loss on Cloze questions
— where the average is taken over the distribution of text pieces–
closely tracks (within a small multiplicative factor like 1.1) the excess
cross-entropy of the model on classical next-word prediction.
Note: As discussed in Section 15.4, if the cloze question is assumed
to be perfectly answerable by a human then any incorrect answers
by the model can be interpreted analogously excess cross entropy.
Our assumption amounts to saying that mistakes on cloze questions
closely capture the excess entropy of the model as defined in (15.3).
The next theorem, shows that there exists a set of cloze questions (al-
beit fairly artificial) where the excess cross-entropy of the model’s
answer tracks the overall excess cross-entropy on next-word predic-
tion.
Theorem 15.8.3. If a model’s excess entropy at the ith place in text is ϵ
then there is a cloze question with binary answer such that the probability
that the model answers it incorrectly is at most
√
2ϵ.
Proof. The proof involves Pinsker’s Inequality (wikipedia version)
which relates variation distance and KL divergence. As in Sec-
tion 15.4 let pi() be the humans’ probability distribution for the
i + 1th word in the text piece and qi() be the model’s distribution.
The probability that the human and the model give different answers
is the variation distance between the two distributions, which is the
maximum (over all subsets A of words) of ∑w∈A(pi(w) −qi(w)). Let
Ai+1 denote the subset for which the previous expression is max-
imised. The cloze question consists of replacing word wi+1 in the text
with the question: Is the next word among the words listed in option (a) or
in option (b), where option (a) lists words in Ai+1 and (b) lists words
in Ai+1. The theorem now follows from Pinsker’s inequality.

184
theory of deep learning
15.8.1
Skills: A Statistical View
Language is assumed to have an underlying set S of skills. Every text-
piece t has an associated set of skills that are required for compre-
hending it. The theory allows this set of skills to be quite large —it
only needs to be (a fair bit) smaller than the number of text-pieces in
the distribution (an enormous number).
Definition 15.8.4 (skill graph). A skill graph is a bipartite graph
(S, T, E) where nodes in S correspond to skills, nodes in T correspond to
text-pieces, and (s, t) is in the edge set E if “comprehending” text-piece t
(i.e., answering its associated cloze questions) requires using skill s. (See
Figure ??)
It is important to realize that we are interested in quantifying the
model’s competence on a skill. For example, while the above defi-
nition assumes there the distribution of text-pieces includes those
whose comprehension requires the skill “anaphora resolution,” a
language model (or even human individuals!) will in general be un-
able to apply the skill correctly in all text pieces. Thus “competence
on anaphora resolution” is not 0/1 —instead it is quantified as the
fraction of text-pieces associated with this skill whose cloze questions
were correctly answered by the model. Quantifying the success rate
of this (in other words, the model’s capabilities) is the goal of the rest
of the paper.
The final element of our theory is that the skill-graph has ran-
dom edges, as made precise in Definition 15.8.5. To understand why
this makes sense, we recall Winograd’s example: The city councilmen
refused the demonstrators a permit because they feared violence. Wino-
grad implicitly assumes that the trickiest skill needed here is pro-
noun/anaphora resolution, but of course, applying that skill in this
context requires other skills: understanding of causality (i.e., inter-
pretation of “because”) as well as world knowledge about “city coun-
cilmen,” “permit,” “demonstrators,” etc. This example highlights
the fact that if we were to look at random text-pieces that require
pronoun disambiguation, we would encounter random real-world
scenarios, whose comprehension requires very different set of skills.
Moreover, the scenarios (and hence the relevant skills) could have
different probabilities of occurring in the corpus.
For simplicity we assume that each text-piece requires exactly k
skills for some k, and this set was drawn by iid sampling from an
underlying measure on the set of skills. (Thinking of k as a random
variable is natural but will not be considered here.) The next defini-
tion formalizes the above framework in form of a skill cluster.
Definition 15.8.5 (Degree-k skill cluster). This is a skill graph (S, T, E)

language models (lms)
185
where the collection of text pieces is generated by “nature” by applying
the following process: pick a subset of k skills via iid sampling from an
underlying measure µ1 on skills, and then use a procedure gen to create a
text-piece t whose comprehension requires these skills, as well as a measure
µ2(t) associated25 with this text piece t. Then nature uses process cloze
25 Note that the measure on text-pieces
has to have the correct marginals
e.g., the µ2-measure of all text-pieces
containing a skill s is µ1(s). There are
many measures satisfying this weak
condition, since the number of text
pieces is way larger than the number of
skills.
to add cloze prompts to test comprehension on t. The prediction loss on
the text-piece is the cross-entropy loss on predicting the answers to the cloze
questions in it. The average prediction loss over all text-pieces is computed
with respect to the measure µ2(). We call the skill-graph thus produced a
degree-k skill cluster.
Now we formalize a simple model of what the full text corpus
looks like. More complicated extensions of this framework (e.g.,
considering a hierarchy among corpora) are left for future work.
Definition 15.8.6. (Text corpus) The text corpus consists of many skill
clusters (e.g., math, newspapers, science, coding, etc.) (S, T1, E1), (S, T2, E2), . . .
which share the same underlying set of skills S but have disjoint sets of text-
pieces T1, T2, . . . that are generated as in Definition 15.8.5.
Definition 15.8.5 allows us to define “competence on a skill” in
the more familiar setting of statistical learning theory, specifically
by letting us associate a statistical task with it. The task involves
predicting answers to cloze questions in a sub-distribution of text
pieces that contain that skill. Our emergence theory will apply to the
family of tasks of the next definition.
Definition 15.8.7 (Competence on Skills). In the setting of Defini-
tion 15.8.5, for each skill cluster and each skill s ∈S statistical task τs
corresponding to s and this cluster is defined as follows. The learner is
given a text-piece created by sampling s1, . . . , sk−1 via iid sampling (k −1)
times from measure µ1, and applying gen and cloze to the skill-tuple
(s, s1, . . . , sk−1) to convert it into a text piece t with an associated mea-
sure µ2(t) (but the measure is re-scaled so that the total measure of the
inputs to this task τs is 1). The error rate of the model at the statistical
tasks is the expected prediction loss on text-pieces drawn from the above
distribution. Since error rate is between 0 and 1, the competence refers to
(1 −error rate).
For every k′-tuple of skills (s1, s2, . . . , sk′) (where k′ ≤k) the statistical
task τs1,s2,...,sk′ corresponding to that k’-tuple is similarly defined. The inputs
to the task are generated by completing the k′-tuple to a k-tuple⃗s by iid
sampling of k −k′ additional skills from µ1 and then using gen and cloze
to convert it into a text-piece.
Competence on the k′-tuple is defined just as above. 26
26 Thus if a text-piece involves 5 skills,
then that text-piece will appear in 5
statistical tasks corresponding to indi-
vidual skills, (5
2) tasks corresponding
to pairs of skills, and so on. How-
ever, our method of measuring the
loss incurred on these statistical tasks
implicitly assumes that if the model
incorrectly answered this cloze question
(i.e., it assigned significant probability
to the wrong answer), then that loss
was incurred in all these statistical
tasks. This is a conservative estimate of
competence on skills.
Note: The definition involves the k-tuple being picked by iid sam-
pling from µ1 which, in principle, allows a skill to be picked twice.

186
theory of deep learning
However, the probability of picking the same skill twice scales as
O(1/|S|). Since the set of skills S is assumed to be large, the distri-
bution is almost the same as sampling distinct k-tuples of skills. The
small difference of O(1/|S|) between the two methods will not affect
any of the random graph theory calculations.
15.9
Analysis of Emergence (uniform cluster)
We have arrived at a core mathematical issue around emergence: As
the model’s excess cross entropy goes down (due to scaling), this improves
the model’s performance on cloze tasks inserted in the test stream (Assump-
tion 15.8.2). How does this improve competence on the skills as well as on
tuples of skills –in other words, performance on the associated cloze ques-
tions?
This section analyzes a simple setting where the test-stream con-
sists of a single degree-k skill cluster, and the skills are uniformly dis-
tributed and so are the text-pieces—in other words, the distributions
µ1 and µ2 in Definition 15.8.5 are uniform. Section ?? will extend the
analysis to the general setting. The calculations below only require
the total number of skills to be much less than the support size of the
distribution of text—in other words, the set of skills can be extremely
large.
Let’s say the model makes a mistake on a text-piece if the total pre-
diction loss on all the cloze-questions27 of that text-piece is at least
27 This is the amount of error incurred
if the incorrect answer is chosen with
noticeable probability on even a single
cloze question).
1/2. As the model is scaled up, there are two distinct phases.
Phase 1: In every text-piece, the error on its cloze questions exceeds 1/2 .
Thus the model has not developed competence in any skill, since it
is making mistakes in every text-piece.
Phase 2: On the average text-piece, the total error on all cloze questions is less than 1/2.
Now one begins to see nontrivial competence on some skills. The
analysis below will kick in.
If the average cross-entropy loss for the text-pieces is δ we con-
clude Y consists of at most 2δ fraction of text pieces. The following
result guarantees that statistical tasks corresponding to most skills do
not assign significant probability to text pieces in Y –in other words,
the model has good performance on statistical tasks connected with
these skills.
Theorem 15.9.1 (Basic). Let α, β, θ > 0, β > 1, αβ < 1, θ < 1 satisfy
H(θ) + kθ

H(βα) −βα log 1
α −(1 −βα) log(
1
1 −α)

< 0
(15.16)
and the distribution on skills and text pieces be uniform in the skill-cluster.
Then irrespective of the details of gen and cloze processes, the following

language models (lms)
187
property holds for every subset Y of text pieces that contains at least θ frac-
tion of text pieces: at least 1 −α fraction of skills have at most βθk|T|/|S|
edges to Y (in other words, at most β times the number of edges a skill
would be expected to have to text-pieces in Y).
Note: Since edges between a skill node s and set Y correspond to er-
rors in the statistical task τs, Theorem 15.9.1 is giving a lower bound
bound for competence on (1 −α) fraction of skills: it is at least 1 −βθ.
15.9.1
Proof of Theorem 15.9.1.
The proof uses the famous Probabilistic Method 28. Here, one is trying
28 N Alon and J Spencer. The Probabilistic
Method (4th Ed). Wiley, 2016
to show that in a certain probability space, there are no bad outcomes.
Denoting by W an integer random variable denoting the number of
bad outcomes, if we show that E[W] ≈0, then it follows that W = 0
with probability at least 1 −E[W]. Concretely, in the proof, W will be
the number of “bad” set pairs (Y, Z) of a certain size that violate the
lemma.
Proof. For Y ⊆V1, |Y| = θ|T| and Z ⊆S, |Z| ≤α|S| we say that (Y, Z)
are bad if Z has at least αβθk|T| edges to Y. Let W denote the number
of such Z’s. The expectation is upper bounded by
|S||T|
 |S|
α|S|

×
 |T|
θ|T|

×
 kθ|T|
βαkθ|T|

× αβαθk|T| × (1 −α)(1−βα)θk|T|
(15.17)
For (15.17) to be ≪1 it suffices for its logarithm to be negative. By
Stirling’s approximation ( N
tN) ≤2(H(t)+ϵN)N where H(t) = −t log t −
(1 −t) log(1 −t) is the binary entropy function and ϵN goes to zero
rapidly as N →∞. Applying this to (15.17) and taking logarithms,
and assuming |S| ≪|T|, we arrive at the condition (15.16) for large
|T|.
15.9.2
Competence on skills and tuples of skills: Performance Curves
Definition 15.9.2 (performance curve). The contour plot (i.e., the bound-
ary) of the region of α, β combinations satisfying Theorem 15.9.1 is called a
performance curve and denoted C(k,θ). A performance curve C is better
than another curve C′ if for every α, β on C there is a corresponding point
(α, β′) on C′ for β′ > β.
We draw performance curves by ploting (1 −α) on the horizon-
tal axis and the vertical axis plots βθ, so point (0.8, 0.16) on a curve
means at least 0.8 fraction of skills have at most 0.16 fraction of their
edges in the “error set” Y (hence 0.84 fraction of their edges are out-
side the error set). The emergence curves shift down noticeably (i.e.,

188
theory of deep learning
imply emergence of more skills) as we increase k. The next lemma
shows this trend always holds.
Lemma 15.9.3 (Monotonicity). If θ′ < θ then the performance curve for
θ′, k lies below that for θ, k.
If k′ > k then the performance curve of θ, k′ lies below that for k, θ.
Proof. Follows from the fact that H(θ)/θ is a decreasing function in
the interval (0, 1).
Figure 15.2: Performance
Curves: Left plot has θ
=
0.1
and varies k = 2, 4, 8, 16. Higher
values of k greatly improve per-
formance (for k
=
2 valid α, β
did not exist). The right plot
has k = 8 and θ = 0.05, 0.1, 0.2.
Section ?? clarifies that it also
describes the model’s perfor-
mance curve for t-tuples of
skills for for θ
=
0.05 and
t = 1, 2, 4 respectively (e.g., blue
curve for 4-tuples).
15.9.3
The tensorization argument
While the above method yields performance curves, better curves
can be derived via a tensorization argument. Consider the follow-
ing k′-wise recombination operation on the test stream. First randomly
partition the test stream into subsets of size k′, and then concatenate
the k′ text pieces within each subset to create a larger piece of text
that we refer to as a “k′-piece,” and whose measure is the sum of the
measures of the component test-pieces. All cloze questions for the
old test-pieces are retained and no new cloze questions are inserted.
Clearly, if the error of the model per average text-piece was δ, then
the error per average b-piece is k′δ. However, each k′-piece is now us-
ing a random k′k-tuple of skills. Importantly, this set of k′k skills con-
sists of iid draws from the skill distribution, which can also be seen
as a k-tuple of k′-tuples. Thus viewing k′-tuples of skills as ‘complex
skills’ we can use these complex skills as the skill set in the setting
of Theorem 15.9.1, which gives us an easy corollary quantifying the
performance on tasks corresponding to k′-tuples of skills.
Lemma 15.9.4 (Emergence for k′-tuples of skills). Consider the skill-
graph (S′, T′, E) where S′ consists of all k′-tuples of skills, T′ consists of
k′-pieces, and E consists of (s′, t′) where s′ is a k′-tuple of skills and t′ is
a k′-piece where this tuple of skills is used. Let Y consist of θ fraction of
k′-pieces. Then for any α, β > 0, β > 1, αβ < 1 satisfying (15.17) there
are at least 1 −α fraction of k′-tuples of skills that have at most αβθθN1 βθ
fraction of their edges connected to Y.

language models (lms)
189
The next problem asks you to derive a somewhat surprising gen-
eral principle that’s also hinted at in caption of Figure 15.2. Assume
(for simplicity) a Chinchilla-like scaling law that 10x up-scaling leads
to factor 2 reduction in excess entropy. If a model is considered to
have reasonable performance on individual skills at current scaling,
then after further up-scaling of 10x one would see similar reasonable
performance on skill-pairs, and scaling up by yet another 10x after
that will yield similar reasonable performance on 4-tuples of skills,
etc. Note that these are provable lower bounds on performance gains—
actual gains could be higher. Figure 15.2 illustrates the phenomenon.
Problem 15.9.5. Show that when a model M1 with loss δ is scaled up (e.g.,
as per equation (15.8)) so that the new model M2 has loss δ/k′, then the
performance curve inferred by our method for k′-tuples of skills using M2 is
identical to the curve inferred for individual skills on model M1.


16
Generative Adversarial Nets
Chapter 14 described some classical approaches to generative models,
which are often trained using a log-likelihood approach. We also saw
that they often do not suffice for high-fidelity learning of complicated
distributions such as the distribution of real-life images. Generative
Adversarial Nets (GANs) is an approach that generates more realistic
samples. It relies upon power of deep nets at discriminative tasks.
For convenience throughout this chapter we assume the data of in-
terest are images, which we think of as points in ℜd for some d. The
model is trying to generate realistic images.
Before developing the theory of GANs we survey various notions
of how to test similarity of two distributions, because that discussion
feeds directly into the setup used in GANs. This is relevant because
Example 14.2.1 illustrated how the the notion of generalization from
supervised learning can lead us astray when it comes to reasoning
about correctness of distribution learning.
16.1
Distance between Distributions
How can we measure how different two distributions P and Q are?
If we have access to a formula for computing the density of each
distribution, then one can compute an f-divergence for any suitable
f : ℜ→ℜthat is convex.
Df (P||Q) =
Z
f ( P(x)
Q(x))Q(x)dx
(16.1)
Problem 16.1.1. (i) Show that f-divergence is nonnegative. (ii) Show that
the f-divergence for f (t) = t log t coincides with KL(P||Q). (iii) Show
that the f-divergence for f = 1
2|t −1| coincides with total variation (or ℓ1)
distance: |P −Q|1 = R |P(x) −Q(x)|dx.
However, in practice one doesn’t have a formula for the probability
density function, and must estimate distance using samples from P

192
theory of deep learning
and Q. A natural idea is to compare the expectation of a class of test
functions on the two distributions.
Example 16.1.2. The expectation EP[x] is the mean of the distribution
P. Similarly expectation of monomials of form xi1xi2 · · · xik constitutes the
kth moment. Moments can be estimated from samples (under fairly general
conditions) and the difference of moments of distributions P, Q can be seen
as some measure of their difference.
Unfortunately, accurate estimation of all higher moments for mul-
tivariate distributions gets expensive with respect to both computa-
tion time and sample complexity. This motivates a notion of distance
that arises in transportation metrics. 1 If F is a class of functions, then
1 Please look up Wasserstein metrics
and Earth-Mover distance on the
internet.
define the distance between P and Q using the highest different in
expectation achievable over functions in F.
d(P, Q) = supf ∈F| E
x~P[ f (x)] −E
x~Q[ f (x)]|
(16.2)
For example F could be polynomials of degree at most k.
Problem 16.1.3. Show that the distance defined above satisfies triangle
inequality.
16.2
Introducing GANs
Generative Adversarial Nets (Goodfellow et al.2) is a framework to
2 I Goodfellow, J Pouget-Abadie,
M Mirza, B Xu, D Warde-Farley,
S Ozair, A Courville, and Y Bengio.
Generative Adversarial Networks.
NeurIPS, 2014
learn generative models via the definition in (16.2). Specifically, one
tries to train a generative model G, which (as in Chapter 14) produces
an image G(u) using random vector u. This yields a distribution on
images, and we check the quality of this distribution by using a class
F of deep nets (with a fixed size and architecture) and estimate the
distance in (16.2) by trying to find a net that is a maximiser of the
expression. Now we spell out the main components of GANs.
Idea 1: Since deep nets are good at recognizing images —e.g., distinguish-
ing pictures of people from pictures of cats—why not let a deep net be the
judge of the outputs of G()?
More concretely, suppose images are represented as vectors in ℜd
and let Preal be the distribution over real images. The generator G
has learned to generate synthetic images from a new distribution
Psynth (i.e., the distribution of G(h) when h is a random seed). We
could try to train a discriminator deep net D that maps images
to numbers in [0, 1] and tries to discriminate between these dis-
tributions in the following sense. Its expected output Ex[D(x)] as
high as possible when x is drawn from Preal and as low as possi-
ble when x is drawn from Psynth. The discriminator can be trained

generative adversarial nets
193
with standard supervised learning (e.g, regression) with two la-
bels. If Psynth = Preal then of course no classifier can achieve a
gap in this expected output, and so the training will fail. If, on
the other hand, we are able to train a good discriminator deep net
—one whose average output is noticeably different between real
and synthetic samples— then this is proof positive that the two
distributions are different. 3
3 There is an in-between case, whereby
the distributions are different but
the discriminator net doesn’t detect
a difference. This case is going to be
important in the story very soon.
Idea 2: If a good discriminator net has been trained, use it to provide “gra-
dient feedback”that improves the generative model.
The natural goal for the generator is to make Eh[D(G(h))] as high
as possible, because that means it is better able to fool the discrim-
inator D. Given a fixed D the natural way to improve G is to pick
a few random seeds h, and slightly adjust the trainable parameters
of G to increase this objective. Note that this gradient computation
involves back-propagation through the composed net D(G(·))). 4
4 These updates to G assume D is fixed
and vice versa. Many papers have
suggested alternative update methods
that anticipate D’s response to this
update, and show evidence that this
makes training more stable in practice.
See Problem 16.2.1.
Idea 3: Turn the training of the generative model and the accompanying
discriminator net into a game of many moves (i.e., rounds of parameter
updates).
Each move for the discriminator consists of taking a few sam-
ples from Preal and Psynth and improving its ability to discriminate
between them. Each move for the generator consists of produc-
ing a few samples from Psynth and updating its parameters so that
Eu[D(G(h))] goes up a bit.
Notice, the discriminator always uses the generator as a black
box —i.e., never examines its internal parameters —whereas the
generator needs the discriminator’s parameters to compute its
gradient direction. Specifically, the gradient for G is computed by
backpropagating through D. Also, the generator does not ever use
real images from Preal for its computation. (Though of course it
does rely on the real images indirectly since the discriminator is
trained using them.)
One can fill in the above framework in multiple ways. The most
obvious is that the generator could try to maximize Eu[ f (D(G(h)))]
where f is some increasing function. (We call this the *measuring
function.*) Concretely, if D, G are deep nets with specified architec-
ture and whose number of parameters is fixed in advance by the
algorithm designer, then the training objective is:
min
G max
D
Ex∼Preal[ f (D(x))] + Eh[ f (1 −D(G(h)))].
(16.3)
Problem 16.2.1. (1) Write an expression for updates for G and D for the
loss in (16.3) when f is the identity map (i.e. f (x) = x). (2) Write an
expression where G and D anticipate the effect of the other’s immediate
response to their update. (This can be done in more than one way.)

194
theory of deep learning
Effect of f: The measuring function has the effect of giving different
importance to different samples. Goodfellow et al. originally used
f (x) = log(x), which, since the derivative of log x is 1/x, implicitly
gives much more importance to synthetic data G(u) where the dis-
criminator outputs very low values D(G(h)). In other words, using
f (x) = log x makes the training more sensitive to instances which the
discriminator finds terrible than to instances which the discriminator
finds so-so. By contrast, f (x) = x gives the same importance to all
samples and results in Wasserstein GAN.
Problem 16.2.2. Show that if the discriminator has unbounded capacity
(i.e., able to compute any function) then for f (x) = log x the optimum
value of the expression in (16.3) is the following quantity (called Jensen-
Shannon Divergence) where µ = Preal, ν = Psynth and KL was defined in
Section 5.6: 5
5 Hint: The optimum D will be unre-
alistically powerful: given input x its
output depends on the probabilities
Preal(x), Psynth(x).
KL(µ∥µ + ν
2
) + KL(ν∥µ + ν
2
).
16.2.1
Game-theoretic interpretation and implications for training
A serious practical difficulty in implementing training as above is
that it can be oscillatory, meaning the above objective can go up and
down. This is unlike usual deep net training, where training (at least
in cases where it works) steadily improves the objective. The reason
is that implicitly the discriminator and generator are playing a 2-
person zero sum game 6 where their “moves ” are the two circuits
6 Please read up about zero sum games
online, including the famous Min-Max
Theorem about equilibria.
D, G and the payoff from generator (minimizer) to discriminator
(maximizer) is the loss. Thus generator is picking moves to minimize
the following payoff
max
D
Ex∼Preal[ f (D(x))] + Eh[ f (1 −D(G(h)))]
whereas discriminator is maximising
min
G
Ex∼Preal[ f (D(x))] + Eh[ f (1 −D(G(h)))].
Such games do not always have an equilibrium, i.e., a point where
both players are reacting optimally to the other and thus lack an
incentive to change. (It is akin to a saddle point in optimization.)
Although equilibrium may not exist when viewed as a two-person
game, of course during training both players are under the control
of the training algorithm. Thus suitable modifications to gradient-
based training could conceivably allow convergence to some solution
even though it is not an equilibrium. (For example, even if D is not
optimal response to the current G, gradients may not allow a way to
improve on the current D.) An extensive list of papers present such
ideas; some examples are: need some references here

generative adversarial nets
195
16.3
"Generalization" for GANs vs Mode Collapse
Section 14.2.1 discussed complications arising when we try to learn
distributions from finite samples. In the GAN setting the training ob-
jective (16.3) was described using the full distribution but of course in
practice the discriminator D is discriminating between finite samples
of the two distributions.
Usually in machine learning good generalization means that the
average loss function on test dataset is similar to that on the train-
ing dataset. However, we saw that for the usual loss functions like
log-likelihood, this notion of generalization does not imply that the
distribution has been learned well. After GANs were introduced
there was extensive study of whether GAN approach could bypass
these issues, and in this effort a large number of training objectives
and algorithms were tried. It was noted that they were learning the
distribution fairly imperfectly7 but it was unclear whether this would
7 A representative problem is mode
collapse: the learnt distribution does
not appear to have the same amount of
diversity as the
go away with bigger training datasets or different objectives.
Example 16.3.1. Since the objective (16.3) allows maximization over all
neural nets D of the allowed architecture, even two different samples from
the same distribution can look very different to a neural net. For example if
we take two finite samples from the d-dimensional Gaussian N (0, I) then
even if the samples have size say d3, they are distinguishable by a deep net
that is somewhat larger than d3. The reason is that the samples are two
discrete sets in which all pairs of points are almost orthogonal (with high
probaility). While this is an artificial example, it is the case that in real-life
GAN training, the objective does not usually drop to zero —the net is indeed
able to somewhat distinguish the samples from the two distributions.
We now describe theoretical analysis from 8 showing that the
8 Sanjeev Arora, Rong Ge, Yingyu
Liang, Tengyu Ma, and Yi Zhang.
Generalization and Equilibrium in
Generative Adversarial Nets (GANs).
Proc. ICML, 2017
quality of the learnt distribution is inherently limited by the represen-
tational capacity of the discriminator regardless of how large we make the
the training dataset.
As usual when discussing generalization, let us assume the net
has some finite size, say N, and the sample size of the distributions
is large enough for nets of size N to generalize. The following two
problems are drawn from
Problem 16.3.2. Suppose the loss is C-Lipschitz and the parameter vector
is in ℜd and has ℓ2-norm at most L. Suppose this discriminator trained
on a training set of size N achieved training loss at most ϵ1. Show that if
N > Ω( L
ϵ2
2 log(C/ϵ2)) then it has test loss at most ϵ1 + ϵ2 on the full
distribution. 9
9 Hint: Use the results of Chapter 5, eg
Theorem 5.2.7.
Does this imply that GANs actually learn the distribution? No,
just as in Example 14.2.1: generalization only implies training and
test loss are close, not that the distributions are close.

196
theory of deep learning
Problem 16.3.3. Under the same conditions as Problem 16.3.2 show that if
the learned distribution Psynth has finite support and is a uniform distribu-
tion on Ω( L
ϵ2
2 log(C/ϵ2)) random iid samples from Preal then no discrimina-
tor can achieve test loss more than ϵ1 + ϵ2 when comparing the distributions
Psynth and Preal.
In the previous problem, let’s think of Preal as the distribution on
all real-life images. Then Psynth is quite different from Preal: it is the
uniform distribution on some small set of real-life images. Never-
theless no discriminator (whose size, norm and Lipschitz constant
are suitably upper bounded) can distinguish between the two distri-
butions. As we will see, such a Psynth is indeed observed in practice.
Furthermore, changes to GANs architecture (e.g. Encoder-Decoder
GANs) do not affect this basic result 10.
10 Do GANs learn the Distribution?
Some Theory and Empirics
The following exercise gives a (probably very loose) upper bound
on the size of a generator that can produce such a Psynth.
Problem 16.3.4. Show that the uniform distribution on M images where
each image is in ℜd an be generated using a net with O(Md2) parameters.
11
11 More precisely the distribution will be
a mixture of gaussians of tiny variance
centered at the M images.
Putting together the previous three problems we conclude that in
the following scenario the GAN objective is insufficient to prevent
the verifier from learning a distribution supported on a small finite
set of images ("mode collapse"): The generator has somewhat larger
"capacity" than the discriminator. 12
12 It is an open question whether mode
collapse can be avoided when the
discriminator is much larger, although
in that case usually the training loss is
hard to reduce, for reasons explored in
Example 16.3.1.
16.3.1
Experimental verification of Mode Collapse: Birthday Paradox
Test
The theory above suggests that GANs trained using discriminators of
a certain “capacity”(in the sense of generalization theory) have solu-
tions with low training and test error where the synthetic distribution
Psynth is supported on a small set of images and thus is quite different
from Preal). This phenomenon of the GAN ending up with a synthetic
distribution consisting of a small set of images is called mode collapse
and was earlier believed to be a result of either failed training or us-
ing a training set of real images that is too small. The results above
suggested that mode collapse is not a surprising outcome with gener-
ators and discriminators of low-ish capacity (as opposed to capacity
that scales with the number of distinct modes in Preal.)
This raised a question whether we can detect such model col-
lapse in real-life GANs. In other words, estimate how many “dis-
tinct”images it can produce. At first glance, such an estimation seems
very difficult. After all, automated/heuristic measures of image simi-
larity can be easily fooled, and we humans surely don’t have enough

generative adversarial nets
197
time to go through millions or billions of images, right?
Luckily, a crude estimate is possible using the simple birthday
paradox, a staple of undergrad discrete math.
Problem 16.3.5 (Birthday paradox). 13 Consider a uniform distribution
13 Do GANs learn the Distribution?
Some Theory and Empirics
on a set of size N. Show that a random sample of size 2
√
N contains a
duplicate probability at least 1 −1/e. (The name for this paradox comes
from its implication that if you put 23 random people in a room, then the
odds are good that two of them have the same birthday is significant.)
Let’s realize the implications for GANs. Imagine for argument’s
sake that Preal is the distribution on pictures of faces. What is the
number of modes in this distribution? At a minimum it is the num-
ber of distinct human faces? This feels like a rather large set, because
all of us know tens of thousands of faces (including those encoun-
tered in the news) and don’t see any unrelated doppelgangers; only
identical twins. More precisely, the birthday paradox says that if the
number of distinct human faces is N then we would expect to have
seen doppelgangers after having seen
√
N faces.
In the GAN setting, the distribution is continuous, not discrete.
Thus our proposed birthday paradox test for GANs 14 is as follows.
14 Sanjeev Arora and Yi Zhang. Theo-
retical Limitations of Encoder-Decoder
GANs Architectures. Proc. ICLR, 2018
(a) Pick a sample of size s from the generated distribution. (b) Use
an automated measure of image similarity to flag the 20 (say) most
similar pairs in the sample. (c) Visually inspect the flagged pairs and
check for images that a human would consider near-duplicates. (d)
Repeat.
If this test reveals that samples of size s have duplicate images
with good probability, then suspect that the distribution has support
size about s2.
16.3.2
Other notes on GANs and mode collapse
While recent GANs (such as Progressive GAN) use very large dis-
criminators and generators to produce images of better visual quality
(as judged by humans) they still suffer from mode collapse, in line
with the above theory.
On the other hand, Florian et al 15 argue that the above analysis
15 F Schaefer, H Zheng, and A Anand-
kumar. Implicit competitive regulariza-
tion in GANs. ICML, 2020
takes assumes static near-equilibrium in training, whereas real-life
training never arrives at an equilibrium, and the training dynamics
resulting from non-equilibrium can act as a power shaper of the
GAN’s behavior.
A recent paper 16 shows that even though GANs suffer from mode
16 Y Zhang, A Gupta, N Saunshi, and
S Arora. On predicting generalization
using gans. ICLR, 2022
collapse, they can be used to predict generalization. In other words,
given a dataset S and a discriminative model trained on it, use the
following predicter of generalization error: train a conditional GAN

198
theory of deep learning
using S and use random samples from the trained GAN in lieu of
held-out data to predict generalization. This is shown to work better
than other predictors of generalization error.
The basic idea of GANs has been extended for other settings, most
notably to learn good image to image maps (e.g., changing a photo-
graph to a painting, or virtually trying on an article of clothing on the
image of a person). This works very well and there is no analog of
the mode collapse result.

17
Self-supervised Learning
Semantic representations (aka semantic embeddings) of complicated
data types (e.g. images, text, video) have become central in machine
learning, and also crop up in machine translation, language models,
GANs, domain transfer, etc. These involve learning a representation
function f such that f (x) is a compact and “high level” representa-
tion of datapoint x —meaning it retains semantic information while
discarding low level details — e.g., the colors of individual pixels in
an image. The test of a good representation is that it should greatly
simplify solving new classification tasks, by allowing them to be
solved via linear classifiers (or other low-complexity classifiers) using
small amounts of labeled data.
Researchers are most interested in unsupervised representation
learning using unlabeled data. A popular early example is word em-
beddings which used simple linear algebra 1 and proved useful in
1 LSI paper
information retrieval for several decades. More recent word em-
beddings such as word2vec2 became the inspiration for semantic
2 word2vec paper; see wikipedia page
of word2vec for links to other similar
algorithms
embeddings of diverse data types such as molecules, social networks,
images, text etc.
In this chapter we encounter self-supervised learning, a family of
methods for learning good representations. Working with unlabeled
data, the learner defines a learning objective for finding a good repre-
sentation function. An important difference from learning paradigms
studied elsewhere in the book is that the training and test tasks are
different, and hence the notion of generalization does not capture the
final goal of learning. This is an example of training on task A to later
do well on task B, which one imagines is actually an important aspect
of intelligent behavior.


18
Adversarial Examples and efforts to combat them
While modern deep nets exhibit superhuman accuracy at solving
classification tasks on images, they have a surprising Achilles heel
that was first reported in 1: for most correctly classified images x
1 C Szegedy, W Zaremba, I Sutskever,
J Bruna, D Erhan, I Goodfellow, and
R Fergus. Intriguing properties of
neural networks. In ICLR, 2014
there is a small perturbation vector δ such that x + δ is mis-classified
by the deep net, and yet to humans x + δ looks pretty similar to
x.
These are called adversarial examples; note that δ is specially
Figure 18.1: Flying pigs? (A)
is image of a pig, and (B) is a
slightly perturbed version of it.
A normally trained ResNet50
classifier labels (B) as "airliner."
The difference between the two
images is tiny; in (C) you see an
image that is 50 times the pixel-
wise difference between (A)
and (B). Without the 50x scaling
(C) would consist of pixels with
values close to 0 (i.e., blank
image). Source: Kolter-Madry
Tutorial.
constructed given x using an optimization technique, so x + δ is
not from the usual input distribution that the classifier was trained
on. Still it is striking that x + δ is misclassifier despite looking like a
normal image to us humans.
Adversarial examples have been extensively documented in a
variety of datasets and neural architectures. Powerful methods (based
upon optimization) have been discovered to find such examples. The
concepts and definitions of the current chapter closely track those of
Kolter and Madry’s online tutorial 2. Because the phenomenon hints
2 Kolter Z and Madry M. Adversarial
robustness –theory and practice
at fragility of current ML-based systems, myriad attempts have been
made to mitigate this issue by changing training protocols, though
progress has been slow.
Figure 18.2: An adversarial 3D
object! This stop sign with a
few stickers on it reliably fooled
image recognition classifiers
to classify it as a 45mph speed
limit.
18.1
Basic Definitions
The classifier f : Rd →Y maps inputs to labels in a finite set Y.
There is an allowed set of perturbations ∆⊆ℜd. In this chapter we
assume ∆is the set of vectors of ℓp norm at most ϵ for some p, where

202
theory of deep learning
p is one of 1, 2, ∞. The adversary has to find a δ ∈∆such that f (x +
δ) ̸= f (x).
The targeted adversary is given a specific target label y′ ̸= f (x) and
has to find a δ ∈∆such that f (x + δ) = y′. Black box attacks involve an
adversary that does not know the internal parameter vector of f; the
adversary can only provide inputs to f and see the answer. White box
attacks allow the adversary access to the internal parameter vector.3
3 While black box attacks may appear
hopeless, in practice they do exist.
Adversaries generate adversarial
images using white box attacks on their
own deep nets trained on the same
dataset. These images are able to fool
other nets with unknown architecture
and parameters. Success of black box
attacks hints that different architectures
are fairly similar in their classification
behavior.
We will focus on white box attacks. We assume there is a natural loss
function ℓ(w, x, y) giving the loss of classifier w on input x and label
y.
Now we describe the basic attack and defense methods.
18.1.1
Attack method: PGD
A representative (and popular) attack method uses Projected Gradient
Descent (PGD) 4 where Projx0+∆(x) is the closest point of type x0 + δ
4 A Madry, A Makelov, L Schmidt,
D Tsipras, and A Vladu. Towards
Deep Learning Models Resistant to
Adversarial Attacks. ICLR, 2018
to x, where ℓp norm of the perturbation δ is no more than ∆. Further-
more, although x has label y, the label assigned by the classifer to
x0 + δ is y′.
Note that if norm is ℓ2, then Projx0+∆(x) is simply x0 + ∆(x−x0)
|x−x0|2 .
Problem 18.1.1. Give methods to compute Projx0+∆(x) for norms ℓ1 and
ℓ∞.
Now we can describe the most popular method to find adversarial
examples.
PGD method: Given input x0, do the following iteration k times:
x ←x + η∇xℓ(w, x, y), followed by x ←Projx0+∆(x).
Note that the gradient is with respect to the input x, and not the
parameter vector w! It can be computed by a simple modification of
backpropagation. 5
5 Often attacks use so-called sign gradi-
ent, which rounds all positive coordi-
nates to +1 and negative coordinates to
−1.
For targeted attacks one can use ∇xℓ(w, x, y) −∇xℓ(w, x, y′).
Figure 18.3: PGD attack on
input x0. The red arrows cor-
respond to gradient-based
updates. When they produce
a point outside Ball(x0, r) a
projection operation (denoted
by the green arrows) finds the
closest point in the ball.
Deep nets trained in the usual way are very susceptible to such
attacks. For most inputs x, algorithms such as the one above can find
a close-by point x + δ the classifier outputs a different label.
18.1.2
Adversarial Defense
To make the net somewhat resistant to the above attack, one has to
train it differently. Specifically it is trained using adversarial exam-
ples and taught to classify them correctly. Using a batch of inputs,
the adversary (such as the one above)is used to generate adversarial
examples. The parameters are now adjusted to make the classifier
output the correct label on these examples. Then the adversary is
used to generate a new set of adversarial examples for the newly ad-
justed parameters. This back and forth is repeated some number of

adversarial examples and efforts to combat them
203
times. Essentially the classifier is being trained to move the decision
boundary away from the datapoints; see Figure 18.4.
The above prototocol has many variants, but the end result is a
utility/robustness trade off: the ability of the adversary to find ad-
versarial examples is blunted a fair bit, so that instead of being able
to flip the answer for almost all inputs x, it can only do so for, say,
40% of the inputs. The downside is that in the process the classifier’s
overall accuracy on the original dataset (i.e., the usual inputs) drops a
fair bit as well (say, from 95% to 80%). Thus the robust classifier has
significantly lower utility for the non-adversarial setting.
Figure 18.4: Conceptual illus-
tration of adversarial examples
for ℓ∞-bounded perturbations.
In the vanilla classifier most
datapoints are close to the de-
cision boundary, as measured
by ℓ∞distance. The red stars
are adversarial examples. After
adversarial training, a small
ℓ∞-ball around the datapoint no
longer intersects the decision
boundary. Credit: Madry et al
2018.
Realize moreover that at the end of training the classifier only has
the ability to evade adversarial examples generated by the particular
adversary (i.e., attack algorithm) it trained with. Often using a differ-
ent attack algorithm new adversarial examples can be generated. This
has been shown many times, and is a very frustrating state of affairs
indeed.
18.1.3
Other defense ideas
A lot of energy was spent on trying to avert adversarial attacks by
building in some form of obfuscation inside the classifier, usually by
performing a non-differentiable transformation inside the classifier
net. The motivation is to make it difficult to implement the gradient-
based attacks mentioned above. However, most such defenses were
broken with some ingenuity; for an introduction see 6.
6
18.2
Provable defense via randomized smoothing
Since many defenses were actually broken, often within months, it
seems advisable to have a more principled defense with some mathe-
matical proof of security. We now describe randomized smoothing, the
best class of such defenses known – albeit it leads to significant low-
ering of classification accuracy. 7 For simplicity we describe the idea
7 We do not survey another approach
to provable defense via theorem-
proving based upon mixed-integer
programming, which has seen extensive
work as well but does not so far match
the guarantees provided by randomized
smoothing.
for ℓ2-attacks.
Definition 18.2.1. If D is a distribution on (input, label) pairs, where
inputs are in ℜd, then classifier g is γ-robust at (x, y) if f (x′) = y for all
x′ such that ∥x′ −x0∥2 ≤γ.

204
theory of deep learning
The idea in randomized smoothing is to try to produce a robust
classifier by taking a local spatial average of the outputs of another
classifier. Below, we will for simplicity equate ℓ2-ball of radius β
√
d
around x with the gaussian distribution N (x, β2I), since the two are
very close.
Definition 18.2.2. If D is a distribution on (input, label) pairs and g is
a classifier, then the β-smoothing of g, denoted gβ, is the classifier that,
given x, outputs the probabilistic answer g(x + δ) where δ is a random
vector from N (0, β2I). The classifier gβ
smooth is a classifier that on input x
gives the plurality label, namely, the label that is given highest probability
by gβ. (It breaks ties among labels arbitrarily.)
While definition of gβ involves an average over a continuous dis-
tribution, the probability that gβ(x) = y for a particular label y can
be estimated with arbitrary additive accuracy by sampling. The out-
put of the classifier gβ
smooth can be similarly determined via sampling
with probability close to 1.8 The goal will be to show that gβ
smooth is
8 In practice this requires a slight
gap between the probability of most
popular label and that of the second
most popular label.
γ-robust for some small γ.
Theorem 18.2.3. If gβ(x) outputs label y with probability pa then gβ(x′)
outputs y with probability at least
Φ(Φ−1(pa) −1
β|x −x′|2)
where Φ() is the cumulative distribution function9 of the univariate Gaus-
9 This means Φ(t) = Prz∼N (0,I)[z ≤t].
sian N (0, 1).
The theorem is proved a few paragraphs later. First we note the
following simple corollary.
Corollary 18.2.4. If pa = Pr[gβ(x) = y] and all other labels are given
probability at most p0, then y is the label given highest probability by
gβ(x + δ) provided |δ|2 ≤β
2 (Φ−1(pa) −Φ−1(p0)).
Let’s understand how the corollary can be used to train a classifier
g that is robust to ℓ2 perturbations. Usually x is an input in the train-
ing set with label y, then in normal training you do gradient updates
to the deep net towards reducing the sum of the losses associated
with assigning label y to x. To ensure robustness to ℓ2 perturbations,
you change training to also assign the same label y to a random sam-
ple of noised inputs x + δ where δ ∼N (0, β2I).
When training ends, using held-out data estimate the fraction ρ
of held out images x where (a) the plurality label of gβ is correct
and (b) there is a noticeable gap between its probability pa and the
probability p0 of the next most likely label. Then Corollary 18.2.4
implies that for such points x the classifier gβ
smooth outputs the same
label y for all x′ where |x −x′|2 ≤β
2 (Φ−1(pa) −Φ−1(p0)).

adversarial examples and efforts to combat them
205
Proof. (Theorem 18.2.3) Letting x′ be any point in the neighborhood
of x we try to upper bound the difference between Pr[gβ(x) = y] and
Pr[gβ(x′) = y]. Then sampling a random u from N (z, β2Id×d) can be
alternatively viewed as first picking the projection of this vector along
x′ −x according to the univariate gaussian N (z, β2) and then picking
the rest of the vector perpendicular to x −x′ according to the the
d −1 dimensional gaussian N (z, β2Id−1×d−1). Say z is the projection
on the infinite line passing through x, x′. Define E(z) = R
u 1g(u)=ydu
where R
u integrates over the d −1 dimensional distribution N (0, β2I)
in such an hyperplane at z. Then we have
Pr[gβ(x) = y] =
Z
z E(z)dz
(18.1)
where R
z integrates over the univariate Gaussian density N (x, β2). A
corresponding expression holds for Pr[gβ(x′) = y], with R
z integrat-
ing over univariate Gaussian density N (x′, β2) centered at x′. What is
the largest difference between the two, assuming x is to the left of x′
on this line?
Figure 18.5: Univariate gaus-
sians centered at x (blue
one)and x′ (the red one), and
the point where E(z) switches
from 1 to 0.
Intuitively it seems clear that Pr[gβ(x) = y] −Pr[gβ(x′) = y] is
maximized when E(z) takes its highest possible value, 1 on points
close to x, and then begins to switch to lower values closer to x′. As-
suming this intuition is correct10 let’s see how low it can get at x′. the
10 Correctness of this intuition is im-
plied by the Neyman Pearson lemma of
statistics.
worst case must be when E(z) is 1 for z = x to z < x + βΦ−1(pa),
and zero to the right of that. Then since gβ(x′) = gβ(x + x′ −x), the
same E(z)’s enter the average at x′ with somewhat different weight-
ing, corresponding to a shift by |x −x′|/β standard deviations in the
standard normal distribution. Thus in this worst-case configuration
Pr[gβ(x′) = y] equals Φ(Φ−1(pa) −|x −x′|/β) and in general is at
least that.
The analysis above can be tightened a bit; see 11. While the above
11 H Salman, G Yang, J Li, P Zhang,
H Zhang, I Razenshteyn, and S Bubeck.
Provably robust deep learning via ad-
versarially trained smoothed classifiers.
NeurIPS, 2019
argument relies upon the close relationship between the Gaussian
distribution and ℓ2 distance, it can be extended to ℓp bounded attacks
using suitable analogs for ℓp distance.
Problem 18.2.5. If g is a function mapping data points in ℜd to ℜand
g(x) ≤1 for all x, then show that there is a constant C (independent of d)
such that the gradient of g1
smooth has norm at most C.


19
Examples of Theorems, Proofs, Algorithms, Tables, Fig-
ures
In this chapter, Zhao provide examples of many things, like Theo-
rems, Lemmas, Algorithms, Tables, and Figures. If anyone has ques-
tion, feel free to contact Zhao directly.
19.1
Example of Theorems and Lemmas
We provide some examples
Theorem 19.1.1 (d-dimension sparse Fourier transform). There is an
algorithm (procedure FourierSparseRecovery in Algorithm 5) that
runs in ??? times and outputs ??? such that ???.
Note that, usually, if we provide the algorithm of the Theorem/Lemma.
Theorem should try to ref the corresponding Algorithm.
For the name of Theorem/Lemma/Corollary ..., let us only capital-
ize the first word,
Lemma 19.1.2 (Upper bound on the gradient). Blah blah.
Problem 19.1.3. This is how you put in a problem. It inherits chapter and
section numbers.
Theorem 19.1.4 (Main result).
19.2
Example of Long Equation Proofs
We can rewrite ∥Ax′ −b∥2
2 in the following sense,
∥Ax′ −b∥2
2 = ∥Ax′ −Ax∗+ AA†b −b∥2
2
= ∥Ax∗−Ax′∥2
2 + ∥Ax∗−b∥2
2
= ∥Ax∗−Ax′∥2
2 + OPT2

208
theory of deep learning
where the first step follows from x∗= A†b, the second step follows
from Pythagorean Theorem, and the last step follows from OPT :=
∥Ax∗−b∥2.

examples of theorems, proofs, algorithms, tables, figures
209
19.3
Example of Algorithms
Here is an example of algorithm. Usually the algorithm should ref
some Theorem/Lemma, and also the corresponding Theorem/Lemma
should ref back. This will be easier to verify the correctness.
Algorithm 5 Fourier Sparse Recovery Algorithm
1: procedure FourierSparseRecovery(x, n, k, µ, R∗)
▷
Theorem 19.1.1
2:
Require that µ =
1
√
k∥bx−k∥2 and R∗≥∥bx∥∞/µ
3:
H ←5, ν ←µR∗/2, y ←⃗0
4:
Let T
=
{T (1), · · · , T (H)} where each T (h) is a list of i.i.d.
uniform samples in [p]d
5:
while true do
6:
ν′ ←21−Hν
7:
z ←LinfinityReduce({xt}t∈T )
8:
if ν′ ≤µ then return y + z
▷We found the solution
9:
y′ ←⃗0
10:
for f ∈supp(y + z) do
11:
y′
f ←Π0.6ν(y f + z f )
▷We want ∥bx −y′∥∞≤ν and the
dependence between y′ and T is under control
12:
end for
13:
y ←y′, ν ←ν/2
14:
end while
15: end procedure

210
theory of deep learning
19.4
Example of Figures
We should make sure all the pictures are plotted by the same soft-
ware. Currently, everyone feel free to include their own picture. Zhao
will re-plot the picture by tikz finally.
x0
x1
x2
x3
xT
z0
z1
z2
z3
zT
y2,1
y2,2
y2,3
y2,4
Figure 19.1: A chasing sequence

examples of theorems, proofs, algorithms, tables, figures
211
19.5
Example of Tables
Reference
Samples
Time
Filter
RIP
[GMS05]
k logO(d) n
k logO(d) n
Yes
No
[CT06]
k log6 n
poly(n)
No
Yes
[RV08]
k log2 k log(k log n) log n
eO(n)
No
Yes
[HIKP12]
k logd n log(n/k)
k logd n log(n/k)
Yes
No
[CGV13]
k log3 k log n
eO(n)
No
Yes
[IK14]
2d log dk log n
eO(n)
Yes
No
[Bou14]
k log k log2 n
eO(n)
No
Yes
[HR16]
k log2 k log n
eO(n)
No
Yes
[Kap16]
2d2k log n
2d2k logd+O(1) n
Yes
No
[KVZ19]
k3 log2 k log2 n
k3 log2 k log2 n
Yes
Yes
[NSW19]
k log k log n
eO(n)
No
No
Table 19.1: We ignore the O
for simplicity. The ℓ∞/ℓ2
is the strongest possible
guarantee, with ℓ2/ℓ2 com-
ing second, ℓ2/ℓ1 third and
exactly k-sparse being the
weaker. We also note that all
[RV08, CGV13, Bou14, HR16]
obtain improved analyses of
the Restricted Isometry prop-
erty; the algorithm is suggested
and analyzed (modulo the RIP
property) in [BD08]. The work
in [HIKP12] does not explicitly
state the extension to the d-
dimensional case, but can easily
be inferred from the arguments.
[HIKP12, IK14, Kap16, KVZ19]
work when the universe size in
each dimension are powers of 2.
19.6
Exercise
This section provides several examples of exercises.
Exercises
Exercise 19.6-1:
Solve the following equation for x ∈C, with C the set
of complex numbers:
5x2 −3x = 5
(19.1)
Exercise 19.6-2:
Solve the following equation for x ∈C, with C the
set of complex numbers:
7x3 −2x = 1
(19.2)


Bibliography
[ADG+16] Marcin Andrychowicz, Misha Denil, Sergio Gomez,
Matthew W Hoffman, David Pfau, Tom Schaul, and
Nando de Freitas. Learning to learn by gradient descent
by gradient descent. In Advances in Neural Information
Processing Systems, 2016.
[ADH+19a] Sanjeev Arora, Simon Du, Wei Hu, Zhiyuan Li, and
Ruosong Wang. Fine-grained analysis of optimization
and generalization for overparameterized two-layer
neural networks. In International Conference on Machine
Learning, pages 322–332, 2019.
[ADH+19b] Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, Ruslan
Salakhutdinov, and Ruosong Wang. On exact compu-
tation with an infinitely wide neural net. arXiv preprint
arXiv:1904.11955, 2019.
[ADL+19] Sanjeev Arora, Simon S Du, Zhiyuan Li, Ruslan
Salakhutdinov, Ruosong Wang, and Dingli Yu. Harness-
ing the power of infinitely wide deep nets on small-data
tasks. arXiv preprint arXiv:1910.01663, 2019.
[AG23] Sanjeev Arora and Anirudh Goyal. A theory for emer-
gence of complex skills in language models. arXiv
preprint arXiv:2307.15936, 2023.
[AGL+17] Sanjeev Arora, Rong Ge, Yingyu Liang, Tengyu Ma, and
Yi Zhang. Generalization and Equilibrium in Generative
Adversarial Nets (GANs). Proc. ICML, 2017.
[AGNZ18] Sanjeev Arora, Rong Ge, Behnam Neyshabur, and
Yi Zhang. Stronger generalization bounds for deep
nets via a compression approach. In Proc. ICML 2018,
pages 254–263, 2018.
[ALL19] S Arora, Z Li, and K Lyu. Theoretical analysis of auto
rate-tuning by batch normalization. ICLR, 2019.

214
theory of deep learning
[aro] Do GANs learn the Distribution? Some Theory and
Empirics.
[AS16] N Alon and J Spencer. The Probabilistic Method (4th Ed).
Wiley, 2016.
[AZ18] Sanjeev Arora and Yi Zhang. Theoretical Limitations of
Encoder-Decoder GANs Architectures. Proc. ICLR, 2018.
[BBV16] Afonso S Bandeira, Nicolas Boumal, and Vladislav
Voroninski. On the low-rank approach for semidefinite
programs arising in synchronization and community
detection. In Conference on learning theory, pages 361–
382, 2016.
[BD08] Thomas Blumensath and Mike E Davies. Iterative
thresholding for sparse approximations. Journal of
Fourier analysis and Applications, 14(5-6):629–654, 2008.
[BDK+21] Yasaman Bahri, Ethan Dyer, Jared Kaplan, Jaehoon Lee,
and Utkarsh Sharma. Explaining neural scaling laws.
arXiv preprint arXiv:2102.06701, 2021.
[Ber24] Sergei Bernstein. On a modification of chebyshev’s
inequality and of the error formula of laplace. Ann. Sci.
Inst. Sav. Ukraine, Sect. Math, 1(4):38–49, 1924.
[BH89] Pierre Baldi and Kurt Hornik. Neural networks and
principal component analysis: Learning from examples
without local minima. Neural networks, 2(1):53–58, 1989.
[BKH16] J Ba, J R Kiros, and G E Hinton. Layer normalization.
NeurIPS, 2016.
[BM02] Peter L Bartlett and Shahar Mendelson. Rademacher
and gaussian complexities: Risk bounds and structural
results. Journal of Machine Learning Research, 3(Nov):463–
482, 2002.
[BM03] P. L. Bartlett and S. Mendelson. Rademacher and Gaus-
sian complexities: Risk bounds and structural results.
Journal of Machine Learning Research, 2003.
[BNS16a] Srinadh Bhojanapalli, Behnam Neyshabur, and Nati
Srebro. Global optimality of local search for low rank
matrix recovery. In Advances in Neural Information Pro-
cessing Systems, pages 3873–3881, 2016.

bibliography
215
[BNS16b] Srinadh Bhojanapalli, Behnam Neyshabur, and Nati
Srebro. Global optimality of local search for low rank
matrix recovery. In Advances in Neural Information Pro-
cessing Systems (NIPS), pages 3873–3881, 2016.
[Bou14] Jean Bourgain. An improved estimate in the restricted
isometry problem. In Geometric Aspects of Functional
Analysis, pages 65–70. Springer, 2014.
[BR89] Avrim Blum and Ronald L Rivest. Training a 3-node
neural network is np-complete. In Advances in neural
information processing systems, pages 494–501, 1989.
[Bre67] L. M. Bregman. The relaxation method of finding the
common point of convex sets and its application to the
solution of problems in convex programming. USSR
computational mathematics and mathematical physics, 1967.
[BT52] Ralph A. Bradley and Milton E. Terry. Rank analysis
of incomplete block designs: I. the method of Paired
Comparisons. Biometrika, 1952.
[BT03] A. Beck and M. Teboulle. Mirror descent and nonlinear
projected subgradient methods for convex optimization.
Operations Research Letters, 2003.
[BV04] S. Boyd and L. Vandenberghe. Convex optimization.
Cambridge university press, 2004.
[CCS+16] Pratik Chaudhari, Anna Choromanska, Stefano Soatto,
Yann LeCun, Carlo Baldassi, Christian Borgs, Jennifer
Chayes, Levent Sagun, and Riccardo Zecchina. Entropy-
sgd: Biasing gradient descent into wide valleys. arXiv
preprint arXiv:1611.01838, 2016.
[CGV13] Mahdi Cheraghchi, Venkatesan Guruswami, and Ameya
Velingker. Restricted isometry of Fourier matrices and
list decodability of random linear codes. SIAM Journal
on Computing, 42(5):1888–1914, 2013.
[Che52] Herman Chernoff. A measure of asymptotic efficiency
for tests of a hypothesis based on the sum of obser-
vations. The Annals of Mathematical Statistics, pages
493–507, 1952.
[CKL+21] Jeremy Cohen, Simran Kaur, Yuanzhi Li, J Zico Kolter,
and Ameet Talwalkar. Gradient descent on neural
networks typically occurs at the edge of stability. ICLR,
2021.

216
theory of deep learning
[CLB+17] P. Christiano, J. Leike, T. Brown, M. Martic, S. Legg, and
D. Amodei. Deep reinforcement learning from human
preferences. NeurIPS, 2017.
[CLG01] Rich Caruana, Steve Lawrence, and C Lee Giles. Over-
fitting in neural nets: Backpropagation, conjugate gradi-
ent, and early stopping. In Advances in Neural Informa-
tion Processing Systems (NIPS), pages 402–408, 2001.
[CT06] Emmanuel J Candes and Terence Tao. Near-optimal
signal recovery from random projections: Universal
encoding strategies? IEEE transactions on information
theory, 52(12):5406–5425, 2006.
[CW82] R D Cook and S Weisberg. Residuals and influence in
regression. 1982.
[DHS11] J. Duchi, E. Hazan, and Y. Singer. Adaptive subgradient
methods for online learning and stochastic optimiza-
tion. Journal of Machine Learning Research, 2011.
[DHS+19] Simon S Du, Kangcheng Hou, Russ R Salakhutdinov,
Barnabas Poczos, Ruosong Wang, and Keyulu Xu.
Graph neural tangent kernel: Fusing graph neural
networks with graph kernels. In Advances in Neural
Information Processing Systems, pages 5724–5734, 2019.
[DKB15] Laurent Dinh, David Krueger, and Yoshua Bengio.
NICE: Nonlinear Independent Component Analysis.
Proc. ICLR, 2015.
[DLL+18] Simon S Du, Jason D Lee, Haochuan Li, Liwei Wang,
and Xiyu Zhai. Gradient descent finds global minima of
deep neural networks. arXiv preprint arXiv:1811.03804,
2018.
[DP94] X Deng and C Papadimitriou. On the complexity of co-
operative solution concepts. Math of Operations Research,
1994.
[DPBB17] Laurent Dinh, Razvan Pascanu, Samy Bengio, and
Yoshua Bengio. Sharp minima can generalize for deep
nets. In International Conference on Machine Learning,
2017.
[DSDB17] Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio.
Density Eestimation using Real NVP. Proc. ICLR, 2017.

bibliography
217
[DZPS18] Simon S Du, Xiyu Zhai, Barnabas Poczos, and Aarti
Singh. Gradient descent provably optimizes over-
parameterized neural networks.
arXiv preprint
arXiv:1810.02054, 2018.
[DZPS19] Simon S Du, Xiyu Zhai, Barnabas Poczos, and Aarti
Singh. Gradient descent provably optimizes over-
parameterized neural networks.
arXiv preprint
arXiv:1810.02054, 2019.
[EHJT04] B. Efron, T. Hastie, I. Johnstone, and R. Tibshirani. Least
angle regression. The Annals of statistics, 2004.
[Fri01] Jerome H Friedman. Greedy function approximation: a
gradient boosting machine. Annals of statistics, 2001.
[FSSS11] Rina Foygel, Ohad Shamir, Nati Srebro, and Ruslan R
Salakhutdinov. Learning with the weighted trace-norm
under arbitrary sampling distributions. In Advances in
Neural Information Processing Systems, pages 2133–2141,
2011.
[GDG+17] Priya Goyal, Piotr Dollár, Ross Girshick, Pieter No-
ordhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew
Tulloch, Yangqing Jia, and Kaiming He. Accurate, large
minibatch sgd: Training imagenet in 1 hour. arXiv
preprint arXiv:1706.02677, 2017.
[GHJY15a] Rong Ge, Furong Huang, Chi Jin, and Yang Yuan. Es-
caping from saddle points—online stochastic gradient
for tensor decomposition. In Conference on Learning
Theory, pages 797–842, 2015.
[GHJY15b] Rong Ge, Furong Huang, Chi Jin, and Yang Yuan. Es-
caping from saddle points—online stochastic gradient
for tensor decomposition. In Conf. Learning Theory
(COLT), 2015.
[GJZ17a] Rong Ge, Chi Jin, and Yi Zheng. No spurious local
minima in nonconvex low rank problems: A unified
geometric analysis. In Proceedings of the 34th International
Conference on Machine Learning-Volume 70, pages 1233–
1242. JMLR. org, 2017.
[GJZ17b] Rong Ge, Chi Jin, and Yi Zheng. No spurious local
minima in nonconvex low rank problems: A unified
geometric analysis. arXiv preprint arXiv:1704.00708, 2017.

218
theory of deep learning
[GLM16a] Rong Ge, Jason D Lee, and Tengyu Ma. Matrix com-
pletion has no spurious local minimum. In Advances in
Neural Information Processing Systems, pages 2973–2981,
2016.
[GLM16b] Rong Ge, Jason D Lee, and Tengyu Ma. Matrix com-
pletion has no spurious local minimum. In Advances in
Neural Information Processing Systems (NIPS), 2016.
[GLM18] Rong Ge, Jason D Lee, and Tengyu Ma. Learning one-
hidden-layer neural networks with landscape design. In
ICLR. arXiv preprint arXiv:1711.00501, 2018.
[GLSS18a] Suriya Gunasekar, Jason Lee, Daniel Soudry, and
Nathan Srebro. Characterizing implicit bias in terms of
optimization geometry. arXiv preprint arXiv:1802.08246,
2018.
[GLSS18b] Suriya Gunasekar, Jason D Lee, Daniel Soudry, and
Nati Srebro. Implicit bias of gradient descent on
linear convolutional networks. In S. Bengio, H. Wal-
lach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and
R. Garnett, editors, Advances in Neural Information Pro-
cessing Systems, volume 31. Curran Associates, Inc.,
2018.
[GMS05] Anna C Gilbert, S Muthukrishnan, and Martin Strauss.
Improved time bounds for near-optimal sparse Fourier
representations. In Optics & Photonics 2005, pages
59141A–59141A. International Society for Optics and
Photonics, 2005.
[GPAM+14] I Goodfellow, J Pouget-Abadie, M Mirza, B Xu,
D Warde-Farley, S Ozair, A Courville, and Y Bengio.
Generative Adversarial Networks. NeurIPS, 2014.
[GWB+17] Suriya Gunasekar, Blake E Woodworth, Srinadh Bho-
janapalli, Behnam Neyshabur, and Nati Srebro. Implicit
regularization in matrix factorization. In Advances in
Neural Information Processing Systems, pages 6151–6159,
2017.
[HBD+20] A Holtzman, J Buys, L Du, M Forbes, and Y Choi. The
curious case of neural text degeneration. ICLR, 2020.
[HBM+22] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch,
Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego
de Las Casas, Lisa Anne Hendricks, Johannes Welbl,

bibliography
219
Aidan Clark, et al. Training compute-optimal large
language models. arXiv preprint arXiv:2203.15556, 2022.
[HHS17] Elad Hoffer, Itay Hubara, and Daniel Soudry. Train
longer, generalize better: closing the generalization gap
in large batch training of neural networks. In Advances
in Neural Information Processing Systems, 2017.
[HIKP12] Haitham Hassanieh, Piotr Indyk, Dina Katabi, and Eric
Price. Nearly optimal sparse Fourier transform. In
Proceedings of the forty-fourth annual ACM symposium on
Theory of computing, pages 563–578. ACM, 2012.
[HLLL19] Wenqing Hu, Chris Junchi Li, Lei Li, and Jian-Guo Liu.
On the diffusion approximation of nonconvex stochastic
gradient descent. Annals of Mathematical Sciences and
Applications, 4(1), 2019.
[HMR18] Moritz Hardt, Tengyu Ma, and Benjamin Recht. Gradi-
ent descent learns linear dynamical systems. In JLMR.
arXiv preprint arXiv:1609.05191, 2018.
[Hoe63] Wassily Hoeffding. Probability inequalities for sums
of bounded random variables. Journal of the American
Statistical Association, 58(301):13–30, 1963.
[HR16] Ishay Haviv and Oded Regev. The restricted isometry
property of subsampled Fourier matrices. In SODA,
pages 288–297. arXiv preprint arXiv:1507.01768, 2016.
[HS97] Sepp Hochreiter and Jürgen Schmidhuber. Flat minima.
Neural Computation, 1997.
[IK14] Piotr Indyk and Michael Kapralov. Sample-optimal
Fourier sampling in any constant dimension. In Founda-
tions of Computer Science (FOCS), 2014 IEEE 55th Annual
Symposium on, pages 514–523. IEEE, 2014.
[IPE+22] Andrew Ilyas, Sung Min Park, Logan Engstrom, Guil-
laume Leclerc, and Aleksander Madry. Datamodels:
Predicting predictions from training data. arXiv preprint
arXiv:2202.00622, 2022.
[IS15a] S Ioffe and C Szegedy. Batch normalization: Accel-
erating deep network training by reducing internal
covariate shift. ICML, 2015.
[IS15b] Sergey Ioffe and Christian Szegedy. Batch normaliza-
tion: Accelerating deep network training by reducing

220
theory of deep learning
internal covariate shift. In International Conference on
Machine Learning (ICML), pages 448–456, 2015.
[JGH18] Arthur Jacot, Franck Gabriel, and Clément Hongler.
Neural tangent kernel: Convergence and generalization
in neural networks. In Advances in neural information
processing systems, pages 8571–8580, 2018.
[JGN+17] Chi Jin, Rong Ge, Praneeth Netrapalli, Sham M Kakade,
and Michael I Jordan. How to escape saddle points
efficiently. arXiv preprint arXiv:1703.00887, 2017.
[JKA+18] S Jastrz˛ebski, Z Kenton, D Arpit, N Ballas, A Fischer,
Y Bengio, and A Storkey. Three Factors Influencing
Minima in SGD. ICANN, 2018.
[JNG+19] Chi Jin, Praneeth Netrapalli, Rong Ge, Sham M Kakade,
and Michael I Jordan. On nonconvex optimization for
machine learning: Gradients, stochasticity, and saddle
points. arXiv preprint arXiv:1902.04811, 2019.
[Kap16] Michael Kapralov. Sparse Fourier transform in any con-
stant dimension with nearly-optimal sample complexity
in sublinear time. In Symposium on Theory of Computing
Conference, STOC’16, Cambridge, MA, USA, June 19-21,
2016, 2016.
[Kaw16] Kenji Kawaguchi. Deep learning without poor local
minima. In Adv in Neural Information Proc. Systems
(NIPS), 2016.
[KD19] Diederik P. Kingma and Prafulla Dhariwal. GLOW:
Generative Flow with Invertible 1 × 1 convolutions.
Proc. Neurips, 2019.
[KGC17] Jan Kukavcka, Vladimir Golkov, and Daniel Cremers.
Regularization for deep learning: A taxonomy. arXiv
preprint arXiv:1710.10686, 2017.
[KKSK11] Sham M Kakade, Varun Kanade, Ohad Shamir, and
Adam Kalai. Efficient learning of generalized linear
and single index models with isotonic regression. In
Advances in Neural Information Processing Systems, pages
927–935, 2011.
[KL17] P W Koh and P Liang. Understanding black-box predic-
tions via influence functions. In Proc. ICML, 2017.

bibliography
221
[KMN+16] Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge No-
cedal, Mikhail Smelyanskiy, and Ping Tak Peter Tang.
On large-batch training for deep learning: Generaliza-
tion gap and sharp minima. In International Conference
on Learning Representations, 2016.
[KS09] Adam Tauman Kalai and Ravi Sastry. The isotron
algorithm: High-dimensional isotonic regression. In
COLT. Citeseer, 2009.
[KST09] Sham M Kakade, Karthik Sridharan, and Ambuj Tewari.
On the complexity of linear prediction: Risk bounds,
margin bounds, and regularization. In Advances in
neural information processing systems, 2009.
[KVZ19] Michael Kapralov, Ameya Velingker, and Amir Zandieh.
Dimension-independent sparse Fourier transform. In
Proceedings of the Thirtieth Annual ACM-SIAM Symposium
on Discrete Algorithms, pages 2709–2728. SIAM, 2019.
[LA19] Z Li and S Arora. An exponential learning rate schedule
for deep learning. ICLR, 2019.
[Lan02] John Langford. Quantitatively tight sample complexity
bounds. PhD Thesis CMU, 2002.
[LBZ+22] Z Li, S Bhojanapalli, M Zaheer, Reddi S, and Kumar S.
Robust training of neural networks using scale invariant
architectures. arxiv, 2022.
[LDM12] Hector Levesque, Ernest Davis, and Leora Morgen-
stern. The winograd schema challenge. In Thirteenth
international conference on the principles of knowledge repre-
sentation and reasoning, 2012.
[LLA20] Zhiyuan Li, Kaifeng Lyu, and Sanjeev Arora. Recon-
ciling modern deep learning with traditional optimiza-
tion analyses: The intrinsic learning rate. In Hugo
Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-
Florina Balcan, and Hsuan-Tien Lin, editors, Advances
in Neural Information Processing Systems 33: Annual Con-
ference on Neural Information Processing Systems 2020,
NeurIPS 2020, December 6-12, 2020, virtual, 2020.
[LMA21] Zhiyuan Li, Sadhika Malladi, and Sanjeev Arora. On
the validity of modeling sgd with stochastic differen-
tial equations (sdes). Advances in Neural Information
Processing Systems, 34, 2021.

222
theory of deep learning
[LMAPH19] Stéphane Lathuilière, Pablo Mesejo, Xavier Alameda-
Pineda, and Radu Horaud. A comprehensive analysis of
deep regression. IEEE transactions on pattern analysis and
machine intelligence, 2019.
[LMZ18] Yuanzhi Li, Tengyu Ma, and Hongyang Zhang. Al-
gorithmic regularization in over-parameterized matrix
sensing and neural networks with quadratic activations.
In Conference On Learning Theory, pages 2–47, 2018.
[LSJR16] Jason D Lee, Max Simchowitz, Michael I Jordan, and
Benjamin Recht. Gradient descent converges to mini-
mizers. arXiv preprint arXiv:1602.04915, 2016.
[LTW19] Qianxiao Li, Cheng Tai, and E Weinan. Stochastic mod-
ified equations and dynamics of stochastic gradient
algorithms i: Mathematical foundations. J. Mach. Learn.
Res., 20:40–1, 2019.
[LWLA22] Zhiyuan Li, Tianhao Wang, Jason D Lee, and San-
jeev Arora.
Implicit bias of gradient descent on
reparametrized models: On equivalence to mirror de-
scent. Advances in Neural Information Processing Systems,
35:34626–34640, 2022.
[MA19] Poorya Mianjy and Raman Arora. On dropout and
nuclear norm regularization. In International Conference
on Machine Learning, 2019.
[MAV18] Poorya Mianjy, Raman Arora, and Rene Vidal. On the
implicit bias of dropout. In International Conference on
Machine Learning, pages 3537–3545, 2018.
[McA99] David A McAllester. Some pac-bayesian theorems.
Machine Learning, 37(3):355–363, 1999.
[MHB17] Stephan Mandt, Matthew D Hoffman, and David M
Blei. Stochastic gradient descent as approximate
bayesian inference. The Journal of Machine Learning
Research, 18(1):4873–4907, 2017.
[MMS+18] A Madry, A Makelov, L Schmidt, D Tsipras, and
A Vladu. Towards Deep Learning Models Resistant
to Adversarial Attacks. ICLR, 2018.
[MRB+23] Niklas Muennighoff, Alexander M. Rush, Boaz Barak,
Teven Le Scao, Aleksandra Piktus, Nouamane Tazi,
Sampo Pyysalo, Thomas Wolf, and Colin Raffel. Scaling
data-constrained language models, 2023.

bibliography
223
[MRT18] Mehryar Mohri, Afshin Rostamizadeh, and Ameet
Talwalkar. Foundations of machine learning. MIT press,
2018.
[NBE17] Agarwal N, Bullins B, and Hazan E. Second-order
stochastic optimization for machine learning in linear
time. 2017.
[NBMS18] Behnam Neyshabur, Srinadh Bhojanapalli, David
McAllester, and Nathan Srebro. A pac-bayesian ap-
proach to spectrally-normalized margin bounds for
neural networks. ICLR, 2018.
[Nes98] Yurii Nesterov. Introductory Lectures on Convex Program-
ming Volume I: Basic Course. Springer, 1998.
[Nes00] Yurii Nesterov. Squared functional systems and opti-
mization problems. In High performance optimization,
pages 405–440. Springer, 2000.
[Ney17] Behnam Neyshabur. Implicit regularization in deep
learning. arXiv preprint arXiv:1709.01953, 2017.
[NH92] Steven J Nowlan and Geoffrey E Hinton. Simplifying
neural networks by soft weight-sharing. Neural computa-
tion, 4(4):473–493, 1992.
[NK19] V Nagarajan and Zico Kolter. Uniform convergence may
be unable to explain generalization in deep learning.
NeurIPS, 2019.
[NP06] Yurii Nesterov and Boris T Polyak. Cubic regulariza-
tion of Newton method and its global performance.
Mathematical Programming, 108(1):177–205, 2006.
[NS23] Mark Braverman Sanjeev Aror Nikunj Saunshi,
Arushi Gupta. Understanding influence functions
and data models via harmonic analysis. ICLR, 2023.
[NSS15] Behnam Neyshabur, Ruslan R Salakhutdinov, and Nati
Srebro. Path-sgd: Path-normalized optimization in
deep neural networks. In Advances in Neural Information
Processing Systems, pages 2422–2430, 2015.
[NSW19] Vasileios Nakos, Zhao Song, and Zhengyu Wang.
(Nearly) Sample-optimal sparse Fourier transform
in any dimension; RIPless and Filterless. In FOCS.
https://arxiv.org/pdf/1909.11123.pdf, 2019.

224
theory of deep learning
[NTS15a] Behnam Neyshabur, Ryota Tomioka, and Nathan Sre-
bro. In search of the real inductive bias: On the role of
implicit regularization in deep learning. In International
Conference on Learning Representations, 2015.
[NTS15b] Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro.
Norm-based capacity control in neural networks. In
Conference on Learning Theory, pages 1376–1401, 2015.
[NY83] A. Nemirovskii and D. Yudin. Problem complexity and
method efficiency in optimization. Wiley, 1983.
[Pea94] Barak Pearlmutter. Fast exact multiplication by the
hessian. Neural Computation, 1994.
[PKCS17] Dohyung Park, Anastasios Kyrillidis, Constantine Cara-
manis, and Sujay Sanghavi. Non-square matrix sensing
without spurious local minima via the burer-monteiro
approach. In AISTATS. arXiv preprint arXiv:1609.03240,
2017.
[RDS04] Cynthia Rudin, Ingrid Daubechies, and Robert E
Schapire. The dynamics of adaboost: Cyclic behavior
and convergence of margins. Journal of Machine Learning
Research, 5(Dec):1557–1595, 2004.
[RSM+23] Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano
Ermon, Christopher D. Manning, and Chelsea Finn.
Direct preference optimization: Your language model is
secretly a reward model. arxiv, 2023.
[RV08] Mark Rudelson and Roman Vershynin. On sparse re-
construction from Fourier and Gaussian measurements.
Communications on Pure and Applied Mathematics: A Jour-
nal Issued by the Courant Institute of Mathematical Sciences,
61(8):1025–1045, 2008.
[SF12] Robert E Schapire and Yoav Freund. Boosting: Founda-
tions and algorithms. MIT press, 2012.
[Sha] Lloyd Shapley. "Notes on the n-Person Game – II: The
Value of an n-Person Game". RAND Corporation.
[SHK+14] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky,
Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: a
simple way to prevent neural networks from overfitting.
Journal of Machine Learning Research (JMLR), 15(1), 2014.

bibliography
225
[SHS17] Daniel Soudry, Elad Hoffer, and Nathan Srebro. The
implicit bias of gradient descent on separable data.
arXiv preprint arXiv:1710.10345, 2017.
[Smi18] Le Smith, Kindermans. Don’t Decay the Learning Rate,
Increase the Batch Size. In ICLR, 2018.
[SMK23] Ryan Schaeffer, Brando Miranda, and Sanmi Koyejo.
Are emergent abilities of large language models a mi-
rage? ArXiv e-prints, April 2023.
[SQW16a] Ju Sun, Qing Qu, and John Wright. Complete dictionary
recovery over the sphere i: Overview and the geomet-
ric picture. IEEE Transactions on Information Theory,
63(2):853–884, 2016.
[SQW16b] Ju Sun, Qing Qu, and John Wright. A geometric analysis
of phase retrieval. In IEEE International Symposium on
Information Theory (ISIT), pages 2379–2383, 2016.
[SQW18] Ju Sun, Qing Qu, and John Wright. A geometric anal-
ysis of phase retrieval. Foundations of Computational
Mathematics, 18(5):1131–1198, 2018.
[SS10] Nathan Srebro and Ruslan R Salakhutdinov. Collabora-
tive filtering in a non-uniform world: Learning with the
weighted trace norm. In Advances in Neural Information
Processing Systems, pages 2056–2064, 2010.
[SSJ20] Bin Shi, Weijie J Su, and Michael I Jordan. On learn-
ing rates and schrödinger operators. arXiv preprint
arXiv:2004.06977, 2020.
[SSS10] Shai Shalev-Shwartz and Yoram Singer. On the equiva-
lence of weak learnability and linear separability: New
relaxations and efficient boosting algorithms. Machine
learning, 2010.
[SY19] Zhao Song and Xin Yang. Quadratic suffices for over-
parametrization via matrix chernoff bound. arXiv
preprint arXiv:1906.03593, 2019.
[SYL+19] H Salman, G Yang, J Li, P Zhang, H Zhang, I Razen-
shteyn, and S Bubeck. Provably robust deep learning
via adversarially trained smoothed classifiers. NeurIPS,
2019.
[SZA20] F Schaefer, H Zheng, and A Anandkumar. Implicit
competitive regularization in GANs. ICML, 2020.

226
theory of deep learning
[SZS+14] C Szegedy, W Zaremba, I Sutskever, J Bruna, D Erhan,
I Goodfellow, and R Fergus. Intriguing properties of
neural networks. In ICLR, 2014.
[Tel13] Matus Telgarsky. Margins, shrinkage, and boosting.
arXiv preprint arXiv:1303.4172, 2013.
[Tro15] Joel A Tropp. An introduction to matrix concentra-
tion inequalities. Foundations and Trends® in Machine
Learning, 8(1-2):1–230, 2015.
[Wer88] P. J. Werbos. Backpropagation: Past and future. In IEEE
InternationalConference on Neural Networks, page 343–353,
1988.
[WGL+20] Blake Woodworth, Suriya Gunasekar, Jason D Lee,
Edward Moroshko, Pedro Savarese, Itay Golan, Daniel
Soudry, and Nathan Srebro. Kernel and rich regimes
in overparametrized models. In Conference on Learning
Theory, pages 3635–3673, 2020.
[WH17] Y Wu and K He. Group Normalization. ECCV, 2017.
[Win71] Terry Winograd. Procedures as a representation for
data in a computer program for understanding natural
language. Technical report, MASSACHUSETTS INST
OF TECH CAMBRIDGE PROJECT MAC, 1971.
[WRS+17] Ashia C Wilson, Rebecca Roelofs, Mitchell Stern, Nati
Srebro, and Benjamin Recht. The marginal value of
adaptive gradient methods in machine learning. In
Advances in Neural Information Processing Systems, 2017.
[WTB+22] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret
Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten
Bosma, Denny Zhou, Donald Metzler, et al. Emer-
gent abilities of large language models. arXiv preprint
arXiv:2206.07682, 2022.
[ZBH+16a] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin
Recht, and Oriol Vinyals. Understanding deep learn-
ing requires rethinking generalization. arXiv preprint
arXiv:1611.03530, 2016.
[ZBH+16b] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin
Recht, and Oriol Vinyals. Understanding deep learn-
ing requires rethinking generalization. arXiv preprint
arXiv:1611.03530, 2016.

bibliography
227
[ZGSA22] Y Zhang, A Gupta, N Saunshi, and S Arora. On predict-
ing generalization using gans. ICLR, 2022.
[ZM] Kolter Z and Madry M. Adversarial robustness –theory
and practice.
[ZY+05] Tong Zhang, Bin Yu, et al. Boosting with early stopping:
Convergence and consistency. The Annals of Statistics,
2005.

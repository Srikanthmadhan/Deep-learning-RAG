M A N N I N G Krishnendu Chaudhury with Ananya H. Ashok Sujay Narumanchi Devashish Shankar Foreword by Prith Banerjee Math and Architectures of Deep Learning Krishnendu Chaudhury with Ananya H. Ashok Sujay Narumanchi Devashish Shankar Foreword by Prith Banerjee M A N N I N G SHELTER ISLAND For online information and ordering of this and other Manning books, please visit www.manning.com. The publisher offers discounts on this book when ordered in quantity. For more information, please contact Special Sales Department Manning Publications Co. 20 Baldwin Road PO Box 761 Shelter Island, NY 11964 Email: orders@manning.com © 2024 by Manning Publications Co. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system, or transmitted, in any form or by means electronic, mechanical, photocopying, or otherwise, without prior written permission of the publisher. Many of the designations used by manufacturers and sellers to distinguish their products are claimed as trademarks. Where those designations appear in the book, and Manning Publications was aware of a trademark claim, the designations have been printed in initial caps or all caps. ∞Recognizing the importance of preserving what has been written, it is Manning’s policy to have the books we publish printed on acid-free paper, and we exert our best efforts to that end. Recognizing also our responsibility to conserve the resources of our planet, Manning books are printed on paper that is at least 15 percent recycled and processed without the use of elemental chlorine. The author and publisher have made every effort to ensure that the information in this book was correct at press time. The author and publisher do not assume and hereby disclaim any liability to any party for any loss, damage, or disruption caused by errors or omissions, whether such errors or omissions result from negligence, accident, or any other cause, or from any usage of the information herein. Manning Publications Co. 20 Baldwin Road PO Box 761 Shelter Island, NY 11964 Development editor: Christina Taylor Technical development editor: Mike Shepard Review editor: Aleksandar Dragosavljevi´c Production editor: Andy Marinkovich Copy editor: Tiffany Taylor Proofreader: Keri Hales Technical proofreader: Lucian Mircea Sasu Typesetter: Westchester Publishing Services Cover designer: Marija Tudor ISBN: 9781617296482 Printed in the United States of America brief contents 1 An overview of machine learning and deep learning 1 2 Vectors, matrices, and tensors in machine learning 18 3 Classifiers and vector calculus 83 4 Linear algebraic tools in machine learning 115 5 Probability distributions in machine learning 149 6 Bayesian tools for machine learning 193 7 Function approximation: How neural networks model the world 239 8 Training neural networks: Forward propagation and backpropagation 272 9 Loss, optimization, and regularization 300 10 Convolutions in neural networks 343 11 Neural networks for image classification and object detection 385 12 Manifolds, homeomorphism, and neural networks 438 13 Fully Bayes model parameter estimation 447 14 Latent space and generative modeling, autoencoders, and variational autoencoders 468 iii contents foreword xiv preface xvi acknowledgments xviii about this book xx about the authors xxv about the cover illustration xxvi 1 An overview of machine learning and deep learning 1 1.1 A first look at machine/deep learning: A paradigm shift in computation 2 1.2 A function approximation view of machine learning: Models and their training 6 1.3 A simple machine learning model: The cat brain 7 Input features 7 Output decisions 7 Model estimation 8 Model architecture selection 8 Model training 8 Inferencing 10 1.4 Geometrical view of machine learning 10 1.5 Regression vs. classification in machine learning 12 1.6 Linear vs. nonlinear models 12 iv CONTENTS v 1.7 Higher expressive power through multiple nonlinear layers: Deep neural networks 14 2 Vectors, matrices, and tensors in machine learning 18 2.1 Vectors and their role in machine learning 19 The geometric view of vectors and its significance in machine learning 21 2.2 PyTorch code for vector manipulations 22 PyTorch code for the introduction to vectors 22 2.3 Matrices and their role in machine learning 23 Matrix representation of digital images 25 2.4 Python code: Introducing matrices, tensors, and images via PyTorch 25 2.5 Basic vector and matrix operations in machine learning 26 Matrix and vector transpose 28 Dot product of two vectors and its role in machine learning 29 Matrix multiplication and machine learning 30 Length of a vector (L2 norm): Model error 34 Geometric intuitions for vector length 36 Geometric intuitions for the dot product: Feature similarity 36 2.6 Orthogonality of vectors and its physical significance 39 2.7 Python code: Basic vector and matrix operations via PyTorch 39 PyTorch code for a matrix transpose 39 PyTorch code for a dot product 40 PyTorch code for matrix vector multiplication 40 PyTorch code for matrix-matrix multiplication 41 PyTorch code for the transpose of a matrix product 42 2.8 Multidimensional line and plane equations and machine learning 42 Multidimensional line equation 42 Multidimensional planes and their role in machine learning 43 2.9 Linear combinations, vector spans, basis vectors, and collinearity preservation 46 Linear dependence 46 Span of a set of vectors 47 Vector spaces, basis vectors, and closure 48 vi CONTENTS 2.10 Linear transforms: Geometric and algebraic interpretations 49 Generic multidimensional definition of linear transforms 51 All matrix-vector multiplications are linear transforms 52 2.11 Multidimensional arrays, multilinear transforms, and tensors 53 Array view: Multidimensional arrays of numbers 53 2.12 Linear systems and matrix inverse 53 Linear systems with zero or near-zero determinants, and ill-conditioned systems 55 PyTorch code for inverse, determinant, and singularity testing of matrices 57 Over- and under- determined linear systems in machine learning 57 Moore Penrose pseudo-inverse of a matrix 59 Pseudo-inverse of a matrix: A beautiful geometric intuition 59 PyTorch code to solve overdetermined systems 62 2.13 Eigenvalues and eigenvectors: Swiss Army knives of machine learning 62 Eigenvectors and linear independence 65 Symmetric matrices and orthogonal eigenvectors 66 PyTorch code to compute eigenvectors and eigenvalues 67 2.14 Orthogonal (rotation) matrices and their eigenvalues and eigenvectors 67 Rotation matrices 67 Orthogonality of rotation matrices 70 PyTorch code for orthogonality of rotation matrices 71 Eigenvalues and eigenvectors of a rotation matrix: Finding the axis of rotation 72 PyTorch code for eigenvalues and vectors of rotation matrices 72 2.15 Matrix diagonalization 73 PyTorch code for matrix diagonalization 74 Solving linear systems without inversion via diagonalization 74 PyTorch code for solving linear systems via diagonalization 76 Matrix powers using diagonalization 76 2.16 Spectral decomposition of a symmetric matrix 77 PyTorch code for the spectral decomposition of a matrix 77 2.17 An application relevant to machine learning: Finding the axes of a hyperellipse 78 PyTorch code for hyperellipses 79 CONTENTS vii 3 Classifiers and vector calculus 83 3.1 Geometrical view of image classification 83 Input representation 83 Classifiers as decision boundaries 84 Modeling in a nutshell 86 Sign of the surface function in binary classification 88 3.2 Error, aka loss function 88 3.3 Minimizing loss functions: Gradient vectors 89 Gradients: A machine learning-centric introduction 90 Level surface representation and loss minimization 97 3.4 Local approximation for the loss function 99 1D Taylor series recap 100 Multidimensional Taylor series and the Hessian matrix 101 3.5 PyTorch code for gradient descent, error minimization, and model training 101 PyTorch code for linear models 101 Autograd: PyTorch automatic gradient computation 103 Nonlinear Models in PyTorch 105 A linear model for the cat brain in PyTorch 108 3.6 Convex and nonconvex functions, and global and local minima 109 3.7 Convex sets and functions 109 Convex sets 110 Convex curves and surfaces 110 Convexity and the Taylor series 112 Examples of convex functions 113 4 Linear algebraic tools in machine learning 115 4.1 Distribution of feature data points and true dimensionality 116 4.2 Quadratic forms and their minimization 118 Minimizing quadratic forms 119 Symmetric positive (semi)definite matrices 121 4.3 Spectral and Frobenius norms of a matrix 122 Spectral norms 122 Frobenius norms 122 4.4 Principal component analysis 123 Direction of maximum spread 125 PCA and dimensionality reduction 127 PyTorch code: PCA and dimensionality reduction 128 Limitations of PCA 129 PCA and data compression 130 viii CONTENTS 4.5 Singular value decomposition 130 Informal proof of the SVD theorem 131 Proof of the SVD theorem 133 Applying SVD: PCA computation 135 Applying SVD: Solving arbitrary linear systems 135 Rank of a matrix 136 PyTorch code for solving linear systems with SVD 137 PyTorch code for PCA computation via SVD 139 Applying SVD: Best low-rank approximation of a matrix 139 4.6 Machine learning application: Document retrieval 140 Using TF-IDF and cosine similarity 141 Latent semantic analysis 142 PyTorch code to perform LSA 145 PyTorch code to compute LSA and SVD on a large dataset 146 5 Probability distributions in machine learning 149 5.1 Probability: The classical frequentist view 150 Random variables 151 Population histograms 152 5.2 Probability distributions 152 5.3 Basic concepts of probability theory 154 Probabilities of impossible and certain events 154 Exhaustive and mutually exclusive events 154 Independent events 155 5.4 Joint probabilities and their distributions 155 Marginal probabilities 157 Dependent events and their joint probability distribution 157 5.5 Geometrical view: Sample point distributions for dependent and independent variables 159 5.6 Continuous random variables and probability density 160 5.7 Properties of distributions: Expected value, variance, and covariance 162 Expected value (aka mean) 162 Variance, covariance, and standard deviation 164 5.8 Sampling from a distribution 167 5.9 Some famous probability distributions 169 Uniform random distributions 170 Gaussian (normal) distribution 173 Binomial distribution 180 Multinomial distribution 185 Bernoulli distribution 188 Categorical distribution and one-hot vectors 189 CONTENTS ix 6 Bayesian tools for machine learning 193 6.1 Conditional probability and Bayes’ theorem 194 Joint and marginal probability revisited 194 Conditional probability 196 Bayes’ theorem 196 6.2 Entropy 198 Geometrical intuition for entropy 201 Entropy of Gaussians 203 6.3 Cross-entropy 204 6.4 KL divergence 207 KLD between Gaussians 208 6.5 Conditional entropy 210 Chain rule of conditional entropy 212 6.6 Model parameter estimation 213 Likelihood, evidence, and posterior and prior probabilities 213 Maximum likelihood parameter estimation (MLE) 214 Maximum a posteriori (MAP) parameter estimation and regularization 215 6.7 Latent variables and evidence maximization 215 6.8 Maximum likelihood parameter estimation for Gaussians 216 Python PyTorch code for maximum likelihood estimation 218 Python PyTorch code for maximum likelihood estimation using gradient descent 219 6.9 Gaussian mixture models 222 Probability density function of the GMM 223 Latent variables for class selection 227 Classification via GMM 230 Maximum likelihood estimation of GMM parameters (GMM fit) 230 7 Function approximation: How neural networks model the world 239 7.1 Neural networks: A 10,000-foot view 240 7.2 Expressing real-world problems: Target functions 241 Logical functions in real-world problems 242 Classifier functions in real-world problems 245 General functions in real-world problems 252 x CONTENTS 7.3 The basic building block or neuron: The perceptron 252 The Heaviside step function 252 Hyperplanes 253 Perceptrons and classification 254 Modeling common logic gates with perceptrons 256 7.4 Toward more expressive power: Multilayer perceptrons (MLPs) 259 MLP for logical XOR 259 7.5 Layered networks of perceptrons: MLPs or neural networks 260 Layering 260 Modeling logical functions with MLPs 260 Cybenko’s universal approximation theorem 261 MLPs for polygonal decision boundaries 268 8 Training neural networks: Forward propagation and backpropagation 272 8.1 Differentiable step-like functions 273 Sigmoid function 273 Tanh function 275 8.2 Why layering? 276 8.3 Linear layers 277 Linear layers expressed as matrix-vector multiplication 277 Forward propagation and grand output functions for an MLP of linear layers 280 8.4 Training and backpropagation 281 Loss and its minimization: Goal of training 282 Loss surface and gradient descent 283 Why a gradient provides the best direction for descent 284 Gradient descent and local minima 285 The backpropagation algorithm 286 Putting it all together: Overall training algorithm 294 8.5 Training a neural network in PyTorch 295 9 Loss, optimization, and regularization 300 9.1 Loss functions 301 Quantification and geometrical view of loss 301 Regression loss 303 Cross-entropy loss 303 Binary cross-entropy loss for image and vector mismatches 305 Softmax 306 Softmax cross-entropy loss 308 Focal loss 310 Hinge loss 312 CONTENTS xi 9.2 Optimization 314 Geometrical view of optimization 316 Stochastic gradient descent and minibatches 316 PyTorch code for SGD 316 Momentum 320 Geometric view: Constant loss contours, gradient descent, and momentum 322 Nesterov accelerated gradients 322 AdaGrad 326 Root-mean-squared propagation 327 Adam optimizer 328 9.3 Regularization 330 Minimum descriptor length: An Occam’s razor view of optimization 330 L2 regularization 332 L1 regularization 333 Sparsity: L1 vs. L2 regularization 333 Bayes’ theorem and the stochastic view of optimization 334 Dropout 336 10 Convolutions in neural networks 343 10.1 One-dimensional convolution: Graphical and algebraical view 345 Curve smoothing via 1D convolution 350 Curve edge detection via 1D convolution 350 One-dimensional convolution as matrix multiplication 351 PyTorch: One-dimensional convolution with custom weights 354 10.2 Convolution output size 356 10.3 Two-dimensional convolution: Graphical and algebraic view 356 Image smoothing via 2D convolution 362 Image edge detection via 2D convolution 362 PyTorch: 2D convolution with custom weights 363 Two-dimensional convolution as matrix multiplication 366 10.4 Three-dimensional convolution 368 Video motion detection via 3D convolution 370 PyTorch: Three-dimensional convolution with custom weights 372 10.5 Transposed convolution or fractionally strided convolution 374 Application of transposed convolution: Autoencoders and embeddings 375 Transposed convolution output size 377 Upsampling via transpose convolution 378 10.6 Adding convolution layers to a neural network 380 PyTorch: Adding convolution layers to a neural network 380 10.7 Pooling 381 xii CONTENTS 11 Neural networks for image classification and object detection 385 11.1 CNNs for image classification: LeNet 386 PyTorch: Implementing LeNet for image classification on MNIST 388 11.2 Toward deeper neural networks 389 VGG (Visual Geometry Group) Net 391 Inception: Network-in-network paradigm 397 ResNet: Why stacking layers to add depth does not scale 401 PyTorch Lightning 406 11.3 Object detection: A brief history 411 R-CNN 411 Fast R-CNN 412 Faster R-CNN 413 11.4 Faster R-CNN: A deep dive 414 Convolutional backbone 414 Region proposal network 415 Fast R-CNN 427 Training the Faster R-CNN 434 Other object-detection paradigms 435 12 Manifolds, homeomorphism, and neural networks 438 12.1 Manifolds 438 Hausdorff property 441 Second countable property 442 12.2 Homeomorphism 443 12.3 Neural networks and homeomorphism between manifolds 444 13 Fully Bayes model parameter estimation 447 13.1 Fully Bayes estimation: An informal introduction 448 Parameter estimation and belief injection 448 13.2 MLE for Gaussian parameter values (recap) 449 13.3 Fully Bayes parameter estimation: Gaussian, unknown mean, known precision 450 13.4 Small and large volumes of training data, and strong and weak priors 453 13.5 Conjugate priors 454 13.6 Fully Bayes parameter estimation: Gaussian, unknown precision, known mean 454 Estimating the precision parameter 455 CONTENTS xiii 13.7 Fully Bayes parameter estimation: Gaussian, unknown mean, unknown precision 457 Normal-gamma distribution 457 Estimating the mean and precision parameters 457 13.8 Example: Fully Bayesian inferencing 459 Maximum likelihood estimation 460 Bayesian inference 460 13.9 Fully Bayes parameter estimation: Multivariate Gaussian, unknown mean, known precision 461 13.10 Fully Bayes parameter estimation: Multivariate, unknown precision, known mean 463 Wishart distribution 463 Estimating precision 464 14 Latent space and generative modeling, autoencoders, and variational autoencoders 468 14.1 Geometric view of latent spaces 469 14.2 Generative classifiers 471 14.3 Benefits and applications of latent-space modeling 472 14.4 Linear latent space manifolds and PCA 474 PyTorch code for dimensionality reduction using PCA 477 14.5 Autoencoders 478 Autoencoders and PCA 481 14.6 Smoothness, continuity, and regularization of latent spaces 481 14.7 Variational autoencoders 483 Geometric overview of VAEs 483 VAE training, losses, and inferencing 485 VAEs and Bayes’ theorem 487 Stochastic mapping leads to latent-space smoothness 487 Direct minimization of the posterior requires prohibitively expensive normalization 487 ELBO and VAEs 488 Choice of prior: Zero-mean, unit-covariance Gaussian 490 Reparameterization trick 492 appendix 497 notations 507 index 509 foreword As a lifelong student of the business of technological innovation, I have often wondered: what sets apart an expert from regular practitioners in any area of technology? An expert tends to have many micro-insights into the subject that often elude the ordinary practitioner. This enables them to come up with solutions that are not visible to others. The primary appeal of this book is to generate that kind of micro-intuitions into the complex subject of machine learning. For all their ubiquitousness, episodic internet recipes do not build such intuitions in a systematic, connected way. This book does. I also agree with the author’s position that such intuitions are impossible to build without a firm grasp of the mathematical understanding of the core principles of machine learning. Of course, all this has to be combined with programming knowledge, without which it becomes idle theory. I like the way this book attends to both theory and practice of machine learning by presenting the mathematics alongside PyTorch code snippets. At present, deep learning is indeed shaping human history. Machine learning and data science jobs are consistently rated as the best. If you are looking for a rewarding career in technology, this may be the area for you. And if you are looking for a book that gives you expert-level understanding but only assumes fairly basic knowledge of mathematics and programming, this is your book. With its joint, side-by-side treatment of math and PyTorch programming, it is perfect for professionals who want to become serious practitioners of the art and science of machine learning. Machine learning lies at the confluence of linear algebra, multivariate statistics, and Python programming, and this book combines them into a single coherent narrative—starting from the basics but rapidly moving into advanced topics. xiv FOREWORD xv A particularly delightful aspect of the book is how it creates geometric intuitions behind complex mathematical concepts. Symbols may be forgotten, but the picture remains in the head. —PRITH BANERJEE, Chief Technology Officer ANSYS, Inc., ex Senior Vice President of Research and Director, HP Labs, formerly Professor and Director of Computa- tional Science and Engineering, University of Illinois at Urbana-Champaign preface Artificial intelligence (machine learning or deep learning to insiders) is quite the rage at this point of time. Media is full of eager and/or paranoid predictions about a world governed by this new technology and quite justifiably so. It’s a knowledge revolution happening in front of our very eyes. Working on computer vision and image processing problems for decades for my PhD, then at Adobe Systems, then at Google, and then at Drishti Technologies (the Silicon Valley start-up that I co-founded), I have been at the bleeding edge of this revolution for a long time. I’ve seen not only what works, but also—perhaps more importantly—what does not work and what almost works. This gives me a unique per- spective. Often when trying to solve practical problems, none of the textbook the- ories will work directly. We must mix various ideas to create a winning concoction. This requires a feel for what works and why and what doesn’t work and why. It is this feel, this understanding of the inner workings of the machine/deep learning theory, along with the insights and intuitions that I hope to transmit to my readers. This brings me to another point. Because of the popularity of the subject, a large volume of “deep-learning-made-easy”-type material exists in print and/or online. These articles don’t do justice to the subject. My reaction to them is “everything should be made as simple as possible, but not simpler.” Deep learning can’t be learned by going through a small fragmented set of simplified recipes from which all math has been scrubbed out. This is a mathematical topic and mastery requires understanding the math along with the programming. What is needed is a resource which presents this xvi PREFACE xvii topic with the requisite amount of math—no more and no less—with the connection between the deep learning and math explicitly spelled out. This is exactly what this book strives to provide with its dual presentation of the math and corresponding PyTorch code snippets. acknowledgments The authors would collectively like to thank all their colleagues at Drishti Technologies, especially Etienne Dejoie and Soumya Dipta Biswas, who actively engaged in many lively discussions of the topics covered in the book; Pinakpani Mukherjee, who created some of the early diagrams; and all the MEAP reviewers whose anonymous contributions made the book possible. They would also like to thank the Manning team for their professionalism and competence, in particular Tiffany Taylor for her sharp and deep reviews. To all the reviewers: Al Krinker, Atul Saurav, Bobby Filar, Chris Giblin, Ekkehard Schnoor, Erik Hansson, Gaurav Bhardwaj, Grigory Sapunov, Ian Graves, James J. Byleckie, Jeff Neumann, Jehad Nasser, Juan Jose Rubio Guillamon, Julien Pohie, Kevin Cheung, Krzysztof Kamyczek, Lucian Mircea Sasu, Matthias Busch, Mike Wall, Mortaza Doulaty, Morteza Kiadi, Nelson González, Nicole Königstein, Ninoslav ˇCerkez, Obiamaka Ag- baneje, Pejvak Moghimi, Peter Morgan, Rauhsan Jha, Sean T. Booker, Sebastián Palma Mardones, Stefano Ongarello, Tony Holdroyd, Vishwesh Ravi Shrimali, and Wiebe de Jong, your suggestions helped make this a better book. From Krish Chaudhury: First and foremost, I would like to thank my family: Devyani (my wife), for covering my back for all these years despite an abundance of reasons not to, and for teaching me the value of pursuing excellence in whatever I do. Anwesa (my daughter), who fills my life with indescribable joy with her love, positive attitude, and empathy. Gouri (my mother), for her unquestioning faith in me. xviii ACKNOWLEDGMENTS xix (Late) Dr. Sujit Chaudhury (my father), for teaching me the value of insights, sincerity, and a life of letters as a goal in itself. I would also like to thank Dr. Vineet Gupta (my former colleague from Google) and Dr. Srayanta Mukherjee (my former colleague from Flipkart), for their valuable comments and encouragement. From Ananya Honnedevasthana Ashok: Writing this book has been much harder than I initially expected. It has been a massive learning experience that wouldn’t have been possible without the unwavering support of my family. In particular, I’d like to thank: Dr. Ashok (my father), for being a perennial role model and always being there for me. Jayanthi (my mother), for her unequivocal belief in me. Susheela (my grandmother), for her unconditional love despite chiding me for spending long hours on the book during weekends. I would also like to thank all my teachers, especially Dr. Viraj Kumar and Prof. N.S. Kumar, for inspiring and indoctrinating a love of learning within me. From Sujay Narumanchi: This book has been a labor of love, requiring more effort than I anticipated but giving me a truly fulfilling learning experience that I will forever cherish. My family and friends have been my pillars of strength throughout this journey. I’d like to thank: Sivakumar (my father), for always believing in me and encouraging me to pursue my dreams. Vinitha (my mother), for being my rock and providing unwavering support through- out my life. Prabhu (my brother), for being a constant source of fun and wisdom. (Late) Ramachandran (my grandfather), for instilling in me a love of mathematics and teaching me the value of learning from first principles. My friends Ambika, Anoop, Bharat, Neel, Pranav, and Sanjana, for providing a listening ear and a shoulder to lean on. From Devashish Shankar: I would like to begin by thanking my parents, Dr. Shiv Shanker and Dr. Sadhana Shanker, for their unwavering support, love, and guidance. Addition- ally, I would like to honor the memory of my late grandfather, Dr. Ajai Shanker, who instilled in me a deep sense of curiosity and a passion for scientific thinking that has guided me throughout my life. I am also deeply grateful to my mentors and colleagues for their guidance and support. about this book Are you the type of person who wants to know why and how things work? Instead of feeling satisfied, even grateful, that a tool solves the problem at hand, do you try to understand what the tool is really doing, why it behaves a certain way, and whether it will work under different circumstances? If yes, you have our sympathy—life won’t be peaceful for you. You also have our best wishes—these pages are dedicated to you. The internet abounds with prebuilt deep learning models and training systems that hardly require you to understand the underlying principles. But practical problems often do not fit any of the publicly available models. These situations call for the development of a custom model architecture. Developing such an architecture requires understanding the mathematical underpinnings of optimization and machine learning. Deep learning and computer vision are very practical subjects, so these questions are relevant: “Is the math necessary? Shouldn’t we spend the time learning, say, the Python nuances of deep learning?” Well, yes and no. Programming skills (in particular, Python) are mandatory. But without an intuitive understanding of the mathematics, the how and why and the answer to “Can I repurpose this model?” will not be visible to you. Mathematics allows you to see the abstractions behind the implementation. In many ways, the ability to form abstractions is the essence of higher intelligence. Abstraction enabled early humans to divine a digging and defending tool from what was merely a sharply pointed stone to other animals. The abstraction of the description of where something is with respect to another thing fixed in the environment (aka coordinate systems and vectors) has done wonders for human civilization. Mathematics is the language for abstractions: the most precise, succinct, and unambiguous known to humankind. Hence, mathematics is absolutely necessary as a tool to study deep learning. But we must remember that it is a tool—no more and no less. The ultimate purpose of xx ABOUT THIS BOOK xxi all the math in the book is to bring out the intuitions and insights that are necessary to gain expertise in the complex world of machine learning. Another equally important tool is the programming language—we have chosen PyTorch—without which all the wisdom cannot be put to practical use. This book connects the two pillars of machine learning—mathematics and programming—via numerous code snippets typically presented together with the math. The book is accom- panied by fully functional code in the GitHub repository. We expect readers to work out the math with paper and pencil and then run the code on a computer to understand the results. This book is not bedtime reading. Having (hopefully) made a case for studying the underlying mathematical principles of deep learning and computer vision, we hasten to add that mathematical rigor is not the goal of this book. Rather, the goal is to provide mathematical (in particular, geometrical) insights that make the subject more intuitive and less like black magic. At the same time, we provide Python coding exercises and visualization aids throughout. Thus, reading this book can be regarded as learning the mathematical foundations of deep learning via geometrical examples and Python exercises. Mastery over the material presented in this book will enable you to Understand state-of-the-art deep learning research papers. The book provides in-depth, intuitive explanations of some of today’s seminal papers. Study and understand a deep learning code base. Use code snippets from the book in your tasks. Prepare for an interview for a role as a machine learning engineer/scientist. Determine whether a real-life problem is amenable to machine/deep learning. Troubleshoot neural network quality issues. Identify the right neural network architecture to solve a real-life problem. Quickly implement a prototype architecture and train a deep learning model for a real-life problem. A word of caution: we often start with the basics but quickly go deeper. It’s important to read individual chapters from beginning to end, even if you’re familiar with the material presented at the start. Finally, the ultimate justification for an intellectual endeavor is to have fun pursuing it. So, the authors will consider themselves successful if you enjoy reading this book. Who should read this book? This book is aimed toward the reader with a basic understanding of engineering math- ematics and Python programming, with a serious intent to learn deep learning. For maximum benefit, the math should be worked out with paper and pencil and the PyTorch programs executed on a computer. Here are some possible reader profiles: A person with a degree in engineering, science, or math, possibly acquired a while ago, who is considering a career switch to deep learning. No prior knowledge of machine learning or deep learning is required. xxii ABOUT THIS BOOK An entry- or mid-level machine learning practitioner who wants to gain deeper insights into the workings of various techniques and graduate from downloading models from the internet and trying them out to developing custom deep learning solutions for real problems, and/or develop the ability to read and understand research publications on the topic. A college student embarking on a career of deep learning. How this book is organized: A road map This book consists of 14 chapters and an appendix. In general, all mathematical concepts are examined from a machine learning point of view. Geometric insights are brought out and PyTorch code is provided wherever appropriate. Chapter 1 is an overview of machine learning and deep learning. Its purpose is to establish the big picture context in the reader’s mind and familiarize the reader with some machine learning concepts like input space, feature space, model training, architecture, loss, and so on. Chapter 2 covers the core concepts of vectors and matrices which form the building blocks for machine learning. It introduces the notions of dot product, vector length, orthogonality, linear systems, eigenvalues and eigenvectors, Moore-Penrose pseudo inverse, matrix diagonalization, spectral decomposition, and so on. Chapter 3 provides an overview of vector calculus concepts needed for under- standing deep learning. We introduce gradients, local approximation of multi- dimensional functions via Taylor expansion in arbitrary dimensional spaces, Hes- sian matrices, gradient descent, convexity, and the connection of all these with the idea of loss minimization in machine learning. This chapter provides the first taste of PyTorch model building. Chapter 4 introduces principal component analysis (PCA) and singular value decomposition (SVD)—key linear algebraic tools for machine learning. We provide end-to-end PyTorch implementation of a SVD-based document retrieval system. Chapter 5 explains the basic concepts of probability distributions from a deep learning point of view. We look at the important properties of distributions like expected value, variance and covariance, and we also cover some of the most popular probability distributions like Gaussian, Bernoulli, binomial, multinomial, categorical, and so on. We also introduce the PyTorch distributions package. Chapter 6 explores Bayesian tools for machine learning. We study the Bayes theo- rem, understand model parameter estimation techniques like maximum likelihood estimation (MLE) and maximum a posteriori (MAP) estimation. We also look at latent variables, regularization, MLE for Gaussian distributions, entropy, cross en- tropy, conditional entropy, and KL divergence. We finally look at Gaussian mixture models (GMMs) and how to model and estimate the parameters of a GMM. Chapter 7 deep dives into neural networks. We study perceptrons, the basic building block of neural networks and how multilayered perceptrons can model ABOUT THIS BOOK xxiii arbitrary polygonal decision boundaries as well as common logic gate operations. This enables them to perform classification. We discuss Cybenko’s universal approximation theorem. Chapter 8 covers activation functions for neural networks, the importance and intuition behind layers. We look at forward propagation and backpropagation (with mathematical proofs) and implement a simple neural network with PyTorch. We study how to train a neural network end to end. Chapter 9 provides an in-depth look into various loss functions which are crucial for effective learning of neural networks. We study the math and the intuitions behind popular loss functions like cross entropy loss, regression loss, focal loss, and so on, implementing them via PyTorch. We look at geometrical insights underlying various optimization techniques like SGD, Nesterov, Adagrad, Adam, and others. Additionally, we understand why regularization is important and its relationship with MLE and MAP. Chapter 10 introduces convolutions, a core operator for computer vision models. We study 1D, 2D, and 3D convolution, as well as transposed convolutions and their intuitive interpretations. We also implement a simple convolutional neural network via PyTorch. Chapter 11 introduces various neural network architectures for image classification and object detection in images. We look at several image classification architectures in detail like LeNet, VGG, Inception, and Resnet. We also provide an in-depth study of Faster R-CNN for object detection. Chapter 12 explores the manifolds, the properties of manifolds like homeomor- phism, Haussdorf property, and second countable property, and also how man- ifolds tie in with neural networks. Chapter 13 provides an introduction to Bayesian parameter estimation. We look at injection of prior belief into parameter estimation and how it can be used in unsupervised/semi-supervised settings. Additionally, we understand conjugate priors and the estimation of Gaussian likelihood parameters under conditions of known/unknown mean and variances. Chapter 14 explores latent spaces and generative modeling. We understand the geometric view of latent spaces and the benefits of latent space modeling. We take another look at PCA with this new lens, along with studying autoencoders and variational autoencoders. We study how variational autoencoders regularize the latent space and hence exhibit superior properties to autoencoders. The appendix covers mathematical proofs and derivations for some of the math- ematical properties introduced in the chapters. About the code This book contains many examples of source code both in numbered listings and in line with normal text. In both cases, source code is formatted in a fixed-width font xxiv ABOUT THIS BOOK like this to separate it from ordinary text. Sometimes code is also in bold to highlight code that has changed from previous steps in the chapter, such as when a new feature adds to an existing line of code. In many cases, the original source code has been reformatted; we’ve added line breaks and reworked indentation to accommodate the available page space in the book. In rare cases, even this was not enough, and listings include line-continuation markers (¯). Additionally, comments in the source code have often been removed from the listings when the code is described in the text. Code annotations accompany many of the listings, highlighting important concepts. You can get executable snippets of code from the liveBook (online) version of this book at https://livebook.manning.com/book/math-and-architectures-of-deep-learning. Fully functional code backing the theory discussed in the book can be found on GitHub at https://github.com/krishnonwork/mathematical-methods-in-deep-learning-ipython and from the Manning website at www.manning.com. The code is presented in the form of Jupyter notebooks (organized by chapter) that can be executed independently. The code is written in Python and uses the popular PyTorch library. Important code snippets are presented as code listings throughout the book, and key concepts are highlighted using code annotations. To get started with the code, clone the repository and follow the steps described in the README. liveBook discussion forum Purchase of Math and Architectures of Deep Learning includes free access to liveBook, Manning’s online reading platform. Using liveBook’s exclusive discussion features, you can attach comments to the book globally or to specific sections or paragraphs. It’s a snap to make notes for yourself, ask and answer technical questions, and receive help from the author and other users. To access the forum, go to https://livebook.manning .com/book/math-and-architectures-of-deep-learning/discussion. You can also learn more about Manning’s forums and the rules of conduct at https://livebook .manning.com/discussion. Manning’s commitment to our readers is to provide a venue where a meaningful dialogue between individual readers and between readers and the author can take place. It is not a commitment to any specific amount of participation on the part of the author, whose contribution to the forum remains voluntary (and unpaid). We suggest you try asking the authors some challenging questions lest their interest stray! The forum and the archives of previous discussions will be accessible from the publisher’s website for as long as the book is in print. about the authors Krishnendu Chaudhury is the CTO and a co-founder of Drishti Technologies in Palo Alto, California, which applies AI to manufacturing. He has been a technology leader and inventor in the field of deep learning and computer vision for decades. Before starting Drishti, Krishnendu spent over 20 years at premier organizations, including Google (2004–2015) and Adobe Systems (1996–2004). He was with Flipkart as head of image sciences from 2015 to 2017. In 2017, he left Flipkart to start Drishti. Krishnendu earned his PhD in computer science from the University of Kentucky in Lexington. He has several dozen patents and publications in leading journals and global conferences to his credit. Ananya Honnedevasthana Ashok, Sujay Narumanchi, and Devashish Shankar are practicing machine learning engineers with multiple patents in the deep learning and computer vision area. They are all members of the founding engineering team at Drishti. xxv about the cover illustration The figure on the cover of Math and Architectures of Deep Learning is “Femme Wotyak,” or “Wotyak Woman,” taken from a collection by Jacques Grasset de Saint-Sauveur, published in 1797. Each illustration is finely drawn and colored by hand. In those days, it was easy to identify where people lived and what their trade or station in life was just by their dress. Manning celebrates the inventiveness and initiative of the computer business with book covers based on the rich diversity of regional culture centuries ago, brought back to life by pictures from collections such as this one. xxvi 1 An overview of machine learning and deep learning This chapter covers A first look at machine learning and deep learning A simple machine learning model: The cat brain Understanding deep neural networks Deep learning has transformed computer vision, natural language and speech process- ing in particular, and artificial intelligence in general. From a bag of semi-discordant tricks, none of which worked satisfactorily on real-life problems, artificial intelligence has become a formidable tool to solve real problems faced by industry, at scale. This is nothing short of a revolution going on under our very noses. To lead the curve of this revolution, it is imperative to understand the underlying principles and abstractions rather than simply memorizing the “how-to” steps of some hands-on guide. This is where mathematics comes in. In this first chapter, we present an overview of deep learning. This will require us to use some concepts explained in subsequent chapters. Don’t worry if there are some open questions at the end of this chapter: it is aimed at orienting your mind toward this difficult subject. As individual concepts become clearer in subsequent chapters, you should consider coming back and re-reading this chapter. 1 2 CHAPTER 1 An overview of machine learning and deep learning 1.1 A first look at machine/deep learning: A paradigm shift in computation Making decisions and/or predictions is a central requirement of life. Doing so essentially involves taking in a set of sensory or knowledge inputs and processing them to generate decisions or estimates. For instance, a cat’s brain is often trying to choose between the following options: run away from the object in front of it vs. ignore the object in front of it vs. approach the object in front of it and purr. The cat’s brain makes that decision by processing sensory inputs like the perceived hardness of the object in front of it, the perceived sharpness of the object in front of it, and so on. This is an instance of a classification problem, where the output is one of a set of possible classes. Some other examples of classification problems in life are as follows: Buy vs. hold vs. sell a certain stock, from inputs like the price history of this stock and the change in price of the stock in recent times Object recognition (from an image): – Is this a car or a giraffe? – Is this a human or a non-human? – Is this an inanimate object or a living object? – Face recognition—is this Tom or Dick or Mary or Einstein or Messi? Action recognition from a video: – Is this person running or not running? – Is this person picking something up or not? – Is this person doing something violent or not? Natural language processing (NLP) from digital documents: – Does this news article belong to the realm of politics or sports? – Does this query phrase match a particular article in the archive? Sometimes life requires a quantitative estimation instead of a classification. A lion’s brain needs to estimate how far to jump so as to land on top of its prey, by processing inputs like speed of the prey and distance to the prey. Another instance of quantitative estimation is estimating a house’s price based on inputs like current income of the house’s owner, crime statistics for the neighborhood, and so on. Machines that make such quantitative estimators are called regressors. Here are some other examples of quantitative estimations required in daily life: Object localization from an image: identifying the rectangle bounding the location of an object Stock price prediction from historical stock prices and other world events Similarity score between a pair of documents Sometimes a classification output can be generated from a quantitative estimate. For instance, the cat brain described earlier can combine the inputs (hardness, sharpness, 1.1 A first look at machine/deep learning: A paradigm shift in computation 3 and so on) to generate a quantitative threat score. If that threat score is high, the cat runs away. If the threat score is near zero, the cat ignores the object in front of it. If the threat score is negative, the cat approaches the object and purrs. Many of these examples are shown in figure 1.1. In each instance, a machine—that is, a brain—transforms sensory or knowledge inputs into decisions or quantitative estimates. The goal of machine learning is to emulate that machine. Note that machine learning has a long way to go before it can catch up with the human brain. The human brain can single-handedly deal with thousands, if not millions, of such problems. On the other hand, at its present state of development, machine learning can hardly create a single general-purpose machine that makes a wide variety of decisions and estimates. We are mostly trying to make separate machines to solve individual tasks (such as a stock picker or a car recognizer). At this point, you may ask, “Wait: converting inputs to outputs—isn’t that exactly what computers have been doing for the last 30 or more years? What is this paradigm shift I am hearing about?” The answer is that it is a paradigm shift because we do not provide a step-by-step instruction Jump length estimate Lion brain Jump length estimator model Speed of prey Distance to prey Image Object recognition model Object class Image Object recognition and localization model Object class + object bounding rectangle Past prices World events Stock price model Stock price Cat brain Sharpness of object in front Hardness of object in front Run away (positive threat) Ignore (near zero threat) Approach and purr (negative threat) Threat score Threat estimator model Thresholder Figure 1.1 Examples of decision making and quantitative estimations in life 4 CHAPTER 1 An overview of machine learning and deep learning set—that is, a program—to the machine to convert the input to output. Instead, we develop a mathematical model for the problem. Let’s illustrate the idea with an example. For the sake of simplicity and concreteness, we will consider a hypothetical cat brain that needs to make only one decision in life: whether to run away from the object in front of it or ignore the object or approach and purr. This decision, then, is the output of the model we will discuss. And in this toy example, the decision is made based on only two quantitative inputs (aka features): the perceived hard- ness and sharpness of the object (as depicted in figure 1.1). We do not provide any step- by-step instructions such as “if sharpness greater than some threshold, then run away.” Instead, we try to identify a parameterized function that takes the input and converts it to the desired decision or estimate. The simplest such function is a weighted sum of inputs: y  hardness, sharpness =w0 × hardness +w1 × sharpness + b The weights w0, w1 and the bias b are the parameters of the function. The output y can be interpreted as a threat score. If the threat score exceeds a threshold, the cat runs away. If it is close to 0, the cat ignores the object. If the threat score is negative, the cat approaches and purrs. For more complex tasks, we will use more sophisticated functions. Note that the weights are not known at first; we need to estimate them. This is done through a process called model training. Overall, solving a problem via machine learning has the following stages: We design a parameterized model function (e.g., weighted sum) with unknown parameters (weights). This constitutes the model architecture. Choosing the right model architecture is where the expertise of the machine learning engineer comes into play. Then we estimate the weights via model training. Once the weights are estimated, we have a complete model. This model can take arbitrary inputs not necessarily seen before and generate outputs. The process in which a trained model processes an arbitrary real-life input and emits an output is called inferencing. In the most popular variety of machine learning, called supervised learning, we prepare the training data before we commence training. Training data comprises example input items, each with its corresponding desired output.1 Training data is often created manually: a human goes over every single input item and produces the desired output (aka target output). This is usually the most arduous part of doing machine learning. For instance, in our hypothetical cat brain example, some possible training data items are as follows 1 If you have some experience with machine learning, you will realize that we are talking about “supervised” learning here. There are also machines that do not need known outputs to learn—so-called “unsupervised” machines—and we will talk about them later. 1.1 A first look at machine/deep learning: A paradigm shift in computation 5 input:  hardness = 0.01, sharpness = 0.02 →threat = −0.90 →decision: “approach and purr” input:  hardness = 0.50, sharpness = 0.60 →threat = 0.01 →decision: “ignore” input:  hardness = 0.99, sharpness = 0.97 →threat = 0.90 →decision: “run away” where the input values of hardness and sharpness are assumed to lie between 0 and 1. What exactly happens during training? Answer: we iteratively process the input train- ing data items. For each input item, we know the desired (aka target) output. On each iteration, we adjust the model weight values in a way that the output of the model func- tion on that specific input item gets at least a little closer to the corresponding target output. For instance, suppose at a given iteration, the weight values are w0 = 20 and w1 = 10, and b = 50. On the input (hardness = 0.01, sharpness = 0.02), we get an output threat score y = 50.3, which is quite different from the desired y = −0.9. We will adjust the weights: for instance, reducing the bias so w0 = 20, w1 = 10, and b = 40. The corre- sponding threat score y = 40.3 is still nowhere near the desired value, but it has moved closer. After we do this on many training data items, the weights will start approaching their ideal values. Note that how to identify the adjustments to the weight values is not discussed here; it requires somewhat deeper math and will be discussed later. As stated earlier, this process of iteratively tuning weights is called training or learning. At the beginning of learning, the weights have random values, so the machine outputs often do not match desired outputs. But with time, more training iterations happen, and the machine “learns” to generate the correct output. That is when the model is ready for deployment in the real world. Given arbitrary input, the model will (hopefully) emit something close to the desired output during inferencing. Come to think of it, that is probably how living brains work. They contain equivalents of mathematical models for various tasks. Here, the weights are the strengths of the connections (aka synapses) between the different neurons in the brain. In the beginning, the parameters are untuned; the brain repeatedly makes mistakes. For example, a baby’s brain often makes mistakes in identifying edible objects—anybody who has had a child will know what we are talking about. But each example tunes the parameters (eating green and white rectangular things with a $ sign on them invites much scolding—should not eat them in the future, etc.). Eventually, this machine tunes its parameters to yield better results. One subtle point should be noted here. During training, the machine is tuning its parameters so that it produces the desired outcome—on the training data input only. Of course, it sees only a small fraction of all possible inputs during training—we are not building a lookup table from known inputs to known outputs. Hence, when this machine is released in the world, it mostly runs on input data it has never seen before. What guarantee do we have that it will generate the right outcome on never-before-seen data? Frankly, there is no guarantee. Only, in most real-life problems, the inputs are not really random. They have a pattern. Hopefully, the machine will see enough during training to capture that pattern. Then its output on unseen input will be close to the desired value. The closer the distribution of the training data is to real life, the more likely that becomes. 6 CHAPTER 1 An overview of machine learning and deep learning 1.2 A function approximation view of machine learning: Models and their training As stated in section 1.1, to create a brain-like machine that makes classifications or estimations, we have to find a mathematical function (model) that transforms inputs into corresponding desired outputs. Sadly, however, in typical real-life situations, we do not know that transformation function. For instance, we do not know the function that takes in past prices, world events, and so on and estimates the future price of a stock—something that stops us from building a stock price estimator and getting rich. All we have is the training data—a set of inputs on which the output is known. How do we proceed, then? Answer: we will try to model the unknown function. This means we will create a function that will be a proxy or surrogate to the unknown function. Viewed this way, machine learning is nothing but function approximation—we are simply trying to approximate the unknown classification or estimation function. Let’s briefly recap the main ideas from the previous section. In machine learning, we try to solve problems that can be abstractly viewed as transforming a set of inputs to an output. The output is either a class or an estimated value. Since we do not know the true transformation function, we try to come up with a model function. We start by designing—using our physical understanding of the problem—a model function with tunable parameter values that can serve as a proxy for the true function. This is the model architecture, and the tunable parameters are also known as weights. The simplest model architecture is one where the output is a weighted sum of the input values. Determining the model architecture does not fully determine the model—we still need to determine the actual parameter values (weights). That is where training comes in. During training, we find an optimal set of weights that transform the training inputs to outputs that match the corresponding training outputs as closely as possible. Then we deploy this machine in the world: its weights are estimated and the function is fully determined, so on any input, it simply applies the function and generates an output. This is called inferencing. Of course, training inputs are only a fraction of all possible inputs, so there is no guarantee that inferencing will yield a desired result on all real inputs. The success of the model depends on the appropriateness of the chosen model architecture and the quality and quantity of training data. Obtaining training data After mastering machine learning, the biggest struggle turns out to be the procurement of training data. When practitioners can afford it, it is common practice to use humans to hand-generate the outputs corresponding to the training data inputs (these target outputs are sometimes referred to as ground truth). This process, known as human labeling or human curation, involves an army of human beings looking at a substantial number of training data inputs and producing the corresponding ground truth outputs. For some well-researched problems, we may be lucky enough to get training data on the internet; otherwise it becomes a daunting challenge. More on this later. 1.3 A simple machine learning model: The cat brain 7 Now, let’s study the process of model building with a concrete example: the cat brain machine shown in figure 1.1. 1.3 A simple machine learning model: The cat brain For the sake of simplicity and concreteness, we will deal with a hypothetical cat that needs to make only one decision in life: whether to run away from the object in front of it, ignore it, or approach and purr. And it makes this decision based on only two quantitative inputs pertaining to the object in front of it (shown in figure 1.1). NOTE This chapter is a lightweight overview of machine/deep learning. As such, it relies some on mathematical concepts that we will introduce later. You are encouraged to read this chapter now, nonetheless, and perhaps re-read it after digesting the chapters on vectors and matrices. 1.3.1 Input features The input features are x0, signifying hardness, and x1, signifying sharpness. Without loss of generality, we can normalize the inputs. This is a pretty popular trick whereby the input values ranging between a minimum possible value vmin and a maximum possible value vmax are transformed to values between 0 and 1. To transform an arbitrary input value v to a normalized value vnorm, we use the formula vnorm = (v −vmin) (vmax −vmin) (1.1) In mathematical parlance, transformation via equation 1.1, v ∈[vmin, vmax] →vnorm ∈ [0, 1] maps the values v from the input domain [vmin, vmax] to the output values vnorm in the range [0, 1]. A two-element vector ®x =  x0 x1  ∈[0, 1]2 represents a single input instance succinctly. 1.3.2 Output decisions The final output is multiclass and can take one of three possible values: 0, implying running away from the object in front of the cat; 1, implying ignoring the object; and 2, implying approaching the object and purring. It is possible in machine learning to compute the class directly. However, in this example, we will have our model estimate a threat score. It is interpreted as follows: threat high positive = run away, threat near zero = ignore, and threat high negative = approach and purr (negative threat is attractive). We can make a final multiclass run/ignore/approach decision based on threat score by comparing the threat score y against a threshold 훿, as follows: y   > 훿→ high threat, run away >= −훿and <= 훿→ threat close to zero, ignore < −훿→ negative threat, approach and purr (1.2) 8 CHAPTER 1 An overview of machine learning and deep learning 1.3.3 Model estimation Now for the all-important step: we need to estimate the function that transforms the input vector to the output. With slight abuse of terms, we will denote this function as well as the output by y. In mathematical notation, we want to estimate y  ®x. Of course, we do not know the ideal function. We will try to estimate this unknown function from the training data. This is accomplished in two steps: 1 Model architecture selection—Designing a parameterized function that we expect is a good proxy or surrogate for the unknown ideal function 2 Training—Estimating the parameters of that chosen function such that the outputs on training inputs match corresponding outputs as closely as possible 1.3.4 Model architecture selection This is the step where various machine learning approaches differ from one another. In this toy cat brain example, we will use the simplest possible model. Our model has three parameters, w0, w1, b. They can be represented compactly with a single two- element vector ®w =  w0 w1  ∈ℝ2 and a constant bias b ∈ℝ(here, ℝdenotes the set of all real numbers, ℝ2 denotes the set of 2D vectors with both elements real, and so on). It emits the threat score, y, which is computed as y (x0, x1) =w0x0 +w1x1 + b = h w0 w1 i  x0 x1  + b = ®wT ®x + b (1.3) Note that b is a slightly special parameter. It is a constant that does not get multiplied by any of the inputs. It is common practice in machine learning to refer to it as bias; the other parameters are multiplied by inputs as weights. 1.3.5 Model training Once the model architecture is chosen, we know the exact parametric function we are going to use to model the unknown function y  ®x that transforms inputs to outputs. We still need to estimate the function’s parameters. Thus, we have a function with unknown parameters, and the parameters are to be estimated from a set of inputs with known outputs (training data). We will choose the parameters so that the outputs on the training data inputs match the corresponding outputs as closely as possible. Iterative training This problem has been studied by mathematicians and is known as a function-fitting problem in mathematics. What changed with the advent of machine learning, however, is the sheer scale. In machine learning, we deal with training data comprising millions 1.3 A simple machine learning model: The cat brain 9 and millions of items. This altered the philosophy of the solution. Mathematicians use a closed-form solution, where the parameters are estimated by directly solving equations involving all the training data items together. In machine learning, we go for iterative solutions, dealing with a few training data items (or perhaps only one) at a time. In the iterative solution, there is no need to hold all the training data in the computer’s memory. We simply load small portions of it at a time and deal with only that portion. We will exemplify this with our cat brain example. Concretely, the goal of the training process is to estimate the parameters w0, w1, b or, equivalently, the vector ®w along with constant b from equation 1.3 in such a way that the output y (x0, x1) on the training data input (x0, x1) matches the corresponding known training data outputs (aka ground truth [GT]) as much as possible. Let the training data consist of N + 1 inputs ®x(0), ®x(1), · · · ®x(N ). Here, each ®x(i) is a 2 × 1 vector denoting a single training data input instance. The corresponding desired threat values (outputs) are y(0) gt , y(1) gt , · · · y(N ) gt , say (here, the subscript gt denotes ground truth). Equivalently, we can say that the training data consists of N + 1  input, output pairs:  ®x(0), y(0) gt  ,  ®x(1), y(1) gt  · · ·  ®x(N ), y(N ) gt  Suppose ®w denotes the (as-yet-unknown) optimal parameters for the model. Then, given an arbitrary input ®x, the machine will estimate a threat value of ypredicted = ®wT ®x + b. On the ith training data pair,  ®x(i), y(i) gt  the machine will estimate y(i) predicted = ®wT ®x(i) + b while the desired output is y(i) gt . Thus the squared error (aka loss) made by the machine on the ith training data instance is2 e2 i =  y(i) predicted −y(i) gt 2 The overall loss on the entire training data set is obtained by adding the loss from each individual training data instance: E2 = i=N Õ i=0 e2 i = i=N Õ i=0  y(i) predicted −y(i) gt 2 = i=N Õ i=0  ®wT ®xi + b −y(i) gt 2 The goal of training is to find the set of model parameters (aka weights), ®w, that minimizes the total error E. Exactly how we do this will be described later. In most cases, it is not possible to come up with a closed-form solution for the optimal ®w, b. Instead, we take an iterative approach depicted in algorithm 1.1. 2 In this context, note that it is a common practice to square the error/loss to make it sign independent. If we desire an output of, say, 10, we are equally happy/unhappy if the output is 9.5 or 10.5. Thus, an error of +5 or −5 is effectively the same; hence we make the error sign independent. 10 CHAPTER 1 An overview of machine learning and deep learning Algorithm 1.1 Training a supervised model Initialize parameters ®w, b with random values ⊲iterate while error not small enough while (E2 = Íi=N i=0  ®wT ®xi + b −y(i) gt 2 > threshold) do ⊲iterate over all training data instances for ∀i ∈[0, N ] do ⊲details provided in section 3.3 after gradients are introduced Adjust ®w, b so that E2 is reduced end for end while ⊲remember the final parameter values as optimal ®w∗←®w, b∗←b In this algorithm, we start with random parameter values and keep tuning the parameters so the total error goes down at least a little. We keep doing this until the error becomes sufficiently small. In a purely mathematical sense, we continue the iterations until the error is minimal. But in practice, we often stop when the results are accurate enough for the problem being solved. It is worth re-emphasizing that error here refers only to error on training data. 1.3.6 Inferencing Finally, a trained machine (with optimal parameters ®w∗, b∗is deployed in the world. It will receive new inputs ®x and will infer ypredicted  ®x = ®wT ∗®x + b∗. Classification will happen by thresholding ypredicted, as shown in equation 1.2. 1.4 Geometrical view of machine learning Each input to the cat brain model is an array of two numbers: x0 (signifying hardness of the object), x1 (signifying sharpness of the object) or, equivalently, a 2 × 1 vector ®x. A good mental picture is to think of the input as a point in a high-dimensional space. The input space is often called the feature space—a space where all the characteristic features to be examined by the model are represented. The feature space dimension is two in this case, but in real-life problems it will be in the hundreds or thousands or more. The exact dimensionality of the input changes from problem to problem, but the intuition that it is a point remains. The output y should also be viewed as a point in another high-dimensional space. In this toy problem, the dimensionality of the output space is one, but in real problems, it will be higher. Typically, however, the number of output dimensions is much smaller than the number of input dimensions. Geometrically speaking, a machine learning model essentially maps a point in the feature space to a point in the output space. It is expected that the classification or 1.4 Geometrical view of machine learning 11 estimation job to be performed by the model is easier in the output space than in the feature space. In particular, for a classification job, input points belonging to separate classes are expected to map to separate clusters in output space. Let’s continue with our example cat brain model to illustrate the idea. As stated earlier, our feature space is 2D, with two coordinate axes X0 signifying hardness and X1 signifying sharpness.3 Individual points in this 2D space are denoted by coordinate values (x0, x1) in lowercase (see figure 1.2). As shown in the diagram, a good way to model the threat score is to measure the distance from line x0 + x1 = 1. From coordinate geometry, in a 2D space with coordinate axes X0 and X1, the signed distance of a point (a, b) from the line x0 + x1 = 1 is y = a+b−1 √ 2 . Examining the sign of y, we can determine which side of the separator line the input point belongs to. In the simple situation depicted in figure 1.2, observation tells us that the threat score can be proxied by the signed distance, y, from the diagonal line x0 + x1 −1 = 0. We can make the run/ignore/approach decision by thresholding y. Values close to zero imply ignore, positive values imply run away, and negative values imply approach and purr. From X0 (hardness) (1,0) (0,1) X1 (sharpness) L: x0 + x1 = 1 Hard and sharp objects $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ P Q y(p) y(q) + + + + + + + + + + + + Soft and roundish objects Figure 1.2 2D input point space for the cat brain model. The bottom-left corner shows objects with low hardness and low sharpness objects (–), while the top-right corner shows objects with high hardness and high sharpness (+). Intermediate values are near the diagonal ($). 3 We use X0, X1 as coordinate symbols instead of the more familiar X, Y so as not to run out of symbols when going to higher-dimensional spaces. 12 CHAPTER 1 An overview of machine learning and deep learning high school geometry, the distance of an arbitrary input point (x0 = a, x1 = b) from line x0 + x1 −1 = 0 is a+b−1 √ 2 . Thus, the function y (x0, x1) = x0+x1−1 √ 2 is a possible model for the cat brain threat estimator function. Training should converge to w0 = 1 √ 2, w1 = 1 √ 2 and b = −1 √ 2. Thus, our simplified cat brain threat score model is y (x0, x1) = 1 √ 2 x0 + 1 √ 2 x1 −1 √ 2 (1.4) It maps the 2D input points, signifying the hardness and sharpness of the object in front of the cat, to a 1D value corresponding to the signed distance from a separator line. This distance, physically interpretable as a threat score, makes it possible to separate the classes (negative threat, neutral, positive threat) via thresholding, as shown in equation 1.2. The separate classes form distinct clusters in the output space, depicted by +, –, and $ signs in the output space. Low values of inputs produce negative threats (the cat will approach and purr): for example, y (0, 0) = −1 √ 2. High values of inputs produce high threats (the cat will run away): for example, y (1, 1) = 1 √ 2. Medium values of inputs produce near-zero threats (the cat will ignore the object): for example, y (0.5, 0.5) = 0. Of course, because the problem is so simple, we could come up with the model parameters via simple observation. In real-life situations, this will need training. The geometric view holds in higher dimensions, too. In general, an n-dimensional input vector ®x is mapped to an m-dimensional output vector (usually m < n) in such a way that the problem becomes much simpler in the output space. An example with 3D feature space is shown in figure 1.3. 1.5 Regression vs. classification in machine learning As briefly outlined in section 1.1, there are two types of machine learning models: regressors and classifiers. In a regressor, the model tries to emit a desired value given a specific input. For instance, the first stage (threat-score estimator) of the cat brain model in section 1.3 is a regressor model. Classifiers, on the other hand, have a set of prespecified classes. Given a specific input, they try to emit the class to which the input belongs. For instance, the full cat brain model has three classes: (1) run away, (2) ignore, and (3) approach and purr. Thus, it takes an input (hardness and sharpness values) and emits an output decision (aka class). In this example, we convert a regressor into a classifier by thresholding the output of the regressor (see equation 1.2). It is also possible to create models that directly output the class without having an intervening regressor. 1.6 Linear vs. nonlinear models In figure 1.2 we faced a rather simple situation where the classes could be separated by a line (a hyperplane in higher-dimensional surfaces). This does not happen often in real 1.6 Linear vs. nonlinear models 13 Feature space Output space X Y Z X Y Model Transform Figure 1.3 A model maps the points from input (feature) space to an output space where it is easier to separate the classes. For instance, in this figure, input feature points belonging to two classes, red (+) and green (–) are distributed over the volume of a cylinder in a 3D feature space. The model unfurls the cylinder into a rectangle. The feature points are mapped onto a 2D planar output space where the two classes can be discriminated with a simple linear separator. life. What if the points belonging to different classes are as shown in figure 1.4? In such cases, our model architecture should no longer be a simple weighted combination. It is a nonlinear function. For instance, check the curved separator in figure 1.4. Nonlinear models make sense from the function approximation point of view as well. Ultimately, our goal is to approximate very complex and highly nonlinear functions that model the classification or estimation processes demanded by life. Intuitively, it seems better to use nonlinear functions to model them. Figure 1.4 The two classes (indicated by light and dark shades) cannot be separated by a line. A curved separator is needed. In 3D, this is equiva- lent to saying that no plane can separate the surfaces; a curved surface is necessary. In still higher-dimensional spaces, this is equivalent to saying that no hyperplane can separate the classes; a curved hypersurface is needed. 14 CHAPTER 1 An overview of machine learning and deep learning A very popular nonlinear function in machine learning is the sigmoid function, so named because it looks like the letter S. The sigmoid function is typically symbolized by the Greek letter 휎. It is defined as 휎(x) = 1 1 + e−x (1.5) The graph of the sigmoid function is shown in figure 1.5. Thus we can use the follow- ing popular model architecture (still kind of simple) that takes the sigmoid (without parameters) of the weighted sum of the inputs: y = 휎  ®wT ®x + b  (1.6) Figure 1.5 The sigmoid graph The sigmoid imparts the nonlinearity. This architecture can handle relatively more complex classification tasks than the weighted sum alone. In fact, equation 1.6 depicts the basic building block of a neural network. 1.7 Higher expressive power through multiple nonlinear layers: Deep neural networks In section 1.6 we stated that adding nonlinearity to the basic weighted sum yielded a model architecture that is able to handle more complex tasks. In machine learning parlance, the nonlinear model has more expressive power. Now consider a real-life problem: say, building a dog recognizer. The input space comprises pixel locations and pixel colors (x, y, r, g, b, where r, g, b denote the red, green, and blue components of a pixel color). The input dimensionality is large (pro- portional to the number of pixels in the image). Figure 1.6 gives a small glimpse of the possible variations in background and foreground that a typical deep learn- ing system (such as a dog image recognizer) has to deal with. We need a machine with really high expressive power here. How do we create such a machine in a princi- pled way? Instead of generating the output from input in a single step, how about taking a cascaded approach? We will generate a set of intermediate or hidden outputs from the 1.7 Higher expressive power through multiple nonlinear layers: Deep neural networks 15 Figure 1.6 A glimpse into background and foreground variations that a typical deep learning system (here, a dog image recognizer) has to deal with inputs, where each hidden output is essentially a single logistic regression unit. Then we add another layer that takes the output of the previous layer as input, and so on. Finally, we combine the outermost hidden layer outputs into the grand output. We describe the system in the following equations. Note that we have added a superscript to the weights to identify the layer (layer 0 is closest to the input; layer L is the last layer, furthest from the input). We have also made the subscripts two- dimensional (so the weights for a given layer become a matrix). The first subscript identifies the destination node, and the second subscript identifies the source node (see figure 1.7). ... ... ... Input layer Layer 0 Layer 1 Final layer h0 (L) h1 (L) h1L (L) σ h1 (L-1) h2 (L-1) h0 (L-1) hlL-1 (L-1) h3 (L-1) ... L – 1 layer ... h1 (0) h2 (0) h0 (0) h10 (0) h3 (0) σ σ σ σ ... h1 (1) h2 (1) h0 (1) h11 (1) h3 (1) ... σ σ σ σ σ ... σ σ σ σ σ ... σ σ σ ... w 00 (0) w10 (0) w10n (0) w11l0 (10) w10 (1) w00 (L) w1LlL-1 (L) w01(L) ... x0 x1 xn Figure 1.7 Multilayered neural network The astute reader may notice that the following equations do not have an explicit bias term. That is because, for simplicity of notation, we have rolled it into the set of weights and assumed that one of the inputs (say, x0 = 1) and the corresponding weight (such as w0) is the bias. 16 CHAPTER 1 An overview of machine learning and deep learning Layer 0: generates n0 hidden outputs from n + 1 inputs h(0) 0 = 휎  w(0) 00 x0 +w(0) 01 x1 + · · · w(0) 0n xn  h(0) 1 = 휎  w(0) 10 x0 +w(0) 11 x1 + · · · w(0) 1n xn  ... h(0) n0 = 휎  w(0) n00x0 +w(0) n01x1 + · · · w(0) n0nxn  (1.7) Layer 1: generates n1 hidden outputs from n0 hidden outputs from layer 0 h(1) 0 = 휎  w(1) 00 h(0) 0 +w(1) 01 h(0) 1 + · · · w(1) 0n0h(0) n0  h(1) 1 = 휎  w(1) 10 h(0) 0 +w(1) 11 h(0) 1 + · · · w(1) 1n0h(0) n0  ... h(1) n1 = 휎  w(1) n10h(0) 0 +w(1) n11h(0) 1 + · · · w(1) n1n0h(0) n0  (1.8) · · · Final layer (L): generates m + 1 visible outputs from nL−1 previous layer hidden outputs h(L) 0 = 휎  w(L) 00 h(L−1) 0 +w(L) 01 h(L−1) 1 + · · · w(L) 0nL−1h(L−1) nL−1  h(L) 1 = 휎  w(L) 10 h(L−1) 0 +w(L) 11 h(L−1) 1 + · · · w(L) 1nL−1h(L−1) nL−1  ... h(L) m = 휎  w(L) m0 h(L−1) 0 +w(L) m1 h(L−1) 1 + · · · w(L) mnL−1h(L−1) nL−1  (1.9) These equations are shown in figure 1.7. The machine depicted in figure 1.7 can be incredibly powerful, with huge expressive power. We can adjust its expressive power systematically to fit the problem at hand. It then is a neural network. We will devote the rest of the book to studying this. Summary In this chapter, we gave an overview of machine learning, leading all the way up to deep learning. The ideas were illustrated with a toy cat brain example. Some mathematical notions (e.g., vectors) were used in this chapter without proper introduction, and you are encouraged to revisit this chapter after vectors and matrices have been introduced. We would like to leave you with the following mental pictures from this chapter: Machine learning is a fundamentally different paradigm of computing. In tradi- tional computing, we provide a step-by-step instruction sequence to the computer, telling it what to do. In machine learning, we build a mathematical model that tries Summary 17 to approximate the unknown function that generates a classification or estimation from inputs. The mathematical nature of the model function is stipulated from the physical nature and complexity of the classification or estimation task. Models have pa- rameters. Parameter values are estimated from training data—inputs with known outputs. The parameter values are optimized so that the model output is as close as possible to training outputs on training inputs. An alternative geometric view of a machine is a transformation that maps points in the multidimensional input space to a point in the output space. The more complex the classification/estimation task, the more complex the ap- proximating function. In machine learning parlance, complex tasks need machines with greater expressive power. Higher expressive power comes from nonlinearity (e.g., the sigmoid function; see equation 1.5) and a layered combination of simpler machines. This takes us to deep learning, which is nothing but a multilayered nonlinear machine. Complex model functions are often built by combining simpler basis functions. Tighten your seat belts: the fun is about to get more intense. 2 Vectors, matrices, and tensors in machine learning This chapter covers Vectors and matrices and their role in data science Working with eigenvalues and eigenvectors Finding the axes of a hyper-ellipse At its core, machine learning, and indeed all computer software, is about number crunching. We input a set of numbers into the machine and get back a different set of numbers as output. However, this cannot be done randomly. It is important to organize these numbers appropriately and group them into meaningful objects that go into and come out of the machine. This is where vectors and matrices come in. These are concepts that mathematicians have been using for centuries—we are simply reusing them in machine learning. In this chapter, we will study vectors and matrices, primarily from a machine learning point of view. Starting from the basics, we will quickly graduate to advanced concepts, restricting ourselves to topics relevant to machine learning. We provide Jupyter Notebook-based Python implementations for most of the concepts discussed in this and other chapters. Complete, fully functional code that can be downloaded and executed (after installing Python and Jupyter Notebook) 18 2.1 Vectors and their role in machine learning 19 can be found at http://mng.bz/KMQ4. The code relevant to this chapter can be found at http://mng.bz/d4nz. 2.1 Vectors and their role in machine learning Let’s revisit the machine learning model for a cat brain introduced in section 1.3. It takes two numbers as input, representing the hardness and sharpness of the object in front of the cat. The cat brain processes the input and generates an output threat score that leads to a decision to run away or ignore or approach and purr. The two input numbers usually appear together, and it will be handy to group them into a single object. This object will be an ordered sequence of two numbers, the first representing hardness and the second representing sharpness. Such an object is a perfect example of a vector. Thus, a vector can be thought of as an ordered sequence of two or more numbers, also known as an array of numbers.1 Vectors constitute a compact way of denoting a set of numbers that together represent some entity. In this book, vectors are represented by lowercase letters with an overhead arrow and arrays by square brackets. For instance, the input to the cat brain model in section 1.3 was a vector ®x =  x0 x1  , where x0 represented hardness and x1 represented sharpness. Outputs to machine learning models are also often represented as vectors. For instance, consider an object recognition model that takes an image as input and emits a set of numbers indicating the probabilities that the image contains a dog, human, or cat, respectively. The output of such a model is a three element vector ®y =  y0 y1 y2  , where the number y0 denotes the probability that the image contains a dog, y1 denotes the probability that the image contains a human, and y2 denotes the probability that the image contains a cat. Figure 2.1 shows some possible input images and corresponding output vectors. In multilayered machines like neural networks, the input and output to a layer can be vectors. We also typically represent the parameters of the model function (see section 1.3) as vectors. This is illustrated in section 2.3. One particularly significant notion in machine learning and data science is the idea of a feature vector. This is essentially a vector that describes various properties of the object being dealt with in a particular machine learning problem. We will illustrate the idea with an example from the world of natural language processing (NLP). Suppose we have a set of documents. We want to create a document retrieval system where, given a new document, we have to retrieve similar documents in the system. This essentially boils down to estimating the similarity between documents in a quantitative fashion. We 1 In mathematics, vectors can have an infinite number of elements. Such vectors cannot be expressed as arrays—but we will mostly ignore them in this book. 20 CHAPTER 2 Vectors, matrices, and tensors in machine learning (a) Output vector [0.9 0.01 0.1] (b) Output vector [0.9 0.01 0.9] (c) Output vector [0.01 0.99 0.01] (d) Output vector [0.88 0.9. 0.001] Figure 2.1 Input images and corresponding output vectors denoting probabilities that the image contains a dog and/or human and/or cat, respectively. Example output vectors are shown. will study this problem in detail later, but for now, we want to note that the most natural way to approach this is to create feature vectors for each document that quantitatively describe the document. In section 2.5.6, we will see how to measure the similarity between these vectors; here, let’s focus on simply creating descriptor vectors for the documents. A popular way to do this is to choose a set of interesting words (we typically exclude words like “and,” “if,” and “to” that are present in all documents from this list), count the number of occurrences of those interesting words in each document, and make a vector of those values. Table 2.1 shows a toy example with six documents Table 2.1 Toy documents and corresponding feature vectors describing them. Words eligible for the feature vector are bold. The first element of the feature vector indicates the number of occurrences of the word gun and the second violence. docid Document Feature vector d0 Roses are lovely. Nobody hates roses. h 0 0 i d1 Gun violence has reached an epidemic proportion in America. h 1 1 i d2 The issue of gun violence is really over-hyped. One can find many instances of violence, where no guns were involved. h 2 2 i d3 Guns are for violence prone people. Violence begets guns. Guns beget violence. h 3 3 i d4 I like guns but I hate violence. I have never been involved in violence. But I own many guns. Gun violence is incomprehensible to me. I do believe gun owners are the most anti violence people on the planet. He who never uses a gun will be prone to senseless violence. h 5 5 i d5 Guns were used in a armed robbery in San Francisco last night. h 1 0 i d6 Acts of violence usually involves a weapon. h 0 1 i 2.1 Vectors and their role in machine learning 21 and corresponding feature vectors. For simplicity, we have considered only two of the possible set of words: gun and violence, plural or singular, uppercase or lowercase. As a different example, the sequence of pixels in an image can also be viewed as a feature vector. Neural networks in computer vision tasks usually expect this feature vector. 2.1.1 The geometric view of vectors and its significance in machine learning Vectors can also be viewed geometrically. The simplest example is a two-element vector ®x =  x0 x1  . Its two elements can be taken to be x and y, Cartesian coordinates in a two- dimensional space, in which case the vector corresponds to a point in that space. Vectors with n elements represent points in an n-dimensional space. The ability to see inputs and outputs of a machine learning model as points allows us to view the model itself as a geometric transformation that maps input points to output points in some high- dimensional space. We have already seen this in section 1.4. It is an enormously powerful concept we will use throughout the book. A vector represents a point in space. Also, an array of coordinate values like  x y  descri- bes the position of one point in a given coordinate system. Hence, an array (of coordinate values) can be viewed as the quantitative representation of a vector. See figure 2.2 to get an intuitive understanding of this. P y x O 90° Figure 2.2 A vector describing the position of point P with respect to point O. The basic mental picture is an arrowed line. This agrees with the definition of a vector that you may have learned in high school: a vector has a magnitude (length of the arrowed line) and direction (indicated by the arrow). On a plane, this is equivalent to the ordered pair of numbers x, y, where the geometric interpretations of x and y are as shown in the figure. In this context, it is worthwhile to note that only the relative positions of the points O and P matter. If both the points are moved, keeping their relationship intact, the vector does not change. For a real life example, consider the plane of a page of this book. Suppose we want to reach the top-right corner point of the page from the bottom-left corner. Let’s call the bottom-left corner O and the top-right corner P. We can travel the width (8.5 inches) to the right to reach the bottom-left corner and then travel the height (11 inches) upward to reach the top-right corner. Thus, if we choose a coordinate system with the bottom-left corner as the origin and the X-axis along the width, and the Y-axis along the height, point P corresponds to the array representation  8.5 11  . But we could also 22 CHAPTER 2 Vectors, matrices, and tensors in machine learning travel along the diagonal from the bottom-left to the top-right corner to reach P from O. Either way, we end up at the same point P. This leads to a conundrum. The vector ® OP represents the abstract geometric notion “position of P with respect to O” independent of our choice of coordinate axes. On the other hand, the array representation depends on the choice of a coordinate system. For example, the array " 8.5 11 # represents the top-right corner point P only under a speci- fic choice of coordinate axes (parallel to the sides of the page) and a reference point (bottom-left corner). Ideally, to be unambiguous, we should specify the coordinate system along with the array representation. Why don’t we ever do this in machine learning? Because in machine learning, it doesn’t exactly matter what the coordinate system is as long as we stick to any fixed coordinate system. Machine learning is about minimizing loss functions (which we will study later). As such, absolute positions of point are immaterial, only relative positions matter. There are explicit rules (which we will study later) that state how the vector transforms when the coordinate system changes. We will invoke them when necessary. All vectors used in a machine learning computation must consistently use the same coordinate system or be transformed appropriately. One other point: planar spaces, such as the plane of the paper on which this book is written, are two-dimensional (2D). The mechanical world we live in is three-dimensional (3D). Human imagination usually fails to see higher dimensions. In machine learning and data science, we often talk of spaces with thousands of dimensions. You may not be able to see those spaces in your mind, but that is not a crippling limitation. You can use 3D analogues in your head. They work in a surprisingly large variety of cases. However, it is important to bear in mind that this is not always true. Some examples where the lower-dimensional intuitions fail at higher dimensions will be shown later. 2.2 PyTorch code for vector manipulations PyTorch is an open source machine learning library developed by Facebook’s artificial intelligence group. It is one of the most elegant practical tools for developing deep learning applications at present. In this book, we aim to familiarize you with PyTorch and similar programming paradigms alongside the relevant mathematics. Knowledge of Python basics will be assumed. You are strongly encouraged to try out all the code snippets in this book (after installing the appropriate packages like PyTorch, that is). All the Python code in this book is produced via Jupyter Notebook. A summary of the theoretical material presented in the code is provided before the code snippet. 2.2.1 PyTorch code for the introduction to vectors Listing 2.1 shows how to create and access vectors and subvectors and slice and dice vectors using PyTorch. NOTE Fully functional code demonstrating how to create a vector and access its elements, executable via Jupyter Notebook, can be found at http://mng.bz/xm8q. 2.3 Matrices and their role in machine learning 23 Listing 2.1 Introduction to vectors via PyTorch torch.tensor represents a multidimensional array. The vector is a 1D tensor that can be initialized by directly specifying values. v = torch.tensor([0.11, 0.01, 0.98, 0.12, 0.98, ,0.85, 0.03, 0.55, 0.49, 0.99, 0.02, 0.31, 0.55, 0.87, 0.63], dtype=torch.float64) Tensor elements are floats by default. We can force tensors to be other types such as float64 (double). first_element = v[0] third_element = v[2] The square bracket operator lets us access individual vector elements. last_element = v[-1] second_last_element = v[-2] Negative indices count from the end of the array. -1 denotes the last element. -2 denotes the second-to-last element. second_to_fifth_elements = v[1:4] The colon operator slices off a range of elements from the vector. first_to_third_elements = v[:2] last_two_elements = v[-2:] Nothing before a colon denotes the beginning of the array. Nothing after a colon denotes the end of the array. num_elements_in_v = len(v) u = np.array([0.11, 0.01, 0.98, 0.12, 0.98, 0.85, 0.03, 0.55, 0.49, 0.99, 0.02, 0.31, 0.55, 0.87, 0.63]) u = torch.from_numpy(u) Torch tensors can be initialized from NumPy arrays. diff = v.sub(u) The difference between the Torch tensor and its NumPy version is zero. u1 = u.numpy() Torch tensors can be converted to NumPy arrays. 2.3 Matrices and their role in machine learning Sometimes it is not sufficient to group a set of numbers into a vector. We have to collect several vectors into another group. For instance, consider the input to training a machine learning model. Here we have several input instances, each consisting of a sequence of numbers. As seen in section 2.1, the sequence of numbers belonging to a single input instance can be grouped into a vector. How do we represent the entire collection of input instances? This is where the concept of matrices comes in handy from the world of mathematics. A matrix can be viewed as a rectangular array of numbers arranged in a fixed count of rows and columns. Each row of a matrix is a vector, and so is each column. Thus a matrix can be thought of as a collection of row vectors. It can also be viewed as a collection of column vectors. We can represent the entire set of numbers that constitute the training input to a machine learning model as a matrix, with each row vector corresponding to a single training instance. 24 CHAPTER 2 Vectors, matrices, and tensors in machine learning Consider our familiar cat-brain problem again. As stated earlier, a single input instance to the machine is a vector ®x =  x0 x1  , where x0 describes the hardness of the object in front of the cat. Now consider a training dataset with many such input instances, each with a known output threat score. You might recall from section 1.1 that the goal in machine learning is to create a function that maps these inputs to their respective outputs with as little overall error as possible. Our training data may look as shown in table 2.2 (note that in real-life problems, the training dataset is usually large—often millions of input-output pairs—but in this toy problem, we will have 8 training data instances). Table 2.2 Example training dataset for our toy machine learning–based cat brain Input value: Hardness Input value: Sharpness Output: Threat score 0 0.11 0.09 −0.8 1 0.01 0.02 −0.97 2 0.98 0.91 0.89 3 0.12 0.21 −0.68 4 0.98 0.99 0.95 5 0.85 0.87 0.74 6 0.03 0.14 −0.88 7 0.55 0.45 0.00 From table 2.2, we can collect the columns corresponding to hardness and sharpness into a matrix, as shown in equation 2.1—this is a compact representation of the training dataset for this problem.2 Example cat-brain dataset matrix X =  0.11 0.09 0.01 0.02 0.98 0.91 0.12 0.21 0.98 0.99 0.85 0.87 0.03 0.14 0.55 0.45  (2.1) Each row of matrix X is a particular input instance. Different rows represent different input instances. On the other hand, different columns represent different feature elements. For example, the 0th row of matrix X is the vector h x00 x01 i representing 2 We usually use uppercase letters to symbolize matrices. 2.4 Python code: Introducing matrices, tensors, and images via PyTorch 25 the 0th input instance. Its elements, x00 and x01 represent different feature elements, hardness and sharpness respectively of the 0th training input instance. 2.3.1 Matrix representation of digital images Digital images are also often represented as matrices. Here, each element represents the brightness at a specific pixel position (x, y coordinate) of the image. Typically, the brightness value is normalized to an integer in the range 0 to 255. 0 is black, 255 is white, and 128 is gray.3 Following is an example of a tiny image, 9 pixels wide and 4 pixels high: I4,9 =  0 8 16 24 32 40 48 56 64 64 72 80 88 96 104 112 120 128 128 136 144 152 160 168 176 184 192 192 200 208 216 224 232 240 248 255  (2.2) The brightness increases gradually from left to right and also from top to bottom. I00 represents the top-left pixel, which is black. I3,8 represents the bottom-right pixel, which is white. The intermediate pixels are various shades of gray between black and white. The actual image is shown in figure 2.3. Figure 2.3 Image corresponding to matrix I4,9 in equation 2.2 2.4 Python code: Introducing matrices, tensors, and images via PyTorch For programming purposes, you can think of tensors as multidimensional arrays. Scalars are zero-dimensional tensors. Vectors are one-dimensional tensors. Matrices are two-dimensional tensors. RGB images are three-dimensional tensors (colorchannels × height ×width). A batch of 64 images is a four-dimensional tensor (64 × colorchannels × height ×width). Listing 2.2 Introducing matrices via PyTorch A matrix is a 2D array of numbers: i.e., a 2D tensor. The entire training data input set for a machine-learning model can be viewed as a matrix. Each input instance is one row. Row count ≡number of training examples, column count ≡training instance size X = torch.tensor( 3 In digital computers, numbers in the range 0..255 can be represented with a single byte of storage; hence this choice. 26 CHAPTER 2 Vectors, matrices, and tensors in machine learning [ [0.11, 0.09], [0.01, 0.02], [0.98, 0.91], [0.12, 0.21], [0.98, 0.99], [0.85, 0.87], [0.03, 0.14], [0.55, 0.45] Cat-brain training data input: 8 examples, each with two values (hardness, sharpness). An 8 × 2 tensor is created by directly specifying values. ] ) print(''Shape of the matrix is: ''.format(X.shape)) The shape of a tensor is a list. For a matrix, the first list element is num rows; the second list element is num columns. first_element = X[0, 0] Square brackets extract individual matrix elements. row_0 = X[0, :] A standalone colon operator denotes all possible indices. row_1 = X[1, 0:2] The colon operator denotes the range of indices. column_0 = X[:, 0] 0th column column_1 = X[:, 1] 1st column Listing 2.3 Slicing and dicing matrices Ranges of rows and columns can be specified via the colon operator to slice off (extract) submatrices. first_3_training_examples = X[:3, ] Extracts the first three training examples (rows) print(''Sharpness of 5-7 training examples is: '' .format(X[5:8, 1])) Extracts the sharpness feature for the 5th to 7th training examples Listing 2.4 Tensors and images in PyTorch PyTorch tensors can be used to represent tensors. A vector is a 1-tensor, a matrix is a 2-tensor, and a scalar is a 0-tensor. tensor = torch.rand((5, 5, 3)) Creates a random tensor of specified dimensions All images are tensors. An RGB image of height H, width W is a 3-tensor of shape [3, H, W]. I49 = torch.tensor([[0, 8, 16, 24, 32, 40, 48, 56, 64], [64, 72, 80, 88, 96, 104, 112, 120, 128], [128, 136, 144, 152, 160, 168, 176, 184, 192], [192, 200, 208, 216, 224, 232, 240, 248, 255]], ) 4 × 9 single-channel image shown in figure 2.3 img = torch.tensor(cv2.imread('../../Figures/dog3.jpg')) Reads a 199 × 256 × 3 image from disk img_b = img[:, :, 0] img_g = img[:, :, 1] Usual slicing dicing operators work. Extracts the red, green, and blue channels of the image as shown in figure 2.4. img_r = img[:, :, 2] img_cropped = img[0:100, 0:100, :] Crops out a 100 × 100 subimage as shown in figure 2.5 2.5 Basic vector and matrix operations in machine learning In this section, we introduce several basic vector and matrix operations along with examples to demonstrate their significance in image processing, computer vision, and 2.5 Basic vector and matrix operations in machine learning 27 0 25 50 75 100 125 150 175 0 50 100 150 200 250 0 25 50 75 100 125 150 175 0 50 100 150 200 250 0 25 50 75 100 125 150 175 0 50 100 150 200 250 0 25 50 75 100 125 150 175 0 50 100 150 200 250 (a) Original image (b) Red channel (c) Green channel (d) Blue channel Figure 2.4 Tensors and images in PyTorch 0 20 40 60 80 0 20 40 60 80 Figure 2.5 Cropped image of dog 28 CHAPTER 2 Vectors, matrices, and tensors in machine learning machine learning. It is meant to be an application-centric introduction to linear algebra. But it is not meant to be a comprehensive review of matrix and vector operations, for which you are referred to a textbook on linear algebra. 2.5.1 Matrix and vector transpose In equation 2.2, we encountered the matrix I4,9 depicting a tiny image. Suppose we want to rotate the image by 90◦so it looks like figure 2.6. The original matrix I4,9 and its transpose IT 4,9 = I9,4 are shown here: I4,9 =  0 8 16 24 32 40 48 56 64 64 72 80 88 96 104 112 120 128 128 136 144 152 160 168 176 184 192 192 200 208 216 224 232 240 248 255  IT 4,9 = I9,4 =  0 64 128 192 8 72 136 200 16 80 144 208 24 88 152 216 32 96 160 224 40 104 168 232 48 112 176 240 56 120 184 248 64 128 192 255  (2.3) Figure 2.6 Image corresponding to the transpose of matrix I4,9 shown in equation 2.3. This is equivalent to rotating the image by 90◦. 2.5 Basic vector and matrix operations in machine learning 29 By comparing equation 2.2 and equation 2.3, you can easily see that one can be obtained from the other by interchanging the row and column indices. This operation is generally known as matrix transposition. Formally, the transpose of a matrix Am,n with m rows and n columns is another matrix with n rows and m columns. This transposed matrix, denoted AT n,m, is such that AT [i, j] = A [j, i]. For instance, the value at row 0 column 6 in matrix I4,9 is 48; in the transposed matrix, the same value appears in row 6 and column 0. In matrix parlance, I4,9[0, 6] = IT 9,4[6, 0] = 48. Vector transposition is a special case of matrix transposition (since all vectors are matrices—a column vector with n elements is an n × 1 matrix). For instance, an arbitrary vector and its transpose are shown next: ®v =  1 2 3  (2.4) ®vT = h 1 2 3 i (2.5) 2.5.2 Dot product of two vectors and its role in machine learning In section 1.3, we saw the simplest of machine learning models where the output is generated by taking a weighted sum of the inputs (and then adding a constant bias value). This model/machine is characterized by the weights w0, w1, and bias b. Take the rows of table 2.2. For example, for row 0, the input values are the hardness of the approaching object = 0.11 and softness = 0.09. The corresponding model output will be y =w0 × 0.11 +w1 × 0.09 + b. In fact, the goal of training is to choose w0, w1, and b such that model outputs are as close as possible to the known outputs; that is, y =w0 × 0.11 + w1 × 0.09 + b should be as close to −0.8 as possible, y =w0 × 0.01 +w1 × 0.02 + b should be as close to −0.97 as possible, that is In general, given an input instance ®x =  x0 x1  , the model output is y = x0w0 + x1w1 + b. We will keep returning to this model throughout the chapter. But first, let’s consider a different question. In this toy example, we have only three model parameters: two weights, w0, w1, and one bias b. Hence it is not very messy to write the model output flat out as y = x0w0 + x1w1 + b. But, with longer feature vectors (that is, more weights) it will become unwieldy. Is there a compact way to represent the model output for a specific input instance, irrespective of the size of the input? Turns out the answer is yes—we can use an operation called dot product from the world of mathematics. We have already seen in section 2.1 that an individual instance of model input can be compactly represented by a vector, say ®x (it can have any number of input values). We can also represent the set of weights as vector ®w—it will have the 30 CHAPTER 2 Vectors, matrices, and tensors in machine learning same number of items as the input vector. The dot product is simply the element-wise multiplication of the two vectors ®x and ®w. Formally, given two vectors ®x =  x0 x1 ... xn  and ®w =  w0 w1 ... wn  , the dot product of the two vectors is defined as ®x · ®w = x0w0 + x1w1 + · · · xnwn (2.6) In other words, the sum of the products of corresponding elements of the two vectors is the dot product of the two vectors, denoted ®a · ®b. NOTE The dot product notation can compactly represent the model output as y = ®w · ®x + b. The representation does not increase in size even when the number of inputs and weights is large. Consider our (by now familiar) cat-brain example again. Suppose the weight vector is ®w =  3 2  and the bias value b = 5. Then the model output for the 0th input instance from table 2.2 will be  0.11 0.09  ·  3 2  = 0.11 × 3 + 0.09 × 2 + 5 = 5.51. It is another matter that these are bad choices for weight and bias parameters, since the model output 5.51 is a far cry from the desired output −0.89. We will soon see how to obtain better parameter values. For now, we just need to note that the dot product offers a neat way to represent the simple weighted sum model output. NOTE The dot product is defined only if the vectors have the same dimensions. Sometimes the dot product is also referred to as inner product, denoted D ®a, ®b E . Strictly speaking, the phrase inner product is a bit more general; it applies to infinite-dimensional vectors as well. In this book, we will often use the terms interchangeably, sacrificing mathematical rigor for enhanced understanding. 2.5.3 Matrix multiplication and machine learning Vectors are special cases of matrices. Hence, matrix-vector multiplication is a special case of matrix-matrix multiplication. We will start with that. 2.5 Basic vector and matrix operations in machine learning 31 MATRIX-VECTOR MULTIPLICATION In section 2.5.2, we saw that given a weight vector, say ®w =  3 2  , and the bias value b = 5, the weighted sum model output upon a single input instance, say  0.11 0.09  , can be represented using a vector-vector dot product ®w · ®x + b =  0.11 0.09  ·  3 2  + 5. As depicted in equation 2.1, during training, we are dealing with many training data instances at the same time. In real life, we typically deal with hundreds of thousands of input instances, each having hundreds of values. Is there a way to represent the model output for the entire training dataset compactly, such that it is independent of the count of input instances and their sizes? The answer turns out to be yes. We can use the idea of matrix-vector multiplication from the world of mathematics. The product of a matrix X and column vector ®w is another vector, denoted X ®w. Its elements are the dot products between the row vectors of X and the column vector ®w. For example, given the model weight vector ®w =  3 2  and the bias value b = 5, the outputs on the toy training dataset of our familiar cat-brain model (equation 2.1) can be obtained via the following steps:  0.11 0.09 0.01 0.02 0.98 0.91 0.12 0.21 0.98 0.99 0.85 0.87 0.03 0.14 0.55 0.45   3 2  =  0.11 × 3 + 0.09 × 2 = 0.51 0.01 × 3 + 0.02 × 2 = 0.07 0.98 × 3 + 0.91 × 2 = 4.76 0.12 × 3 + 0.21 × 2 = 0.78 0.98 × 3 + 0.99 × 2 = 4.92 0.85 × 3 + 0.87 × 2 = 4.29 0.03 × 3 + 0.14 × 2 = 0.37 0.55 × 3 + 0.45 × 2 = 2.55  (2.7) Adding the bias value of 5, the model output on the toy training dataset is  5 + 0.51 = 5.51 5 + 0.07 = 5.07 5 + 4.76 = 9.76 5 + 0.78 = 5.78 5 + 4.92 = 9.92 5 + 4.29 = 9.29 5 + 0.37 = 5.37 5 + 2.55 = 7.55  (2.8) 32 CHAPTER 2 Vectors, matrices, and tensors in machine learning In general, the output of our simple model (biased weighted sum of input elements) can be expressed compactly as ®y = X ®w + ®b. MATRIX-MATRIX MULTIPLICATION Generalizing the notion of matrix times vector, we can define matrix times matrix. A matrix with m rows and p columns, say Am,p, can be multiplied with another matrix with p rows and n columns, say Bp,n, to generate a matrix with m rows and n columns, say Cm,n: for example, Cm,n = Am,p Bp,n. Note that the number of columns in the left matrix must match the number of rows in the right matrix. Element i, j of the result matrix, Ci, j, is obtained by point-wise multiplication of the elements of the ith row vector of A and the jth column vector of B. The following example illustrates the idea: A3,2 =  a11 a12 a21 a22 a31 a32  B2,2 =  b11 b12 b21 b22  C3,2 =  a11 a12 a21 a22 a31 a32   b11 b12 b21 b22  =  c11 = a11b11 + a12b21 c12 = a11b12 + a12b22 c21 = a21b11 + a22b21 c22 = a21b12 + a22b22 c31 = a31b11 + a32b21 c32 = a31b12 + a32b22  The computation for C2,1 is shown via bolding by way of example. NOTE Matrix multiplication is not commutative. In general, AB ≠BA. At this point, the astute reader may already have noted that the dot product is a spe- cial case of matrix multiplication. For instance, the dot product between two vectors ®w =  w0 w1  and ®x =  x0 x1  is equivalent to transposing either of the two vectors and then doing a matrix multiplication with the other. In other words, ®w · ®x = ®wT ®x =  w0 w1  T  x0 x1  = h w0 w1 i  x0 x1  = ®xT ®w =w0x0 +w1x1 2.5 Basic vector and matrix operations in machine learning 33 The idea works in higher dimensions, too. In general, given two vectors ®x =  x0 x1 ... xn  and ®w =  w0 w1 ... wn  , the dot product of the two vectors is defined as ®x · ®w = ®wT ®x = h w0 w1 · · · wn i  x0 x1 ... xn  = ®xT ®w = h x0 x1 · · · xn i  w0 w1 ... wn  = x0w0 + x1w1 + · · · xnwn (2.9) Another special case of matrix multiplication is row-vector matrix multiplication. For example, ®bT A = ®c or h b1 b2 b3 i  a11 a12 a21 a22 a31 a32  = h c1 = a11b1 + a21b2 + a31b3 c2 = a12b1 + a22b2 + a32b3 i TRANSPOSE OF MATRIX PRODUCTS Given two matrices A and B, where the number of columns in A matches the number of rows in B (that is, it is possible to multiply them), the transpose of the product is the product of the individual transposes, in reversed order. The rule also applies to matrix-vector multiplication. The following equations capture this rule: (AB)T = BT AT  A®xT = ®xT AT  ®xT A T = AT ®x (2.10) 34 CHAPTER 2 Vectors, matrices, and tensors in machine learning 2.5.4 Length of a vector (L2 norm): Model error Imagine that a machine learning model is supposed to output a target value ¯y, but it outputs y instead. We are interested in the error made by the model. The error is the difference between the target and the actual outputs. Squared error When a computing error occurs, we are only interested in how far the computed value is from ideal. We do not care whether the computed value is bigger or smaller than ideal. For instance, if the target (ideal) value is 2, the computed values 1.5 and 2.5 are equally in error—we are equally happy or unhappy with either of them. Hence, it is common practice to square error values. Thus for instance, if the target value is 2 and the computed value is 1.5, the error is (1.5 −2)2 = 0.25. If the target value is 2.5, the error is (2.5 −2)2 = 0.25. The squaring operation essentially eliminates the sign of the error value. We can then follow it up with a square root, but it is OK not to. You might ask, “But wait: squaring alters the value of the quantity. Don’t we care about the exact value of the error?” The answer is, we usually don’t; we only care about relative values of errors. If the target is 2, we want the error for an output value of, say, 2.1 to be less than the error for an output value of 2.5; the exact values of the errors do not matter. Let’s apply this idea of squaring to machine learning model error. As seen earlier in section 2.5.3, given a model weight vector, say ®w =  3 2  , and the bias value b = 5, the weighted sum model output upon a single input instance, say  0.11 0.09  , is  0.11 0.09  ·  3 2  + 5 = 5.51. The corresponding target (ideal) output, from table 2.2, is −0.8. The squared error e2 = (−0.8 −5.51)2 = 39.82 gives us an idea of how good or bad the model parameters 3, 2, 5 are. For instance, if we instead use a weight vector ®w =  1 1  and bias value −1, we get model output ®w · ®x + b =  0.11 0.09  ·  1 1  −1 = −0.8. The output is exactly the same as the target. The corresponding squared error e2 = (−0.8 −(−0.8))2 = 0. This (zero error) immediately tells us that 1, 1, −1 are much better choices of model parameters than 3, 2, 5. In general, the error made by a biased weighted sum model can be expressed as follows. If ®w denotes the weight vector and ®b denotes the bias, the output corresponding to an input instance ®x can be expressed as y = ®w · ®x + b. Let ¯y denote the corresponding target (ground truth). Then the error is defined as e = (y −¯y)2. 2.5 Basic vector and matrix operations in machine learning 35 Thus we see that we can compute the error on a single training instance by taking the difference between the model output and the ground truth and squaring it. How do we extend this concept over the entire training dataset? The set of outputs corresponding to the entire set of training inputs can be expressed as the output vector ®y = X ®w + ®b. The corresponding target output vector, consisting of the entire set of ground truths can be expressed as ¯®y. The differences between the target and model output over the entire training set can be expressed as another vector ¯®y −®y. In our particular example: ®y =  5.51 5.07 9.76 5.78 9.92 9.29 5.37 7.55  ¯®y =  −0.8 −0.97 0.89 −0.67 0.97 0.72 −0.83 0.00  and ¯®y −®y =  5.51 5.07 9.76 5.78 9.92 9.29 5.37 7.55  −  −0.8 −0.97 0.89 −0.67 0.97 0.72 −0.83 0.00  =  6.31 6.04 8.87 6.45 8.95 8.57 6.2 7.55  Thus the total error over the entire training dataset is obtained by taking the difference between the output and the ground truth vector, squaring its elements and adding them up. Recalling equation 2.9, this is exactly what will happen if we take the dot product of the difference vector with itself. That happens to be the definition of the squared magnitude or length or L2 norm of a vector: the dot product of the vector with itself. In the previous example, the overall training (squared) error is: E2 = ¯®y −®y  · ¯®y −®y  = ¯®y −®y T ¯®y −®y  =  6.31 6.04 8.87 6.45 8.95 8.57 6.2 7.55  ·  6.31 6.04 8.87 6.45 8.95 8.57 6.2 7.55  = (6.31)2 + (6.04)2 + (8.87)2 + (6.45)2 + (8.95)2 + (8.57)2 + (6.2)2 + (7.55)2 Formally, the length of a vector ®v =  v1 v2 ... vn  , denoted ∥®v∥, is defined as ∥®v∥= √ ®vT ®v = q v2 1 +v2 2 + · · · v2 n. This quantity is sometimes called the L2 norm of the vector. 36 CHAPTER 2 Vectors, matrices, and tensors in machine learning In particular, given a machine learning model with output vector ®y and a target vector ¯®y, the error is the same as the magnitude or L2 norm of the difference vector e = ∥¯®y −®y∥= r¯®y −®y  · ¯®y −®y  = r¯®y −®y T ¯®y −®y  2.5.5 Geometric intuitions for vector length For a 2D vector ®v =  x y  , as seen in figure 2.2, the L2 norm ∥®v∥= p x2 + y2 is nothing but the hypotenuse of the right-angled triangle whose sides are elements of the vector. The same intuition holds in higher dimensions. A unit vector is a vector whose length is 1. Given any vector ®v, the corresponding unit vector can be obtained by dividing every element by the length of that vector. For example, given ®v =  1 1  , length ∥®v∥= √ 12 + 12 = √ 2 and the corresponding unit vector ˆv =  1 √ 2 1 √ 2  . Unit vectors typically represent a direction. NOTE Unit vectors are conventionally depicted with the hat symbol as opposed to the little overhead arrow, as in ˆuT ˆu = 1. In machine learning, the goal of training is often to minimize the length of the error vector (the difference between the model output vector and the target ground truth vector). 2.5.6 Geometric intuitions for the dot product: Feature similarity Consider the document retrieval problem depicted in table 2.1 one more time. We have a set of documents, each described by its own feature vector. Given a pair of such documents, we must find their similarity. This essentially boils down to estimating the similarity between two feature vectors. In this section, we will see that the dot product between a pair of vectors can be used as a measure of similarity between them. For instance, consider the feature vectors corresponding to d5 and d6 in table 2.1. They are  1 0  and  0 1  . The dot product between them is 1 × 0 + 0 × 1 = 0. This is low and agrees with our intuition that there is no common word of interest between them, so the documents are very dissimilar. On the other hand, the dot product between feature vectors of d3 and d4 is  3 3  ·  5 5  = 3 × 5 + 3 × 5 = 30. This is high and agrees with our intuition that they have many commonalities in words of interest and are similar documents. Thus, we get the first glimpse of an important concept. Loosely speaking, similar vectors have larger dot products, and dissimilar vectors have near-zero dot products. 2.5 Basic vector and matrix operations in machine learning 37 We will keep revisiting this problem of estimating similarity between feature vectors and solve it with more and more finesse. As a first attempt, we will now study in greater detail how dot products measure similarities between vectors. First we will show that the component of a vector along another is yielded by the dot product. Using this, we will show that the “similarity/agreement” between a pair of vectors can be estimated using the dot product between them. In particular, we will see that if the vectors point in more or less the same direction, their dot products are higher than when the vectors are perpendicular to each other. If the vectors point in opposite directions, their dot product is negative. DOT PRODUCT MEASURES THE COMPONENT OF ONE VECTOR ALONG ANOTHER Let’s examine a special case first: the component of a vector along a coordinate axis. This can be obtained by multiplying the length of the vector with the cosine of the angle between the vector and the relevant coordinate axis. As shown for 2D in figure 2.7a, a vector ®v can be broken into two components along the X and Y axes as ®v =  ∥v∥cos 휃 ∥v∥cos (90◦−휃)  =  ∥v∥cos 휃 ∥v∥sin 휃  Note how the length of the vector is preserved:  ∥v∥cos 휃 ∥v∥sin 휃  = ∥v∥  cos2 휃+ sin2 휃  = ∥v∥ Y X (a) Components of a 2D vector along coordinate axes. Note that ∥®a∥is the length of hypotenuse. Y X (b) Dot product as a component of one vector along another ®a · ®b = ®aT ®b = axbx + ayby = ∥®a∥∥®b∥cos (휃). Figure 2.7 Vector components and dot product Now let’s study the more general case of the component of one vector in the direction of another arbitrary vector (figure 2.7b). The component of a vector ®a along another vector ®b is ®a · ®b = ®aT ®b. This is equivalent to ∥®a∥∥®b∥cos (휃), where 휃is the angle between 38 CHAPTER 2 Vectors, matrices, and tensors in machine learning the vectors ®a and ®b. (This has been proven for the two-dimension case discussed in section A.1 of the appendix. You can read it if you would like deeper intuition.) DOT PRODUCT MEASURES THE AGREEMENT BETWEEN TWO VECTORS The dot product can be expressed using the cosine of the angle between the vectors. Given two vectors ®a and ®b, if 휃is the angle between them, we have (see figure 2.7b) ®a · ®b = axbx + ayby for two dimensions ®a · ®b = ®aT ®b = ∥®a∥∥®b∥cos (휃) for all dimensions (2.11) Expressing the dot product using cosines makes it easier to see that it measures the agreement (aka correlation) between two vectors. If the vectors have the same direction, the angle between them is 0 and the cosine is 1, implying maximum agreement. The cosine becomes progressively smaller as the angle between the vectors increases, until the two vectors become perpendicular to each other and the cosine is zero, implying no correlation—the vectors are independent of each other. If the angle between them is 180◦, the cosine is −1, implying that the vectors are anti-correlated. Thus, the dot product of two vectors is proportional to their directional agreement. What role do the vector lengths play in all this? The dot product between two vectors is also proportional to the lengths of the vectors. This means agreement scores between bigger vectors are higher (an agreement between the US president and the German chancellor counts more than an agreement between you and me). If you want the agreement score to be neutral to the vector length, you can use a normalized dot product between unit-length vectors along the same directions: cosine_similarity  ®a, ®b  = ®aT ®b ∥®a∥∥®b∥ = cos (휃) The normalized dot product (aka cosine similarity measure) indicates pure directional agreement. It is often used in document processing. Suppose we have some query text that we want to match against various archive documents and retrieve them rank-ordered by their similarity to the query text. A descriptor vector corresponds to the query text as well as to each archived document. We can use the dot product between descriptor vectors as a measure of similarity, but we do not want longer documents to automatically score higher in similarity. Hence, we use cosine similarity to make the similarity score independent of the length of the document. Document retrieval and cosine similarity are discussed in detail in section 4.6.1. DOT PRODUCT AND THE DIFFERENCE BETWEEN TWO UNIT VECTORS To obtain further insight into how the dot product indicates agreement or correla- tion between two directions, consider the two unit vectors ˆu =  ux uy uz  and ˆv =  vx vy vz  . The difference between them is ˆu −ˆv =  ux −vx uy −vy uz −vz  . 2.7 Python code: Basic vector and matrix operations via PyTorch 39 Note that since they are unit vectors, ∥ˆu∥= q u2 x + u2 y + u2 z = ∥ˆv∥= q v2 x +v2 y +v2 z = 1. The length of the difference vector ∥ˆu −ˆv∥= q (ux −vx)2 +  uy −vy 2 + (uz −vz)2 = q u2 x + u2 y + u2 z +v2 x +v2 y +v2 z −2  uxvx + uyvy + uzvz  = p 2 −2ˆuTˆv = p 2 (1 −ˆu · ˆv) From the last equality, it is evident that a larger dot product implies a smaller difference: that is, more agreement between the vectors. 2.6 Orthogonality of vectors and its physical significance Try moving an object at right angles to the direction in which you are pushing it. You will find it impossible. The larger the angle, the less effective your force vector becomes (finally becoming totally ineffective at a 90◦angle). This is why it is easy to walk on a horizontal surface (you are moving at right angles to the direction of gravitational pull, so the gravity vector is ineffective) but harder on an upward incline (the gravity vector is having some effect against you). These physical notions are captured mathematically in the notion of a dot product. The dot product between two vectors ®a (say, the push vector) and ®b (say, the displacement of the pushed object vector) is ∥®a∥∥®b∥cos휃, where 휃is the angle between the two vectors. When 휃is 0 (the two vectors are aligned), cos휃= 1, the maximum possible value of cos휃, so push is maximally effective. As 휃increases, cos휃decreases, and push becomes less and less effective. Finally, at 휃= 90◦, cos휃= 0, and push becomes completely ineffective. Two vectors are orthogonal if their dot product is zero. Geometrically, this means the vectors are perpendicular to each other. Physically, this means the two vectors are independent: one cannot influence the other. You can say there is nothing in common between orthogonal vectors. For instance, the feature vector for d5 is  1 0  and that for d6 is  0 1  in table 2.1. These are orthogonal (dot product is zero), and you can easily see that none of the feature words (gun, violence) are common to both documents. 2.7 Python code: Basic vector and matrix operations via PyTorch In this section, we use Python PyTorch code to illustrate many of the concepts discussed earlier. NOTE Fully functional code for this section, executable via Jupyter Notebook, can be found at http://mng.bz/ryzE. 2.7.1 PyTorch code for a matrix transpose The following listing shows the PyTorch code for a matrix transpose. 40 CHAPTER 2 Vectors, matrices, and tensors in machine learning Listing 2.5 Transpose The torch.arange function creates a vector whose elements go from start to stop in increments of step. Here we create a 4 × 9 image corresponding to I4,9 in equation 2.2, shown in figure 2.3. I49 = torch.stack([torch.arange(0, 72, 8), torch.arange(64, 136, 8), torch.arange(128, 200, 8), torch.arange(192, 264, 8)]) The transpose operator interchanges rows and columns. The 4 × 9 image becomes a 9 × 4 image (see figure 2.6). The element at position (i, j) is interchanged with the element at position ( j, i). I49_t = torch.transpose(I49, 0, 1) for i in range(0, I49.shape[0]): for j in range(0, I49.shape[1]): assert I49[i][j] == I49_t[j][i] Interchanged elements of the original and transposed matrix are equal. assert torch.allclose(I49_t, I49.T, 1e-5) The .T operator retrieves the transpose of an array. 2.7.2 PyTorch code for a dot product The dot product of two vectors ®a and ®b represents the components of one vector along the other. Consider two vectors ®a = [a1 a2 a3] and ®b = [b1 b2 b3]. Then ®a.®b = a1b1 + a2b2 + a3b3. Listing 2.6 Dot product a = torch.tensor([1, 2, 3]) b = torch.tensor([4, 5, 6]) a_dot_b = torch.dot(a, b) print(''Dot product of these two vectors is: '' ''''.format(a_dot_b)) Outputs 32: 1 ∗4 + 2 ∗5 + 3 ∗6 # Dot product of perpendicular vectors is zero vx = torch.tensor([1, 0]) # a vector along X-axis vy = torch.tensor([0, 1]) # a vector along Y-axis print(''Example dot product of orthogonal vectors:'' '' ''.format(torch.dot(vx, vy))) Outputs 0: 1 ∗0 + 0 ∗1 2.7.3 PyTorch code for matrix vector multiplication Consider a matrix Am,n with m rows and n columns that is multiplied with a vector ®bn with n elements. The result is a m element column vector ®cm . In the following example, m = 3 and n = 2.  a11 a12 a21 a22 a31 a32   b1 b2  =  c1 = a11b1 + a12b2 c2 = a21b2 + a22b2 c3 = a31b2 + a32b2  In general, ci = ai1b1 + ai2b2 + · · · + ainbn 2.7 Python code: Basic vector and matrix operations via PyTorch 41 Listing 2.7 Matrix vector multiplication A linear model comprises a weight vector ®w and bias b. For each training data instance ®xi, the model outputs yi = ®xT i ®w + b. For the training data matrix X (whose rows are training data instances), the model outputs X ®w + ®b = ®y. X = torch.tensor([[0.11, 0.09], [0.01, 0.02], [0.98, 0.91], [0.12, 0.21], [0.98, 0.99], [0.85, 0.87], [0.03, 0.14], [0.55, 0.45], [0.49, 0.51], [0.99, 0.01], [0.02, 0.89], [0.31, 0.47], [0.55, 0.29], [0.87, 0.76], [0.63, 0.24]]) Cat-brain 15 × 2 training data matrix (equation 2.7) w = torch.rand((2, 1)) Random initialization of weight vector b = 5.0 Model training output: ®y = X ®w + b. The scalar b is automatically replicated to create a vector. y = torch.matmul(X, w) + b 2.7.4 PyTorch code for matrix-matrix multiplication Consider a matrix Am,p with m rows and p columns. Let’s multiply it with another matrix Bp,n with p rows and n columns. The resultant matrix Cm,n contains m rows and n columns. Note that the number of columns in the left matrix A should match the number of rows in the right matrix B:  c11 c12 c21 c22 c31 c32  =  a11 a12 a21 a22 a31 a32  " b11 b12 b21 b22 # c11 = a11b11 + a12b21 c12 = a11b12 + a12b22 c21 = a21b11 + a22b21 c22 = a21b12 + a22b22 c31 = a31b11 + a32b21 c32 = a31b12 + a32b22 In general, ci j = p Õ i=1 aipbpj Listing 2.8 Matrix-matrix multiplication A = torch.tensor([[1, 2], [3, 4], [5, 6]]) B = torch.tensor([[7, 8], [9, 10]]) C = AB =⇒C[i, j] is the dot product of the ith row of A and jth column of B. C = torch.matmul(A, B)  1 2 3 4 5 6  " 7 8 9 10 # =  25 28 57 64 89 100  42 CHAPTER 2 Vectors, matrices, and tensors in machine learning w = torch.tensor([1, 2, 3]) x = torch.tensor([4, 5, 6]) assert torch.dot(w, x) == torch.matmul(w.T, x) The dot product can be viewed as a row matrix multiplied by a column matrix. 2.7.5 PyTorch code for the transpose of a matrix product Given two matrices A and B, where the number of columns in A matches the number of rows in B, the transpose of their product is the product of the individual transposes in reversed order: (AB)T = BT AT. Listing 2.9 Transpose of a matrix product Asserts equality between (AB)T and BT AT assert torch.all(torch.matmul(A, B).T == torch.matmul(B.T, A.T)) Applies to matrix-vector multiplication, too:  AT ®xT = ®xT A assert torch.all(torch.matmul(A.T, x).T == torch.matmul(x.T, A)) 2.8 Multidimensional line and plane equations and machine learning Geometrically speaking, what does a machine learning classifier really do? We provided the outline of an answer in section 1.4. You are invited to review that and especially figures 1.2 and 1.3. We will briefly summarize here. Inputs to a classifier are feature vectors. These vectors can be viewed as points in some multidimensional feature space. The task of classification then boils down to separating the points belonging to different classes. The points may be all jumbled up in the input space. It is the model’s job to transform them into a different (output) space where it is easier to separate the classes. A visual example of this was provided in figure 1.3. What is the geometrical nature of the separator? In a very simple situation, such as the one depicted in figure 1.2, the separator is a line in 2D space. In real-life situations, the separator is often a line or a plane in a high-dimensional space. In more complicated situations, the separator is a curved surface, as depicted in figure 1.4. In this section, we will study the mathematics and geometry behind two types of separators, lines, and planes in high-dimensional spaces, aka hyperlines and hyperplanes. 2.8.1 Multidimensional line equation In high school geometry, we learned y = mx + c as the equation of a line. But this does not lend itself readily to higher dimensions. Here we will study a better representation of a straight line that works equally well for any finite-dimensional space. As shown in figure 2.8, a line joining vectors ®a and ®b can be viewed as the set of points we will encounter if we Start at point ®a Travel along the direction ®b −®a Different points on the line are obtained by traveling different distances. Denoting this arbitrary distance by 훼, the equation of the line joining vectors ®a and ®b can be 2.12 Linear systems and matrix inverse 43 X Y 0 Figure 2.8 Any point ®x on the line joining two vectors ®a, ®b is given by ®x = ®a + 휶 ®b −®a  . expressed as ®x = ®a + 훼  ®b −®a  = (1 −훼) ®a + 훼®b or (1 −훼) ®a + 훼®b −®x = 0 (2.12) Equation 2.12 says that any point on the line joining ®a and ®b can be obtained as a weighted combination of ®a and ®b, the weights being 훼and 1 −훼. By varying 훼, we obtain different points on the line. Also, different ranges of 훼values yield different segments on the line. As shown in figure 2.8, values of 훼between 0 and 1 yield points between ®a and ®b. Negative values of 훼yield points to the left of ®a. Values of 훼greater than 1 yield points to the right of ®b. This equation for a line works for any dimensions, not just two. 2.8.2 Multidimensional planes and their role in machine learning In section 1.5, we encountered classifiers. Let’s take another look at them. Suppose we want to create a classifier that helps us make buy or no-buy decisions on stocks based on only three input variables: (1) momentum, or the rate at which the stock price is changing (positive momentum means the stock price is increasing and vice versa); 44 CHAPTER 2 Vectors, matrices, and tensors in machine learning (2) the dividend paid last quarter; and (3) volatility, or how much the price has fluctuated in the last quarter. Let’s plot all training points in the feature space with coordinate axes corresponding to the variables momentum, dividend, volatility. Figure 2.9 shows that the classes can be separated by a plane in the three-dimensional feature space. Volatility Momentum Dividend Figure 2.9 A toy machine learning classifier for stock buy vs. no-buy decision-making. A plus (+) indicates no-buy, and a dash (-) indicates buy. The decision is made based on three input variables: momentum, dividend, and volatility. Geometrically speaking, our model simply corresponds to this plane. Input points above the plane indicate buy decisions (dashes [-]), and input points indicate no-buy decisions (pluses [+]). In general, you want to buy high-positive-momentum stocks, so points at the higher end of the momentum axis are likelier to be buy. However, this is not the only indicator. For more volatile stocks, we demand higher momentum to switch from no-buy to buy. This is why the plane slopes upward (higher momentum) as we move rightward (higher volatility). Also, we demand less momentum for stocks with higher dividends. This is why the plane slopes downward (lower momentum) as we go toward higher dividends. Real problems have more dimensions, of course (since many more inputs are involved in the decision), and the separator becomes a hyperplane. Also, in real-life problems, the points are often too intertwined in the input space for any separator to work. We first have to apply a transformation that maps the point to an output space where it is easier to separate. Given their significance as class separators in machine learning, we will study hyperplanes in this section. In high school 3D geometry, we learned ax + by + cz + d = 0 as the equation of a plane. Now we will study a version of it that works in higher dimensions. 2.12 Linear systems and matrix inverse 45 Geometrically speaking, given a plane (in any dimension), we can find a direction called the normal direction, denoted ˆn, such that If we take any pair of points on the plane, say ®x0 and ®x, … The line joining ®x and ®x0—i.e., the vector ®x −®x0—is orthogonal to ˆn. Thus, if we know a fixed point on the plane, say ®x0, then all points on the plane will satisfy ˆn ·  ®x −®x0  = 0 or ˆnT  ®x −®x0  = 0 Thus we can express the equation of a plane as ˆnT ®x −ˆnT ®x0 = 0 (2.13) Equation 2.13 is depicted pictorially in figure 2.10. Plane: Figure 2.10 The normal to the plane is the same at all points on the plane. This is the fundamental property of a plane. ˆn depicts that normal direction. Let ®x0 be a point on the plane. All other points on the plane, depicted as ®x, will satisfy the equation   ®x −®x0  · ˆn = 0. This physically says that the line joining a known point ®x0 on the plane and any other arbitrary point ®x on the plane is at right angles to the normal ˆn. This formulation works for any dimension. In section 1.3, equation 1.3, we encountered the simplest machine learning model: a weighted sum of inputs along with a bias. Denoting the inputs as ®x, the weights as ®w, and the bias as b, this model was depicted as ®wT ®x + b = 0 (2.14) Comparing equations 2.13 and 2.14 , we get the geometric significance: the simple model of equation 1.3 is nothing but a planar separator. Its weight vector ®w corresponds to the plane’s orientation (normal). The bias b corresponds to the plane’s location (a fixed point on the plane). During training, we are learning the weights and biases—this is essentially learning the orientation and position of the optimal plane that will separate the training inputs. To be consistent with the machine learning paradigm, henceforth we will write the equation of a hyperplane as equation 2.14 for some constant ®w and b. Note that ®w need not be a unit-length vector. Since the right-hand side is zero, if necessary, we can divide both sides by ∥®w∥to convert to a form like equation 2.13. The sign of the expression ®wT ®x + b has special significance. All points ®x for which ®wT ®x + b < 0 lie on the same side of the hyperplane. All points ®x for which ®wT ®x + b > 0 lie on the other side of the hyperplane. And of course, all points ®x for which ®wT ®x + b = 0 lie on the hyperplane. 46 CHAPTER 2 Vectors, matrices, and tensors in machine learning It should be noted that the 3D equation ax + by + cz + d = 0 is a special case of equa- tion 2.14 because ax + by + cz + d = 0 can be rewritten as h a b c i  x y z  + d = 0 which is same as ®wT ®x + b = 0 with ®w =  a b c  and ®x =  x y z  . Incidentally, this tells us that in 3D, the normal to the plane ax + by + cz + d = 0 is ˆn = 1 √ a2+b2+c2  a b c  . 2.9 Linear combinations, vector spans, basis vectors, and collinearity preservation By now, it should be clear that machine learning and data science are all about points in high-dimensional spaces. Consequently, it behooves us to have a decent understanding of these spaces. For instance, given a space, we may need to ask, “Would it be possible to express all points in the space in terms of a set of a few vectors? What is the smallest set of vectors we really need for that purpose?” This section is devoted to the study of these questions. 2.9.1 Linear dependence Consider the vectors (points) shown in figure 2.11. The corresponding vectors in 2D are ®v0 =  1 1  ®v1 =  2 2  ®v2 =  3 3  ®v3 =  4 4  We can find four scalars 훼0 = 2, 훼1 = 2, 훼2 = 2, and 훼3 = −3 such that 훼0®v0 + 훼1®v1 + 훼2®v2 + 훼3®v3 =  0 0  If we can find such scalars, not all zero, we say the vectors ®v0, ®v1, ®v2, and ®v3 are linearly dependent. The geometric picture to keep in mind is that points corresponding to linearly dependent vectors lie on a single straight line in the space containing them. 2.12 Linear systems and matrix inverse 47 (1, 1) (2, 2) (3, 3) (0, 0) Line L: (4, 4) Figure 2.11 Linearly dependent points in a 2D plane COLLINEARITY IMPLIES LINEAR DEPENDENCE Proof: Let ®a, ®b and ®c be three collinear vectors. From equation 2.12, there exists some 훼∈ℝsuch that ®c = (1 −훼) ®a + 훼®b This equation can be rewritten as 훼1®a + 훼2®b + 훼3®c = 0 where 훼1 = (1 −훼), 훼2 = 훼and 훼3 = −1. Thus we have proven that three collinear vectors ®a, ®b, and ®c must also be linearly dependent. LINEAR COMBINATION Given a set of vectors ®v1, ®v2, …. ®vn and a set of scalar weights 훼1,훼2, …훼n, the weighted sum 훼1 ®v1 + 훼2 ®v2 + + · · · 훼n ®vn is called a linear combination. GENERIC MULTIDIMENSIONAL DEFINITION OF LINEAR DEPENDENCE A set of vectors ®v1, ®v2, …. ®vn are linearly dependent if there exists a set of weights 훼1,훼2, …훼n, not all zeros, such that 훼1 ®v1 + 훼2 ®v2 + + · · · 훼n ®vn = 0. For example, the row vectors h 1 1 i and h 2 2 i are linearly dependent, since −2 h 1 1 i + h 2 2 i = 0. 2.9.2 Span of a set of vectors Given a set of vectors ®v1, ®v2, …. ®vn, their span is defined as the set of all vectors that are linear combinations of the original set . This includes the original vectors. 48 CHAPTER 2 Vectors, matrices, and tensors in machine learning For example, consider the two vectors ®vx⊥=  1 0  and ®vy⊥=  0 1  . The span of these two vectors is the entire plane containing the two vectors. Any vector, for instance, the vector  18 97  can be expressed as a weighted sum 18®vx⊥+ 97®vy⊥. You can probably recognize that  1 0  and  0 1  are the familiar Cartesian coordinate axes (X-axis and Y-axis, respectively) in the 2D plane. 2.9.3 Vector spaces, basis vectors, and closure We have been talking informally about vector spaces. It is time to define them more precisely. VECTOR SPACES A set of vectors (points) in n dimensions form a vector space if and only if the operations of addition and scalar multiplication are defined on the set. In particular, this implies that it is possible to take linear combinations of members of a vector space. BASIS VECTORS Given a vector space, a set of vectors that span the space is called a basis for the space. For instance, for the space ℝ2, the two vectors  1 0  and  0 1  are basis vectors. This essentially means any vector in ℝ2 can be expressed as a linear combination of these two. The notion can be extended to higher dimensions. For ℝn, the vectors  1 0 ... 0  ,  0 1 ... 0  , · · · ,  0 0 ... 1  form a basis. The alert reader has probably guessed by now that the basis vectors are related to coordinate axes. In fact, the basis vectors just described constitute the Cartesian coordinate axes. So far, we have only seen examples of basis vectors that are mutually orthogonal, such as the dot product of the two basis vectors in ℝ2 shown earlier:  0 1  ·  1 0  = h 0 1 i  1 0  = 0. However, basis vectors do not have to be orthogonal. Any pair of linearly independent vectors forms a basis in ℝ2. Basis vectors, then, are by no means unique. That said, orthogonal vectors are most convenient, as we shall see later. MINIMAL AND COMPLETE BASIS Exactly n vectors are needed to span a space with dimensionality n. This means the basis set for a space will have at least as many elements as the dimensionality of the space. 2.12 Linear systems and matrix inverse 49 That many basis vectors are also sufficient to form a basis. For instance, exactly n vectors are needed to form a basis in (that is, span) ℝn. A related fact is that in ℝn, any set of m vectors with m > n will be linearly dependent. In other words, the largest size of a set of linearly independent vectors in an n-dimensional space is n. CLOSURE A set of vectors is said to be closed under linear combination if and only if the linear combination of any pair of vectors in the set also belongs to the same set. Consider the set of points ℝ2. Recall that this is the set of vectors with two real elements. Take any pair of vectors ®a and ®b in ℝ2: for instance, ®a =  11.2 31.766  and ®b =  177.01 1031.99  . Any linear combination of these two vectors will also comprise two real numbers—that is, will belong to ℝ2. We say ℝ2 is a vector space since it is closed under linear combination. Consider the space ℝ2. Geometrically speaking, this represents a two dimensional plane. Let’s take two points on this plane, ®a and ®b. Linear combinations of ®a, ®b geomet- rically correspond to points on the line joining them. We know that if two points lie on a plane, the entire line will also lie on the plane. Thus, in two dimensions, a plane is closed under linear combinations. This is the geometrical intuition behind the notion of closure on vector spaces. It can be extended to arbitrary dimensions. On the other hand, the set of points on the surface of a sphere is not closed under linear combination because the line joining an arbitrary pair of points on this set will not wholly lie on the surface of that sphere. 2.10 Linear transforms: Geometric and algebraic interpretations Inputs to a machine learning or data science system are typically feature vectors (in- troduced in section 2.1) in high-dimensional spaces. Each individual dimension of the feature vector corresponds to a particular property of the input. Thus, the feature vector is a descriptor for the particular input instance. It can be viewed as a point in the feature space. We usually transform the points to a friendlier space where it is easier to perform the analysis we are trying to do. For instance, if we are building a classifier, we try to transform the input into a space where the points belonging to different classes are more segregated (see section 1.3 in general and figure 1.3 in particular for simple examples). Sometimes we transform to simplify the data, eliminating axes along which there is scant variation in the data. Given their significance in machine learning, in this section we will study the basics of transforms. Informally, a transform is an operation that maps a set of points (vectors) to another. Given a set S of n × 1 vectors, any m × n matrix T can be viewed as a transform. If ®v belongs to the set S, multiplication with the matrix T will map (transform) ®v to a vector T®v. We will later see that matrix multiplication is a subclass of transforms that preserve collinearity—points that lie on a straight line before the transformation will continue to lie on a (possibly different) straight line post the transformation. For instance, consider 50 CHAPTER 2 Vectors, matrices, and tensors in machine learning the matrix R =  1 √ 2 1 √ 2 −1 √ 2 1 √ 2  In section 2.14, we will see that this is a special kind of matrix called a rotation matrix; for now, simply consider it an example of a matrix. R is a transformation operator that maps a point in a 2D plane to another point in the same plane. In mathematical notation, R : ℝ2 →ℝ2. In fact, as depicted in figure 2.14, this transformation (multipli- cation by matrix R) rotates the position vector of a point in the 2D plane by an angle of 45◦. The output and input points may belong to different spaces in such transforms. For instance, consider the matrix P =  1 0 0 0 1 0  It is not hard to see that this matrix projects 3D points to the 2D X-Y plane: P  x y z  =  x y  Hence, this transformation (multiplication by matrix P) projects points from three to two dimensions. In mathematical parlance, P : ℝ3 →ℝ2. The transforms R and P share a common property: they preserve collinearity. This means a set of vectors (points) ®a, ®b, ®c, · · · that originally lay on a straight line remain so after the transformation. Let’s check this out for the rotation transformation in the example from section 2.9. There we saw four vectors: ®o =  0 0  ®a =  1 1  ®b =  2 2  ®c =  3 3  These vectors all lie on a straight L : x = y. The rotation transformed versions of these vectors are ®o ′ = R®o =  0 0  ®a ′ = R®a =  √ 2 0  2.12 Linear systems and matrix inverse 51 ®b ′ = R®b =  2 √ 2 0  ®c ′ = R®c =  3 √ 2 0  It is trivial to see that the transformed vectors also lie on a (different) straight line. In fact, ®o ′, ®a ′, ®b ′, ®c ′ lie on the Y-axis, which is the 45◦rotated version of the original line y = x. The projection transform represented by matrix P also preserves collinearity. Con- sider four collinear vectors in 3D: ®o =  0 0 0  ®a =  1 1 1  ®b =  2 2 2  ®c =  3 3 3  The corresponding transformed vectors ®o ′ = P®o =  0 0  ®a ′ = P ®a =  1 1  ®b ′ = P ®b =  2 2  ®c ′ = P®c =  3 3  also lie on a straight line in 2D. The class of transforms that preserves collinearity are known as linear transforms. They can always be represented as a matrix multiplication. Conversely, all matrix mul- tiplications represent a linear transformation. A more formal definition is provided later. 2.10.1 Generic multidimensional definition of linear transforms A function 휙is a linear transform if and only if it satisfies 휙  훼®a + 훽®b  = 훼휙 ®a + 훽휙  ®b  ∀훼, 훽∈ℝ (2.15) In other words, a transform is linear if and only if the transform of the linear combination of two vectors is the same as the linear combination (with the same weights) of the transforms of individual vectors. (This can be remembered as: Linear transform means transforms of linear combinations are same as linear combinations of transforms.) Multiplication with a rotation or projection matrix (shown earlier) is a linear transform. 52 CHAPTER 2 Vectors, matrices, and tensors in machine learning 2.10.2 All matrix-vector multiplications are linear transforms Let’s verify that matrix multiplication satisfies the definition of linear mapping (equa- tion 2.15). Let ®a, ®b ∈ℝn be two arbitrary n-dimensional vectors and Am,n be an arbitrary matrix with n columns. Then following the standard rules of matrix-vector multiplication, A  훼®a + 훽®b  = 훼 A®a + 훽  A®b  which mimics equation 2.15 with 휙replaced with matrix A. Thus we have proven that all matrix multiplications are linear transforms. The reverse is not true. In particular, linear transforms that operate on infinite-dimensional vectors are not matrices. But all linear transforms that operate on finite-dimensional vectors can be expressed as matrices. (The proof is a bit more complicated and will be skipped.) Thus, in finite dimensions, multiplication with a matrix and linear transformation are one and the same thing. In section 2.3, we saw the array view of matrices. The corresponding geometric view, that all matrices represent linear transformation, was presented in this section. Let’s finish this section by studying an example of a transform that is not linear. Consider the function 휙 ®x = ∥®x∥ for ®x ∈ℝn. This function 휙maps n-dimensional vectors to a scalar that is the length of the vector, 휙: ℝn →ℝ. We will examine if it satisfies equation 2.15 with 훼1 = 훼2 = 1. For two specific vectors ®a, ®b ∈ℝn, 휙 ®a = 휙 ©­­­­­­ «  a1 a2 · · · an  ª®®®®®® ¬ = q a2 1 + a2 2 + · · · a2 n 휙  ®b  = 휙 ©­­­­­­ «  b1 b2 · · · bn  ª®®®®®® ¬ = q b2 1 + b2 2 + · · · b2 n Now 휙 ®a + 휙  ®b  = q a2 1 + a2 2 + · · · a2 n + q b2 1 + b2 2 + · · · b2 n and 휙  ®a + ®b  = 휙 ©­­­­­­ «  a1 + b1 a2 + b2 ... an + bn  ª®®®®®® ¬ = q (a1 + b1)2 + (a2 + b2)2 + · · · (an + bn)2 2.12 Linear systems and matrix inverse 53 Clearly, these two are not equal; hence, we have violated equation 2.15: 휙is a nonlinear mapping. 2.11 Multidimensional arrays, multilinear transforms, and tensors We often hear the term tensor in connection with machine learning. Google’s famous machine learning platform is named TensorFlow. In this section, we will introduce you to the concept of a tensor. 2.11.1 Array view: Multidimensional arrays of numbers A tensor may be viewed as a generalized n-dimensional array—although, strictly speaking, not all multidimensional arrays are tensors. We will learn more about the distinction between multidimensional arrays and tensors when we study multilinear transforms. For now, we will not worry too much about the distinction. A vector can be viewed as a 1 tensor, a matrix is a 2 tensor, and a scalar is a 0 tensor. In section 2.3, we saw that digital images are represented as 2D arrays (matrices). A color image—where each pixel is represented by three colors, R, G, and B (red, green, and blue)—is an example of a multidimensional array or tensor. This is because it can be viewed as a combination of three images: the R, G, and B images, respectively. The inputs and outputs to each layer in a neural network are also tensors. 2.12 Linear systems and matrix inverse Machine learning today is usually an iterative process. Given a set of training data, you want to estimate a set of machine parameters that will yield target values (or close approximations to them) on training inputs. The number of training inputs and the size of the parameter set are often very large. This makes it impossible to have a closed-form solution where we solve for the unknown parameters in a single step. Solutions are usually iterative. We start with a guessed set of values for the parameters and iteratively improve the guess by processing training data. Having said that, we often encounter smaller problems in real life. We are better off using more traditional closed-form techniques here since they are much faster and more accurate. This section is devoted to gaining some insights into these techniques. Let’s go back to our familiar cat-brain problem and refer to its training data in ta- ble 2.2. As before, we are still talking about a weighted sum model with three parameters: weights w0, w1 and bias b. Let’s focus on the top three rows from the table, repeated here in table 2.3 for convenience. Table 2.3 Example training dataset for our toy machine learning–based cat brain Input value: Hardness Input value: Sharpness Output: Threat score 0 0.11 0.09 −0.8 1 0.01 0.02 −0.97 2 0.98 0.91 0.89 54 CHAPTER 2 Vectors, matrices, and tensors in machine learning The training data says that with a hardness value 0.11 and a sharpness value 0.09, we expect the system’s output to match (or closely approximate) the target value −0.8, and so on. In other words, our estimated values for parameters w0, w1, b should ideally satisfy 0.11w0 + 0.09w1 + b = −0.8 0.01w0 + 0.02w1 + b = −0.97 0.98w0 + 0.91w1 + b = 0.89 We can express this via matrix multiplication as the following equation:  0.11 0.09 1 0.01 0.02 1 0.98 0.91 1   w0 w1 b  =  −0.08 −0.97 0.89  How do we obtain the values of w0, w1, b that make this equation true? That is, how do we solve this equation? There are formal methods (discussed later) to directly solve such equations for w0, w1, and b (in this very simple example, you might just “see” that w0 = 1, w1 = 1, b = −1 solves the equation, but we need a general method). This equation is an example of a class of equations called a linear system. A linear system in n unknowns x1, x2, x3, · · · , xn, a11x1 + a12x2 + a13x3 + · · · + a1nxn = b1 a21x1 + a22x2 + a23x3 + · · · + a2nxn = b2 ... an1x1 + an2x2 + an3x3 + · · · + annxn = bn can be expressed via matrix and vectors as A®x = ®b where A =  a11 a12 a13 · · · a1n a21 a22 a23 · · · a2n ... ... ... ... ... an1 an2 an3 · · · ann  and ®x =  x1 x2 · · · xn  and ®b =  b1 b2 · · · bn  Although equivalent, the matrix depiction is more compact and dimension-independent. In machine learning, we usually have many variables (thousands), so this compactness makes a significant difference. Also, A®x = ®b looks similar to the one-variable equation we know so well: ax = b. In fact, many intuitions can be transferred from 1D to higher dimensions. 2.12 Linear systems and matrix inverse 55 What is the solution to the 1D equation? You may have learned it in fifth grade: The solution of ax = b is x = a−1b where a−1 = 1 a, a ≠0. We can use the same notation in all dimensions. The solution of A®x = ®b is ®x = A−1®b, where A−1 is the matrix inverse. The inverse matrix A−1 has the determinant of the ma- trix, 1 det(A) , as a factor. We will not discuss determinant and inverse matrix computation here—you can obtain that in any standard linear algebra textbook—but will state some facts that lend insights into determinants and inverse matrices: The inverse matrix A−1 is related to matrix A in the same way the scalar a−1 is related to the scalar a. a−1 exists if and only if a ≠0. Analogously, A−1 exists if det (A) ≠0, where det (A) refers to the determinant of a matrix. The product of a scalar a and its inverse a−1 is 1. Analogously, AA−1 = A−1A = I, where I denotes the identity matrix that is the higher-dimension analog for 1 in scalar arithmetic. It is a matrix in which the diagonal terms are 1 and all other terms are 0. The n-dimensional identity matrix is as follows: In,n =  1 0 0 · · · 0 0 1 0 · · · 0 0 0 1 · · · 0 ... 0 0 0 · · · 1  When there is no subscript, the dimensionality can be inferred from the context. For any matrix A, IA = AI = A. For any vector ®a, I®a = ®aTI = ®a. These can be easily verified using the rules of matrix multiplication. There are completely precise but tedious rules for computing determinants and matrix inverses. Despite the importance of the concept, we rarely need to compute them in life as all linear algebra software packages provide routines to do this. Furthermore, computing matrix inverses is not good programming practice because it is numerically unstable. We will not discuss the direct computation of determinant or matrix inverse here (except that in section A.2 of the appendix, we show how to compute the determi- nant of a 2 × 2 matrix). We will discuss pseudo-inverses, which have more significance in machine learning. 2.12.1 Linear systems with zero or near-zero determinants, and ill-conditioned systems We saw earlier that a linear system A®x = ®b has the solution ®x = A−1®b. But A−1 has 1 det(A) as a factor. What if the determinant is zero? The short answer: when the determinant is zero, the linear system cannot be exactly solved. We may still attempt to come up with an approximate answer (see section 2.12.3), but an exact solution is not possible. 56 CHAPTER 2 Vectors, matrices, and tensors in machine learning Let’s examine the situation a bit more closely with the aid of an example. Consider the following system of equations: x1 + x2 = 2 2x1 + 2x2 = 4 It can be rewritten as a linear system with a square matrix:  1 1 2 2   x1 x2  =  2 4  But you can quickly see that the system of equations cannot be solved. The second equation is really the same as the first. In fact, we can obtain the second by multiplying the first by a scalar, 2. Hence, we don’t really have two equations: we have only one, so the system cannot be solved. Now examine the row vectors of matrix A. They are h 1 1 i and h 2 2 i . They are linearly dependent because −2 h 1 1 i + h 2 2 i = 0. Now examine the determinant of matrix A (section A.2 of the appendix shows how to compute the determinant of a 2 × 2 matrix). It is 2 × 1 −1 × 2 = 0. These results are not coincidences. Any one of them implies the other. In fact, the following statements about the linear system A®x = ®b (with a square matrix) are equivalent: Matrix A has a row/column that can be expressed as a weighted sum of the others. Matrix A has linearly dependent rows or columns. Matrix A has zero determinant (such matrices are called singular matrices). The inverse of matrix A (i.e., A−1) does not exist. A is called singular. The linear system cannot be solved. The system is trying to tell you that you have fewer equations than you think you have, and you cannot solve the system of equations. Sometimes the determinant is not exactly zero but close to zero. Although solvable in theory, such systems are numerically unstable. Small changes in input cause the result to change drastically. For instance, consider this nearly singular matrix: A =  2 1 4 2.001  (2.16) Its determinant is 0.002, close to zero. Let ®b =  3 6  be a vector. A−1 =  1000.5 −500.0 −2000. 1000.0  (2.17) (Note how large the elements of A−1 are. This is due to division by an extremely small determinant and, in turn, causes the instability illustrated next.) The solution to the 2.12 Linear systems and matrix inverse 57 equation A®x = ®b is ®x = A−1®b =  1.5 0  . But if we change ®b just a little and make ®b =  3 6.01  , the solution changes to a drastically different ®x = A−1®b =  −3.5 10.0  . This is inherently unstable and arises from the near singularity of the matrix A. Such linear systems are called ill-conditioned. 2.12.2 PyTorch code for inverse, determinant, and singularity testing of matrices Inverting a matrix and computing its determinant can be done with a single function call from the linear algebra package linalg. Listing 2.10 Matrix inverse for an invertible matrix (nonzero determinant) def determinant(A): return torch.linalg.det(A) def inverse(A): return torch.linalg.inv(A) A = torch.tensor([[2, 3], [2, 2]], dtype=torch.float) A = " 2 3 3 2 # A_inv = inverse(A) A = " −1 1.5 1 −1 # I = torch.eye(2) The PyTorch function torch.eye(n) generates an identity matrix I of size n. assert torch.all(torch.matmul(A, A_inv) == I) Verify " 2 3 3 2 # " −1 1.5 1 −1 # = " 1 0 0 1 # assert torch.all(torch.matmul(A_inv, A) == I) assert torch.all(torch.matmul(I, A) == A) assert torch.all(A == torch.matmul(A,I)) I is like 1. Verify AI = IA = A. A singular matrix is a matrix whose determinant is zero. Such matrices are non-invertible. Linear systems of equations with singular matrices cannot be solved. Listing 2.11 Singular matrix B = torch.tensor([[1, 1], [2, 2]], dtype=torch.float) B = " 1 1 2 2 # try: B_inv = inverse(B) Determinant = 1 × 2 −2 × 1 = 0. Singular matrix; attempting to compute the inverse causes a runtime error. except RuntimeError as e: print(''B cannot be inverted: ''.format(B, e)) 2.12.3 Over- and under-determined linear systems in machine learning What if the matrix A is not square? This implies that the number of equations does not match the number of unknowns. Does such a system even make sense? Surprisingly, it does. As a rule, machine learning systems fall in this category: the number of equations 58 CHAPTER 2 Vectors, matrices, and tensors in machine learning corresponds to the number of training data instances collected, while the number of unknowns is a function of the number of weights in the model which is a function of the particular model family chosen to represent the system. These are independent of each other. As stated earlier, we often solve these systems iteratively. Nonetheless, it is important to understand linear systems with nonsquare matrices A, to gain insight. There are two possible cases, assuming that the matrix A is m × n (m rows and n columns): Case 1: m > n (more equations than unknowns; overdetermined system) Case 2: m < n (fewer equations than unknown; underdetermined system) For instance, table 2.2 leads to an overdetermined linear system. Let’s write the system of equations: 0.11w0 + 0.09w1 + b = −0.8 0.01w0 + 0.02w1 + b = −0.97 0.98w0 + 0.91w1 + b = 0.89 0.12w0 + 0.21w1 + b = −0.68 0.98w0 + 0.99w1 + b = 0.95 0.85w0 + 0.87w1 + b = 0.74 0.03w0 + 0.14w1 + b = −0.88 0.55w0 + 0.45w1 + b = 0.00 These yield the following overdetermined linear system:  0.11 0.09 1 0.01 0.02 1 0.98 0.91 1 0.12 0.21 1 0.98 0.99 1 0.85 0.87 1 0.03 0.14 1 0.55 0.45 1   w0 w1 b  =  −0.8 −0.97 0.89 −0.68 0.95 0.74 −0.88 0.00  (2.18) This is a nonsquare 15 × 3 linear system. There are only 3 unknowns to solve for (w0, w1, b), and there are 15 equations. This is highly redundant: we needed only three equations and could have solved it via linear system solution techniques (section 2.12). But the important thing to note is this: the equations are not fully consistent. There is no single set of values for the unknown that will satisfy all of them. In other words, the training data is noisy—an almost universal occurrence in real-life machine learning systems. Consequently, we have to find a solution that is optimal (causes as little error as possible) over all the equations. 2.12 Linear systems and matrix inverse 59 We want to solve it such that the overall error ∥A®x −®b∥is minimized. In other words, we are looking for ®x such that A®x is as close to ®b as possible. This closed-form (that is, non-iterative) method is an extremely important precursor to machine learning and data science. We will revisit this multiple times, most notably in sections 2.12.4 and 4.5. 2.12.4 Moore Penrose pseudo-inverse of a matrix The pseudo-inverse is a handy technique to solve over- or under-determined linear systems. Suppose we have an overdetermined system with the not-necessarily square m × n matrix A: A®x = ®b Since A is not guaranteed to be square, we can take neither the determinant nor the inverse in general. So the usual A−1®b does not work. At this point, we observe that although the inverse cannot be taken, transposing the matrix is always possible. Let’s multiply both sides of the equation with AT: A®x = ®b ⇔AT A®x = AT ®b Notice that AT A is a square matrix: its dimensions are (m × n) × (n × m) = m × m. Let’s assume, without proof for the moment, that it is invertible. Then A®x = ®b ⇔AT A®x = AT ®b ⇔®x =  AT A −1 AT ®b Hmmm, not bad; we seem to be onto something. In fact, we just derived the pseudo-inverse of matrix A, denoted A+ =  AT A−1 AT. Unlike the inverse, the pseudo-inverse does not need the matrix to be square with linearly independent rows. Much like the regular linear system, we get the solution of the (possibly nonsquare) system of equations as A®x = ®b ⇔®x = A+®b. The pseudo-inverse-based solution actually minimizes the error ∥A®x −®b∥. We will provide an intuitive proof of that in section 2.12.5. Meanwhile, you are encouraged to write the Python code to evaluate  AT A−1 AT ®b and verify that it approximately yields the expected answer  1 1 −1  for equation 2.18. 2.12.5 Pseudo-inverse of a matrix: A beautiful geometric intuition A matrix Am×n can be rewritten in terms of its column vectors as  ®a1, ®a2, . . . ®an  , where ®a1 . . . ®an are all m-dimensional vectors. Then if ®x =  x1 x2 . . . xn  , we get A®x = x1®a1 + x2®a2 + · · · xn ®an. In other words, A®x is just a linear combination of the column vectors of A with the elements of ®x as the weights (you are encouraged to write out a small 3 × 3 system 60 CHAPTER 2 Vectors, matrices, and tensors in machine learning and verify this). The space of all vectors of the form A®x (that is, the linear span of the column vectors of A) is known as the column space of A. The solution to the linear system of equations A®x = ®b can be viewed as finding the ®x that minimizes the difference of A®x and ®b: that is, minimizes ∥A®x −®b∥. This means we are trying to find a point in the column space of A that is closest to the point ®b. Note that this interpretation does not assume a square matrix A. Nor does it assume a nonzero determinant. In the friendly case where the matrix A is square and invertible, we can find a vector ®x such that A®x becomes exactly equal to ®b, which makes ∥A®x −®b∥= 0. If A is not square, we will try to find ®x such that A®x is closer to ®b than any other vector in the column space of A. Mathematically speaking,4 ∥A®x −®b∥≤∥A®y −®b∥∀®y ∈ℜn (2.19) From geometry, we intuitively know that the closest point to ®b in the column space of A is obtained by dropping a perpendicular from ®b to the column space of A (see figure 2.12). The point where this perpendicular meets the column space is called the projection of ®b on the column space of A. The solution vector ®x to equation 2.19 that we are looking for should correspond to the projection of ®b on the column space of A. This in turn means ®b −A®x is orthogonal (perpendicular) to all vectors in the column space of A (see figure 2.12). We represent arbitrary vectors in the column space of A as A®y for arbitrary ®y. Hence, for all such ®y,  A®y ⊥  ®b −A®x  ⇔ A®yT  ®b −A®x  = 0 ⇔yT AT  ®b −A®x  = 0 Column space of A Figure 2.12 Solving a linear system A®x = ®b is equivalent to finding the point on the column space of A that is closest to ®b. This means we have to drop a perpendicular from ®b to column space of A. If A®x represents the point where that perpendicular meets the column space (aka projection), the difference vector ®b −A®x corresponds to the line joining ®b and its projection A®x. This line will be perpendicular to all vectors in the column space of A. Equivalently, it is perpendicular to A®y for any arbitrary ®y. 4 The mathematical symbol ∀stands for “for all.” Thus, ∀®y ∈ℜn means “all vectors y in the n-dimensional space.” 2.12 Linear systems and matrix inverse 61 For the previous equation to be true for all vectors ®y, we must have AT  ®b −A®x  = 0. Thus, we have AT  ®b −A®x  = 0 ⇔AT A®x = AT ®b ⇔®x =  AT A −1 AT ®b which is exactly the Moore-Penrose pseudo-inverse. For a machine learning-centric example, consider the overdetermined system corre- sponding to the cat brain earlier in the chapter. There are 15 training examples, each with input and desired outputs specified. Our goal is to determine three unknowns w0, w1, and b such that for each training input ®xi =  xi,0 xi,1  , the model output yi = xi,0w0 + xi,1w1 + b = h xi,0 xi,1 i  w0 w1  + b = h xi,0 xi,1 1 i  w0 w1 b  (2.20) matches the desired output (aka ground truth) ¯yi as closely as possible. NOTE We employed a neat trick here: we added a 1 to the right of the input, which allows us to depict the entire system (including the bias) in a single compact matrix-vector multiplication. We call this augmentation—we augment the input row vector with an extra 1 on the right. Collating all the training examples together, we get  x0,0 x0,1 1 x1,0 x1,1 1 ... ... ... xN,0 xN,1 1   w0 w1 b  =  y0 y1 · · · yN  (2.21) which can be expressed compactly as X ®w = ®y where X is the augmented input matrix with a rightmost column of all 1s. The goal is to minimize ∥®y −¯®y∥. To this end, we formulate the over-determined linear system X ®w = ¯®y 62 CHAPTER 2 Vectors, matrices, and tensors in machine learning Note that this is not a classic system of equations—it has more equations than unknowns. We cannot solve this via matrix inversion. We can, however, use the pseudo-inverse mechanism to solve it. The resulting solution yields the “best fit” or “best effort” solution, which minimizes the total error over all the training examples. The exact numerical system (repeated here for ease of reference) is X =  0.11 0.09 1.00 0.01 0.02 1.00 0.98 0.91 1.00 0.12 0.21 1.00 0.98 0.99 1.00 0.85 0.87 1.00 0.03 0.14 1.00 0.55 0.45 1.00  ¯®y =  −0.8 −0.97 0.89 −0.67 0.97 0.72 −0.83 0.00  ®w =  w0 w1 b  (2.22) We solve for ®w using the pseudo-inverse formula ®w = (XT X)−1XT¯®y 2.12.6 PyTorch code to solve overdetermined systems NOTE Fully functional code for this section, executable via Jupyter Notebook, can be found at http://mng.bz/PPJ2. Listing 2.12 Solving an overdetermined system using the pseudo-inverse def pseudo_inverse(A): return torch.matmul(torch.linalg.inv(torch.matmul(A.T, A)), A.T) X is the augmented data matrix from equation 2.22. X = torch.column_stack((X, torch.ones(15))) The Pytorch column stack operator adds a column to a matrix. Here, the added column is all 1s. It is easy to verify that the solution to equation 2.22 is roughly w0 = 1, w1 = 1, b = −1. But the equations are not consistent: no one solution perfectly fits all of them. The pseudo-inverse finds the “best fit” solution: it minimizes total error for all the equations. w = torch.matmul(pseudo_inverse(X), y) Expect the solution to be close to [1, 1, -1] print(''The solution is ''.format(w)) The solution is [1.08, 0.90, -0.96] 2.13 Eigenvalues and eigenvectors: Swiss Army knives of machine learning Machine learning and data science are all about finding patterns in large volumes of high-dimensional data. The inputs are feature vectors (introduced in section 2.1) in high- dimensional spaces. Each feature vector can be viewed as a point in the feature space descriptor for an input instance. Sometimes we transform these feature vectors—map the feature points to a friendlier space—to simplify the data by reducing dimensionality. This is done by eliminating axes along which there is scant variation in the data. Eigenvalues 2.13 Eigenvalues and eigenvectors: Swiss Army knives of machine learning 63 and eigenvectors are invaluable tools in the arsenal of a machine learning engineer or a data scientist for this purpose. In chapter 4, we will study how to use these tools to simplify and find broad patterns in a large volume of multidimensional data. Let’s take an informal look at eigenvectors first. They are properties of square matrices. As seen earlier, matrices can be viewed as linear transforms which map vectors (points) in one space to different vectors (points) in the same or a different space. But a typical linear transform leaves a few points in the space (almost) unaffected. These points are called eigenvectors. They are important physical aspects of the transform. Let’s look at a simple example. Suppose we are rotating points in 3D space about the Z-axis (see figure 2.13). The points on the Z-axis will stay where they were despite the rotation. In general, points on the axis of rotation (Z in this case) do not go anywhere after rotation. The axis of rotation is an eigenvector of the rotation transformation. Z X Y Figure 2.13 During rotation, points on the axis of rotation do not change position. Extending this idea, when transforming vectors ®x with a matrix A, are there vectors that do not change, at least in direction? Turns out the answer is yes. These are the so-called eigenvectors—they do not change direction when undergoing linear transformation by a matrix A. To be precise, if ®e is an eigenvector of the square matrix A,5 then A®e = 휆®e Thus the linear transformation (that is, multiplication by matrix A) has changed the length but not the direction of ®e because 휆®e is parallel to ®e. How do we obtain 휆and ®e? Well, A®e = 휆®e ⇔A®e −휆®e = ®0 ⇔(A −휆I) ®e = ®0 where I denotes the identity matrix. 5 You can compute eigenvectors and eigenvalues only of square matrices. 64 CHAPTER 2 Vectors, matrices, and tensors in machine learning Of course, we are only interested in nontrivial solutions, where ®e ≠®0. In that case, A −휆I cannot be invertible, because if it were, we could obtain the contradictory solution ®e = (A −휆I)−1 ®0 = ®0. Thus, (A −휆I) is non-invertible, implying the determinant det (A −휆I) = 0 For an n × n matrix A, this yields an nth-degree polynomial equation with n solutions for the unknown 휆. Thus, an n × n matrix has n eigenvalues, not necessarily all distinct. Let’s compute eigenvalues and eigenvectors of a 3 × 3 matrix, just for kicks. The matrix we use is carefully chosen, as will be evident soon. But for now, think of it as an arbitrary matrix: A =  1 √ 2 1 √ 2 0 −1 √ 2 1 √ 2 0 0 0 1  (2.23) We will compute the eigenvalues and eigenvectors of A: (A −휆I) =  1 √ 2 1 √ 2 0 −1 √ 2 1 √ 2 0 0 0 1  −  휆 0 0 0 휆 0 0 0 휆  =  1 √ 2 −휆 1 √ 2 0 −1 √ 2 1 √ 2 −휆 0 0 0 1 −휆  Thus, det (A −휆I) = 0 ⇔(1 −휆)  1 √ 2 −휆   1 √ 2 −휆  + 1 √ 2 1 √ 2  = 0 ⇔(1 −휆)  휆2 −2 1 √ 2 휆+ 1 2 + 1 2  = 0 ⇔(1 −휆)  휆2 − √ 2휆+ 1  = 0 ⇔휆= 1 or 휆=  1 √ 2 + i 1 √ 2  or 휆=  1 √ 2 −i 1 √ 2  ⇔휆= 1 or 휆= ei 휋 4 or 휆= e−i 휋 4 using De Moivre’s rule Here, i = √ −1. If necessary, you are encouraged to refresh your memory of imaginary and complex numbers from high school algebra. Thus, we have found (as expected) three eigenvalues: 1, ei 휋 4 , and e−i 휋 4 . Each of them will yield one eigenvector. Let’s compute the eigenvector corresponding to the 2.13 Eigenvalues and eigenvectors: Swiss Army knives of machine learning 65 eigenvalue of 1 by way of example: A ®e1 = 1 · ®e1 ⇔  1 √ 2 1 √ 2 0 −1 √ 2 1 √ 2 0 0 0 1  ®e1 = ®e1 ⇔  1 √ 2 1 √ 2 −1 √ 2 1 √ 2   e11 e12  =  e11 e12  ⇔e11 = e12 = 0 ⇔®e1 =  0 0 1  Thus,  0 0 1  is an eigenvector for the eigenvalue 1 for matrix A. So is  0 0 k  for any real k. In fact, if 휆, ®e is an eigenvalue, eigenvector pair for matrix A, then A®e = 휆®e ⇔A  k®e = 휆 k®e That is, 휆,  k®e is also an eigenvalue, eigenvector pair of A. In other words, we can only determine the eigenvector up to a fixed scale factor. We take the eigenvector to be of unit length (®eT ®e = 1) without loss of generality. The eigenvector for our example matrix turns out to be the Z-axis. This is not an accident. Our matrix A was, in fact, a rotation about the Z-axis. A rotation matrix will always have 1 as an eigenvalue. The corresponding eigenvector will be the axis of rotation. In 3D, the other two eigenvalues will be complex numbers yielding the angle of rotation. This is detailed in section 2.14. 2.13.1 Eigenvectors and linear independence Two eigenvectors of a matrix corresponding to unequal eigenvalues are linearly in- dependent. Let’s prove this to get some insights. Let 휆1, ®e1 and 휆2, ®e2 be eigenvalue, eigenvector pairs for a matrix A with 휆1 ≠휆2. Then A ®e1 = 휆1 ®e1 A ®e2 = 휆2 ®e2 If possible, let there be two constants 훼1 and 훼2 such that 훼1 ®e1 + 훼2 ®e2 = 0 (2.24) In other words, suppose the two eigenvectors are linearly dependent. We will show that this assumption leads to an impossibility. 66 CHAPTER 2 Vectors, matrices, and tensors in machine learning Multiplying equation 2.24 by A, we get 훼1A ®e1 + 훼2A ®e2 = 0 ⇔훼1휆1 ®e1 + 훼2휆2 ®e2 = 0 Also, we can multiply equation 2.24 by 휆2. Thus we get 훼1휆1 ®e1 + 훼2휆2 ®e2 = 0 훼1휆2 ®e1 + 훼2휆2 ®e2 = 0 Subtracting, we get 훼1 (휆1 −휆2) ®e1 = 0 By assumption, 훼1 ≠0, 휆1 ≠휆2 and ®e1 is not all zeros. Thus it is impossible for their prod- uct to be zero. Our original assumption (the two eigenvectors are linearly dependent) must have been wrong. 2.13.2 Symmetric matrices and orthogonal eigenvectors Two eigenvectors of a symmetric matrix that correspond to different eigenvalues are mutually orthogonal. Let’s prove this to get additional insight. A matrix A is symmetric if AT = A. If 휆1, ®e1 and 휆2, ®e2 are eigenvalue, eigenvector pairs for a symmetric matrix A, then A ®e1 = 휆1 ®e1 (2.25) A ®e2 = 휆2 ®e2 (2.26) Transposing equation 2.25, ®e1 T AT = 휆1 ®e1 T Right-multiplying by ®e2, we get ®e1 T AT ®e2 = 휆1 ®e1 T ®e2 ⇔®e1 T A ®e2 = 휆1 ®e1 T ®e2 where the last equation follows from the matrix symmetry. Also, left-multiplying equa- tion 2.26 by ®e1 T, we get ®e1 T A ®e2 = 휆2 ®e1 T ®e2 Thus ®e1 T A ®e2 = 휆1 ®e1 T ®e2 ®e1 T A ®e2 = 휆2 ®e1 T ®e2 Subtracting the equations, we get 0 = (휆1 −휆2) ®e1 T ®e2 Since 휆1 ≠휆2, we must have ®e1 T ®e2 = 0, which means the two eigenvectors are orthogonal. Thus, if A is an n × n symmetric matrix with eigenvectors ®e1, ®e2, · · · ®en, then ®ei T ®ej = 0 for all i, j satisfying 휆i ≠휆j. 2.14 Orthogonal (rotation) matrices and their eigenvalues and eigenvectors 67 2.13.3 PyTorch code to compute eigenvectors and eigenvalues NOTE Fully functional code for this section, executable via Jupyter Notebook, can be found at http://mng.bz/1rEZ. Listing 2.13 Eigenvalues and vectors from torch import linalg as LA A = torch.tensor([[0.707, 0.707, 0], [-0.707, 0.707, 0], [0, 0, 1]]) A =  cos (45◦) sin (45◦) 0 −sin (45◦) cos (45◦) 0 0 0 1  Rotates points in 3D space around the Z-axis. The axis of rotation is the Z-axis: h 0 0 1 iT l, e = LA.eig(A) Function eig() in the torch linalg package computes eigenvalues and vectors. print(''Eigen values are ''.format(l)) print(''Eigen vectors are ''.format(e.T)) Eigenvalues or vectors can contain complex numbers involving j = √ −1. 2.14 Orthogonal (rotation) matrices and their eigenvalues and eigenvectors Of all the transforms, rotation transforms have a special intuitive appeal because of their highly observable behavior in the mechanical world. Furthermore, they play a significant role in developing and analyzing several machine learning tools. In this section, we overview rotation (aka orthogonal) matrices. (Fully functional code for the Jupyter notebook for this section can be found at http://mng.bz/2eNN.) 2.14.1 Rotation matrices Figure 2.14 shows a point (x, y) rotated about the origin by an angle 휃. The original point’s position vector made an angle 훼with the X-axis. Post-rotation, the point’s new coordinates are  x ′, y ′. Note that by definition, rotation does not change the distance from the center of rotation; that is what the circle indicates. Some well-known rotation matrices are as follows: Planar rotation by angle 휃about the origin (see figure 2.14): R2d =  cos 휃 −sin 휃 sin 휃 cos 휃  (2.27) Rotation by angle 휃in 3D space about the Z-axis: R3dz =  cos 휃 −sin 휃 0 sin 휃 cos 휃 0 0 0 1  (2.28) 68 CHAPTER 2 Vectors, matrices, and tensors in machine learning (using trigonometric identities) or or or (easily verifiable) Figure 2.14 Rotation in a plane about the origin. By definition, rotation does not change the distance from the center of rotation (indicated by the circle). Note that the z coordinate remains unaffected by this rotation:  cos 휃 −sin 휃 0 sin 휃 cos 휃 0 0 0 1   x y z  =  · · z  This rotation matrix has an eigenvalue of 1, and the corresponding eigenvector is the Z-axis—you should verify this. This implies that a point on the Z-axis maps to itself when transformed (rotated) by the previous matrix, which is in keeping with the property that the z coordinate remains unchanged by this rotation. Rotation by angle 휃in 3D space about the X-axis: R3dx =  1 0 0 0 cos 휃 −sin 휃 0 sin 휃 cos 휃  (2.29) 2.14 Orthogonal (rotation) matrices and their eigenvalues and eigenvectors 69 Note that the X coordinate remains unaffected by this rotation and the X-axis is an eigenvector of this matrix:  1 0 0 0 cos 휃 −sin 휃 0 sin 휃 cos 휃   x y z  =  x · ·  Rotation by angle 휃in 3D space about the Y-axis: R3dy =  cos 휃 0 −sin 휃 0 1 0 sin 휃 0 cos 휃  (2.30) Note that theY coordinate remains unaffected by this rotation and theY-axis is an eigenvector of this matrix:  cos 휃 0 −sin 휃 0 1 0 sin 휃 0 cos 휃   x y z  =  · y ·  Listing 2.14 Rotation matrices Returns the matrix that performs in-plane 2D rotation by angle theta about the origin. Thus, multiplication with this matrix moves a point to a new location. The angle between the position vectors of the original and new points is theta (figure 2.14). def rotation_matrix_2d(theta): return torch.tensor([[cos(radians(theta)), -sin(radians(theta))], [sin(radians(theta)), cos(radians(theta))]]) Returns the matrix that rotates a point in 3D space about the chosen axis by angle theta degrees. The axis of rotation can be 0, 1, or 2, corresponding to the X-, Y-, or Z-axis, respectively. def rotation_matrix_3d(theta, axis): if axis == 0: R3dx from equation 2.29 return torch.tensor([[1, 0, 0], [0, cos(radians(theta)),-sin(radians(theta))], [0, sin(radians(theta)),cos(radians(theta))]]) elif axis == 1: R3dy from equation 2.30 return torch.tensor([[cos(radians(theta)),0,-sin(radians(theta))], [0, 1, 0], [sin(radians(theta)),0,cos(radians(theta))]]) elif axis == 2: R3dz from equation 2.28 return torch.tensor([[cos(radians(theta)),-sin(radians(theta)),0], [sin(radians(theta)),cos(radians(theta)),0], [0, 0, 1]]) 70 CHAPTER 2 Vectors, matrices, and tensors in machine learning Listing 2.15 Applying rotation matrices u = torch.tensor.array([1, 1, 1], dtype=torch.float) Creates vector ®u (see figure 2.15) R3dz = rotation_matrix_3d(45, 2) R3dz from equation 2.28, 45◦about Z-axis v = torch.matmul(R3dz, u_row) ®v (see figure 2.15) is ®u rotated by R3dz. R3dx = rotation_matrix_3d(45, 0) R3dx from equation 2.28, 45◦about X-axis w = torch.matmul(R3dx, u_row) ®w (see figure 2.15) is ®v rotated by R3dx. Figure 2.15 Rotation visualized. Here the original vector u is first rotated by 45 degrees around the Z-axis to get vector v, which is subsequently rotated again by 45 degrees around the X-axis to get vector w. 2.14.2 Orthogonality of rotation matrices A matrix R is orthogonal if and only if it its transpose is also its inverse: that is, RTR = RRT = I. All rotations matrices are orthogonal matrices. All orthogonal matrices represent some rotation. For instance:  cos 휃 −sin 휃 sin 휃 cos 휃  T  cos 휃 −sin 휃 sin 휃 cos 휃  =  cos 휃 sin 휃 −sin 휃 cos 휃   cos 휃 −sin 휃 sin 휃 cos 휃  =  cos2 휃+ sin2 휃 0 0 cos2 휃+ sin2 휃  =  1 0 0 1  = I You are encouraged to verify, likewise, that all the rotation matrices shown here are orthogonal. ORTHOGONALITY AND LENGTH-PRESERVATION Orthogonality implies that rotation is length-preserving. Given any vector ®x and rotation matrix R, let ®y = R®x be the rotated vector. The lengths (magnitudes) of the two vectors ®x, ®y are equal since it is easy to see that ∥®y∥= ®yT ®y =  R®xT  R®x = ®xTRTR®x = ®xTI®x = ®xT ®x = ∥®x∥ 2.14 Orthogonal (rotation) matrices and their eigenvalues and eigenvectors 71 From elementary matrix theory, we know that (AB)T = BT AT NEGATING THE ANGLE OF ROTATION Negating the angle of rotation is equivalent to inverting the rotation matrix, which is equivalent to transposing the rotation matrix. For instance, consider in-plane rotation. Say a point ®x is rotated about the origin to vector ®y via matrix R. Thus R =  cos 휃 −sin 휃 sin 휃 cos 휃  ®y = R®x Now we can go back from ®y to ®x by rotating by −휃. The corresponding rotation matrix is  cos (−휃) −sin (−휃) sin (−휃) cos (−휃)  =  cos 휃 sin 휃 −sin 휃 cos 휃  = RT In other words, RT inverts the rotation: that is, rotates by the negative angle. 2.14.3 PyTorch code for orthogonality of rotation matrices Let’s verify the orthogonality of the rotation matrix by creating one in PyTorch, im- parting a transpose to it, and verifying that the product of the original matrix and the transpose is the identity matrix. Listing 2.16 Orthogonality of rotation matrices R_30 = rotation_matrix_2d(30) Creates a rotation matrix, R30 assert torch.allclose( torch.linalg.inv(R_30), R_30.T ) The inverse of a rotation matrix is the same as its transpose. assert torch.allclose( torch.matmul(R_30, R_30.T), torch.eye(2) Multiplying a rotation matrix and its inverse yields the identity matrix. ) u = torch.tensor([[4],[0]], dtype=torch.float) v = torch.matmul(R_30, u) A vector ®u rotated by matrix R30 to yield vector ®v, R30 ®u = ®v. assert torch.linalg.norm(u)==torch.linalg.norm(v) The norm of a vector is the same as its length. Rotation preserves the length of a vector ∥R®u∥= ∥®u∥. 72 CHAPTER 2 Vectors, matrices, and tensors in machine learning R_neg30 = rotation_matrix_2d(-30) w = torch.matmul(R_neg30, v) Rotation by an angle followed by rotation by the negative of that angle takes the vector back to its original position. Rotation by a negative angle is equivalent to inverse rotation. assert torch.all(w == u) assert torch.allclose(R_30, R_neg30.T) assert torch.allclose( torch.matmul(R_30, R_neg30), torch.eye(2)) A matrix that rotates by an angle is the inverse of the matrix that rotates by the negative of the same angle. 2.14.4 Eigenvalues and eigenvectors of a rotation matrix: Finding the axis of rotation Let 휆, ®e be an eigenvalue, eigenvector pair of a rotation matrix R. Then R®e = 휆®e Transposing both sides, ®eTRT = 휆®eT Multiplying the left and right sides, respectively, with the equivalent entities R®e and 휆®e, we get ®eTRT  R®e = 휆®eT  휆®e ⇔®eT  RTR  ®e = 휆2®eT ®e ⇔®eT (I) ®e = 휆2®eT ®e ⇔®eT ®e = 휆2®eT ®e ⇔휆2 = 1 ⇔휆= 1 (The negative solution 휆= −1 corresponds to reflection.) Thus, all rotation matrices have 1 as one of their eigenvalues. The corresponding eigenvector ®e satisfies R®e = ®e. This is the axis of rotation: the set of points that stay where they were post-rotation. 2.14.5 PyTorch code for eigenvalues and vectors of rotation matrices The following listing shows the code for the axis of rotation. Listing 2.17 Axis of rotation R = torch.tensor([[0.7071, 0.7071, 0], [-0.7071, 0.7071, 0], Matrix R =  cos (45◦) sin (45◦) 0 −sin (45◦) cos (45◦) 0 0 0 1  rotates 45◦ about the Z-axis. All rotation matrices will have an eigenvalue 1. The corresponding eigenvector is the axis of rotation (here, the Z-axis). [0, 0, 1]]) l, e = LA.eig(R) The PyTorch function eig() computes eigenvalues and eigenvectors. The PyTorch function where() returns the indices at which the specified condition is true. axis_of_rotation = e[:, torch.where(l == 1.0)] Obtains the eigenvector for eigenvalue 1 axis_of_rotation = torch.squeeze(axis_of_rotation) assert torch.allclose( axis_of_rotation.real, torch.tensor([0, 0, 1], dtype=torch.float) The axis of rotation is the Z-axis. 2.15 Matrix diagonalization 73 ) p = torch.randint(0, 10, (1,)) * axis_of_rotation assert torch.allclose(torch.matmul(R, p.real), p.real) Takes a random point on this axis and applies the rotation to this point; its position does not change. 2.15 Matrix diagonalization In section 2.12, we studied linear systems and their importance in machine learning. We also remarked that the standard mathematical process of solving linear systems via matrix inversion is not very desirable from a machine learning point of view. In this section, we will see one method of solving linear systems without matrix inversion. In addition, this section will help us develop the insights necessary to understand quadratic forms and, eventually, principal component analysis (PCA), one of the most important tools in data science. Consider an n × n matrix A with n linearly independent eigenvectors. Let S be a matrix with these eigenvectors as its columns. That is, A ®e1 = 휆1 ®e1 A ®e2 = 휆2 ®e2 ... ... A ®en = 휆n ®en and S = h ®e1 ®e2 · · · ®en i Then AS = A h ®e1 ®e2 · · · ®en i = h A ®e1 A ®e2 · · · A ®en i = h 휆1 ®e1 휆2 ®e2 · · · 휆n ®en i = h ®e1 ®e2 · · · ®en i  휆1 0 · · · 0 0 휆2 · · · 0 ... ... ... ... 0 0 · · · 휆n  = SΛ where Λ =  휆1 0 · · · 0 0 휆2 · · · 0 ... ... ... ... 0 0 · · · 휆n  is a diagonal matrix with the eigenvalues of A on the diagonal and 0 everywhere else. Thus, we have AS = SΛ 74 CHAPTER 2 Vectors, matrices, and tensors in machine learning which leads to A = SΛS−1 and Λ = S−1AS If A is symmetric, then its eigenvectors are orthogonal. Then STS = SST = I ⇔S−1 = ST, and we get the diagonalization of A: A = SΛST Note that diagonalization is not unique: a given matrix may be diagonalized in multiple ways. 2.15.1 PyTorch code for matrix diagonalization Now we will study the PyTorch implementation of the math we learned in section 2.15. As usual, we will only show the directly relevant bit of code here. NOTE Fully functional code for this section, executable via Jupyter Notebook, can be found at http://mng.bz/RXJn. Listing 2.18 Diagonalization of a matrix def diagonalize(matrix): Diagonalization is factorizing a matrix A = S횺S−1. S is a matrix with eigenvectors of A as columns. 횺is a diagonal matrix with eigenvalues of A in the diagonal. try: l, e = torch.linalg.eig(matrix) The PyTorch function eig() returns eigenvalues and vectors. sigma = torch.diag(l) The PyTorch function diag() creates a diagonal matrix of given values. return e, torch.diag(l), torch.linalg.inv(e) Returns the three factors except np.linalg.LinAlgError: print(''Cannot diagonalize matrix!'') A = torch.tensor([[0.7071, 0.7071, 0], [-0.7071, 0.7071, 0], Creates a matrix A [0, 0, 1]]) S, sigma, S_inv = diagonalize(A) A1 = torch.matmul(S, torch.matmul(sigma, S_inv)) Reconstructs A from its factors assert torch.allclose(A, A1.real) Verifies that the reconstructed matrix is the same as the original 2.15.2 Solving linear systems without inversion via diagonalization Diagonalization has many practical applications. Let’s study one now. In general, matrix inversion (that is, computing A−1) is a very complex process that is numerically unstable. Hence, solving A®x = ®b via ®x = A−1®b is to be avoided when possible. In the case of a square 2.15 Matrix diagonalization 75 symmetric matrix with n distinct eigenvalues, diagonalization can come to the rescue. We can solve this in multiple steps. We first diagonalize A: A = SΛST Then A®x = ®b can be written as: SΛST ®x = ®b where S is the matrix with eigenvectors of A as its columns: S = h ®e1 ®e2 · · · ®en i (Since A is symmetric, these eigenvectors are orthogonal. Hence STS = SST = I.) The solution can be obtained in a series of very simple steps: S Λ ST ®x |{z} y2 | {z } y1 = ®b First solve S ®y1 = ®b as ®y1 = ST ®b Notice that both the transpose and matrix-vector multiplications are simple and numer- ically stable operations, unlike matrix inversion. Then we get Λ  ST ®x  = ®y1 Now solve Λ ®y2 = ®y1 as ®y2 = Λ−1 ®y1 Note that since Λ is a diagonal matrix, inverting it is trivial:  휆1 0 · · · 0 0 휆2 · · · 0 ... ... ... ... 0 0 · · · 휆n  −1 =  1 휆1 0 · · · 0 0 1 휆2 · · · 0 ... ... ... ... 0 0 · · · 1 휆n  (2.31) As a final step, solve ST ®x = ®y2 as ®x = S ®y2 Thus we have obtained ®x without a single complex or unstable step. 76 CHAPTER 2 Vectors, matrices, and tensors in machine learning 2.15.3 PyTorch code for solving linear systems via diagonalization Let’s try solving the following set of equations: x + y + z = 8 2x + 2y + 3z = 15 x + 3y + 3z = 16 This can be written using matrices and vectors as A®x = ®b where A =  1 2 1 2 2 3 1 3 3  ®x =  x y z  ®b =  8 15 16  Note that A is a symmetric matrix. It has orthogonal eigenvectors. The matrix with eigenvectors of A in columns is orthogonal. Its transpose and inverse are the same. Listing 2.19 Solving linear systems using diagonalization A = torch.tensor([[1, 2, 1], [2, 2, 3], [1, 3, 3]], dtype=torch.float) Creates a symmetric matrix A assert torch.all(A == A.T) Asserts that A may be symmetric b = torch.tensor([8, 15, 16], dtype=torch.cfloat) Creates a vector ®b x_0 = torch.matmul(torch.linalg.inv(A), b.real) Solves A®x = ®b using matrix inversion, ®x = A−1 ®b. Note: matrix inversion is numerically unstable. w, S = torch.linalg.eig(A) Solves A®x = ®b via diagonalization. A = S횺ST . S 횲ST ®x |{z} y2 | {z } y1 = ®b. y1 = torch.matmul(S.T, b) 1. Solve: S ®y1 = ®b as ®y1 = ST ®b (no matrix inversion) y2 = torch.matmul(torch.diag(1/ w), y1) 2. Solve: 횲®y2 = ®y1 as ®y2 = 횲−1 ®y1 (inverting a diagonal matrix is easy; see equation 2.31.) x_1 = torch.matmul(S, y2) 3. Solve: ST ®x = ®y2 as ®x = S ®y2 (no matrix inversion) assert torch.allclose(x_0, x_1.real) Verifies that the two solutions are the same 2.15.4 Matrix powers using diagonalization If matrix A can be diagonalized, then A = SΛS−1 A2 = SΛS−1SΛS−1 = SΛIΛS−1 = SΛ2S−1 ... An = · · · = · · · = SΛnS−1 2.16 Spectral decomposition of a symmetric matrix 77 For a diagonal matrix Λ =  휆1 0 · · · 0 0 휆2 · · · 0 ... ... ... ... 0 0 · · · 휆n  the nth power is simply Λn =  휆n 1 0 · · · 0 0 휆n 2 · · · 0 ... ... ... ... 0 0 · · · 휆n n  If we need to compute various powers of an m × m matrix A at different times, we should precompute the matrix S and compute any power with only O (m) operations— compared to the  nm3 operations necessary for naive computations. 2.16 Spectral decomposition of a symmetric matrix We have seen in section 2.15 that a square symmetric matrix with distinct eigenvalues can be decomposed as A = SΛST where S = h ®e1 ®e2 · · · ®en i Thus, A = h ®e1 ®e2 · · · ®en i  휆1 0 · · · 0 0 휆2 · · · 0 ... ... ... ... 0 0 · · · 휆n   ®e1 T ®e2 T ... ®en T  This equation can be rewritten as A = 휆1 ®e1 ®e1 T + 휆2 ®e2 ®e2 T + · · · + 휆n ®en ®en T (2.32) Thus a square symmetric matrix can be written in terms of its eigenvalues and eigenvec- tors. This is the spectral resolution theorem. 2.16.1 PyTorch code for the spectral decomposition of a matrix The following listing shows the relevant code for this section. Listing 2.20 Spectral decomposition of a matrix def spectral_decomposition(A): assert len(A.shape) == 2 Asserts that A is a 2D tensor (i.e., matrix) 78 CHAPTER 2 Vectors, matrices, and tensors in machine learning and A.shape[0] == A.shape[1] A is square: i.e., A.shape[0] (num rows) ≜A.shape[1] (num columns) and torch.all(A == A.T) Asserts that A is symmetric: i.e., A == AT l, e = torch.linalg.eig(A) The PyTorch function eig() returns eigenvectors and values. assert len(torch.unique(l.real)) == A.shape[0], “Eigen values are not distinct!'' C = torch.zeros((A.shape[0], A.shape[0], Defines a 3D tensor C of shape n × n × n to hold the n components from equation 2.32. Each term 흀i ®ei ®eT is an n × n matrix. There are n such terms, all compactly held in tensor C. A.shape[0])) for i, lambda_i in enumerate(l): e_i = e[:, i] e_i = e_i.reshape((3, 1)) C[i, :, :] = (lambda_i * torch.matmul(e_i, e_i.T)).real Computes C[i] = 흀i ®ei ®eT return C A = torch.tensor([[1, 2, 1], [2, 2, 3], [1, 3, 3]]).float() C = spectral_decomposition(A) A1 = C.sum(axis=0) Reconstructs A by adding its components stored in C assert torch.allclose(A, A1) Verifies that the matrix reconstructed from spectral components matches the original 2.17 An application relevant to machine learning: Finding the axes of a hyperellipse The notion of an ellipse in high-dimensional space (aka hyperellipse) keeps coming back in various forms in machine learning. Here we will make a preliminary review of them. We will revisit these concepts later. Recall the equation of an ellipse from high school math: x2 a2 + y2 b2 = 1 This is a rather simple ellipse: it is two-dimensional and centered at the origin, and its major and minor axes are aligned with the coordinate axes. Denoting ®x =  x y  as the position vector, the same equation can be written as ®xTΛ®x = 1 where Λ =  1 a2 0 0 1 b2  is a diagonal matrix. Written in this form, the equation can be extended beyond 2D to an n-dimensional axis-aligned ellipse centered at the origin. Now let’s apply a rotation R to the axis. Then every vector ®x transforms to R®x. The 2.17 An application relevant to machine learning: Finding the axes of a hyperellipse 79 equation of the ellipse in the new (rotated) coordinate system is  R®xT Λ  R®x = 1 ⇔®xT  RTΛR  ®x = 1 where A =  RTΛR. The generalized equation of the ellipse is ®xT A®x = 1 Note the following: The ellipse is no longer axis aligned. The matrix A is no longer diagonal. A is symmetric. We can easily verify that AT =  RTΛRT = RTΛTR = RTΛR (remem- ber, the transpose of a diagonal matrix is itself). If, in addition, we want to get rid of the “centered at the origin” assumption, we get  ®x −휇T A  ®x −휇 = 1 (2.33) Now let’s flip the problem around. Suppose we have a generic n-dimensional ellipse. How do we compute its axes’ directions? Clearly, if we can rotate the coordinate system so that the matrix in the middle is diagonal, we are done. Diagonalization (see section 2.15) is the answer. Specifically, we find the matrix S with eigenvectors of A in its columns. This is a rotation matrix (being orthogonal, since A is symmetric). We transform (rotate) the coordinate system by applying this matrix. In this new coordinate system, the ellipse is axis aligned. Stated another way, the new coordinate axes—these are the eigenvectors of A—yield the axes of the ellipse. 2.17.1 PyTorch code for hyperellipses Let’s try finding the axes of the hyperellipse described by the equation 5x2 + 6xy + 5y2 = 20. Note that the actual ellipse we use as an example is 2D (to facilitate visualization), but the code we develop will be general and extensible to multiple dimensions. The ellipse equation can be written using matrices and vectors as ®xT A®x = 1, where A =  5 3 3 5  ®x =  x y  To find the axes of the hyperellipse, we need to transform the coordinate system so that the matrix in the middle becomes diagonal. Here is how this can be done: if we diagonalize A into SΣS−1, then the ellipse equation becomes ®xTSΣS−1®x = 1, where Σ is a diagonal matrix. Since A is symmetric, its eigenvectors are orthogonal. Hence, the matrix containing these eigenvectors as columns is orthogonal: i.e., S−1 = ST. In other words, S is a rotation matrix. So the ellipse equation becomes ®xTSΣST ®x = 1 or 80 CHAPTER 2 Vectors, matrices, and tensors in machine learning  ®xTS Σ  ST ®x = 1 or ®yTΣ®y = 1 where ®y = ST ®x. This is of the desired form since Σ is a diagonal matrix. Remember, S is a rotation matrix. Thus, rotating the coordinate system by S aligns the coordinate axes with the ellipse axes. Listing 2.21 Axes of a hyperellipse ellipse_eq = sy.Eq(5*x**2 + 5*y**2 + 6*x*y, 20) Equation of the ellipse: 5x2 + 6xy + 5y2 = 20 or ®xT A®x = 20, where A = " 5 3 3 5 # , ®x = " x y # A = torch.tensor([[5, 3], [3, 5]]).float() l, S = torch.linalg.eig(A) x_axis_vec = torch.zeros((A.shape[0])) X-axis vector first_eigen_vec = S[:, 0] Major axis of the ellipse dot_prod = torch.dot(x_axis_vec, first_eigen_vec) The dot product between two vectors is the cosine of the angle between them. theta = math.acos(dot_prod) theta = math.degrees(theta) The angle between the ellipse’s major axis and the X-axis: 45◦(see figure 2.16) Figure 2.16 Note that the ellipse’s major axis forms an angle of 45 degrees with the X-axis. Rotating the coordinate system by this angle will align the ellipse axes with the coordinate axes. Subsequently, the first principal vector will also lie along this direction. Summary In machine learning, a vector is a one-dimensional array of numbers and a matrix is a two-dimensional array of numbers. Inputs and outputs of machine learning models are typically represented as vectors or matrices. In multilayered models, inputs and outputs of each individual layer are also represented as vectors or matrices. Images are two-dimensional arrays of numbers corresponding to pixel color values. As such, they are represented as matrices. Summary 81 An n-dimensional vector can be viewed as point in ℝn space. All models can be viewed as functions that map points from input to output space. The model is designed so that it is easier to solve the problem of interest in the output space. A dot product between a pair of vectors ®x = h x1 x2 · · · xn i and ®y = h y1 y2 · · · yn i is the scalar quantity ®x · ®y = x1y1 + x2y2 + · · · + xnyn. It is a mea- sure of how similar the vectors are. Dot products are widely used in machine learning. For instance, in supervised machine learning, we train the model so that its output is as similar as possible to the known output for a sample set of input points known as training data. Here, some variant of the dot prod- uct is often used to measure the similarity of the model output and the known output. Two vectors are orthogonal if their dot product is zero. This means the vectors have no similarity and are independent of each other. A vector’s dot product with itself is the square of the magnitude or length of the vector ®x · ®x = ∥®x∥2 = x1x1 + x2x2 + · · · + xnxn. Given a set of vectors ®x1, ®x2, · · · , ®xn, the weighted sum a1®x1 + a2®x2 + · · · + an ®xn (where a1, a2, · · · , an are arbitrary scalars) is known as a linear combination. In particular, if the coefficients a1, a2, · · · , an are non-negative and they sum to 1, the linear combination is called a convex combination. If it is possible to find a set of coefficients a1, a2, · · · , an, not all zero, such that the linear combination is a null vector (meaning all its elements are zeros), then the vectors ®x1, ®x2, · · · , ®xn are said to linearly dependent. On the other hand, if the only way to obtain a linear combination that is a null vector is to make every coefficient zero, the vectors are said to be linearly independent. One important application of matrices and vectors is to solve a system of linear equations. Such a system can be expressed in matrix vector terms as A®x = ®b, where we solve for an unknown vector ®x satisfying the equation. This system has an exact solution if and only if A is invertible. This means A is a square matrix (the number of rows equals the number of columns) and the row vectors are linearly independent. If the row vectors are linearly independent, so are the column vectors, and vice versa. If the rows and columns are linearly independent, the determinant of A is guaranteed to be nonzero. Hence, linear independence of rows/columns and nonzero determinant are equivalent conditions. If any one of them is satisfied, the linear system has an exact and unique solution. In practice, this requirement is often not met, and we have an over- or under- determined system. In such situations, the Moore-Penrose inverse leads to a form of best approximation. Geometrically, the Moore-Penrose method yields the point that is closest to ®b in the space of vectors spanned by columns of A. Equivalently, the Moore-Penrose solution ®x∗yields the point closest to ®b on the space of vectors spanned by the columns of A. For a square matrix A, if and only if Aˆe = 휆ˆe, we say 휆is an eigenvalue (a scalar) and ˆe is an eigenvector (a unit vector) of A. Physically, the eigenvector ˆe is a unit vector 82 CHAPTER 2 Vectors, matrices, and tensors in machine learning whose direction does not change when transformed by the matrix A. The transform can magnify its length by the scalar scale factor 휆, which is the eigenvalue. An n × n matrix A has n eigenvalue/eigenvector pairs. The eigenvalues need not all be unique. The eigenvectors corresponding to different eigenvalues are linearly independent. If the matrix A is symmetric, satisfying AT = A, the eigenvectors corresponding to different eigenvalues are orthogonal. A rotation matrix is a matrix in which the rows are orthogonal to each other and so are the columns. Such a matrix is also known as an orthogonal matrix. An orthogonal matrix R satisfies the equation RTR = I, where I is the identity matrix. In the special case when the matrix A is a rotation matrix R, one of the eigenvalues is always 1. The corresponding eigenvector is the axis of rotation. A matrix A with n linearly independent eigenvectors can be decomposed as A = SΛS−1, where S = h ®e1 ®e2 · · · ®en i is the matrix with eigenvectors of A as its columns and Λ is a diagonal matrix with the eigenvalues of A as its diagonal. This decomposition is called matrix diagonalization and leads to a numerically stable way to solve linear systems. A square symmetric matrix A can be expressed in terms of its eigenvectors and eigenvalues as A = 휆1 ®e1 ®e1 T + 휆2 ®e2 ®e2 T + · · · + 휆n ®en ®en T. This is known as the spectral decomposition of the matrix A. 3 Classifiers and vector calculus We took a first look at the core concept of machine learning in section 1.3. Then, in section 2.8.2, we examined classifiers as a special case. But so far, we have skipped the topic of error minimization: given one or more training examples, how do we adjust the weights and biases to make the machine closer to the desired ideal? We will study this topic in this chapter by discussing the concept of gradients. NOTE The complete PyTorch code for this chapter is available at http://mng.bz /4Zya in the form of fully functional and executable Jupyter notebooks. 3.1 Geometrical view of image classification To fix our ideas, consider a machine that classifies whether an image contains a car or a giraffe. Such classifiers, with only two classes, are known as binary classifiers. The first question is how to represent the input. 3.1.1 Input representation The car-versus-giraffe scenario belongs to a special class of problems where we are analyzing a visual scene. Here, the inputs are the brightness levels of various points in the 3D scene projected onto a 2D image plane. Each element of the image represents a point in the actual scene and is referred to as a pixel. The image is a two-dimensional array representing the collection of pixel values at a given instant in time. It is usually scaled to a fixed size, say 224 × 224. As such, the image can be viewed as a matrix: 83 84 CHAPTER 3 Classifiers and vector calculus X =  X0,0 X0,1 · · · X0,223 X1,0 X1,1 · · · X1,223 ... ... ... ... X223,0 X223,1 · · · X223,223  Each element of the matrix, Xi, j, is a pixel color value in the range [0, 255]. IMAGE RASTERIZATION In the previous chapters, we have always seen a vector as the input to a machine learning system. The vector representation of the input allowed us to view it as a point in a high-dimensional space. This led to many geometric insights about classification. But here, our input is an image, which is akin to a matrix rather than a vector. Can we represent an image (matrix) as a vector? The answer is yes. A matrix can always be converted into a vector by a process called rasterization. During rasterization, we iterate over the elements of the matrix from left to right and top to bottom, storing successive encountered elements into a vector. The result is the rasterized vector. It has the same elements as the original matrix, but they are organized differently. The length of the rasterized vector is equal to the product of the row count and column count of the matrix. The rasterized vector for the earlier matrix X has 224 × 224 = 50176 elements ®x =  x0 = X0,0 x1 = X0,1 ... x223 = X0,223 x224 = X1,0 x225 = X1,1 ... x50175 = X223,223  where xi ∈[0, 255] are values of the image pixels. Thus, a 224 × 224 input image can be viewed as a vector (equivalently, a point) in a 50,176-dimensional space. 3.1.2 Classifiers as decision boundaries We see that input images can be converted to vectors via rasterization. Each vector can be viewed as a point in a high-dimensional space. But the points corresponding to any given object or class, say giraffe or car, are not distributed randomly all over the space. Rather, they occupy a small portion (subspace) in the vast high-dimensional space of inputs. This is because there is always inherent commonality in members of a class. For instance, all giraffes are predominantly yellow with a bit of black, and cars have a somewhat fixed shape. This causes the pixel values in images containing the same object to have somewhat similar values. Overall, this means points belonging to a class loosely form a cluster. 3.1 Geometrical view of image classification 85 NOTE Geometrically speaking, a classifier is a hypersurface that separates the point clusters for the classes we want to recognize. This surface forms a decision boundary—the decision about which class a specific input point belongs to is made by looking at which side of the surface the point belongs to. Figure 3.1a shows an example of a rasterized space for the giraffe and car classification problem. The points corresponding to a giraffe are marked g, and those corresponding to a car are marked c. This is a simple case. Here, the classifier surface (aka decision boundary) that separates the cluster of points corresponding to car from those corre- sponding to giraffe is a hyperplane, depicted in figure 3.1a. NOTE We often call surfaces hypersurfaces and planes hyperplanes in greater than three dimensions. g g g g g g g g g g g g g g g g g g g g g g g g g g g g g Classifier surface (hyperplane) . . . . . . . .. g = giraffe c = car c c c c c c c c c c c c c c c c c c c c c c c c c cc c c c c c c c (a) Car vs. giraffe classifier z . . . . . . . .. z = zebra h = horse z z z z z z z z z z z z z z z z z z z z z z z z z z z z z z z z z z z z z z z z Classifier surface (hypersphere) h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h h (b) Horse vs. zebra classifier Figure 3.1 Geometric depiction of a classification problem. In the multidimensional input space, each data instance corresponds to a point. In figure 3.1a, the points marked c denote cars, and points marked g denote giraffes. This is a simple case: the points form reasonably distinct clusters, so the classifica- tion can be done with a relatively simple surface, a hyperplane. The exact parameters of the hyperplane— orientation and position—are determined via training. In figure 3.1b, the points marked h denote horses, and those marked z denote zebras. This case is a bit more difficult: the classification has to be done with a curved (nonplanar) surface, a hypersphere. The parameters of the hypersphere—radius and center—are determined via training. 86 CHAPTER 3 Classifiers and vector calculus Figure 3.1b shows a more difficult example: horse and zebra classification in images. Here the points corresponding to horses are marked h and those corresponding to zebras are marked z. In this example, we need a nonlinear (curved) surface (such as a hypersphere) to separate the two classes. 3.1.3 Modeling in a nutshell Unfortunately, in the typical scenario, we do not know the separating surface. In fact, we do not even know all the points belonging to a class of interest. All we know is a sampled set of inputs ®xi (training inputs) and corresponding classes ¯®yi (the ground truth). The complete set of training inputs plus ground truth— n ®xi, ¯®yi o for a large set of i values—is called the training data. When we want to teach a baby to recognize a car, we show the baby several example cars and say “This is a car.” The training data plays the same role for a neural network. From only this training dataset n ®xi, ¯®yi o ∀i ∈[1, n], we have to identify a good enough approximation of the general classifying surface that when presented with a random scene, we can map it to an input point ®x, check which side of the surface that point lies on, and identify the class (car or giraffe). This process of developing a best guess for a surface that forms a decision boundary between various classes of interest is called modeling the classifier. NOTE The ground truth labels (¯®yi) for the training images ®xi are often created manually. This process of manually generating labels for the training images is one of the most painful aspects of machine learning, and significant research effort is going on at the moment to mitigate it. As indicated in section 1.3, modeling has two steps: 1 Model architecture selection: Choose the parametric model function 휙 ®x; ®w, b. This function takes an input vector ®x and emits the class y. It has a set of parameters ®w, b, which are unknown at first. This function is typically chosen from a bank of well-known functions that are tried and tested; for simple problems, we may choose a linear model, and for more complex problems, we choose nonlinear models. The model designer makes the choice based on their understanding of the problem. Remember, at this point the parameters are still unknown—we have only decided on the function family for the model. 2 Model training: Estimate the parameters ®w, b such that 휙emits the known correct output (as closely as possible) on the training data inputs. This is typically done via an iterative process. For each training data instance ®xi, we evaluate yi = 휙 ®xi; ®w, b. This emitted output is compared with the corresponding known outputs ¯yi. Their difference, ei = ∥yi −¯yi ∥, is called the training error. The sum of training errors over all training data is the aggregate training error. We iteratively adjust the parameters ®w, b such that the aggregate training error keeps going down. This means at each iteration, we adjust the parameters so the model output yi moves a little closer to the target output ¯yi for all i. Exactly how to adjust the parameters to reduce the error forms the bulk of this chapter and will be introduced in section 3.3. 3.1 Geometrical view of image classification 87 The function 휙 ®x; ®w, b represents the decision boundary hypersurface. For example, in the binary classification problem depicted in figure 3.1, 휙 ®x; ®w, b may represent a plane (shown by the dashed line). Points on one side of the plane are classified as cars, while points on the other side are classified as giraffes. Here, 휙 ®x; ®w, b = ®wT ®x + b From equation 2.14 we know this equation represents a plane. In figure 3.1b, a good planar separation does not exist—we need a nonlinear separa- tor, such as the spherical separator shown with dashed lines. Here, 휙 ®x; ®w, b = ®xT  w0 0 · · · 0 0 w1 · · · 0 ... ... ... ... 0 0 · · · wn  ®x + b = 0 This equation represents a sphere. It should be noted that in typical real-life cases, the separating surface does not correspond to any known geometric surface (see figure 3.2). But in this chapter, we will continue to use simple examples to bring out the underlying concepts. . . . . . . . .. 0 = class 2 + = class 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + Figure 3.2 In real-life problems, the separating surface is often not a well-known surface like a plane or sphere. And often, the classification is not perfect—some points fall on the wrong side of the separator. 88 CHAPTER 3 Classifiers and vector calculus 3.1.4 Sign of the surface function in binary classification In the special case of binary classifiers, the sign of the expression 휙 ®x; ®w, b representing the decision boundary has a special significance. To see this, consider a line in a 2D plane corresponding to the equation y + 2x + 1 = 0 All points on the line have x, y coordinate values satisfying this equation. The line divides the 2D plane into two half planes. All points on one half plane have x, y values such that y + 2x + 1 is negative. All points in the other half plane have x, y values such that y + 2x + 1 is positive. This is shown in figure 3.3. This idea can be extended to other surfaces and higher dimensions. Thus, in binary classification, once we have estimated an optimal decision surface 휙 ®x; ®w, b, given any input vector ®x, we can compute the sign of 휙 ®x; ®w, b to predict the class. Figure 3.3 Given a point (x0, y0) and a separator y + 2x + 1 = 0, we can tell which side of the separator the point lies on from the sign of y0 + 2x0 + 1. 3.2 Error, aka loss function As stated earlier, during training, we adjust the parameters ®w, b so that the error keeps going down. Let’s derive a quantitative expression for this error (aka loss function). Later, we will see how to minimize it. 3.3 Minimizing loss functions: Gradient vectors 89 Overall, training data consists of a set of labeled inputs (training data instances paired with known ground truths): (®x(0), ¯y(0)) (®x(1), ¯y(1)) ... , ... (®x(N ), ¯y(N )) Now we define a loss function. On a specific training data instance, the loss function effectively measures the error made by the machine on that particular training data— input-target pair  ®x(i), y(i) . Although there are many sophisticated error functions more suitable for this problem, for now, let’s use a squared error function for the sake of simplicity (introduced in section 2.5.4). The squared error on the ith training data element is the squared difference between the output yielded by the model and the expected or target output:  e(i)2 =  휙  ®x(i); ®w, b  −y(i)2 (3.1) The total loss (aka squared error) during training is L   ®w, b = E2   ®w, b = i=N Õ i=0  e(i)2 (3.2) Note that this total error is not a function of any specific training data instance. Rather, it is the overall error over the entire training data set. This is what we minimize by adjusting ®w and b. To be precise, we estimate the ®w and b that will minimize L   ®w, b. 3.3 Minimizing loss functions: Gradient vectors The goal of training is to estimate the weights and bias parameter ®w, b that will minimize L. This is usually done by an iterative process. We start with random values of ®w, b and adjust these values so that the loss L   ®w, b = E2   ®w, b declines rapidly. Doing this many times is likely to take us close to the optimal values for ®w, b. This is the essential idea behind the process of training a model. It is important to note that we are minimizing the total error: this prevents us from over-indexing on any particular training instance. If the training data is a well-sampled set, the parameters ®w, b that minimize loss over the training dataset will also work well during inferencing. How do we “adjust” ®w, b so that the value of loss L = E2 declines? This is where gradients come in. For any function L   ®w, b, the gradient with respect to ®w, b—that is, ∇®w,bL   ®w, b—indicates the direction along which the maximum change in L occurs. The gradient is the analog of a derivative in 1D calculus. Intuitively, going down along the direction of the gradient of a function seems like the best strategy for minimizing the function value. Geometrically speaking, if we start at an arbitrary point on the surface corresponding to L   ®w, b and move along the direction of the gradient ∇®w,bL   ®w, b, we will go toward the minimum at the fastest rate (this is discussed in detail throughout the rest of this 90 CHAPTER 3 Classifiers and vector calculus section). Hence, during training, we iteratively move toward the minimum by taking steps along ∇®w,bL   ®w, b. Note that the gradient is with respect to weights, not the input. The overall algorithm is shown in algorithm 3.1. Algorithm 3.1 Training a supervised model (overall idea) Initialize ®w, b with random values while L   ®w, b > threshold do  ®w b  =  ®w b  −휇∇®w,bL   ®w, b Recompute L on new ®w, b. end while ®w∗←®w, b∗←b Note the following points: In each iteration, we are adjusting ®w, b along the gradient of the error function. We will see in section 3.3 that this is the direction of maximum change for L. Thus, L is reduced at a maximal rate. 휇is the learning rate: larger values imply longer steps, and smaller values imply shorter steps. The simplest approach, outlined in algorithm 3.1, takes equal-sized steps everywhere. In later chapters, we will study more sophisticated approaches where we try to sense how close to the minimum we are and vary the step size accordingly: – We take longer steps when far from the minimum, to progress quickly. – We take shorter steps when near the minimum, to avoid overshooting it. Mathematically, we should keep iterating until the loss becomes minimal (that is, the gradient of the loss is zero). But in practice, we simply iterate until the accuracy is good enough for the purpose at hand. 3.3.1 Gradients: A machine learning-centric introduction In machine learning, we model the output as a parametric function of the inputs. We define a loss function that quantifies the difference between the model output and the known ideal output on the set of training inputs. Then we try to obtain the parameter values that will minimize this loss. This effectively identifies the parameters that will result in the model function emitting outputs as close as possible to the ideal on the set of training inputs. The loss function depends on ®x (the model inputs), ¯y (the known ideal outputs on the training data—aka ground truth), and ®w (the parameters). Here only the behavior of the loss function with respect to the parameters is of interest to us, so we are ignoring everything else and denoting the loss function as a function of the parameters as L   ®w. NOTE For the sake of brevity, here we use the symbol w to denote all parameters— weight as well as bias. 3.3 Minimizing loss functions: Gradient vectors 91 The core question we are trying to answer is this: given a loss L   ®w and current para- meter values ®w, what is the optimal change in the parameters ® 훿w that maximally reduces the loss? Equivalently, we want to determine ® 훿w that will make 훿L = L  ®w + ® 훿w  −L   ®w as negative as possible. Toward that goal, we will study the relationship between the loss function L (w) and change in parameter values ® 훿w in several scenarios of increasing complexity.1 ONE-DIMENSIONAL LOSS FUNCTIONS For simplicity, we begin by examining this topic in one dimension—meaning there is a single parameter w. The first example we will study is the simplest possible case: a linear one-dimensional loss function, shown in figure 3.4a. A linear loss function in one dimension can be written as L (w) = mw + c. If we change the parameter w by a small amount 훿w, what is the corresponding change in loss 훿L? We have 훿L = L (w + 훿w) −L (w) = (m (w + 훿w) + c) −(m (w) + c) = m 훿w which gives us 훿L 훿w = m, a con- stant. By definition, the derivative dL dw = lim훿w→0 훿L 훿w, which leads to dL dw = m. Thus, for the straight line L (w) = mw + c, the rate of change of L with respect to w is constant everywhere and equals the slope m. Putting all this together, we get 훿L = m훿w = dL dw 훿w. Let’s now study a slightly more complex, non-linear but still one dimensional case—a parabolic loss function illustrated in figure 3.4b. This parabola can be written as L (w) = w2. If we change the parameter w by a small amount 훿w, what is the corresponding change in in loss 훿L? We have 훿L = L (w + 훿w) −L (w) = (w + 훿w)2 −w2 =  2w훿w + 훿w2. For infinitesimally small 훿w, 훿w2 becomes negligibly small and we get lim훿w→0 훿L = lim훿w→0  2w훿w + 훿w2 = 2w훿w and dL dw = lim훿w→0 훿L 훿w = 2w. Combining all these we get the same equation as the linear case 훿L = dL dw 훿w. Of course, in case of the straight line this expression holds for all 훿w while in the non-linear curves the expression holds only for small 훿w. 휹L and 휹w In general, for all one-dimensional loss functions L (w), the change 훿L caused by a change 훿w in parameters can be expressed as follows: 훿L = dL dw 훿w (3.3) To decrease L, 훿L must be negative. From equation 3.3, we can see that this requires 훿w (change in w) and dL dw (derivative) to have opposite signs. Geometrically speaking, the loss function represents a curve with the loss L (w) plotted along the Y axis against the parameter w plotted along the X axis (see figure 3.4 for examples). The tangent at any point can be viewed as the local approximation to the curve itself for an infinitesimally small neighborhood around the point. The derivative at any point represents the slope of the tangent to the curve at that point. 1 If the change in a quantity such as w is infinitesimally small, we use the symbol dw to denote the change. If the change is small but not infinitesimally so, we use the symbol 훿w. 92 CHAPTER 3 Classifiers and vector calculus (a) Line: L (w) = 2w + 1, dL dw = m (b) Parabola: L (w) =w2, dL dw = 2w Figure 3.4 휹L in terms of 휹w in one dimension, illustrated with two example curves: a straight line and a parabola. In general, 휹L = dL dw 휹w. To decrease loss, 휹w must have the opposite sign of the derivative dL dw . In (a), this implies we always have to move left (decrease w) to decrease L. In (b), if we are in the left half (e.g., point Q), the derivative is negative, and we have to move to the right to decrease L. But if we are in the right half, the derivative is positive, and we have to move to the left to decrease L. Geometrically, this is equivalent to following the tangent “downward.” 3.3 Minimizing loss functions: Gradient vectors 93 NOTE Equation 3.3 basically tells us that to reduce the loss value, we have to follow the tangent, moving to the right (i.e., positive 훿w) if the derivative is negative and moving to the left (i.e., negative 훿w) if the derivative is positive. MULTIDIMENSIONAL LOSS FUNCTIONS If there are many tunable parameters, our loss function will be a function of many vari- ables, which implies that we have a high-dimensional vector ®w and a loss function L   ®w. Our goal is to compute the change 훿L in L   ®w caused by a small vector displacement ® 훿w. We immediately note a fundamental difference from the one-dimensional case: the parameter change is a vector, ® 훿w, which has not only a magnitude denoted ∥® 훿w∥but also a direction denoted by the unit vector ˆ 훿w. We can take a step of the same size in the w space, and the change in L   ®w will be different for different directions. The situation is illustrated in figure 3.5, which shows an example loss function L   ®w ≡L (w0, w1) = 2w2 0 + 3w2 1 for two independent variables w0 and w1. Let’s examine how this loss function changes with a few concrete examples. Figure 3.5 Plot for sur- face L   ®w ≡L (w0, w1) = 2w2 0 + 3w2 1 against ®w ≡(w0, w1). From an example point P ≡(w0 = 3, w1 = 4, L = 66) on the surface, we can travel in many directions to reduce L. Some of these are shown by arrows. The maximum reduction occurs when we travel along the dark arrow: this happens when ®w is changed along ® 휹w = [−12, −24]T, which is the negative of the gradient of L   ®w at P. Suppose we are at ®w =  w0 = 3 w1 = 4  . The corresponding value of L   ®w is 2 ∗32 + 3 ∗42 = 66. Now, suppose we undergo a small displacement from this point: ® 훿w =  0.0003 0.0004  . The new value is L  ®w + ® 훿w  = L (3.0003, 4.0004) = 2 ∗3.00032 + 3 ∗4.00042 ≈66.0132066. Thus this displacement vector ® 훿w =  0.0003 0.0004  causes a change 훿L = 66.01320066 −66 = 0.01320066 in L. 94 CHAPTER 3 Classifiers and vector calculus On the other hand, if the displacement vector is ® 훿w =  0.0004 0.0003  , we get L  ®w + ® 훿w  = L (3.0004, 4.0003) = 2 ∗3.00042 + 3 ∗4.00032 ≈66.0120006. Thus, this displacement vector causes a change 훿L = 66.0120006 −66 = 0.0120006 in L. The displacement vectors ® 훿w =  0.0003 0.0004  and ® 훿w =  0.0004 0.0003  have the same length √ 0.00032 + 0.00042 = √ 0.00042 + 0.00032 = 0.0005 but different directions. The change they cause to the func- tion value is different. This exemplifies our thesis that in multivariable loss function, the change in the loss function depends not only on the magnitude but also on the direction of the displacement in the parameter space. What is the general relationship between the displacement vector ® 훿w in the parameter space and the overall change in loss L   ®w? To examine this question, we need to know what a partial derivative is. PARTIAL DERIVATIVES The derivative dL dw of a function L (w) indicates the rate of change of the function with respect to w. But if L is a function of many variables, how does it change if only one of those variables is changed? This question leads to the notion of partial derivatives. The partial derivative of a function of many variables is a derivative taken with respect to exactly one variable, treating all other variables as constants. For instance, given L   ®w ≡L (w0, w1) = 2w2 0 + 3w2 1, the partial derivatives with respect to w0 , w1 are 휕L 휕w0 = 4w0 휕L 휕w1 = 6w1 TOTAL CHANGE IN A MULTIDIMENSIONAL FUNCTION Partial derivatives estimate the change in a function if a single variable changes and the others stay constant. How do we estimate the change in a function’s value if all the variables change together? The total change can be estimated by taking a weighted combination of the partial derivatives. Let ®w and ® 훿w denote the point and the displacement vector, respectively: ®w =  w0 w1 ... wn  3.3 Minimizing loss functions: Gradient vectors 95 ® 훿w =  훿w0 훿w1 ... 훿wn  Then 훿L   ®w = L  ®w + ® 훿w  −L   ®w = 휕L 휕w0 훿w0 + 휕L 휕w1 훿w1 + · · · + 휕L 휕wn 훿wn (3.4) Equation 3.4 essentially says that the total change in L is obtained by adding up the changes caused by displacements in individual variables. The rate of change of L with respect to the change in wi only is 휕L 휕wi . The displacement along the variable wi is 훿wi. Hence, the change caused by the ith element of the displacement is 휕L 휕wi 훿wi— this follows from equation 3.3. The total change is obtained by adding the changes caused by individual elements of the displacement vector: that is, summing over all i from 0 to n. This leads to equation 3.4. Thus equation 3.4 is simply the multidimensional version of equation 3.3. GRADIENTS It would be nice to be able to represent equation 3.4 compactly. To do this, we define a quantity called a gradient: the vector of all the partial derivatives. Given an n-dimensional function L   ®w, its gradient is defined as ∇L   ®w =  휕L 휕w0 휕L 휕w1... 휕L 휕wn  (3.5) Using gradients, we can rewrite equation 3.4 as 훿L   ®w = L  ®w + ® 훿w  −L   ®w = 휕L 휕w0 훿w0 + 휕L 휕w1 훿w1 + · · · + 휕L 휕wn 훿wn =  ∇L   ®wT ® 훿w = ∇L   ®w · ® 훿w (3.6) Equation 3.6 tells us that the total change, 훿L in L   ®w, caused by displacement ® 훿w from ®w in parameter space is the dot product between the gradient vector ∇L   ®w and the displacement vector ® 훿w. This is the exact multidimensional analog of equation 3.3. 96 CHAPTER 3 Classifiers and vector calculus Recall from section 2.5.6 that the dot product of two vectors (of fixed magnitude) attains a maximum value when the vectors are aligned in direction. This yields a physical interpretation of the gradient vector: its direction is the direction in parameter space along which the multidimensional function is changing fastest. It is the multidimensional counterpart of the derivative. This is why, in machine learning, when we want to minimize the loss function, we change the parameter values along the direction of the gradient vector of the loss function. THE GRADIENT IS ZERO AT THE MINIMUM Any optimum (that is, maximum or minimum) of a function is a point of inflection. This means the function turns around at the optimum point. In other words, the gradient direction on one side of the optimum is the opposite of that on the other side. If we try to travel smoothly from positive values to negative values, we must cross zero somewhere in between. Thus, the gradient is zero at the exact point of inflection (maximum or minimum). This is easiest to see in 2D and is depicted in figure 3.6. However, the idea is general: it works in higher dimensions, too. The fact that the gradient becomes zero at the optimum is often used to algebraically compute the optimum. The following example illustrates this. P– gradient (slope of tangent) is negative P+ gradient (slope of tangent) is positive P0 gradient (slope of tangent) is zero Gradient Minimum Figure 3.6 The minimum is always a point of inflection, meaning the func- tion turns around at that point. If we consider any two points P−and P+ on both sides of the minimum, the gradient is positive on one side and negative on the other. Assuming the gradient changes smoothly, it must be zero in between, at the minimum. Consider the simple example function L(w0, w1) = q w2 0 +w2 1. Its optimum occurs when its gradient is zero: ∇®wL =  휕L 휕w0 휕L 휕w1  = 1 2 q w2 0 +w2 1  2w0 2w1  =  0 0  The solution is w0 = 0, w1 = 0 The function attains its minimum value at the origin, which agrees with our intuition. 3.3 Minimizing loss functions: Gradient vectors 97 3.3.2 Level surface representation and loss minimization In figure 3.5, we plotted the loss function L   ®w against the parameter values ®w. In this section, we study a different way of visualizing loss surfaces. This will lend further insight into gradients and minimization. We will continue with our simple example function from the last subsection. Consider a field L(w0, w1) = q w2 0 +w2 1. Its domain is the infinite 2D plane defined by the axes W0 and W1. Note that the function has constant values along concentric circles centered on the origin. For instance, at all points on the circumference of the circle w2 0 +w2 1 = 1, the function has a constant function value of 1. At all points on the circumference of the circle w2 0 +w2 1 = 25, the function has a constant function value of 5. Such constant function value curves on the domain are called level contours in 2D. This is shown as a heat map in figure 3.7. The idea of level contours can be generalized to higher dimensions where we have level surfaces or level hypersurfaces. Note that while the ®w, L   ®w in figure 3.5 was on an (n + 1)-dimensional space (where n is the dimensionality of ®w), the level surface/contour representation is in n-dimensional space. At any point on the domain, what is the direction along which the biggest change in the function value occurs? The answer is along the direction of the gradient. The magnitude of the change corresponds to the magnitude of the gradient. In the current example, say we are at a point (w0, w1). There exists a level contour through this point: the circle with origin at the center passing through (w0, w1). If we move along the circumference of this circle—that is, along the tangent to this circle—the function value does not change. In other words, at any point, the tangent to the level contour through that point is the direction of minimal change. On the other hand, if we move perpendicular to the tangent, maximum change in the function value occurs. The perpendicular to the tangent is known as a normal. This is the direction of the gradient. The gradient at any point on the domain is always normal to the level contour through that point, indicating the direction of maximum change in the function value. In figure 3.7, the gradients are all parallel to the radii of the concentric circles. Recall that while training a machine learning model, we essentially define a loss function in terms of a tunable set of parameters and try to minimize the loss by adjusting (tuning) the parameters. We start at a random point and iteratively progress toward the minimum. Geometrically, this can be viewed as starting at an arbitrary point on the domain and continuing to move in a direction that minimizes the function value. Of course, we would like to progress to the minimum of the function value in as few iterations as possible. In figure 3.7, the minimum is at the origin, which is also the center of all the concentric circles. Wherever we start, we will have to always travel radially inward to reach the minimum (0, 0) of the function q w2 0 +w2 1. In higher dimensions, level contours become level surfaces. Given any function L   ®w with ®w] ∈ℝn, we define level surfaces as L   ®w = constant. If we move along the level surface, the change in L   ®w is minimal (0). The gradient of a function at any point is normal to the level surface through that point. This is the direction along which the function value is changing fastest. Moving along the gradient, we pass from one level surface to another, as shown in figure 3.8. Here the function is 98 CHAPTER 3 Classifiers and vector calculus Figure 3.7 The domain of L(w0, w1) = q w2 0 +w2 1 shown as a heat map of func- tion values. Gradients point radially outward, as shown by the arrowed line. The intensity of the heat map changes fastest along the gradient (that is, radii). This is the direction to follow to rapidly reach lower values of the function represented by the heat map. Figure 3.8 Gradient example in 3D: the function L (w0, w1, w2) = L   ®w =w2 0 +w2 1 +w2 2. The level surfaces L   ®w = constant are concentric spheres with the origin as their center. One such surface is partially shown in the diagram. ∇L   ®w = k [w0 w1 w2]T —the gradient points radially outward. Moving along the gradient, we go from one level surface to another, corresponding to maximum change in L   ®w. Moving along any direction orthogonal to the gradient, we stay on the same level surface (sphere), which corresponds to zero change in the function value. D휽   ®w denotes the directional derivative along the displacement direction making angle 휽with the gradient. If ˆl denotes this displacement direction, D휽   ®w = ∇L   ®w · ˆl. 3.4 Local approximation for the loss function 99 3D: L   ®w = L (w0, w1, w2) =w2 0 +w2 1 +w2 2. The level surfaces w2 0 +w2 1 +w2 2 = constant for various values of the constant are concentric spheres, with the origin as their center. The gradient vector at any point is along the outward-pointing radius of the sphere through that point. Another example is shown in figure 3.9. Here the function is 3D: L   ®w = f (w0, w1, w2) =w2 0 +w2 1. The level surfaces w2 0 +w2 1 = constant for various values of the constant are coaxial cylinders, with w2 as the axis. The gradient vector at any point is along the outward-pointing radius of the planar circle belonging to the cylinder through that point. Figure 3.9 Gradient example in 3D: the function L (w0, w1, w2) = L   ®w =w2 0 +w2 1. The level surfaces f   ®w = constant are coaxial cylinders. One such surface is partially shown in the diagram: ∇L   ®w = k [w0 w1 0]T . The gradient is normal to the curved surface of the cylinder along the outward radius of the circle. Moving along the gradient, we go from one level surface to another, corresponding to the maximum change in L   ®w. Moving along any direction orthogonal to the gradient, we stay on the same level surface (cylinder), which corresponds to zero change in the function value. So far, we have studied the change in loss value resulting from infinitesimally small displacements in the parameter space. In practice, the programmatic displacements undergone during parameter updates while training are small, but not infinitesimally so. Is there any way to improve the approximation in these cases? This is discussed in the following section. 3.4 Local approximation for the loss function Equation 3.6 expresses the change 훿L in the loss value corresponding to displacement ® 훿w in the parameter space. The equation is exactly true if and only if the loss function 100 CHAPTER 3 Classifiers and vector calculus is linear or the magnitude of the displacement is infinitesimally small. In practice, we adjust parameter values by small—but not infinitesimally small—amounts. Under these circumstances, equation 3.6 is only approximately true: the larger the magnitude of ∥® 훿w∥, the worse the approximation. A Taylor series offers a way to approximate a multidimensional function in the local neighborhood of any point by expressing it in terms of the displacements in the parameter space. It is an infinite series, meaning the equation is exactly true (zero approximation) only when we have summed an infinite number of terms. Of course, we cannot add an infinite number of terms with a computer program. But we can improve the accuracy of the approximation as much as we like by including more and more terms. In practice, we include at most up to the second term. Anything beyond that is redundant because the improvement is too small to be realized by the floating point system of current computers. First we will study a Taylor series in one dimension. 3.4.1 1D Taylor series recap Suppose we are trying to describe the curve L (w) in the neighborhood of a particular point w. If we stay infinitesimally close to w, then, as described in section 3.3, we can approximate the curve with a straight line: L (w + 훿w) = L (w) + dL dw 훿w But in the general case, if we are describing a continuous (smooth) function in the neighborhood of a specific point, we use a Taylor series. A Taylor series allows us to describe a function in the neighborhood of a specific point in terms of the value of the function and its derivatives at that point. Doing so is relatively simple in 1D: L (w + 훿w) = L (w) + (훿w) 1! dL dw + (훿w)2 2! d2L dw2 + · · · (3.7) Note that the terms become progressively smaller (since they involve higher and higher powers of a small number 훿w). Hence, although the series goes on to infinity, in practice we entail a negligible loss in accuracy by dropping higher-order terms. We often use the first-order approximation (or, at most, second-order). Equation 3.7 can be rewritten as 훿L = L (w + 훿w) −L (w) = (훿w) 1! dL dw + (훿w)2 2! d2L dw2 + · · · Note that the second term has (훿w)2 as a factor, which is nearly zero at small values of the displacement 훿w. So, for really small 훿w, we include only the first term. Then we get 훿L = (훿w) 1! dL dw , which is the same as equation 3.3. If 훿w is a bit larger and we want greater accuracy, we can include the second-order term. In practice, as mentioned earlier, that is hardly ever done. A handy example of a Taylor series is the expansion of the exponential function ex near x = 0 et = e0+t = 1 + t + t2 2! + t3 3! · · · where we use the fact that dn dxn (ex) |x=0 = ex|x=0 = 1 for all n. 3.5 PyTorch code for gradient descent, error minimization, and model training 101 3.4.2 Multidimensional Taylor series and the Hessian matrix In equation 3.7, we express a function of one variable in a small neighborhood around a point in terms of the derivatives. Can we do a similar thing in higher dimensions? Yes. We simply need to replace the first derivative with the gradient. We replace the second derivative with its multidimensional counterpart: the Hessian matrix. The multidimen- sional Taylor series is as follows L  ®w + ® 훿w  = L   ®w + 1 1!  ® 훿w T ∇L   ®w + 1 2!  ® 훿w T H  L   ®w  ® 훿w  + · · · (3.8) where H  L   ®w, called the Hessian matrix, is defined as H  L   ®w =  휕2L 휕w2 1 휕2L 휕w1휕w2 · · · 휕2L 휕w1휕wn 휕2L 휕w2휕w1 휕2L 휕w2 2 · · · 휕2L 휕w1휕wn ... ... ... 휕2L 휕wn휕w1 휕2L 휕wn휕w2 · · · 휕2L 휕w2n  (3.9) The Hessian matrix is symmetric since 휕2L 휕wi휕wj = 휕2L 휕wj휕wi . Also, note that the Taylor expan- sion assumes that the function is continuous in the neighborhood. Equation 3.8 allows us to compute the value of L in a small neighborhood around point ®w in the parameter space. If we displace from ®w by the vector ® 훿w, we arrive at ®w + ® 훿w. The loss there is L  ®w + ® 훿w  , which is expressed by equation 3.8 in terms of the loss L   ®w at the original point and the displacement ® 훿w. This leads to 훿L = L  ®w + ® 훿w  −L   ®w = 1 1!  ® 훿w T ∇L   ®w + 1 2!  ® 훿w T H  L   ®w  ® 훿w  + · · · (3.10) Note that the first term is same as equation 3.6 and the second term has squares of the displacement. Since the square of a small quantity is even smaller, for very small displacements, the second term disappears, and we essentially get back equation 3.6. This is called first-order approximation. For slightly larger displacements, we can include the second term, involving Hessians to improve the approximation. As stated earlier, this is hardly ever done in practice. 3.5 PyTorch code for gradient descent, error minimization, and model training In this section, we study PyTorch examples in which models are trained by minimizing errors via gradient descent. Before we present the code, we briefly recap the main ideas from a practical point of view. (Complete code for this section can be found at http://mng.bz/4Zya.) 3.5.1 PyTorch code for linear models If the true underlying function we are trying to predict is very simple, linear models suffice. Otherwise, we require nonlinear models. Here we will look at the linear model. 102 CHAPTER 3 Classifiers and vector calculus In machine learning, we identify the input and output variables pertaining to the problem at hand and cast the problem as generating outputs from input variables. All the inputs are represented together by the vector ®x. Sometimes there are multiple outputs, and sometimes there is a single output. Accordingly, we have an output vector ®y or an output scalar y. Let’s denote the function that generates the output from the input vector as f : that is, y = f  ®x. In real-life problems, we do not know f . The crux of machine learning is to estimate f from a set of observed inputs ®xi and their corresponding outputs yi. Each observation can be depicted as a pair ⟨®xi, yi⟩. We model the unknown function f with a known function 휙. 휙is a parameterized function. Although the nature of 휙is known, its parameter values are unknown. These parameter values are “learned” via training. This means we estimate the parameter values such that the overall error on the observations is minimized. If ®w, b denotes the current set of parameters (weights, bias), then the model will output 휙 ®xi, ®w, b on the observed input ®xi. Thus the error on this ith observation is e2 i =  휙 ®xi, ®w, b −yi 2. We can batch several observations and add up the errors into a batch error L = Íi=N i=0  e(i)2 . The error is a function of the parameter set ®w. The question is, how do we adjust ®w so that the error e2 i decreases? We know a function’s value changes most when we move along the direction of the gradient of the parameters. Hence, we adjust the parameters ®w, b as follows:  ®w b  =  ®w b  −휇∇®w,bL   ®w, b Each adjustment reduces the error. Starting from a random set of parameter values and doing this a sufficiently large number of times yields the desired model. A simple and popular model 휙is the linear function (the predicted value is the dot product between the input vector and parameters vector plus bias): ˜yi = 휙 ®xi, ®w, b = ®wT ®x + b = Í j wjxj + b. Our initial implementation (listing 3.1) simply mimics this formula. For more complicated models 휙(with millions of parameters and nonlinearities), we cannot obtain closed-form gradients like this. In such cases, we use a technique called autograd (automatic gradient computation), which does not required closed form gradients. This is discussed in the next section. NOTE In real-world problems, we will not know the true underlying function mapping inputs to outputs. But here, for the sake of gaining insight, we will assume known output functions and perturb them with noise to make them slightly more realistic. Listing 3.1 PyTorch linear model (closed-form gradient formula needed) x = 10 * torch.randn(N) Generates random input values 3.5 PyTorch code for gradient descent, error minimization, and model training 103 y = 1.5 * x + 2.73 y_obs = y + (0.5 * torch.randn(N)) Generates output values by applying a simple known function to the input and then adds noise. Let’s see if our learned function matches the known underlying function. for step in range(num_steps): y_pred = w*x + b Our model, initialized with arbitrary parameter values mean_squared_error = torch.mean( (y_pred - y_obs) ** 2) Model error is the (squared) difference between the observed and predicted values. w_grad = torch.mean(2 * ((y_pred - y_obs)* x)) b_grad = torch.mean(2 * (y_pred - y_obs)) Calculates the gradient of the error using calculus. Possible only with such simple models. w = w - learning_rate * w_grad b = b - learning_rate * b_grad Adjusts the weight, bias along the gradient of error print(''True function: y = 1.5*x + 2.73'') print(''Learned function: y_pred = *x + ''.format(w[0], b[0])) The output is as follows: True function: y = 1.5*x + 2.73 Learned function: y_pred = 1.50059*x + 2.746823 3.5.2 Autograd: PyTorch automatic gradient computation In the PyTorch code in listing 3.1, for this specific model architecture, we computed the gradient using calculus. This approach does not scale to more complex models with millions of weights and perhaps nonlinear complex functions. For scalability, we use an automatic differentiation software library like PyTorch Autograd. Users of the library need not worry about how to compute the gradients—they just construct the model function. Once the function is specified, PyTorch figures out how to compute its gradient through the Autograd technology. To use Autograd, we explicitly tell PyTorch to track gradients for a variable by setting requires_grad = True when creating the variable. PyTorch remembers a computa- tion graph that is updated every time we create an expression using tracked variables. Figure 3.10 shows an example of a computation graph. The following listing, which implements a linear model in PyTorch, relies on Py- Torch’s Autograd for gradient computation. Note that this method does not require the closed-form gradient. Listing 3.2 Linear modeling with PyTorch def update_parameters(params, learning_rate): Updates parameters: adjusts the weight, bias along the gradient of error with torch.no_grad(): Doesn’t track gradients during parameter updates 104 CHAPTER 3 Classifiers and vector calculus ^2 Figure 3.10 Autograd analysis for i, p in enumerate(params): params[i] = p - learning_rate * p.grad for i in range(len(params)): params[i].requires_grad = True Restores gradient tracking x = 10 * torch.randn(N) Generates random training input y = 1.5 * x + 2.73 y_obs = y + (0.5 * torch.randn(N)) Generates training output: applies a simple known function to the input and then adds noise. Let’s see if our learned function matches the known underlying function. w = torch.randn(1, requires_grad=True) b = torch.randn(1, requires_grad=True) params = [b, w] Our model, initialized with arbitrary parameter values 3.5 PyTorch code for gradient descent, error minimization, and model training 105 for step in range(num_steps): y_pred = params[0] + params[1] * x mean_squared_error = torch.mean((y_pred - y_obs) ** 2) The model error is the (squared) difference between the observed and predicted values. mean_squared_error.backward() Backpropagates: computes the partial derivatives of the error with respect to each variable and stores them in the “grad” field within the variable update_parameters(params, learning_rate) Updates parameters using those partial derivatives print(''True function: y = 1.5*x + 2.73'') print(''Learned function: y_pred = *x + ''\ .format(params[1].data[0], params[0].data.[0])) The output is as follows: True function: y = 1.5*x + 2.73 Learned function: y_pred = 1.50059*x + 2.74783 3.5.3 Nonlinear Models in PyTorch In listings 3.1 and 3.2, we fit a linear model to a data distribution that we know to be linear. From the output, we can see that those models converged to a pretty good approximation of the underlying output function. This is also shown graphically in figure 3.11. But what happens if the underlying output function is nonlinear? Figure 3.11 Linear approximation of linear data. By step 1,000, the model has more or less converged to the true underlying function. 106 CHAPTER 3 Classifiers and vector calculus First, listing 3.3 tries to use a linear model on a nonlinear data distribution. As expected (and demonstrated via the output as well as figure 3.12), this model does not do well, because we are using an inadequate model architecture. Further training will not help. Listing 3.3 Linear approximation of nonlinear data x = 10 * torch.rand(N, 1) Generates random input training data y = x**2 - x + 2.0 y_obs = y + (0.5 * torch.rand(N, 1) - 0.25) Generates training output: applies a known nonlinear function to the input and then perturbs it with noise w = torch.rand(1, requires_grad=True) b = torch.rand(1, requires_grad=True) params = [b, w] for step in range(num_steps): y_pred = params[0] + params[1] * x Trains a linear model as in listing 3.2 mean_squared_error = torch.mean((y_pred - y_obs) ** 2) mean_squared_error.backward() update_parameters(params, learning_rate) print(''True function: y = 1.5*x + 2.73'') print(''Learned function: y_pred = *x + ''\ .format(params[1].data[0], params[0].data[0])) Figure 3.12 Linear approximation of nonlinear data. Clearly the model is not converging to anything close to the desired/true function. Our model architecture is inadequate. 3.5 PyTorch code for gradient descent, error minimization, and model training 107 Here is the output: True function: y=x^2 -x + 2 Learned function: y_pred = 8.79633331299*x + -13.4027605057 Next, listing 3.4 tries a nonlinear model. As expected (and demonstrated via the output as well as figure 3.13), the nonlinear model does well. In real-life problems, we usually assume nonlinearity and choose a model architecture accordingly. Listing 3.4 Nonlinear modeling with PyTorch params = [w0, w1, w2] for step in range(num_steps): y_pred = params[0] + params[1] * x + params[2] * (x**2) mean_squared_error = torch.mean((y_pred -y_obs) ** 2) mean_squared_error.backward() update_parameters(params, learning_rate) print(''True function: y= 2 - x + x^2'') print(''Learned function: y_pred = + *x + *x^2''\ .format(params[0].data[0], params[1].data[0], params[2].data[0])) Here is the output: True function: y= 2 - x + x^2 Learned function: y_pred = 1.87116754055+-0.953767299652*x+0.996278882027*x^2 Figure 3.13 If we use a nonlinear model, it more or less converges to the true underlying function. 108 CHAPTER 3 Classifiers and vector calculus 3.5.4 A linear model for the cat brain in PyTorch In section 2.12.6, we solved the cat-brain problem directly via pseudo-inverse. Now, let’s train a PyTorch model over the same dataset. As expected, the model parameters will converge to a solution close to that obtained by the pseudo-inverse technique (this being a simple training dataset); but in the following listing, we demonstrate our first somewhat sophisticated PyTorch model. Listing 3.5 Our first realistic PyTorch model (solves the cat-brain problem) X = torch.tensor([[0.11, 0.09], ... [0.63, 0.24]]) X = torch.column_stack((X, torch.ones(15))) Adds a column of all 1s to augment the data matrix X y = torch.tensor([-0.8, ... 0.37]) X, ®y created (see section 2.12.3) as per equation 2.22 It is easy to verify that the solution to equation 2.22 is roughly w0 = 1, w1 = 1, b = −1. But the equations are not consistent: no one solution perfectly fits all of them. We expect the learned model to be close to y = x0 + x1 −1. class LinearModel(torch.nn.Module): def __init__(self, num_features): super(LinearModel, self).__init__() self.w = torch.nn.Parameter( Parameter is a type (subclass) of Torch Tensor suitable for model parameters (weights+bias). torch.randn(num_features, 1)) def forward(self, X): y_pred = torch.mm(X, self.w) Linear model: ®y = X ®w (X is augmented, and ®w includes bias) return y_pred model = LinearModel(num_features=num_unknowns) loss_fn = torch.nn.MSELoss(reduction='sum') Ready-made class for computing squared error loss optimizer = torch.optim.SGD(model.parameters(), lr=1e-2) Ready-made class for updating weights using the gradient of error for step in range(num_steps): y_pred = model(X) loss = loss_fn(y_pred, y) optimizer.zero_grad() Zeros out all partial derivatives loss.backward() Computes partial derivatives via autograd optimizer.step() Updates the parameters using gradients computed in the backward() step solution_gd = torch.squeeze(model.w.data) print(''The solution via gradient descent is ''.format(solution_gd)) The output is as follows: The solution via gradient descent is [ 1.0766 0.8976 -0.9581] 3.7 Convex sets and functions 109 3.6 Convex and nonconvex functions, and global and local minima A convex surface (see figure 3.14) has a single optimum (maximum/minimum): the global one.2 Wherever we are on such a surface, if we keep moving along the gradient in parameter space, we will eventually reach the global minimum. On the other hand, on a nonconvex surface, we might get stuck in a local minimum. For instance, in figure 3.14b, if we start at the point marked with the arrowed line indicating a gradient and move downward following the gradient, we will arrive at a local minimum. At the minimum, the gradient is zero, and we will never move out of that point. Gradient Global minimum (a) A convex function Gradient Global minimum Local minimum (b) A nonconvex function Figure 3.14 Convex vs. nonconvex func- tions. Convex functions have only a global optimum (minimum or maximum), no local optimum. Following the gradient downward is guaranteed to reach the global minimum. Friendly error functions are convex. A nonconvex function has one or more local optima. Following the gradi- ent may reach a local minimum and never discover the global minimum. Unfriendly error functions are nonconvex. There was a time when researchers put a lot of effort into trying to avoid local minima. Special techniques (such as simulated annealing) were developed to avoid them. How- ever, neural networks typically do not do anything special to deal with local minima and nonconvex functions. Often, the local minimum is good enough. Or we can retrain by starting from a different random point, which may help us escape the local minimum. 3.7 Convex sets and functions In section 3.6, we briefly encountered convex functions and how convexity tells us whether a function has local minima. In this section, we look at convex functions in 2 Although the theory applies to either optimum, maximum or minimum, for brevity’s sake, here we will only talk in terms of the minimum 110 CHAPTER 3 Classifiers and vector calculus more detail. In particular, we learn how to tell whether a given function is convex. We also discuss some important properties of convex functions that will come in handy later—for instance, when we study Jensen’s inequality in probability and statistics, in the appendix. We will mostly illustrate the ideas in 2D space, but they can be easily extended to higher dimensions. 3.7.1 Convex sets Informally speaking, a set of points is said to be convex if and only if the straight line joining any pair of points in the set lies entirely within the set. For example, if we join any pair of points in the shaded region on the left-hand side of figure 3.15 with a straight line, all points on that line will also be in the shaded region. This is illustrated by points A and B in the figure. The complete set of points in any such region constitutes a convex set. -15 -10 -5 0 5 10 15 1000 750 500 250 0 -250 -500 -750 -1000 Convex function: y = x2 100 80 60 40 20 0 -15 -10 -5 0 5 10 15 Non-Convex function: y = x3 A B C D Figure 3.15 Convex and nonconvex sets. The points in the left-hand shaded region form a convex set. The line joining any pair of points in that shaded region lies entirely in the shaded region: for example, AB. The points in the right-hand shaded region form a nonconvex set. For instance, the line joining points C and D passes through a nonshaded region even though both end points belong to a shaded region. Conversely, a set of points is nonconvex if it contains at least one pair of points whose joining line contains a point not belonging to the set. For instance, the shaded region on the right-hand side of figure 3.15 contains a pair of points C and D whose joining line passes through points not belonging to the shaded region. The boundary of a convex set is always a convex curve. 3.7.2 Convex curves and surfaces Consider a function g (x). Let’s pick any two points on the curve y = g(x): A ≡(x1, y1 = g(x1)) and B ≡(x2, y2 = g(x2)). Now consider the line segment L joining A and B. From section 2.8.1 (equation 2.12 and figure 2.8), we know that all points C on L can be expressed as a weighted average of the coordinates of A and B, with the sum of weights being 1. Thus, C ≡(훼1x1 + 훼2x2, 훼1y1 + 훼2y2), where 훼1 + 훼2 = 1. Compare 3.7 Convex sets and functions 111 C with its corresponding point D on the curve, which has the same X coordinate: D ≡(훼1x1 + 훼2x2, g (훼1x1 + 훼2x2)). If and only if g (x) is a convex function, C will always be above D, or 훼1y1 + 훼2y2 = 훼1g (x1) + 훼2g (x2) ≥g (훼1x1 + 훼2x2) Viewed another way, if we drop a perpendicular to the X-axis from any point on the secant line joining a pair of points on the curve, that perpendicular will cut the curve at a lower point (that is, smaller in its Y-coordinate). This is illustrated on the left-hand side of figure 3.16 with the function g (x) = x2 (known to be convex) and A ≡(−3, 9) and B ≡(5, 25), 훼1 = 0.3, 훼2 = 0.7. It can be seen that the weighted average point C on the line lies above the corresponding point on the curve D. The right-hand side illustrates the nonconvex function g (x) = x3, with A ≡ (−8, −512) and B ≡(5, 125), 훼1 = 0.3, 훼2 = 0.7. The figure shows one weighted average point (C) on the line joining points A and B on the curve: C lies below point D on the curve, which has the same X-coordinate. Point C = 0.3A + 0.7B = (2.6, 20.2) lies above the point on the curve with the same X coordinate, D = (2.6, 2.6˄2) = (2.6, 6.76) Point C = 0.3A + 0.7B = (1.1, -66.1) lies above the point on the curve with the same X coordinate, D = (1.1, 1.1˄3) = (1.1, 1.331) -15 -10 -5 0 5 10 15 -15 -10 -5 0 5 10 15 100 80 60 40 20 0 1000 500 0 -500 -1000 A(-3, 9) C(2.6, 20.2) B(5, 25) A(-8, -512) D(1.1, 1.331) B(5, 125) C(1.1, -66.1) D(2.6, 6.76) Figure 3.16 Convex and nonconvex curves. A and B are a pair of points on the curve. C = 0.3A + 0.7B is a weighted average of the coordinates of A and B, with weights summing to 1. C lies on the line joining A and B. The left-hand curve is convex: C lies above the corresponding curve point D. The right-hand curve is nonconvex: C lies below the corresponding curve point D. We need not restrict ourselves to two points. We can take the weighted average of an arbitrary number of points on the curve, with the weights summing to one. The point corresponding to the weighted average will lie above the curve (that is, above the point on the curve with the same X-coordinate). The idea also extends to higher dimensions, as discussed next. DEFINITION 1 In general, a multidimensional function g  ®x is convex if and only if 112 CHAPTER 3 Classifiers and vector calculus Given an arbitrary set of points on the function surface (curve, if the function is 1D),  ®x1, g   ®x1 ,  ®x2, g   ®x2 , · · · ,  ®xn, g   ®xn , And given an arbitrary set of n weights 훼1, 훼2, · · · , 훼n that sum to 1 (that is, Ín i=1 훼i = 1), The weighted sum of the function outputs exceeds or equals the function output on the weighted sums: n Õ i=1 훼i g  ®xi  ≥g n Õ i=1 훼i ®xi ! (3.11) A little thought will reveal that definition 1 implies that convex curves always curl upward and/or rightward everywhere. This leads to another equivalent definition of convexity. DEFINITION 2 In general, a multidimensional function g  ®x is convex if and only if A 1D function g (x) is convex if and only if its curvature is positive everywhere: d2g dx2 ≥0 ∀x (3.12) A multidimensional function g  ®x is convex if and only if its Hessian matrix (see sec- tion 3.4.2, equation 3.9) is positive semi-definite (that is, all the eigenvalues of the Hessian matrix are greater than or equal to zero). This is just the multidimensional analog of equation 3.12. One subtle point to note is that if the second derivative is negative everywhere or the Hessian is negative semi-definite, the curve or surface is said to be concave. This is different from nonconvex curves, where the second derivative is positive in some places and negative in others. The negative of a concave function is a convex function. But the negative of a nonconvex function is again nonconvex. A function that curves upward everywhere always lies above its tangent. This leads to another equivalent definition of a convex function. DEFINITION 3 In general, a multidimensional function g  ®x is convex if and only if A function g (x) is convex if and only if all the points on the curve S ≡(x, g (x)) lie above the tangent line T at any point A on S, with S touching T only at A. A function g  ®x is convex if and only if all the points on the surface S ≡ ®x, g  ®x lie above the tangent plane T at any point A on S, with S touching T only at A. This is illustrated in figure 3.17. 3.7.3 Convexity and the Taylor series In section 3.4.1, equation 3.7, we saw the one-dimensional Taylor expansion for a function in the neighborhood of a point x. If we retain the terms in the Taylor expansion only up to the first derivative and ignore all subsequent terms, that is equivalent to approximating the function at x with its tangent at x (see figure 3.17). This is the linear Summary 113 Convex function: y = x2 Non-Convex function: y = x3 1000 750 500 250 0 -250 -500 -750 -1000 100 75 50 25 0 -25 -50 -75 -100 -15 -10 -5 0 5 10 15 -15 -10 -5 0 5 10 15 A A Figure 3.17 The left-hand curve is convex. If we draw a tangent line at any point A on the curve, the entire curve is above the tangent line, touching it only at A. The right-hand cuve is nonconvex: part of the curve lies above the tangent and part of it below. approximation to the curve. If we retain one more term (that is, up to the second derivative), we get the quadratic approximation to the curve. If the second derivative of the function is always positive (as in convex functions), the quadratic approximation to the function will always be greater than or equal to the linear approximation. In other words, locally, the curve will curve so that it lies above the tangent. This connects the second derivative definition (definition 2) with the tangent definition (definition 3) of convexity. 3.7.4 Examples of convex functions The function g (x) = x2 is convex. The easiest way to verify this is to compute d2g dx2 = d(2x) dx = 2, which is always positive. In fact, any even power of x, g (x) = x2n for an integer n, such as x4 or x6, is convex. g (x) = ex is also convex. This can be easily verified by taking its second derivative. g (x) = logx is concave. Hence, g (x) = −logx is convex. Multiplication by a positive scalar preserves convexity. The sum of convex functions is also a convex function. Summary We would like to leave you with the following mental pictures from this chapter: Inputs for a machine learning problem can be viewed as vectors or, equivalently, points in a high-dimensional feature space. Classification is nothing but separating clusters of points belonging to individual classes in this space. A classifier is can be viewed geometrically as the hypersurface (aka decision bound- ary) in the high-dimensional feature space, separating the point clusters corre- sponding to individual classes. During training, we collect sample inputs with known classes and identify the surface that best separates the corresponding points. During inferencing, given an unknown input, we determine which side of the decision boundary this point lies in—this tells us the class. 114 CHAPTER 3 Classifiers and vector calculus For two-class classifiers (aka binary classifiers), if we plug in the point in the function for the classifier hypersurface, the sign of the corresponding output yields the class. To compute the hypersurface decision boundary that best separates the training data, we first choose a parametric function family to model this surface (for exam- ple, a hyperplane for simple problems). Then we estimate the optimal parameter values that best separate the training data, usually in an iterative fashion. To estimate the parameter values that optimally separates the training data, we define a loss function that measures the difference between the model output and the known desired output over the entire training dataset. Then, starting from random initial values, we iteratively adjust the parameter values so that the loss value decreases progressively. At every iteration, the adjustment to the parameter values that optimally reduces the loss is estimated by computing the gradient of the loss function. The gradient of a multidimensional function identifies the direction in the pa- rameter space corresponding to the maximum change in the function. Thus, the gradient of the loss function identifies the direction in which we can adjust the parameters to maximally decrease the loss. The gradient is zero at the maximum or minimum point of a function, which is always a point of inflection. This can be used to recognize when we have reached the minimum. However, in practice, in machine learning we often do an early stop: terminate training iterations when the loss is sufficiently low. A multidimensional Taylor series can be used to create local approximations to a smooth function in the neighborhood of a point. The function is expressed in terms of the displacement from the point, the first-order derivatives (gradient), second- order derivatives (Hessian matrix), and so on. This can be used to make higher- accuracy approximations to the change in loss value resulting from a displacement in the parameter space. Loss functions can beconvex or nonconvex. In a convex function, there is no local minimum, only a single global minimum. Hence, gradient descent is guaranteed to converge to the global minimum. A nonconvex function can have both a local and a global minimum. So, gradient-based descent may get stuck in a local minimum. 4 Linear algebraic tools in machine learning This chapter covers Quadratic forms Applying principal component analysis (PCA) in data science Retrieving documents with a machine learning application Finding patterns in large volumes of high-dimensional data is the name of the game in machine learning and data science. Data often appears in the form of large matrices (a toy example of this is shown in section 2.3 and also in equation 2.1). The rows of the data matrix represent feature vectors for individual input instances. Hence, the number of rows matches the count of observed input instances, and the number of columns matches the size of the feature vector—that is, the number of dimensions in the feature space. Geometrically speaking, each feature vector (that is, row of the data matrix) represents a point in feature space. These points are not distributed uniformly over the space. Rather, the set of points belonging to a specific class occupies a small subregion of that space. This leads to certain structures in the data matrices. Linear algebra provides us the tools needed to study matrix structures. 115 116 CHAPTER 4 Linear algebraic tools in machine learning In this chapter, we study linear algebraic tools to analyze matrix structures. The chapter presents some intricate mathematics, and we encourage you to persevere through it, including the theorem proofs. An intuitive understanding the proofs will give you significantly better insights into the rest of the chapter. NOTE The complete PyTorch code for this chapter is available at http://mng.bz/aoYz in the form of fully functional and executable Jupyter notebooks. 4.1 Distribution of feature data points and true dimensionality For instance, consider the problem of determining the similarity between documents. This is an important problem for document search companies like Google. Given a query document, the system needs to retrieve from an archive—in ranked order of similarity— documents that match the query document. To do this, we typically create a vector representation of each document. Then the dot product of the vectors representing a pair of documents can be used as a quantitative estimate of the similarity between the documents. Thus, each document is represented by a document descriptor vector in which every word in the vocabulary is associated with a fixed index in the vector. The value stored in that index position is the frequency (number of occurrences) of that word in the document. NOTE Prepositions and conjunctions are typically excluded and singular; plural and other variants of words originating from the same stem are usually collapsed into one word. Every word in the vocabulary gets its own dimension in the document space. If a word does not occur in a document, we put a zero at that word’s index location in the descriptor vector for that document. We store one descriptor vector for every document in the archive. In theory, the document descriptor is an extremely long vector: its length matches the size of the vocabulary of the documents in the system. But this vector only exists notionally. In practice, we do not explicitly store descriptor vectors in their entirety. We store a <word, frequency> pair for every unique word that occurs in a document—but we do not explicitly store words that do not occur. This is a sparse representation of a document vector. The corresponding full representation can be constructed from the sparse one whenever necessary. In documents, certain words often occur together (for example, Michael and Jackson, or gun and violence). For example, in a given set of documents, the number of occurrences of gun will more or less match the number of occurrences of violence: if one appears, the other also appears most of the time. For a descriptor vector or, equivalently, a point in a feature space representing a document, the value at the index position corresponding to the word gun will be more or less equal to that for the word violence. If we project those points on the hyperplane formed by the axes for these correlated words, all the points fall around a 45-degree straight line (whose equation is x = y), as shown in figure 4.1. In figure 4.1, all the points representing documents are concentrated near the 45-degree line, and the rest of the plane is unpopulated. Can we collapse the two axes defining that plane and replace them with the single line 4.1 Distribution of feature data points and true dimensionality 117 Roses Violence Gun America . . . . . . . . . . . . . 1 2 3 4 5 Topic vector “gun-violence”: a principal component of the document matrix Figure 4.1 Document descriptor space. Each word in the vocabulary corresponds to a separate dimension. Dots show projections of document feature vectors on the plane formed by the axes corresponding to the terms gun and violence. around which most data is concentrated? It turns out that yes, we can do this. Doing so reduces the number of dimensions in the data representation—we are replacing a pair of correlated dimensions with a single one—thereby simplifying the representation. This leads to lower storage costs and, more importantly, provides additional insights. We have effectively discovered a new topic called gun-violence from the documents. As another example, consider a set of points in 3D, represented by coordinates X,Y , Z. If the Z coordinate is near zero for all the points, the data is concentrated around the X,Y plane. We can (and should) represent these points in two dimensions by projecting them onto the Z = 0 plane. Doing so approximates the positions of the points only slightly (they are projected onto a plane that they were close to in the first place). In a more realistic example, the data points may be clustered around an arbitrary plane in the 3D space (as opposed to the Z = 0 plane). We can still reduce the dimensionality of these data points to 2D by projecting on the plane they are close to. In general, if a set of data points is distributed in a space so that the points are clustered around a lower-dimensional subspace within that space (such as a plane or line), we can project the points onto the subspace and perform a dimensionality reduction on the data. We effectively approximate the distances from the subspace with 118 CHAPTER 4 Linear algebraic tools in machine learning zero: since these distances are small by definition, the approximation is not too bad. Viewed another way, we eliminate smaller from-subspace variations and retain the larger in-subspace variations. The resulting representation is more compressed and also lends itself more easily to better analysis and insights as we have eliminated unimportant perturbations and are focusing on the main pattern. These ideas form the basis of the technique called principal component analysis (PCA). It is one of the most important tools in the repertoire of a data scientist and machine learning practitioner. These ideas also underlie the latent semantic analysis (LSA) tech- nique for document retrieval—a fundamental approach for solving natural language processing (NLP) problems in machine learning. This chapter is dedicated to studying a set of methods leading to PCA and LSA. We examine a basic document retrieval system along with Python code. 4.2 Quadratic forms and their minimization Given a square symmetric matrix A, the scalar quantity Q = ®xT A®x is called a quadratic form. These are seen in various situations in machine learning. For instance, recall the equation for a circle that we learned in high school (x0 −훼0)2 + (x1 −훼1)2 = r2 where the center of the circle is (훼0, 훼1) and the radius is r. This equation can be rewritten as h (x0 −훼0) (x1 −훼1) i  1 0 0 1   (x0 −훼0) (x1 −훼1)  = r2 If we denote the position vector  x0 x1  as ®x and the center of the circle as  훼0 훼1  as ®훼, the previous equation can be written compactly as  ®x −®훼T I  ®x −®훼 = r2 Note that left hand side of this equation is a quadratic form. The original x0, x1-based equation only works for two dimensions. The matrix based equation is dimension agnostic: it represents a hypersphere in an arbitrary-dimensional space. For a two- dimensional space, the two equations become identical. Now, consider the equation for an ellipse: (x0 −훼0)2 훽2 0 + (x1 −훼1)2 훽2 1 = 1 You can verify that this can be written compactly in matrix form as h (x0 −훼0) (x1 −훼1) i  1 훽2 0 0 0 1 훽2 1   (x0 −훼0) (x1 −훼1)  = 1 4.2 Quadratic forms and their minimization 119 or, equivalently,  ®x −®훼T A  ®x −®훼 = 1 (4.1) where A =  1 훽2 0 0 0 1 훽2 1  . Once again, the matrix representation is dimension independent. In other words, equation 4.1 represents a hyperellipsoid. Note that if the ellipse axes are aligned with the coordinate axes, matrix A is diagonal. If we rotate the coordinate system, each position vector is rotated by an orthogonal matrix R. Equation 4.1 is transformed as follows (we have used the rules for transposing the products of matrices from equation 2.10):  R  ®x −®훼T A  R  ®x −®훼 = 1  ®x −®훼T  RT AR   ®x −®훼 = 1 Replacing RT AR with A, we get the same equation as equation 4.1, but A is no longer a diagonal matrix. For a generic ellipsoid with arbitrary axes, A has nonzero off-diagonal terms but is still symmetric. Thus, the multidimensional hyperellipsoid is represented by a quadratic form. The hypersphere is a special case of this. Quadratic forms are also found in the second term of the multidimensional Taylor expansion shown in equation 3.8: 1 2!  ®훿x T H  ®x  ®훿x  is a quadratic form in the Hessian matrix. Another huge application of quadratic forms is PCA, which is so important that we devote a whole section to it (section 4.4). 4.2.1 Minimizing quadratic forms An important question is, what choice of ®x maximizes or minimizes the quadratic form? For instance, because the quadratic form is part of the multidimensional Taylor series, we need to minimize quadratic forms when we want to determine the best direction to move in to minimize the loss L  ®x. Later, we will see that this question also lies at the heart of PCA computation. If ®x is a vector with arbitrary length, we can make Q arbitrarily big or small by simply changing the length of ®x. Consequently, optimizing Q with arbitrary length ®x is not a very interesting problem: rather, we want to know which direction of ®x optimizes Q. For the rest of this section, we discuss quadratic forms with unit vectors Q = ˆxT Aˆx (recall that ˆx denotes a unit-length vector satisfying ˆxT ˆx = ∥ˆx∥2 = 1). Equivalently, we could use a different flavor, Q = ®xT A®x ®xT ®x , but we will use the former expression here. We are essentially searching over all possible directions ˆx, examining which direction minimizes Q = ˆxT Aˆx. Using matrix diagonalization (section 2.15), Q = ˆxT Aˆx = ˆxTSΛST ˆx where S = h ®e1 ®e2 · · · ®en i is the matrix with eigenvectors of A as its columns and Λ is a diagonal matrix with the eigenvalues of A on the diagonal and 0 everywhere else. 120 CHAPTER 4 Linear algebraic tools in machine learning Substituting ˆy = ST ˆx we get Q = ˆxT Aˆx = ˆxTSΛST ˆx = ˆyTΛˆy (4.2) Note that since A is symmetric, its eigenvectors are orthogonal. This implies that S is an orthogonal matrix: that is, STS = SST = I. Recall from section 2.14.2 that for an orthogonal matrix S, the transformation ST ˆx is length preserving. Consequently, ˆy = ST ˆx is a unit-length vector. To be precise, ∥ˆy∥2 = ∥ST ˆx∥2 =  ST ˆx T  ST ˆx  = ˆxTSST ˆx = ˆxT ˆx = 1 since SST = I So, expanding the right-hand side of equation 4.2, we get Q = h y1 y2 · · · yn i  휆1 0 · · · 0 0 휆2 · · · 0 ... ... ... ... 0 0 · · · 휆n   y1 y2 ... yn  = n Õ i=1 휆iy2 i (4.3) We can assume that the eigenvalues are sorted in decreasing order of magnitude (if not, we can always renumber them). Consider this lemma (small proof): The quantity Ín i=1 휆iy2 i , where Ín i=1 y2 i = 1 and 휆1 ≥휆2 ≥· · · 휆n, attains its maximum value when y1 = 1, y2 = · · · yn = 0. An intuitive proof follows. If possible, let that the maximum value occur at some other value of ˆy. We are constrained by the fact that ˆy is an unit vector, so we must maintain Ín i=1 y2 i = 1. In particular, none of the elements of ˆy can exceed 1. If we reduce the first term from 1 to a smaller value, say √ 1 −휖, some other element(s) must go up by an equivalent amount to compensate (i.e., maintain the unit length property). Accordingly, suppose the hypothesized ˆy maximizing Q is given by ˆy =  √ 1 −휖 ... √ 훿 ...  where 훿> 0. 4.2 Quadratic forms and their minimization 121 What happens if we transfer the entire mass from the later term to the first term so that ˆy =  √ 1 −휖+ 훿 ... 0 ...  Doing this does not alter the length of ˆy as the sum of the squares of the first and the other term remains 1 −휖+ 훿. But the value of Q = Ín i=1 휆iy2 i is higher in the second case (where y1 has been beefed up at the expense of another term), since 휆1 (1 −휖+ 훿) > 휆1 (1 −휖) + 휆j훿for any j > 1 (since, 휆1 > 휆2 · · · by assumption). Thus, whenever we have less than 1 in the first term and greater than zero in some other term, we can increase Q without losing the unit length property of ˆy by transferring the entire mass to the first term. This means to maximize the right hand side of equation 4.3, we must have 1 as the first element (corresponding to the largest eigenvalue) of the unit vector ˆy and zeros everywhere else. Anything else violates the condition that the corresponding quadratic form Q = Ín i=1 휆iy2 i is a maximum. Thus we have established that the maximum ofQ occurs at ˆy =  1 0 ... 0  . The corresponding ˆx = Sˆy = ®e1 - the eigenvector corresponding to the largest eigenvalue of A. Thus, the quadratic form Q = ˆxT Aˆx attains its maximum when ˆx is along the eigen- vector corresponding to the largest eigenvalue of A. The corresponding maximum Q is equal to the largest eigenvalue of A. Similarly, the minimum of the quadratic form occurs when ˆx is along the eigenvector corresponding to the smallest eigenvalue. As stated above, many machine learning problems boil down to minimizing a quadratic form. We will study a few of them in later sections. 4.2.2 Symmetric positive (semi)definite matrices A square symmetric n × n matrix A is positive semidefinite if and only if ®xT A®x ≥0 ∀®x ∈ℝn In other words, a positive semidefinite matrix yields a non-negative quadratic form with all n × 1 vectors ®x. If we disallow the equality, we get symmetric positive definite matrices. Thus a square symmetric n × n matrix A is positive definite if and only if ®xT A®x > 0 ∀®x ∈ℝn 122 CHAPTER 4 Linear algebraic tools in machine learning From equations 4.2 and 4.3, Q is positive or zero if all 휆is are positive or zero (since the y2 i s are non-negative). Hence, symmetric positive (semi)definiteness is equivalent to the condition that all eigenvalues of the matrix are greater than (or equal to) zero. 4.3 Spectral and Frobenius norms of a matrix A vector is an entity with a magnitude and direction. The norm ∥®x∥of a vector ®x represents its magnitude. Is there an equivalent notion for matrices? The answer is yes, and we will study two such ideas. 4.3.1 Spectral norms In section 2.5.4, we saw that the length (aka magnitude) of a vector ®x is ∥®x∥= ®xT ®x. Is there an equivalent notion of magnitude for a matrix A? Well, a matrix can be viewed as an amplifier of a vector. The matrix A amplifies the vector ®x to ®b = A®x. So we can take the maximum possible value of ∥A®x∥over all possible ®x; that is a measure for the magnitude of A. Of course, if we consider arbitrary- length vectors, we can make ®b arbitrarily large by simply scaling ®x for any A. That is uninteresting. Rather, we want to examine which direction of ®x is amplified most and by how much. We examine this question with unit vectors ˆx: what is the maximum (or minimum) value of ∥Aˆx∥, and what direction ˆx materializes it? The quantity ∥A∥2 = max ˆx ∥Aˆx∥2 is known as the spectral norm of the matrix A. Note that A®x is a vector and ∥A®x∥2 is its length. (We will sometimes drop the subscript 2 and denote the spectral norm as ∥A∥.) Now consider the vector Aˆx. Its magnitude is ∥Aˆx∥= (Aˆx)T (Aˆx) = ˆxT AT Aˆx This is a quadratic form. From section 4.2, we know it will be maximized (minimized) when ˆx s aligned with the largest (smallest) eigenvalue of AT A. Thus the spectral norm is given by the largest eigenvalue of AT A ∥A∥2 = max ˆx ∥Aˆx∥= 휎1 (4.4) where 휎1 is the largest eigenvalue of AT A. It is also (the square of) the largest sin- gular value of A. We will see 휎1 again in section 4.5, when we study singular value decomposition (SVD). 4.3.2 Frobenius norms An alternative measure for the magnitude of a matrix is the Frobenius norm, defined as ∥A∥F = v u t m Õ i=1 n Õ j=1 ∥ai j∥2 (4.5) In other words, it is the root mean square of all the matrix elements. 4.4 Principal component analysis 123 It can be proved that the Frobenius norm is equal to the root mean square of the sum of all the singular values (eigenvalues of AT A) of the matrix ∥A∥F = v u tmin(m,n) Õ i=1 휎2 i (4.6) 4.4 Principal component analysis Suppose we have a set of numbers, X =  x(0), x(1), · · · , x(n) . We want to get a sense of how tightly packed these points are. In other words, we want to measure the spread of these numbers. Figure 4.2 shows such a distribution. Figure 4.2 A 1D distribution of points. The distance between extreme points is not a fair representation of the spread of points: the distribution is not uniform, and the extreme points are far from the others. Most points are within a more tightly packed region. Note that the points need not be uniformly distributed. In particular, the extreme points (xmax, xmin) may be far from most other points (as in figure 4.2). Thus, xmax−xmin n+1 is not a fair representation of the average spread of points here. Most points are within a more tightly packed region. The statistically sensible way to obtain the spread is to first obtain the mean: 휇= 1 n n Õ i=0 x(i) Then obtain the average distance of the numbers from the mean: 휎2 = 1 n n Õ i=0  x(i) −휇 2 (If we want to, we can take the square root and use 휎, but it is often not necessary to incur that extra computational burden). This scalar quantity, 휎, is a good measure of the mean packing density or spread of the points in 1D. You may recognize that the 124 CHAPTER 4 Linear algebraic tools in machine learning previous equation is nothing but the famous variance formula from statistics. Can we extend the notion to higher-dimensional data? Let’s first examine the idea in two dimensions. As usual, we name our coordinate axes X0, X1, and so on, instead of X,Y, to facilitate the extension to multiple dimensions. An individual 2D data point is denoted ®x(i) =  x(i) 0 x(i) 1  . The dataset is  ®x(0), ®x(1), · · · , ®x(n) . The mean is straightforward. Instead of one means, we have two: 휇0 = 1 n n Õ i=0 x(i) 0 휇1 = 1 n n Õ i=0 x(i) 1 Thus we now have a mean vector: ®휇=  휇0 휇1  = 1 n n Õ i=0 ®x(i) Now let’s do the variance. The immediate problem we face is that there are infinite possible directions in the 2D plane. We can measure variance along any of them, and it will be different for each choice. We can, of course, find the variance along the X0 and X1 axes: 휎2 00 = 1 n n Õ i=0  x(i) 0 −휇0 2 휎2 11 = 1 n n Õ i=0  x(i) 1 −휇1 2 휎00 and 휎11 tells us the variance along only one of the axes X0 and X1, respectively. But in general, there will be joint variation along both axes. To deal with joint variation, let’s introduce a cross term: 휎2 01 = 휎2 10 = 1 n n Õ i=0  x(i) 0 −휇0   x(i) 1 −휇1  These equations can be written compactly in matrix vector notation: ®휇= 1 n n Õ i=0 ®x(i) C =  휎00 휎01 휎10 휎11  = 1 n n Õ i=0  ®x(i) −®휇   ®x(i) −®휇 T NOTE In the expression for C, we are not taking the dot product of the vectors  ®x(i) −®휇  and  ®x(i) −®휇  . The dot product would be  ®x(i) −®휇 T  ®x(i) −®휇  . Here, 4.4 Principal component analysis 125 the second element of the product is transposed, not the first. Consequently, the result is a matrix. The dot product would yield a scalar.) The previous equations are general, meaning they can be extended to any dimension. To be precise, given a set of n multidimensional data points X =  ®x(0), ®x(1), · · · , ®x(n) , we can define ®휇= 1 n n Õ i=0 ®x(i) (4.7) C = 1 n n Õ i=0  ®x(i) −®휇   ®x(i) −®휇 T (4.8) Note how the mean has become a vector (it was a scalar for 1D data) and the scalar variance of 1D, 휎, has become a matrix C. This matrix is called the covariance matrix. The (n + 1)-dimensional mean and covariance matrix can also be defined as ®휇=  휇0 휇1 · · · 휇n  C =  휎00 휎01 · · · 휎0n 휎10 휎11 · · · 휎1n ... ... ... ... 휎n0 휎n1 · · · 휎nn  (4.9) where 휎i j = n Õ k=0  x(k) i −휇i   x(k) j −휇j  (4.10) For i = j, 휎ii is essentially the variance of the data along the ith dimension. Thus the diagonal elements of matrix C contain the variance along the coordinate axes. Off- diagonal elements correspond to cross-covariances. NOTE Equations 4.8 and 4.9 are equivalent. 4.4.1 Direction of maximum spread What is the direction of maximum spread/variance? Let’s first consider an arbitrary direction specified by the unit vector ˆl. Recalling that the component of any vector along a direction is yielded by the dot product of the vector with the unit direction vector, the components of the data points along ˆl are given by X ′ = n ˆl T ®x(0), ˆl T ®x(1), · · · , ˆl T ®x(n)o 126 CHAPTER 4 Linear algebraic tools in machine learning NOTE Remember figure 2.8b, which showed that the component of one vector along another is given by the dot product between them? ˆl T ®x(i) are dot products and hence scalar values. The spread along direction ˆl is given by the variance of the scalar values in X ′. The mean of the values in X ′ is given by 휇 ′ = 1 n n Õ i=0 ˆl T ®x(i) =ˆl T 1 n n Õ i=0 ®x(i) ! =ˆl T ®휇 and the variance is C ′ = 1 n n Õ i=0  ˆl T ®x(i) −ˆl T ®휇   ˆl T ®x(i) −ˆl T ®휇 T = 1 n n Õ i=0 ˆl T  ®x(i) −®휇   ˆl T  ®x(i) −®휇 T = 1 n n Õ i=0 ˆl T  ®x(i) −®휇   ®x(i) −®휇 T ˆl =ˆl T 1 n n Õ i=0  ®x(i) −®휇   ®x(i) −®휇 T ! ˆl =ˆl TCˆl Note that C ′ =ˆl TCˆl is the variance of the data components along the direction ˆl. As such, it represents the spread of the data along that direction. What is the direction ˆl along which this spread ˆl TCˆl is maximal? It is the direction ˆl that maximizes C ′ =ˆl TCˆl. This maximizing direction can be identified using the quadratic form optimization technique we discussed in 4.2. Applying that, we have the following results: Variance is maximal when ˆl is along the eigenvector corresponding to the largest eigenvalue of the covariance matrix C. This direction is called the first principal axis of the multidimensional data. The components of the data vectors along the principal axis are known as first principal components. The value of the variance along the first principal axis, given by the corresponding eigenvalue of the covariance matrix, is called the first principal value. The second principal axis is the eigenvector of the covariance matrix correspond- ing to the second largest eigenvalue of the covariance matrix. Second principal components and values are defined likewise. The principal axes are orthogonal to each other because they are eigenvectors of the symmetric covariance matrix. 4.4 Principal component analysis 127 What is the practical significance of PCA? Why would we like to know the direction along which the spread is maximum for a point distribution? Sections 4.4.2 through 4.4.5 are devoted to answering this question. 4.4.2 PCA and dimensionality reduction In section 4.1, we saw that when data points are clustered around a lower-dimensional subspace, it is beneficial to project them onto the subspace and reduce the dimension- ality of the data representation. The dimensionally reduced data is more compactly representable and more amenable to deriving insights and analysis. In the particular case where the data points are clustered around a straight line or hyperplane, PCA can be used to generate a lower-dimensional data representation by getting rid of the principal components corresponding to relatively small principal values. The technique is agnostic to the dimensionality of the data. The line or hyperplane can be anywhere in the space, with arbitrary orientation. For instance, consider the 2D distribution shown in figure 4.3a. Here, the data is 2D and plotted on a plane, but the main spread of the data is along a 1D line (shown by the thick two-arrowed line in the figure). There is very little spread in the direction orthogonal to that line (indicated by the little perpendiculars from the data points to the line in the figure). PCA reveals this internal structure. There are two principal values (because the data is 2D), but one of them is much smaller than the other: this reveals that dimensionality reduction is possible. The principal axis corresponding to the larger principal value is along the line of maximum spread. The small perturbations along the other principal axis can be eliminated with little loss of information. Replacing each data point with its projection on the first principal axis converts the 2D dataset into a 1D dataset, brings out the true underlying pattern in the data (straight line), eliminates noise (little perpendiculars), and reduces storage costs. First principal axis (reflects the underlying structure of the data) Original data point Projection of data point on principal axis (only a few shown for clarity) Legend Second principal axis (variation along this direction is caused by measurement errors) (a) Dimensionality reduction from 2D to 1D (b) Dimensionality reduction from 3D to 2D Figure 4.3 Dimensionality reduction via PCA. Original data points are shown with filled little circles, and hollow circles represent lower-dimensional representations. 128 CHAPTER 4 Linear algebraic tools in machine learning In figure 4.3b, the data is 3D, but the data points are clustered around a plane in 3D space (shown as the rectangle in the figure). The main spread of the data is along the plane, while the spread in the direction normal to that plane (shown with little perpendiculars from data points to the plane) is small. PCA reveals this: there are three principal values (because the data is 3D), but one of them is much smaller than the other two, revealing that dimensionality reduction is possible. The principal axis corresponding to the small principal value is normal to the plane. We can ignore these perturbations (perpendiculars in figure 4.3b) with little loss of information. This is equivalent to projecting the data onto the plane formed by the first two principal axes. Doing so brings out the underlying data pattern (plane), eliminates noise (little perpendiculars), and reduces storage costs. 4.4.3 PyTorch code: PCA and dimensionality reduction In this section, we provide a PyTorch code sample for PCA computation in listing 4.1. Then we provide PyTorch code for applying PCA on a correlated dataset and an uncorrelated dataset in listings 4.2 and 4.3, respectively. The results are plotted in figure 4.4. (a) PCA on correlated data (b) PCA on uncorrelated data Figure 4.4 PCA results. In (a), the data points are around the straight line y = 2x. Consequently, one principal value is much larger than the other, indicating that dimensionality reduction will work. In (b), both principal values are large. Dimensionality reduction will not work. NOTE The complete PyTorch code for this section is available at http://mng.bz/aoYz in the form of fully functional and executable Jupyter notebooks. Listing 4.1 PCA computation def pca(X): Returns principal values and vectors covariance_matrix = torch.cov(X.T) l, e = torch.linalg.eig(covariance_matrix) return l, e 4.4 Principal component analysis 129 NOTE Fully functional code for the PCA computation in listing 4.1 is available at http://mng.bz/DRYR. Listing 4.2 PCA on synthetic correlated data x_0 = torch.normal(0, 100, (N,)) Random feature vector x_1 = 2 * x_0 + torch.normal(0, 20, (N,)) Correlated feature vector + minor noise X = torch.column_stack((x_0, x_1)) Data matrix spread mostly along y = 2x principal_values, principal_vectors = pca(X) One large principal value and one small First principal vector along y = 2x X_proj = torch.matmul(X, first_princpal_vec) Dimensionality reduction by projecting on the first principal vector The output is as follows: Principal values are: [62.6133, 48991.0469] First Principal Vector is: [-0.44, -0.89] NOTE Fully functional code for the PCA computation in listing 4.2 is available at http://mng.bz/gojl. Listing 4.3 PCA on synthetic uncorrelated data x_0 = torch.normal(0, 100, (N,)) x_1 = torch.normal(0, 100, (N,)) Random uncorrelated feature-vector pair X = torch.column_stack((x_0, x_1)) principal_values, principal_vectors = pca(X) Principal values close to each other. The spread of the data points is comparable in both directions. Here is the output: Principal values are [ 9736.4033, 7876.6592] NOTE Fully functional code for the PCA computation in listing 4.3 is available at http://mng.bz/e5Kz. 4.4.4 Limitations of PCA PCA assumes that the underlying pattern is linear in nature. Where this is not true, PCA will not capture the correct underlying pattern. This is illustrated schematically in figure 4.5a and via experimental results from listing 4.3. Figure 4.5b shows the results of running listing 4.4, where we synthetically generate non-linearly correlated data and perform PCA. The straight line at the base shows the first principal axis. Projecting data 130 CHAPTER 4 Linear algebraic tools in machine learning (a) Schematic 2D data distribution with a curved underlying pattern (b) PCA results on synthetic (computer generated) non-linearly correlated data. The line at the base shows the first principal axis. Figure 4.5 Non-linearly correlated data. The points are distributed around a curve as opposed to a straight line. It is impossible to find a straight line such that all points are near it. on this axis results in a large error in the data positions (loss of information). Linear PCA will not do well. Listing 4.4 PCA on synthetic nonlinearly correlated data x_0 = torch.normal(0, 100, (N,)) x_1 = 2 * (x_0 ** 2) + torch.normal(0, 5, (N,)) X = torch.column_stack((x_0, x_1)) principal_values, principal_vectors = pca(X) Principal vectors fail to capture the underlying distribution. The output is as follows: Principal values are [9.3440e+03, 5.3373e+08] Mean loss in information: 68.0108526887 - high 4.4.5 PCA and data compression If we want to represent a large multidimensional dataset within a fixed byte budget, what information can we can get rid of with the least loss of accuracy? Clearly, the answer is the principal components along the smaller principal values—getting rid of them actually helps, as described in section 4.4.2. To compress data, we often perform PCA and then replace the data points with their projections on first few principal axes; doing so reduces the number of data components to store. This is the underlying principle in JPEG 98 image compression techniques. 4.5 Singular value decomposition Singular value decomposition (SVD) may be the most important linear algebraic tool in machine learning. Among other things, PCA and LSA implementations are built based on SVD. We illustrate the basic idea in this section. 4.5 Singular value decomposition 131 NOTE There are several slightly different forms of SVD. We have chosen the one that seems intuitively simplest. The SVD theorem states that any matrix A, singular or nonsingular, rectangular or square, can be decomposed as the product of three matrices A =UΣV T (4.11) where (assuming that the matrix A is m × n) Σ is an m × n diagonal matrix. Its diagonal elements contain the square roots of the eigenvalues of AT A. These are also known as the singular values of A. The singular values appear in decreasing order in the diagonal of Σ. V is an n × n orthogonal matrix containing eigenvectors of AT A in its columns. U is an m × m orthogonal matrix containing eigenvectors of AAT in its columns. 4.5.1 Informal proof of the SVD theorem We will provide an informal proof of the SVD theorem through a series of lemmas. Going through these will provide additional insights. LEMMA 1 AT A is symmetric positive semidefinite. Its eigenvalues—aka singular values—are non- negative. Its eigenvectors—aka singular vectors—are orthogonal. PROOF OF LEMMA 1 Let’s say A has m rows and n columns. Then AT A is an n × n square matrix  AT A T = AT  AT T = AT A which proves that AT A is symmetric. Also, for any ®x, ®xT AT A®x =  A®xT  A®x = ∥A®x∥2 > 0 which, as per section 4.2.2, proves that the matrix AT A is symmetric and positive semi- definite. Hence, its eigenvalues are all positive or zero. We proved in section 2.13 that symmetric matrices have orthogonal eigenvectors. That proves that singular vectors are orthogonal. Let (휆i, ˆvi), for i ∈[1, n] be the set of eigenvalue, eigenvector pairs of AT A—aka the singular value, singular vector pair of A. Note that without loss of generality, we can assume 휆1 ≥휆2 ≥· · · 휆n (because if not, we can always renumber the eigenvalues and eigenvectors). Now, by definition, AT Aˆvi = 휆iˆvi ∀i ∈[1, n] From lemma 1, singular vectors are orthogonal, and hence ˆvT i ˆvj = ( 0 i ≠j 1 i = j (4.12) 132 CHAPTER 4 Linear algebraic tools in machine learning Note that ˆvis are unit vectors (that is why we are using the hat sign as opposed to the overhead arrow). As described in section 2.13, eigenvectors remain eigenvectors if we change their length. We are free to choose any length for eigenvectors as long as we choose it consistently. We are choosing unit-length eigenvectors here. LEMMA 2 AAT is symmetric positive semidefinite. Its eigenvalues are non-negative and eigen- vectors are orthogonal. PROOF OF LEMMA 2  AAT T =  AT T AT = AAT Also, ®xT AAT ®x =  AT ®x T  AT ®x  = ∥  AT ®x  ∥≥0 and so on. LEMMA 3 1 √휆i Aˆvi, ∀i ∈[1, n] is a set of orthogonal unit vectors. PROOF OF LEMMA 3 Let’s take the dot product of a pair of these vectors:  1 √휆i Aˆvi T 1 p휆j Aˆvj ! = 1 p휆i휆j ˆvT i AT Aˆvj = 1 p휆i휆j ˆvT i  AT Aˆvj  Since 휆j, ˆvj are eigenvalue, eigenvector pairs of AT A, the previous equation can be rewritten as 1 p휆i휆j ˆvT i 휆jˆvj which, using equation 4.12, can be rewritten as s 휆j 휆i ˆvT i ˆvj = ( 0 i ≠j 1 i = j LEMMA 4 If (휆i, ˆvi) is an eigenvalue, eigenvector pair of AT A, then  휆i, ˆui = 1 √휆i Aˆvi  is an eigen- value, eigenvector pair of AAT. PROOF OF LEMMA 4 Given AT Aˆvi = 휆iˆvi left-multiplying both sides of the equation by A, we get AAT Aˆvi = 휆iAˆvi AAT (Aˆvi) = 휆i (Aˆvi) 4.5 Singular value decomposition 133 Substituting ®fi = Aˆvi in the last equation, we get AAT ®fi = 휆i ®fi which proves that ®fi = Aˆvi is an eigenvector of AAT with 휆i as a corresponding eigenvalue. Multiplying by 1 √휆i converts it into a unit vector as per lemma 3. This completes the proof of the lemma. 4.5.2 Proof of the SVD theorem Now we are ready to examine the proof of the SVD theorem. CASE 1: MORE ROWS THAN COLUMNS IN A If m, the number of rows in A, is greater than or equal to n, the number of columns in A, we define U = h ˆu1 ˆu2 · · · ˆun ˆun+1 · · · ˆum i Σ =  √휆1 0 · · · 0 0 √휆2 · · · 0 ... 0 0 · · · √휆n 0 0 · · · 0 ... 0 0 · · · 0  V = h ˆv1 ˆv2 · · · ˆvn i Note the following: From lemma 1, we know that the eigenvalues of AT A are positive. This makes the square roots, √휆is, real. U is an m × m orthogonal matrix whose columns are the eigenvectors of AAT. Since, AAT is m × m, it has m eigenvalues and eigenvectors. The first n of them are ˆu1 = 1 √휆1 Aˆv1, ˆu2 = 1 √휆2 Aˆv2, · · · , ˆun = 1 √휆i Aˆvn (from lemma 4, we know these are eigenvectors of AAT). In this case, by our initial assumption, n < m. Thus AAT has (m −n) more eigenvectors, ˆun+1, · · · ˆum. V is an n × n orthogonal matrix with the eigenvectors of AT A (that is, ˆv1, ˆv2, · · · , ˆvn) as its columns. Consider the matrix product UΣ. From basic matrix multiplication rules (section 2.5, we can see that 134 CHAPTER 4 Linear algebraic tools in machine learning UΣ = h ˆu1 ˆu2 · · · ˆun ˆun+1 · · · ˆum i  √휆1 0 · · · 0 0 √휆2 · · · 0 ... 0 0 · · · √휆n 0 0 · · · 0 ... 0 0 · · · 0  = h√휆1ˆu1 √휆2ˆu2 · · · √휆nˆun i Note that the last columns of U, ˆun+1, · · · , ˆum, are multiplied by all zeros in Σ and vanishing. Thus, UΣ = h√휆1ˆu1 √휆2ˆu2 · · · √휆nˆun i = h Aˆv1 Aˆv2 · · · Aˆvn i = A h ˆv1 ˆv2 · · · ˆvn i = AV The later columns ofU—those named with us—fail to survive because they are multiplied by the zeros at the bottom of Σ. Thus we have proved that AV =UΣ Then AVV T =UΣV T Since V is orthogonal, VV T = I. Hence A =UΣV T which completes the proof of the singular value theorem. CASE 2: FEWER ROWS THAN COLUMNS IN A If m, the number of rows in A, is less than or equal to n, the number of columns in A, we have U = h ˆu1 ˆu2 · · · · · · ˆum i Σ =  √휆1 0 · · · 0 · · · 0 0 √휆2 · · · 0 · · · 0 ... 0 0 · · · √휆n · · · 0  V = h ˆv1 ˆv2 · · · ˆvn i The proof follows along similar lines. 4.5 Singular value decomposition 135 4.5.3 Applying SVD: PCA computation We will illustrate the idea first with a toy dataset. Consider a 3D dataset with five points. We use a superscript to denote the index of the data instance and a subscript to denote the component. Thus the ith data instance vector is denoted as h x(i) 0 x(i) 1 x(i) 2 i . We denote the entire data set with a matrix in which each feature instance appears as a row vector. The data matrix is X =  x(0) 0 x(0) 1 x(0) 2 x(1) 0 x(1) 1 x(1) 2 x(2) 0 x(2) 1 x(2) 2 x(3) 0 x(3) 1 x(3) 2 x(4) 0 x(4) 1 x(4) 2  We will assume that the data is already mean-subtracted. Now examine the matrix product XT X, using ordinary rules of matrix multiplication: XT X =  Í4 i=0  x(i) 0 2 Í4 i=0 x(i) 0 x(i) 1 Í4 i=0 x(i) 0 x(i) 2 Í4 i=0 x(i) 1 x(i) 0 Í4 i=0  x(i) 1 2 Í4 i=0 x(i) 1 x(i) 2 Í4 i=0 x(i) 2 x(i) 0 Í4 i=0 x(i) 2 x(i) 1 Í4 i=0  x(i) 2 2  From equations 4.10 and 4.9, XT X =  휎00 휎01 휎02 휎10 휎11 휎12 휎20 휎21 휎22  = C Thus XT X is the covariance matrix of the dataset X. This holds for arbitrary dimensions and arbitrary feature instance counts. If we create a data matrix X with each data instance forming a row, XT X yields the covariance matrix of the dataset. The eigenvalues and eigenvectors of this matrix are the principal components. Hence, performing SVD on X yields PCA of the data (assuming prior mean subtraction). 4.5.4 Applying SVD: Solving arbitrary linear systems A linear system is a system of simultaneous linear equations A®x = ®b We first encountered a linear system in section 2.12. It is possible to use matrix inversion to solve such a system: ®x = A−1®b 136 CHAPTER 4 Linear algebraic tools in machine learning However, solving a linear system with matrix inversion is undesirable for many reasons. To begin with, it is numerically unstable. The matrix inverse contains the determi- nant of the matrix in its denominator. If the determinant is near zero, the inverse will contain very large numbers. Minor noise in ®b will be multiplied by these large numbers and cause large errors in the computed solution ®x. In this case, the inverse- based solution can be very inaccurate. Furthermore, the determinant can be zero: this can happen when one row of the matrix is a linear combination of others, in- dicating that we have fewer equations than we think. And what if the matrix is not square to begin with? This can happen when we have more equations than unknowns (overdetermined system) or fewer equations than unknowns (underdetermined sys- tem). In these cases, the inverse is not computable, and the system cannot be solved fully. Even in these cases, we would like to obtain a solution that is the best approximation in some sense; and in the case of a square matrix, we would like to get the exact solution. How do we do this? Answer: we use SVD. The steps are as follows: 1 A®x = ®b can be rewritten as U  ΣV T ®x = ®b. We then solve U®y1 = ®b. This can be easily done using orthogonality of U, as ®y1 =UT ®b. 2 Now we have Σ  V T ®x = ®y1 Solve Σ®y2 = ®y1. This can be easily done because for any dia- gonal matrix Σ =  d1 0 · · · 0 0 d2 · · · 0 ... ... ... ... 0 · · · · · · dn  we can easily compute Σ−1 =  1 d1 0 · · · 0 0 1 d2 · · · 0 ... ... ... ... 0 · · · · · · 1 dn  Hence, ®y2 = Σ−1®y1. 3 Now we have V T ®x = ®y2. This too can be solved easily using the orthogonality of V : ®x =V ®y2 Thus we have solved for ®x without inverting the matrix A: For invertible square matrices A, this method yields the same solution as the matrix-inverse-based method. For nonsquare matrices, this boils down to the Moore-Penrose inverse and yields the best-effort solution. 4.5.5 Rank of a matrix In section 2.12, we studied linear systems of equations. Such a system can be represented in matrix-vector form: A®x = ®b Each row of A and ®b contributes one equation. If we have as many independent equations as unknowns, the system is solvable. This is the simplest case; matrix A is square and invertible. det(A) is nonzero, and A−1 exists. 4.5 Singular value decomposition 137 Sometimes the situation is misleading. Consider the following system:  1 0 0 0 1 0 1 1 0   x0 x1 x2  =  5 7 12  Although there are three rows and apparently three equations, the equations are not independent. For instance, the third equation can be obtained by adding the first two. We really have only two equations, not three. We say this linear system is degenerate. All of the following statements are true for such a system A®x = ®b: The linear system is degenerate. det(A) = 0. A−1 cannot be computed, and A is not invertible. Rows of A are linearly dependent. There exists a linear combination of the rows that sum to zero. For example, in the previous example, ®r0 + ®r1 −®r2 = 0. At least one of the singular values of A (eigenvalues of AT A) is zero. The number of linearly independent rows is equal to the number of nonzero eigenvalues. The number of linearly independent rows in a matrix is called its rank. It can be proved that a matrix has as many nonzero singular values as its rank. It can also be proved that the number of linearly independent columns in a matrix matches the number of linearly independent rows. Hence, rank can also be defined as the number of linearly independent columns in a matrix. A nonsquare rectangular matrix with m rows and n columns has a rank r = min(m, n). Such matrices are never invertible. We usually resort to SVD to solve them. A square matrix with n rows and n columns is invertible (nonzero determinant) if and only if it has rank n. Such a matrix is said to have full rank. Full-rank matrices are invertible. They can be solved via matrix inverse computation, but inverse computation is not always numerically stable. SVD can be applied here as well, with good numerical properties. Non-full-rank matrices are degenerate. So, rank is a measure of the non-degeneracy of the matrix. 4.5.6 PyTorch code for solving linear systems with SVD The listings in this section show a PyTorch-based implementation of SVD and demon- strate an application that solves a linear system via SVD. Listing 4.5 Solving an invertible linear system with matrix inversion and SVD A = torch.tensor([[1, 2, 1], [2, 2, 3], [1, 3, 3]]).float() b = torch.tensor([8, 15, 16]).float() Simple test linear system of equations x_0 = torch.matmul(torch.linalg.inv(A), b) Matrix inversion is numerically unstable; SVD is better. 138 CHAPTER 4 Linear algebraic tools in machine learning U, S, V_t = torch.linalg.svd(A) A =U SV T =⇒A®x = ®b ≜U SV T ®x = ®b y1 = torch.matmul(U.T, b) Solves U ®y1 = ®b. Remember U −1 =UT as U is orthogonal. S_inv = torch.diag(1 / S) y2 = torch.matmul(S_inv, y1) Solves S®y2 = ®y1. Remember S−1 is easy as S is diagonal. x_1 = torch.matmul(V_t.T, y2) Solves V T ®x = ®y2. Remember V −T =V as V is orthogonal. assert torch.allclose(x_0, x_1) The two solutions are the same. Here is the output: Solution via inverse: [1.0, 2.0, 3.0] Solution via SVD: [1.0, 2.0, 3.0] Listing 4.6 Solving an overdetermined linear system by pseudo-inverse and SVD A = torch.tensor([[0.11, 0.09], [0.01, 0.02], [0.98, 0.91], [0.12, 0.21], [0.98, 0.99], [0.85, 0.87], [0.03, 0.14], [0.55, 0.45], [0.49, 0.51], [0.99, 0.01], [0.02, 0.89], [0.31, 0.47], [0.55, 0.29], [0.87, 0.76], [0.63, 0.24]]) Cat-brain dataset: nonsquare matrix A = torch.column_stack((A, torch.ones(15))) b = torch.tensor([-0.8, -0.97, 0.89, -0.67, 0.97, 0.72, -0.83, 0.00, 0.00, 0.00, -0.09, -0.22, -0.16, 0.63, 0.37]) x_0 = torch.matmul(torch.linalg.pinv(A), b) Solution via pseudo-inverse U, S, V_t = torch.linalg.svd(A, full_matrices=False) Solution via SVD y1 = torch.matmul(U.T, b) S_inv = torch.diag(1 / S) y2 = torch.matmul(S_inv, y1) x_1 = torch.matmul(V_t.T, y2) assert torch.allclose(x_0, x_1) The two solutions are the same. The output is as follows: Solution via pseudo-inverse: [ 1.0766, 0.8976, -0.9582] Solution via SVD: [ 1.0766, 0.8976, -0.9582] 4.5 Singular value decomposition 139 Fully functional code for solving the SVD-based linear system can be found at http://mng .bz/OERn. 4.5.7 PyTorch code for PCA computation via SVD The following listing demonstrates PCA computations using SVD. Listing 4.7 Computing PCA directly and using SVD principal_values, principal_vectors = pca(X) Eigenvalues of the covariance matrix yield principal values. Eigenvectors of the covariance matrix yield principal vectors. Direct PCA computation from a covariance matrix Data matrix X_mean = X - torch.mean(X, axis=0) U, S, V_t = torch.linalg.svd(X_mean) Diagonal elements of matrix S yield principal values. PCA from SVD V = V_t.T Columns of matrix V yield principal vectors. The output is as follows: Principal components obtained via PCA: [[-0.44588404 -0.89509073] [-0.89509073 0.44588404]] Principal components obtained via SVD: [[-0.44588404 0.89509073] [-0.89509073 -0.44588404]] 4.5.8 Applying SVD: Best low-rank approximation of a matrix Given a matrix A of some rank p, we sometimes want to approximate it with a ma- trix of lower rank r, where r < p. How do we obtain the best rank r approximation of A? MOTIVATION Why would we want to do this? Well, consider a data matrix X as shown in section 4.5.3. As explained in section 4.4.2, we often want to eliminate small variances in the data (likely due to noise) and get the pattern underlying large variations. Replacing the data matrix with a lower-rank matrix often achieves this. However, we must bear in mind that this does not work when the underlying pattern is nonlinear (such as in figure 4.5a). 140 CHAPTER 4 Linear algebraic tools in machine learning APPROXIMATION ERROR What do we mean by best approximation? The Frobenius norm can be taken as the magnitude of the matrix. Accordingly, given a matrix A and its rank r approximation Ar, the approximation error is e = ∥A −Ar ∥F. METHOD To solidify our ideas, let’s consider an m × n matrix A. From section 4.5, we know it will have min(m, n) singular values. Let its rank be p ≤min(m, n). We want to approximate this matrix with a rank r(< p) matrix. Let’s rewrite the SVD expression. We will assume m > n. Also, as usual, we have the singular values sorted in decreasing order: 휆1 ≥휆2 ≥휆n. We will partition U , Σ, V : A =UΣV T = h U1 U2 i  Σ1 0 0 Σ2   V T 1 V T 2  =U1Σ1V T 1 +U2Σ2V T 2 It can be proved that U1Σ1V T 1 is a rank r matrix. Furthermore, it is the best rank r approximation of A. 4.6 Machine learning application: Document retrieval We will now bring together several of the concepts we have discussed in this chapter with an illustrative toy example: the document retrieval problem we first encountered in section 2.1. Briefly recapping, we have a set of documents {d0, · · · , d6}. Given an incoming query phrase, we have to retrieve documents that match the query phrase. We will use the bag of words model: that is, our matching approach does not pay attention to where a word appears in a document; it simply pays attention to how many times the word appears in the document. Although this technique is not the most sophisticated, it is popular because of its conceptual simplicity. Our documents are as follows: d0: Roses are lovely. Nobody hates roses. d1: Gun violence has reached epidemic proportions in America. 4.6 Machine learning application: Document retrieval 141 d2: The issue of gun violence is really over-hyped. One can find many instances of violence where no guns were involved. d3: Guns are for violence prone people. Violence begets guns. Guns beget violence. d4: I like guns but I hate violence. I have never been involved in violence. But I own many guns. Gun violence is incomprehensible to me. I do believe gun owners are the most anti violence people on the planet. He who never uses a gun will be prone to senseless violence. d5: Guns were used in an armed robbery in San Francisco last night. d6: Acts of violence usually involve a weapon. 4.6.1 Using TF-IDF and cosine similarity Before discussing PCA, let’s look at some more elementary techniques for document retrieval. These are based on term frequency-inverse document frequency (TF-IDF) and cosine similarity. TERM FREQUENCY Term frequency (TF) is defined as the number of occurrences of a particular term in a document. (In this context, note that in this book, we use term and word somewhat in- terchangeably.) In a slightly looser definition, any quantity proportional to the number of occurrences of the term is also known as term frequency. For example, the TF of the word gun in d0, d6 is 0, in d1 is 1, in d3 is 3, and so on. Note that we are being case indepen- dent. Also, singular/plural (gun and guns) and various flavors of the words originating from the same stem (such as violence and violent) are typically mapped to the same term. INVERSE DOCUMENT FREQUENCY Certain terms, such as the, appear in pretty much all documents. These should be ignored during document retrieval. How do we down-weight them? The IDF is obtained by inverting and then taking the absolute value of the logarithm of the fraction of all documents in which the term occurs. For terms that occur in most documents, the IDF weight is very low. It is high for relatively esoteric terms. DOCUMENT FEATURE VECTORS Each document is represented by a document feature vector. It has as many elements as the size of the vocabulary (that is, the number of distinct words over all the documents). Every word has a fixed index position in the vector. Given a specific document, the value at the index position corresponding to a specific word contains the TF of the corresponding word multiplied by that word’s IDF. Thus, every document is a point in a space that has as many dimensions as the vocabulary size. The coordinate value along a specific dimension is proportional to the number of times the word is repeated in the document, with a weigh-down factor for common words. For real-life document retrieval systems like Google, this vector is extremely long. But not to worry: this vector is notional—it is never explicitly stored in the computer’s memory. We store a sparse version of the document feature vector: a list of unique words along with their TF×IDF scores. 142 CHAPTER 4 Linear algebraic tools in machine learning COSINE SIMILARITY In section 2.5.6, we saw that the dot product between two vectors measures the agreement between them. Given two vectors ®a and ®b, we know ®a · ®b = ∥®a∥∥®b∥cos (휃), where the operator ∥· ∥implies the length of a vector and 휃is the angle between the two vectors (see figure 2.7b). The cosine is at its maximum possible value, 1, when the vectors are pointing in the same direction and the angle between them is zero. It becomes progressively smaller as the angle between the vectors increases until the two vectors are perpendicular to each other and the cosine is zero, implying no correlation: the vectors are independent of each other. The magnitude of the dot product is also proportional to the length of the two vectors. We do not want to use the full dot product as a measure of similarity between the vectors because two long vectors would have a high similarity score even if they were not aligned in direction. Rather, we want to use the cosine, defined as cosine_similarity  ®a, ®b  = ®aT ®b ∥®a∥∥®b∥ (4.13) The cosine similarity between document vectors is a principled way of measuring the degree of term sharing between the documents. It is higher if many repeated words are shared between the two documents. 4.6.2 Latent semantic analysis Cosine similarity and similar techniques suffer from a significant drawback. To see this, examine the cosine similarity between d5 and d6. It is zero. But it is obvious to a human that the documents are similar. What went wrong? Answer: we are measuring only the direct overlap between terms in documents. The words gun and violence occur together in many of the other documents, indicating some degree of similarity between them. Hence, documents containing only gun have some similarity with documents containing only violence—but cosine similarity between document vectors does not look at such secondary evidence. This is the blind spot that LSA tries to overcome. Words are known by the company they keep. That is, if terms appear together in many documents (like gun and violence in the previous examples), they are likely to share some semantic similarity. Such terms should be grouped together into a common pool of se- mantically similar terms. Such a pool is called a topic. Document similarity should be mea- sured in terms of common topics rather than explicit common terms. We are particularly interested in topics that discriminate the documents in our corpus: that is, there should be a high variation in the degree to which different documents subscribe to the topic. Geometrically, a topic is a subspace in the document feature space. In classical latent semantic analysis, we only look at linear subspaces, and a topic can be visualized as a direction or linear combination of directions (hyperplane) in the document feature space. In particular, any direction line in the space is a topic: it is a subspace representing a weighted combination of the coordinate axis directions, which means it is a weighted combination of vocabulary terms. We are, of course, interested in topics with high 4.6 Machine learning application: Document retrieval 143 variance. These correspond to a direction along which the document vectors are well spread, which means the document vectors are well discriminated over this topic. We typically prune the set of topics, eliminating those with insufficient variance. From this discussion, a mathematical definition of topic begins to emerge. Topics are principal components of the matrix of document vectors with individual document descriptor vectors along its rows. Measuring document similarity in terms of topic has the advantage that two documents may not have many exact words in common but may still have a common topic. This happens when they share words belonging to the same topic. Essentially, they share a lot of words that occur together in other documents. So even if the number of common words is low, we can have high document similarity. For instance, in our toy document corpus, gun and violence are very correlated (both or neither is likely to occur in a document). Gun-violence emerges as a topic. If we express the document vector in terms of this topic instead of the individual words, we see similarities that otherwise would have escaped us. That is, we see latent semantic similarities. For instance, the cosine similarity between d5 and d6 is nonzero. This is the core idea of latent semantic analysis and is illustrated in figure 4.6. Roses Violence Gun America . . . . . . . . . . . . . 1 2 3 4 5 Topic vector “gun-violence”: a principal component of the document matrix Figure 4.6 Document vectors from our toy dataset d0, · · · d6. Each word in the vocabulary corresponds to a separate dimension. Dots show projections of document feature vectors on the plane formed by the axes corresponding to the terms gun(s) and violence. 144 CHAPTER 4 Linear algebraic tools in machine learning Table 4.1 Document matrix for the toy example dataset Violence Gun America · · · Roses d0 0 0 0 · · · 2 d1 1 1 1 · · · 0 d2 2 2 0 · · · 0 d3 3 3 0 · · · 0 d4 5 5 0 · · · 0 d5 0 1 0 · · · 0 d6 1 0 0 · · · 0 Let’s revisit our example document-retrieval problem in light of topic extraction. The document matrix (with document vectors as rows) looks like table 4.1. Rows correspond to documents, and columns correspond to terms. Each cell contains the term frequency. The terms gun and violence occur an equal number of times in most documents, in- dicating clear correlation. Hence gun-violence is a topic. The principal components (right eigenvectors) identify topics. As usual, we have omitted prepositions, conjunc- tions, commas, and so on. The overall steps are as follows (see listing 4.8 for the Python code): 1 Create a document term matrix X of dimension m × n. Its rows correspond to documents (m documents), and its columns correspond to terms (n terms). 2 Perform SVD on the matrix. This yields U, S, and V matrices. V is an n × n orthog- onal matrix, and S is a diagonal matrix. 3 The columns of matrix V yield topics. These are principal vectors for the rows of X: that is, eigenvectors of XT X or, equivalently, the covariance matrix of X. 4 The successive elements of each topic vector (column in matrix V ) tell us the contribution of corresponding terms to that topic. Each column is n × 1, depicting the contributions of the n terms in the system. 5 The diagonal elements of S tell us the weights (importance) of corresponding topics. These are the eigenvalues of XT X: that is, principal values of the row vectors of X. 6 Inspect the weights, and choose a cutoff. All topics below that weight are discarded— the corresponding columns of V are thrown away. This yields a matrix V with fewer columns (but the same number of rows); these are the topic vectors of interest to us. We have reduced the dimensionality of the problem. If the number of retained topics is t, the reduced V is m × t. 7 By projecting (multiplying) the original matrix X of document terms to this new matrix V, we get an m × t matrix of document topics (it has same number of rows as X but fewer columns). This is the projection of X to the topic space: that is, a topic-based representation of the document vectors. 8 Rows of the document topic matrix will henceforth be taken as document repre- sentations. Document similarities will be computed by taking the cosine similarity of these rows rather than the rows of the original document term matrix. This 4.6 Machine learning application: Document retrieval 145 cosine similarity, in the topic space, will capture many indirect connections that were not visible in the original input space. 4.6.3 PyTorch code to perform LSA The following listing demonstrates how to compute the LSA for our toy dataset from table 4.1. Fully functional code for this section can be found at http://mng.bz/E2Gd. Listing 4.8 Computing LSA terms = [''violence'', ''gun'', ''america'', ''roses''] Considers only four terms for simplicity X = torch.tensor([[0, 0, 0, 2], [1, 1, 1, 0], [2, 2, 0, 0], [3, 3, 0, 0], [5, 5, 0, 0], [0, 1, 0, 0], [1, 0, 0, 0]]).float() Document term matrix. Each row describes a document. Each column contains TF scores for one term. IDF is ignored for simplicity. U, S, V_t = torch.linalg.svd(X) Performs SVD on the doc-term matrix. Columns of the resulting matrix V correspond to topics. These are eigenvectors of XT X: principal vectors of the doc-term matrix. A topic corresponds to the direction of maximum variance in the doc feature space. V = V_t.T rank = 1 U = U[:, :rank] V = V[:, :rank] S indicates the diagonal matrix of principal values. These signify topic weights (importance). We choose a cut-off and discard all topics below that weight (dimensionality reduction). Only the first few columns of V are retained. Principal values (topic weights) for this dataset are shown in the output. Only one topic is retained in this example. topic0_term_weights = list(zip(terms, V[:, 0])) Elements of the topic vector show the contributions of corresponding terms to the topic. def cosine_similarity(vec_1, vec_2): vec_1_norm = torch.linalg.norm(vec_1) vec_2_norm = torch.linalg.norm(vec_2) return torch.dot(vec_1, vec_2) / (vec_1_norm * vec_2_norm) d5_d6_cosine_similarity = cosine_similarity(X[5], X[6]) Cosine similarity in the feature space fails to capture d, d6 similarity. LSA succeeds. doc_topic_projection = torch.dot(X, V) d5_d6_lsa_similarity = cosine_similarity(doc_topic_projection[5], doc_topic_projection[6]) The output is as follows: Principal Values from S matrix: 8.89, 2.00, 1.00, 0.99 (Topic 0 has disproportionately high weight. We discard others) 146 CHAPTER 4 Linear algebraic tools in machine learning topic0_term_weights (Topic zero is about ''gun'' and ''violence''): [ ('violence', -0.706990662151775) ('gun', -0.7069906621517749) ('america', -0.018122010384881156) ('roses', 2.9413274625621952e-18) ] Document 5, document 6 Cosine similarity in original space: 0.0 Document 5, document 6 Cosine similarity in topic space: 1.0 4.6.4 PyTorch code to compute LSA and SVD on a large dataset Suppose we have a set of 500 documents over a vocabulary of 3 terms. This is an unrealistically short vocabulary, but it allows us to easily visualize the space of document vectors. Each document vector is a 3 × 1 vector, and there are 500 such vectors. Together they form a 500 × 3 data matrix X. In this dataset, the terms x0 and x1 are correlated: x0 occurs randomly between 0 and 100 times in a document, and x1 occurs twice as many times as x0 except for small random fluctuations. The third term’s frequency varies between 0 and 5. From section 4.6, we know that x0, x1 together form a single topic, while x2 by itself forms another topic. We expect a principal component along each topic. Listing 4.9 creates the dataset, computes the SVD, plots the dataset, and shows the first two principal components. The third singular value is small compared to the first. We can ignore that dimension—it corresponds to the small random variation within the x0 −x1 topic. The singular values are printed out and also shown graphically along with the data points in figure 4.7. Figure 4.7 Latent se- mantic analysis. Note that the vertical axis line is actually much smaller than it appears to be in the diagram. Listing 4.9 LSA using SVD num_examples = 500 x0 = torch.normal(0, 100, (num_examples,)).round() Summary 147 random_noise = torch.normal(0, 2, (num_examples,)).round() x1 = 2*x0 + random_noise x2 = torch.normal(0, 5, (num_examples,)).round() X = torch.column_stack((x0, x1, x2)) 3D dataset: the first two axes are linearly correlated; the third axis has small near-zero random values. U, S, V_t = torch.linalg.svd(X) V = V_t.T The third singular value is relatively small; we ignore it. The first two principal vectors represent topics. Projecting data points on them yields document descriptors in terms of the two topics. Here is the output: Singular values are: 4867.56982, 118.05858, 19.68604 Summary In this chapter, we studied several linear algebraic tools used in machine learning and data science: The direction (unit vector) that maximizes (minimizes) the quadratic form ˆxT Aˆx is the eigenvector corresponding to the largest (smallest) eigenvalue of matrix A. The magnitude of the quadratic form when ˆx is along those directions is the largest (smallest) eigenvalue of A. Given a set of points X =  ®x(0), ®x(1), ®x(2), · · · , ®x(n) in an n + 1-dimensional space, we can define the mean vector and covariance matrix as ®휇= 1 n n Õ i=0 ®x(i) C = 1 n n Õ i=0  ®x(i) −®휇   ®x(i) −®휇 T The variance along an arbitrary direction (unit vector) ˆl is ˆl TCˆl. This is a quadratic form. Consequently, the maximum (minimum) variance of a set of data points in multidimensional space occurs along the eigenvector corresponding to the largest (smallest) eigenvalue of the covariance matrix. This direction is called the first principal axis of the data. The subsequent eigenvectors, sorted in order of decreasing eigenvalues, are mutually orthogonal (perpendicular) and yield the subsequent direction of maximum variance. This technique is known as principal component analysis (PCA). In many real-life cases, larger variances correspond to the true underlying pattern of the data, while smaller variances correspond to noise (such as measurement error). Projecting the data on the principal axes corresponding to the larger eigenvalues yields lower-dimensional data that is relatively noise-free. The projected data points also match the true underlying pattern more closely, yielding better insights. This is known as dimensionality reduction. Singular value decomposition (SVD) allows us to decompose an arbitrary m × n matrix A as a product of three matrices: A =UΣV T, where U , V are orthogonal 148 CHAPTER 4 Linear algebraic tools in machine learning and Σ is diagonal. Matrix V has the eigenvectors of AT A as its columns. U has eigenvectors of AAT as columns. Σ has the eigenvalues of AT A (sorted in decreasing order) in its diagonal. SVD provides a numerically stable way to solve the linear system of equations A®x = ®b. In particular, for nonsquare matrices, it provides the closest approximations: that is, ®x that minimizes ∥A®x −®b∥. Given a dataset X whose rows are data vectors corresponding to individual instances and columns correspond to feature values, XT X yields the covariance matrix. Thus eigenvectors of XT X yield the data’s principal components. Since the SVD of X has eigenvectors of XT X as columns of the matrix V, SVD is an effective way to compute PCA. When using machine learning data science for document retrieval, the bag-of- words model represents documents with document vectors that contain the term frequency (number of occurrences) of each term in the document. TF-IDF is a cosine similarity technique for document matching and retrieval. Latent semantic analysis (LSA) does topic modeling: we perform PCA on the document vectors to identify topics. Projecting document vectors onto topic axes allows LSA to see latent (indirect) similarities beyond the direct overlapping of terms. 5 Probability distributions in machine learning This chapter covers The role of probability distributions in machine learning Working with binomial, multinomial, categorical, Bernoulli, beta, and Dirichlet distributions The significance of entropy and cross-entropy in machine learning Life often requires us to estimate the chances of an event occurring or make a decision in the face of uncertainty. Probability and statistics form the common toolbox to use in such circumstances. In machine learning, we take large feature vectors as inputs. As stated earlier, we can view these feature vectors as points in a high-dimensional space. For instance, gray-level images of size 224 × 224 can be viewed as points in a 50,176- dimensional space, with each pixel corresponding to a specific dimension. Inputs with common characteristics, such as images of animals, will correspond to a cluster of points in that space. Probability distributions provide an effective tool for analyzing such loosely structured point distributions in arbitrarily high-dimensional spaces. Instead of simply developing a machine that emits a class given an input, we can 149 150 CHAPTER 5 Probability distributions in machine learning fit a probability distribution to the clusters of input points (or a transformed version of them) satisfying some property of interest. This often lends more insight into the problem we are trying to solve. For instance, suppose we are trying to design a recommendation system. We could design one or more classifiers that emit yes/no decisions about whether to recommend product X to person Y. On the other hand, we can fit probability distributions to specific groups of people. Doing so can lead to the discovery of significant overlap between the point clusters representing various groups—for instance, people who drink black coffee and start-up founders. We may not know the explanation or even the direction in which causality (if any) flows in the relationship. But we see the correlation and may design a better recommendation system using it. Another situation in which probabilistic models are used in machine learning is when the problem involves a very large number of (perhaps infinitely many) classes. For instance, suppose we are creating a machine that not only recognizes cats in an image but also labels each pixel as belonging or not belonging to a cat. Effectively, the machine segments the image pixels into foreground versus background. This is called semantic segmentation. It is hard to cast this problem as a classification problem: we typically design a system that emits a probability of being foreground for each pixel. Probabilistic models are also used in unsupervised and minimally supervised learning: for instance, in variational autoencoders (VAEs), which we discuss in chapter 14. This chapter introduces the fundamental notion of probability and discusses probabil- ity distributions (including multivariates), with specific examples, in a machine learning- centric way. As usual, we emphasize the geometrical view of multivariate statistics. An equally important goal of this chapter is to familiarize you with PyTorch distributions, the PyTorch statistical package, which we use throughout the book. All the distributions discussed are accompanied by code snippets from PyTorch distributions. NOTE The complete PyTorch code for this chapter is available at http://mng .bz/8NVg in the form of fully functional and executable Jupyter notebooks. 5.1 Probability: The classical frequentist view Consider a mythical city called Statsville. Suppose we choose a random adult inhab- itant of Statsville. What are the chances of this person’s height being greater than 6 ft? Less than 3 ft? Between 5 ft 5 in. and 6 ft? What are the chances of this person’s weight being between 50 and 70 kg (physicists would rather use the term mass here, but we have chosen to stick to the more common word weight)? Greater than 100 kg? What is the probability of the person’s home being exactly 6 km from the city cen- ter? What are the chances of the person’s weight being in the 50–70 kg range and their height being in the 5 ft 5 in. to 6 ft range? What are the chances of the per- son’s weight being greater than 90 kg and their home being within 5 km of the city center? 5.1 Probability: The classical frequentist view 151 All these questions can be answered in the frequentist paradigm by adopting the following approach: Count the size of the population belonging to the desired event (satisfying the criterion or criteria of interest): for instance, the number of Statsville adults taller than 6 ft. Divide that by the total size of the population (here, the number of adults in Statsville). This is the probability (chance) of that criterion/criteria being satisfied. Formally, Probability of an event = Size of population belonging to that event Total size of population = Number of favorable outcomes Number of possible outcomes (5.1) For instance, suppose there are 100,000 adults in the city. Of them, 25,000 are 6 ft or taller. Then the size of the population satisfying the event of interest (aka the number of favorable outcomes) is 25,000. The total population size (aka the number of possible outcomes) is 100,000. So, Probability of a random adult Statsville resident being taller than 6 ft = Number of adult Statsville residents taller than 6 ft Total number of adult Statsville residents = 25000 100000 = 0.25 Since the total population is always a superset of the population belonging to any event, the denominator is always greater than or equal to the numerator. Consequently, probabilities are always lesser than or equal to 1. 5.1.1 Random variables When we talk about probability, a relevant question is, “The probability of what?” The simplest answer is, “The probability of the occurrence of an event.” For example, in the previous subsection, we discussed the probability of the event that the height of an adult Statsville resident is less than 6 ft. A little thought reveals that an event always corresponds to a numerical entity of interest taking a particular value or lying in a particular range of values. This entity is called a random variable. For instance, the height of adult Statsville residents can be a random variable, and we can talk about the probability of it being less than 6 ft, or the weight of adult Statsville residents can be a random variable, and we can talk about the probability of it being less than 60 kg. When predicting the performance of stock markets, the Dow Jones index maybe a random variable: we can talk about the probability of this random variable crossing 19,000. And when discussing the spread of a virus, the total number of infected people may be a random variable, and we can talk about the probability of it being less than 2,000, and so on. The defining characteristic of a random variable is that every allowed value (or range of values) is associated with a probability (of the random variable taking that value or value range). For instance, we may allow a set of only three weight ranges for adults of 152 CHAPTER 5 Probability distributions in machine learning Statsville: S1, less than 60 kg; S2, between 60 and 90 kg; and S3, greater than 90 kg. Then we can have a corresponding random variable X representing the quantized weight. It can take one of only three values: X = 1 (corresponding to the weight in S1), X = 2 (corresponding to the weight in S2), or X = 3 (corresponding to the weight in S3). Each value comes with a fixed probability: for example, p(X = 1) = 0.25, p(X = 2) == 0.5, and p(X = 3) = 0.25, respectively, in the example from section 5.1. Such random variables that take values from a countable set are known as discrete random variables. Random variables can also be continuous. For a continuous random variable X, we asso- ciate a probability with its value being in an infinitesimally small range, p (x ≤X < x + 훿x), with 훿x →0. This is called probability density and is explained in more detail in section 5.6. NOTE In this book, we always use uppercase letters to denote random variables. Usually, the same letter in lowercase refers to a specific value of the random vari- able: for example, p (X = x) denotes the probability of random variable X taking the value x and p (X ∈{x, x + 훿x}) denotes the probability of random variable X taking a value between x and x + 훿x. Also note that sometimes we use the letter X to denote a data set. This popular but confusing convention is rampant in the literature—generally, the usage is obvious from the context. 5.1.2 Population histograms Histograms help us to visualize discrete random variables. Let’s continue with our Statsville example. We are only interested in three weight ranges for Statsville adults: S1: less than 60 kg; S2: between 60 and 90 kg; and S3: greater than 90 kg. Suppose the counts of Statsville adults in these weight ranges are as shown in table 5.1. Table 5.1 Frequency table for the weights of adults in the city of Statsville S1: Less than 60 kg S2: Between 60 and 90 kg S3: More than 90 kg 25,000 50,000 25,000 The same information can be visualized by the histogram shown in figure 5.1. The X-axis of the histogram corresponds to possible values of the discrete random variable from section 5.1.1. The Y-axis shows the size of the population in the corresponding weight range. There are 25,000 people in range S1, 50,000 people in range S2, and 25,000 people in range S3. Together, these categories account for the entire adult population of Statsville—every adult belongs to one category or another. This can be verified by adding the Y-axis values for all the categories: 25,000 + 50,000 + 25,000 = 100,000, the adult population of Statsville. 5.2 Probability distributions Figure 5.1 and its equivalent, table 5.1, can easily be converted to probabilities, as shown in table 5.2. The table shows the probabilities corresponding to allowed values of the discrete random variable X representing the quantized weight of a randomly chosen adult resident of Statsville. Table 5.2 represents what is formally known as a probability 5.2 Probability distributions 153 Figure 5.1 Histogram depicting the weights of adults in Statsville, corresponding to table 5.1 Table 5.2 Probability distribution for quantized weights of Statsville adults S1: Less than 60 kg S2: Between 60 and 90 kg S3: More than 90 kg p (X = 1) = 25,000 100,000 = 0.25 p (X = 2) = 50,000 100,000 = 0.5 p (X = 3) = 25,000 100,000 = 0.25 distribution: a mathematical function that takes a random variable as input and outputs the probability of it taking any allowed value. It must be defined over all possible values of the random variable. Note that the set of ranges {S1, S2, S3} is exhaustive in the sense that all possible values of X belong to one range or other—we cannot have a weight that does not belong to any of them. In set-theoretical terms, the union of these ranges, S1 ∪S2 ∪S3, covers a space that contains the entire population (all possible values of X). NOTE The set-theoretic operator ∪denotes set union. The ranges are also mutually exclusive in that any given observation of X can belong to only a single range, never more. In set-theoretic terms, the intersection of any pair of ranges is null: S1 ∩S2 = S1 ∩S3 = S2 ∩S3 = 휙. NOTE The set-theoretic operator ∩denotes set intersection. For a set of exhaustive and mutually exclusive events, the function yielding the probabili- ties of these events is a probability distribution. For instance, the probability distribution in our tiny example comprises three probabilities: P (X = 1) = 0.25, P (X = 2) = 0.5, and P (X = 3) = 0.25. This is shown in figure 5.2, which is a three-point graph. 154 CHAPTER 5 Probability distributions in machine learning Figure 5.2 Probability distribution graph for the weights of adults in Statsville, corresponding to table 5.2. Event E1 ≡X = 1 =⇒a weight in the range S1, Event E2 ≡X = 2 =⇒a weight in the range S2, and Event E3 ≡x = 3 =⇒a weight in the range S3. 5.3 Basic concepts of probability theory In this section, we briefly touch on impossible and certain events; the sum of probabilities of exhaustive, mutually exclusive events; and independent events. 5.3.1 Probabilities of impossible and certain events The probability of an impossible event is zero (for example, the probability that the sun will rise in the west). The probability of an event that occurs with certitude is 1 (the probability that the sun will rise in the east). Improbable events (such as this author beating Roger Federer in competitive tennis) have low but nonzero probabilities, like 0.001. Highly probable events (such as Roger Federer beating this author in competitive tennis) have probabilities close to but not exactly equal to 1, like 0.999. 5.3.2 Exhaustive and mutually exclusive events Consider the events E1, E2, E3 corresponding to the quantized weight of a Statsville adults from section 5.2 belonging to the range S1, S2, or S3, respectively (equivalently, E1 is the event corresponding to X = 1, E2 is the event corresponding to X = 2, and E3 is the event corresponding to X = 3). The events are exhaustive: their union covers the entire population space. This means all quantized weights of Statsville adults belong to one of the ranges S1, S2, S3. The events are also mutually exclusive: their mutual intersections are null, meaning no member of the population can belong to more than 5.4 Joint probabilities and their distributions 155 one range. For example, if a weight belongs to S1, it cannot belong to S2 or S3. For such events, the following holds true: The sum of the probabilities of mutually exclusive events yields the probability of one or the other of them occurring. For instance, for events E1, E2, E3, p (E1 or E2) = p (E1) + p (E2) (5.2) the sum rule says that The sum of the probabilities of an exhaustive, mutually exclusive set of events is always 1. For example, p (E1) + p (E2) + p (E3) = p (E1 or E2 or E3) = 1 This is intuitively obvious. We are merely saying that we can say with certainty that either E1 or E2 or E3 will occur. In general, given a set of exhaustive, mutually exclusive events E1, E2, · · · , En, i=n Õ i=1 p (Ei) = 1 (5.3) 5.3.3 Independent events Consider the two events E1 ≡“weight of an adult inhabitant of Statsville is less than 60 kg” and G1 ≡“home of an adult inhabitant of Statsville is within 5 km of the city center.” These events do not influence each other at all. The knowledge that a member of the population weighs less than 60 kg does not reveal anything about the distance of their home from the city center and vice versa. We say E1 and G1 are independent events. Formally, A set of events are independent if the occurrence of one does not affect the probability of the occurrence of another. 5.4 Joint probabilities and their distributions Given an adult Statsville resident, let E1 be, as before, the event that their weight is less than 60 kg. The corresponding probability is p (E1). Also, let G1 be the event that the distance of their home from the city center is less than 5 km. The corresponding probability is p (G1). Now consider the probability that a resident weights less than 60 kg and their home is less than 5 km from the city center. This probability, denoted p (E1, G1), is called a joint probability. Formally, The joint probability of a set of events is the probability of all those events occurring together. The product rule says that the joint probability of independent events can be obtained by multiplying their individual probabilities. Thus, for the current example, p (E1, G1) = p (E1) p (G1). 156 CHAPTER 5 Probability distributions in machine learning Let’s continue our discussion of joint probabilities with a slightly more elaborate example. We have consolidated the weight categories, corresponding populations, and probability distributions in table 5.3 for quick reference. Similarly, we quantize the distance of residents’ homes from the city center into three ranges: D1 ≡less than 5 km, D2 ≡between 5 and 15 km, and D3 ≡greater than 15 km. Table 5.4 shows the corre- sponding population and probability distributions. The joint probability distribution of the events {E1, E2, E3} and {G1, G2, G3} is shown in table 5.5. Table 5.3 Population probability distribution table for the weights of adult residents of Statsville. E1, E2, E3 are exhaustive, mutually exclusive events, p (E1) + p (E2) + p (E3) = 1. Less than 60 kg (range S1) Between 60 and 90 kg (range S2) More than 90 kg (range S3) Event E1 ≡weight ∈S1 Event E2 ≡weight ∈S2 Event E3 ≡weight ∈S3 Population size = 25,000 Population size = 50,000 Population size = 25,000 p (E1) = 25,000 100,000 = 0.25 p (E2) = 50,000 100,000 = 0.5 p (E3) = 25,000 100,000 = 0.25 Table 5.4 Population probability distribution table for the distance of adult Statsville residents’ homes from the city center. G1, G2, G3 are exhaustive, mutually exclusive events, p (G1) + p (G2) + p (G3) = 1. Less than 5 km (range D1) Between 5 and 15 km (range D2) Greater than 15 km (range D3) Event G1 ≡distance ∈D1 Event G2 ≡distance ∈D2 Event G3 ≡distance ∈D3 Population size = 20,000 Population size = 60,000 Population size = 20,000 p (G1) = 20,000 100,000 = 0.20 p (G2) = 60,000 100,000 = 0.6 p (G3) = 20,000 100,000 = 0.20 Table 5.5 Joint probability distribution of independent events. The sum of all elements in the table is 1. Less than Between 60 More than 60 kg (E1) and 90 kg (E2) 90 kg (E3) Less than p (E1, G1) p (E2, G1) p (E3, G1) 5 km (G1) = 0.25 × 0.2 = 0.5 × 0.2 = 0.25 × 0.2 = 0.05 = 0.1 = 0.05 Between 5 and p (E1, G2) p (E2, G2) p (E3, G2) 15 km (G2) = 0.25 × 0.6 = 0.5 × 0.6 = 0.25 × 0.6 = 0.15 = 0.3 = 0.15 More than p (E1, G3) p (E2, G3) p (E3, G3) 15 km (G3) = 0.25 × 0.2 = 0.5 × 0.2 = 0.25 × 0.2 = 0.05 = 0.1 = 0.05 We can make the following statements about table 5.5: The sum total of all elements in table 5.5 is 1. In other words, p  Ei, Gj  is a proper probability distribution indicating the probabilities of event Ei and event Gj occurring together: here, (i, j) ∈{1, 2, 3} × {1, 2, 3}. 5.4 Joint probabilities and their distributions 157 p  Ei, Gj  = p (Ei) p  Gj  ∀(i, j) ∈{1, 2, 3} × {1, 2, 3}. This is because the events are independent. NOTE The symbol × denotes the Cartesian product. The Cartesian product of two sets {1, 2, 3} × {1, 2, 3} is the set {(1, 1) , (1, 2) , (1, 3) , (2, 1) , (2, 2) , (2, 3) , (3, 1) , (3, 2) , (3, 3)}. And the symbol ∀indicates “for all.” Read ∀(i, j) ∈{1, 2, 3} × {1, 2, 3} as follows: for all pairs (i, j) in the Cartesian product, {1, 2, 3} × {1, 2, 3}. In general, given a set of independent events E1, E2, · · · , En, the joint probability p (E1, E2, · · · , En) of all the events occurring together is the product of their individual probabilities of occurring: p (E1, E2, · · · , En) = p (E1) p (En) · · · p (E1) = i=n Ö i=1 p (Ei) (5.4) NOTE The symbol Î stands for “product.” 5.4.1 Marginal probabilities Suppose we do not have the individual probabilities p (Ei) and p  Gj . All we have is the joint probability distribution: that is, table 5.5. Can we find the individual probabilities from them? If so, how? To answer this question, consider a particular row or column in table 5.5—say, the top row. In this row, the E values iterate over all possibilities (the entire space of Es), but G is fixed at G1. If G1 is to occur, there are only three possibilities: it occurs with E1,E2, or E3. The corresponding joint probabilities are p (E1, G1), p (E2, G1), and p (E3, G1). If we add them, we get the probability of G1 occurring with E1 or E2, or E3: that is, event (E1, G1) or (E2, G1) or (E3, G1). Thus we have considered all situations under which G1 can occur. The sum represents the probability of event G1 occurring. Thus, p (G1) can be obtained by adding all the probabilities in the row corresponding to G1 and writing it in the margin (this is why it is called the marginal probability). Similarly, by adding all the probabilities in the middle column, we obtain the probability p (E2), and so forth. Table 5.6 shows table 5.5 updated with marginal probabilities. In general, given a set of exhaustive, mutually exclusive events E1, E2, · · · , En, another event G, and joint probabilities p (E1, G), p (E2, G), · · · , p (En, G), p (G) = i=n Õ i=1 p (Ei, G) (5.5) By summing over all possible values of Eis, we factor out the Es. This is because the Es are mutually exclusive and exhaustive; summing over them results in a certain event that is factored out (remember, the probability of a certain event is 1). 5.4.2 Dependent events and their joint probability distribution So far, the events we have considered jointly are “weights” and “distance of a resi- dent’s home from the city center.” These are independent of each other—their joint probability is the product of their individual probabilities. Now, let’s discuss a different 158 CHAPTER 5 Probability distributions in machine learning Table 5.6 Joint probability distribution with marginal probabilities shown Less than Between 60 More than Marginals 60 kg (E1) and 90 kg (E2) 90 kg (E3) for G’s Less than p (E1, G1) p (E2, G1) p (E3, G1) p (G1) 5 km (G1) = 0.25 × 0.2 = 0.5 × 0.2 = 0.25 × 0.2 = 0.05 + 0.1 + 0.05 = 0.05 = 0.1 = 0.05 = 0.2 Between p (E1, G2) p (E2, G2) p (E3, G2) p (G2) 5 and 15 km = 0.25 × 0.6 = 0.5 × 0.6 = 0.25 × 0.6 0.15 + 0.3 + 0.15 (G2) = 0.15 = 0.3 = 0.15 = 0.6 More than p (E1, G3) p (E2, G3) p (E3, G3) p (G3) 15 km (G3) = 0.25 × 0.2 = 0.5 × 0.2 = 0.25 × 0.2 = 0.05 + 0.1 + 0.05 = 0.05 = 0.1 = 0.05 = 0.2 Marginals p (E1) p (E2) p (E3) for Es = 0.05 + 0.15 + 0.05 = 0.1 + 0.3 + 0.1 = 0.05 + 0.15 + 0.05 = 0.25 = 0.5 = 0.25 situation where the variables are connected and knowing the value of one does help us predict the other. For instance, the weights and heights of adult residents of Statsville are not independent: typically, taller people weigh more, and vice versa. As usual, we use a toy example to understand the idea. We quantize heights into three ranges, H1 ≡less than 5 ft 5 in., H2 ≡between 5 ft 5 in. and 6 ft, and H3 ≡greater than 6 ft. Let z be the random variable corresponding to height. We have three possible events with respect to height: F1 ≡z ∈H1, F2 ≡z ∈H2, and F3 ≡z ∈H3. The joint probability distribution of height and weight is shown in table 5.7. Table 5.7 Joint probability distribution of dependent events Less than Between 60 More than 60 kg (E1) and 90 kg (E2) 90 kg (E3) Less than 5 ft p (E1, F1) p (E2, F1) p (E3, F1) 5 in. (F1) = 0.25 = 0 = 0 Between 5 ft p (E1, F2) p (E2, F2) p (E3, F2) 5 in. and 6 ft (F2) = 0. = 0.5 = 0 More than 6 ft (F3) p (E1, F3) p (E2, F3) p (E3, F3) = 0 = 0 = 0.25 Note the following about table 5.7: The sum total of all elements in table 5.7 is 1. In other words, p  Ei, Fj  is a proper probability distribution indicating the probabilities of event Ei and event Fj occur- ring together. Here (i, j) ∈{1, 2, 3} × {1, 2, 3}. p  Ei, Fj  = 0 i f i ≠j ∀(i, j) ∈{1, 2, 3} × {1, 2, 3}. This essentially means the events are perfectly correlated: the occurrence of E1 implies the occurrence of F1 and 5.5 Geometrical view: Sample point distributions for dependent and independent variables 159 vice versa, the occurrence of E2 implies the occurrence of F2 and vice versa, and the occurrence of E3 implies the occurrence of F3 and vice versa. In other words, every adult resident of Statsville who weighs less than 60 kg is also shorter than 5 ft 5 in., and so on. (In life, such perfect correlations rarely exist; but Statsville is a mythical town.) 5.5 Geometrical view: Sample point distributions for dependent and independent variables Let’s look at a graphical view of the point distributions corresponding to tables 5.5 and 5.7. There is a fundamental difference in how the point distributions look for independent and dependent variables; it is connected to principal component analysis (PCA) and dimensionality reduction, which we discussed in section 4.4. We use a rectangular bucket-based technique to visualize joint 2D discrete events. For instance, we have three weight-related events, E1, E2, E3, and three distance-related events, G1, G2, G3. Hence the joint distribution has 3 × 3 = 9 possible events  Ei, Gj , ∀(i, j) ∈{1, 2, 3} × {1, 2, 3}, as shown in table 5.5. Each of these nine events is repre- sented by a small rectangle (bucket for the joint event); altogether, we have a 3 × 3 grid of rectangular buckets. To visualize the sample point distribution, we have drawn 1,000 samples from the joint distribution. A joint event sample is placed at a random location within its bucket (that is, all points within the bucket have an equal probability of being selected). Notice that the concentration of points is greater inside high-probability buckets and vice versa. Graphical views of the point distribution for the independent (table 5.5) and non- independent (table 5.7) joint variable pairs are shown in figures 5.3a and 5.3b, respec- tively. We see that the sample point distribution for the independent events is spread somewhat symmetrically over the domain, while that for the dependent events is spread narrowly around a particular line (in this case, the diagonal). This holds true in general and for higher dimensions, too. You should have this mental picture with respect to independent versus non-independent point distributions. If we sample independent events (uncor- related), all possible combinations of events {E1, G1}, {E1, G2}, {E1, G3}, · · · , {E3, G3} have a non-negligible probability of occurrence (see table 5.5), which is equivalent to saying that none of the events have a very high probability of occurring (remember that probabilities sum to 1, so if some events have very low probabilities [close to zero], other events must have high probabilities [near one] to compensate). This precludes the concentration of points in a small region of the space. All buckets will have many points. In other words, the joint probability samples of independent events are diffused throughout the population space (see figure 5.3a, for instance). On the other hand, if the events are correlated, the joint probability samples are concentrated in certain high-probability regions of the joint space. For instance, in table 5.7, events (E1, F1), (E2, F2), (E3, F3) are far more likely than the other combina- tions. Hence, the sample points are concentrated along the corresponding diagonal (see figure 5.3b). 160 CHAPTER 5 Probability distributions in machine learning (a) (b) Figure 5.3 Graphical visualization of joint probability distributions. Rectangles represent buckets of different discrete events. (a) From table 5.5 (independent events). The probabilities of all nine events are non-negligible, and all nine rectangles have a relatively high concentration of sample points. Not suitable for PCA. (b) From table 5.7 (non-independent events). Events (E1, F1), (E2, F2), and (E3, F3) have very high probabilities, and other events have negligible probabilities. Sample points are concentrated along the rectangles on the diagonal. Suitable for PCA. If this does not remind you of PCA (section 4.4), you should re-read that section. De- pendent events such as that shown in figure 5.3a are good candidates for dimensionality reduction: the two dimensions essentially carry the same information, and if we know one, we can derive the other. We can drop one of the highly correlated dimensions without losing significant information. 5.6 Continuous random variables and probability density So far, we have quantized our random variables and made them discrete. For instance, weight has been quantized into three buckets—less than 60 kg, between 60 and 90 kg, and greater than 90 kg—and probabilities have been assigned to each bucket. What if we want to know probabilities at a more granular level like 0 to 10 kg, 10 to 20 kg, 20 to 30 kg, and so on? Well, we have to create more buckets. Each bucket covers a narrower range of values (a smaller portion of the population space), but there are more of them. In all cases, following the frequentist approach, we count the number of adult Statsvilleans in each bucket, divide that by the total population size, and call that the probability of belonging to that bucket. What if we want even further granularity? We create even more buckets, each covering an even smaller portion of the population space. In the limit, we have an infinite number of buckets, each covering an infinitesimally small portion of the population space. Together they still cover the population space—a very large number of very small pieces 5.6 Continuous random variables and probability density 161 can cover arbitrary regions. At this limit, the probability distribution function is called a probability density function. Formally, The probability density function p (x) for a continuous random variable X is defined as the probability that X lies between x and x + 훿x with 훿x →0 p (x) = lim 훿x→0 probability (x ≤X < x + 훿x) NOTE It is slightly unfortunate that the typical symbol for a random variable, X, collides with that for a dataset (collection of data vectors), also X. But the context is usually enough to tell them apart. There is a bit of theoretical nuance here. We are saying that p (x) is the probability of the random variable X lying between x and x + 훿x. This is not exactly the same as saying that p (x) is the probability that X is equal to x. But because 훿x is infinitesimally small, they amount to the same thing. Consider the set of events E = lim훿x→0 {x ≤X < x + 훿x} for all possible values of x. All possible values of x range from negative infinity to infinity: x ∈[−∞, ∞]. There are infinite such events, each of which is infinitesimally narrow, but together they cover the entire domain x ∈[−∞, ∞]. In other words, they are exhaustive. They are also mutually exclusive because x cannot belong to more than one of them at the same time. They are continuous counterparts of the discrete events E1, E2, E3 that we have seen before. The fact that the set of events E = lim훿x→0 {x ≤X < x + 훿x} in continuous space is exhaustive and mutually exclusive means we can apply equation 5.3 but the sum will be replaced by an integral as the variable is continuous. The sum rule in a continuous domain is expressed as ∞ ∫ x=−∞ p (x) dx = 1 (5.6) Equation 5.6 is the continuous analog of equation 5.3. It physically means we can say with certainty that x lies somewhere in the interval −∞to ∞. The random variable can also be multidimensional (that is, a vector). Then the probability density function is denoted as p  ®x. The sum rule for a continuous multidimensional probability density function is ∫ ®x∈D p  ®x d®x = 1 (5.7) where D is the domain of ®x—that is, the space containing all possible values of the vector ®x. For instance, the 2D vector  x y  has the XY plane as its domain. Note that the integral in equation 5.7 is a multidimensional integral (for example, for 2D ®x, it is ∬ ®x∈D p  ®x d®x = 1). 162 CHAPTER 5 Probability distributions in machine learning NOTE For simplicity of notation, we usually use a single integral sign to denote multidimensional integrals. The vector sign in the domain (for example, ®x ∈D), as well the vector sign in d®x, indicates multiple dimensions. You may remember from elementary integral calculus that equation 5.6 corresponds to the area under the curve for p (x) (or p  ®x). In higher dimensions, equation 5.7 corresponds to the volume under the hypersurface for p  ®x. Thus, the total area under a univariate probability density curve is always 1. And in higher dimensions, the volume under the hypersurface for a multivariate probability density function is always 1. 5.7 Properties of distributions: Expected value, variance, and covariance Toward the beginning of this chapter, we stated that generative machine learning models are often developed by fitting a distribution from a known family to the available training data. Thus, we postulate a parameterized distribution from a known family and estimate the exact parameters that best fit the training data. Most distribution families are parameterized in terms of intuitive properties like the mean, variance, and so on. Understanding these concepts and their geometric significance is essential for understanding the models based on them. In this section, we explain a few properties/parameters common to all distributions. Later, when we discuss individual distributions, we connect them to the parameters of those distributions. We also show how to programmatically obtain the values of these for each individual distribution via the PyTorch distributions package. 5.7.1 Expected value (aka mean) If we sample a random variable with a given distribution many times and take the average of the sampled values, what value do we expect to end up with? The average will be closer to the values with higher probabilities (as these appear more often during sampling). If we sample enough times, for a given probability distribution, this average always settles down to a fixed value for that distribution: the expected value of the distribution. Formally, given a discrete distribution D where a discrete random variable X can take any value from the sets {x1, x2, · · · , xn} with respective probabilities {p (x1) , p (x2) · · · , p (xn)}, the expected value is given by the formula 피(X) = lim N→∞ 1 N N Õ k=1 xk →D = n Õ i=1 p (xi) xi (5.8) where xk →D denotes the kth sample drawn from the distribution D. Overall, equation 5.8 says that the average or expected value of a very large number of samples drawn from the distribution approaches the probability-weighted sum of all possible sample values. When we sample, the higher-probability values appear more frequently than the lower-probability values, so the average over a large number of samples is pulled closer to the higher-probability values. For multivariate random variables: 5.7 Properties of distributions: Expected value, variance, and covariance 163 Given a discrete distribution where a discrete multidimensional random variable X can take any value from the sets  ®x1, ®x2, · · · , ®xn with respective probabilities  p  ®x1  , p  ®x2  , · · · , p  ®xn  , the expected value is given by the formula 피(X) = n Õ i=1 p  ®xi  ®xi (5.9) For continuous random variables (note how the sum is replaced by an integral): The expected value of a continuous random variable X that takes values from −∞ to ∞(that is, x ∈{−∞, ∞}) is 피(X) =   ∫∞ x=−∞x p (x) dx ⇒for continuous univariate distributions ∫ ®x∈D ®x p  ®x d®x ⇒for continuous multivariate distributions (5.10) EXPECTED VALUE AND CENTER OF MASS IN PHYSICS In physics, we have the concept of the center of mass or centroid. If we have a set of points, each with a mass, the entire point set can be replaced by a single point. This point is called the centroid. The position of the centroid is the weighted average of the positions of the individual points, weighted by their individual masses. If we mentally think of the probabilities of individual points as masses, the notion of expected value in statistics corresponds to the notion of centroid in physics. EXPECTED VALUE OF AN ARBITRARY FUNCTION OF A RANDOM VARIABLE So far, we have seen the expected value of the random variable itself. The notion can be extended to functions of the random variable. The expected value of a function of a random variable is the probability-weighted sum of the values of that function at all possible values of the random variable. Formally, 피( f (X)) = n Õ i=1 f (xi) p (xi) ⇒for discrete univariate distributions 피( f (X)) = n Õ i=1 f  ®xi  p  ®xi  ⇒for discrete multivariate distributions 피( f (X)) = ∞ ∫ x=−∞ f (x) p (x) dx ⇒for continuous univariate distributions 피( f (X)) = ∫ ®x∈D f  ®x p  ®x d®x ⇒for continuous multivariate distributions (5.11) EXPECTED VALUE AND DOT PRODUCT In equation 2.6, we looked at the dot product between two vectors. Further, in sec- tion 2.5.6, we saw that the dot product between two vectors measures the agreement between the two vectors. If both point in the same direction, the dot product is larger. In this section, we show that the expected value of a function of a random variable can be viewed as a dot product between a vector representing the probability and another vector representing the function itself. 164 CHAPTER 5 Probability distributions in machine learning First let’s consider the discrete case. Our random variable can take values xi, i ∈{1, n}. Now, imagine a vector ®f =  f (x1) f (x2) · · · f (xn)  and a vector ®p =  p (x1) p (x2) · · · p (xn)  . From equation 5.11, we see that the expected value of the function 피(f (X)) of random variable X is the same as ®f T ®p = ®f · ®p. This is high when ®f and ®p are aligned; thus, the expected value of the function of the random variable is high when the high function values coincide with high probabilities of the random variable and vice versa. In the continuous case, these vectors have an infinite number of components and the summation is replaced by an integral, but the idea stays the same. EXPECTED VALUE OF LINEAR COMBINATIONS OF RANDOM VARIABLES The expected value is a linear operator. This means the expected value of a linear combination of random variables is a linear combination (with the same weights) of the expected values of the random variables. Formally, 피(훼1X1 + 훼2X2 · · · 훼nXn) = 훼1피(X1) + 훼2피(X2) + · · · 훼n피(Xn) (5.12) 5.7.2 Variance, covariance, and standard deviation When we draw a very large number of samples from a given point distribution, we often like to know the spread of the point set. The spread is not merely a matter of measuring the largest distance between two points in the distribution. Rather, we want to know how densely packed the points are. If most of the points fit within a very small ball, then even if one or two points are far from the ball, we call that a small spread or high packing density. Why is this important in machine learning? Let’s start with a few informal examples. If we discover that the points are tightly packed in a small region around a single point, we may want to replace the entire distribution with that point without causing much error. Or if the points are packed tightly around a single straight line, we can replace the entire distribution with that line. Doing so gives us a simpler (lower-dimensional) representation and often leads to a view of the data that is more amenable to under- standing the big picture. This is because small variations about a particular point or direction are usually caused by noise, while large variations are caused by meaningful things. By eliminating small variations and focusing on the large ones, we capture the main information content. (This could be why older people tend to be better at form- ing big-picture views: perhaps there are too few neurons in their heads to retain the huge amount of memory data they have accumulated over the years. Their brain per- forms dimensionality reduction.) This is the basic idea behind PCA and dimensionality reduction, which we saw in section 4.4. Variance—or its square root, standard deviation—measures how densely packed around the expected value the points in the distribution are: that is, the spread of the point distribution. Formally, the variance of a probability distribution is defined as follows: 5.7 Properties of distributions: Expected value, variance, and covariance 165 var (X) =   Ín i=1 (xi −휇)2 p (xi) ⇒for a discrete n point distribution ∫∞ x=−∞(x −휇)2 p (x) dx ⇒for a continuous distribution (5.13) By comparing equation 5.13 to equations 5.10 and 5.11, we see that the variance is the expected value of the distance (x −휇)2 of sample points x from the mean 휇. So if the more probable (more frequently occurring) sample points lie within a short distance of the mean, the variance is small, and vice versa. That is to say, the variance measures how tightly packed the points are around the mean. COVARIANCE: VARIANCE IN HIGHER DIMENSIONS Extending the notion of the expected value from the univariate case to the multivariate case was straightforward. In the univariate case, we take a probability-weighted average of a scalar quantity, x. The resulting expected value is a scalar, 휇= ∫∞ x=−∞x p (x) dx. In the multivariate case, we take the probability-weighted average of a vector quantity, ®x. The resulting expected value is a vector, ®휇= ∫ ®x∈D ®x p  ®x d®x. Extending the notion of variance to the multivariate case is not as straightforward. This is because we can traverse the multidimensional random vector’s domain (the space over which the vector is defined) in an infinite number of possible directions—think how many possible directions we can walk on a 2D plane—and the spread or packing density can be different for each direction. For instance, in figure 5.3b, the spread along the main diagonal is much larger than the spread in a perpendicular direction. The covariance of a multidimensional point distribution is a matrix that allows us to easily measure the spread or packing density in any desired direction. It also allows us to easily figure out the direction in which the maximum spread occurs and what that spread is. Consider a multivariate random variable X that takes vector values ®x. Let ˆl be an arbitrary direction (as always, we use overhead hats to denote unit-length vectors signifying directions) in which we want to measure the packing density of X. We discussed in sections 2.5.2 and 2.5.6 that the dot product of ®x in the direction ˆl (that is, ®xTˆl) is the projection or component (effective value) of x along ˆl. Thus the spread or packing density of the random vector ®x in direction ˆl is the same as the spread of the dot product (aka component or projection) ˆl T ®x. This projection ˆl T ®x is a scalar quantity: we can use the univariate formula to measure its variance. NOTE In this context, we can use ®xTˆl and ˆl T ®x interchangeably. The dot product is symmetric. The expected value of the projection is ®휇l = ∫ ®x∈D  ˆl T ®x  p  ®x d®x =ˆl T ∫ ®x∈D ®x p  ®x d®x =ˆl T ®휇 166 CHAPTER 5 Probability distributions in machine learning The variance is given by var  ˆl T ®x  = ∫ ®x∈D  ˆl T ®x −ˆl T ®휇 2 d®x Now, since the transpose of a scalar is the same scalar, we can write the square term within the integral as the product of the scalar ˆl T  ®x −®휇 and its transpose: var  ˆl T ®x  = ∫ ®x∈D  ˆl T ®x −ˆl T ®휇   ˆl T ®x −ˆl T ®휇 T d®x = ∫ ®x∈D ˆl T  ®x −®휇  ˆl T  ®x −®휇T d®x Using equation 2.10, var  ˆl T ®x  = ∫ ®x∈D ˆl T  ®x −®휇  ®x −®휇T  ˆl T T d®x = ∫ ®x∈D ˆl T  ®x −®휇  ®x −®휇T ˆl d®x Since ˆl is independent of ®x, we can take it out of the integral. Hence, var  ˆl T ®x  =ˆl T ©­ « ∫ ®x∈D  ®x −®휇  ®x −®휇T d®xª® ¬ ˆl =ˆl Tℂ(X)ˆl where ℂ(X) = ( Ín i=1  ®x −®휇  ®x −®휇T ⇒for discrete n point distributions ∫ ®x∈D  ®x −®휇  ®x −®휇T d®x ⇒for continuous distributions (5.14) For simplicity, we drop the X in parentheses and simply write ℂ(X) as ℂ. An equivalent way of looking at the covariance matrix of a d-dimensional random variable X taking vector values ®x is as follows: ℂ=  휎11 휎12 휎13 · · · 휎1d 휎21 휎22 휎23 · · · 휎2d ... 휎d1 휎d2 휎d3 · · · 휎dd  (5.15) where 휎i, j = (∫ xi ∈Di ∫ xj ∈Dj (xi −휇i)  xj −휇j  dxidxj ⇒for continuous distributions Ín i=1 Ín j=1 (xi −휇i)  xj −휇j  ⇒for discrete n point distributions is the co-variance of the ith and jth dimensions of the random vector ®x. ℂ(X) or ℂis the covariance matrix of the random variable X. A little thought reveals that equations 5.14 and 5.15 are equivalent. The following things are noteworthy: From equation 5.14, ℂis the sum of the products of d × 1 vectors  ®x −®휇 and their transpose  ®x −®휇T, 1 × d vectors. Hence, ℂis a d × d matrix. 5.8 Sampling from a distribution 167 This matrix is independent of the direction, ˆl, in which we are measuring the variance or spread. We can precompute ℂ; then, when we need to measure the variance in any direction ˆl, we can evaluate the quadratic form ˆl Tℂˆl to obtain the variance in that direction. Thus ℂis a generic property of the distribution, much like ®휇. ℂis called the covariance of the distribution. Covariance is the multivariate peer of the univariate entity variance. That covariance is the multivariate analog of variance is evident by comparing the expressions in equations 5.13 and 5.14. VARIANCE AND EXPECTED VALUE As outlined previously, the variance is the expected value of the distance (x −휇)2 of sample points x from the mean 휇. This can be easily seen by comparing equations 5.13, 5.10, and 5.11 and leads to the following formula (where we use the principle of the expected value of linear combinations): var (X) = 피  (X −휇)2 = 피  X2 −피(2휇X) + 피  휇2 Since 휇is a constant, we can take it out of the expected value (a special case of the principal of the expected value of linear combinations). Thus we get var (X) = 피  X2 −2휇피(X) + 휇2피(1) But 휇= 피(X). Also, the expected value of a constant is that constant. So, 피(1) = 1. Hence, var (X) = 피  X2 −2휇2 + 휇2피(1) = 피  X2 −휇2 or var (X) = 피  X2 −피(X)2 (5.16) 5.8 Sampling from a distribution Drawing a sample from the probability distribution of a random variable yields an arbitrary value from the set of possible values. If we draw many samples, the higher- probability values show up more often than lower-probability values. The sampled points form a cloud in the domain of possible values, and the region where the probabilities are higher is more densely populated than lower-probability regions. In other words, in a sample point cloud, higher-probability values are overrepresented. Thus a collection of sample points is often referred to as a sample point cloud. The hope, of course, is that the sample point cloud is a good representation of the entire population so that analyzing the points in the cloud will yield insights about the entire population. In univariate cases, the sample value is a scalar and represented by a point on the number line. In multivariate cases, the sample value is a vector and represented as a point in a higher-dimensional space. It is often useful to compute aggregate statistics (such as the mean and variance) to describe the population. If we know a distribution, we can use closed-form expressions to obtain these properties. Many standard distributions and closed-form equations for 168 CHAPTER 5 Probability distributions in machine learning obtaining their means and variance are discussed in section 5.9. But often, we don’t know the underlying distribution. Under those circumstances, the sample mean and sample variance can be used. Given a set of n samples X = ®x1, ®x2 · · · ®xn from any distribution, the sample mean and variance are computed as ®휇n = 1 n n Õ i=1 ®xi 휎2 n = 1 n n Õ i=1  ®xi −®휇n 2 In some situations, like Gaussian distributions (which we discuss shortly), it can be theoretically proved that the sample mean and variance are optimal (the best possible guesses of the true mean and variance, given the sampled data). Also, the sample mean approaches the true mean as the number of samples increases, and with enough samples, we get a pretty good approximation of the true mean. In the next subsection, we learn more about how much is “enough.” LAW OF LARGE NUMBERS: HOW MANY SAMPLES ARE ENOUGH? Informally speaking, the law of large numbers says that if we draw a large number of sample values from a probability distribution, their average should be close to the expected value of the distribution. In the limit, the average over an infinite number of samples will match the mean. In practice, we cannot draw an infinite number of samples, so there is no guarantee that the sample mean will coincide with the expected value (true mean) in real-life sampling. But if the number of samples is large, they will not be too different. This is not a matter of mere theory. Casinos design games where the probability of the house winning a bet against the guest is slightly higher than the probability of the guest winning. The expected value of the outcome is that the casino wins rather than the guest. Over the very large number of bets placed in a casino, this is exactly what happens—and that is why casinos make money on the whole, even though they may lose individual bets. How many samples is “a large number of samples?” Well, it is not defined precisely. But one thing is known: if the variance is larger, more samples need to be drawn to make the law of large numbers apply. Let’s illustrate this with an example. Consider a betting game. Suppose that the famous soccer club FC Barcelona, for unknown reasons, has agreed to play a very large number of matches against the Machine Learning Experts’ Soccer Club of Silicon Valley. We can place a bet of $100 on a team. If that team wins, we get back $200: that is, we make $100. If that team loses, we lose the bet: that is, we make –$100. The betting game is happening in a country where nobody knows anything about the reputations of these clubs. A bettor bets on FC Barcelona in the first game and wins $100. Based on this one observation, can the bettor say that by betting on Barcelona, they expect to win $100 every time? Obviously not. But suppose the bettor places 100 bets and wins $100 99 times and loses $100 once. Now the bettor can expect with some confidence that they will win $100 (or close to it) by 5.9 Some famous probability distributions 169 betting on Barcelona. Based on these observations, the sample mean winnings from a bet on FC Barcelona are 0.99 × (100) + 0.01 × (−100) = 98. The sample standard deviation is r .99 × (98 −100)2 + 0.01 × (98 −(−100))2 = 19.8997. Relative to the sample mean, the sample standard deviation is 19.8997 98 = 0.203. Next, consider the same game, except now FC Barcelona is playing the Real Madrid football club. Since the two teams are evenly matched (the theoretical win proba- bility of Barcelona is 0.5), the results are no longer one-sided. Suppose that after 100 games, FC Barcelona has won 60 times and Real Madrid has won 40 times. The sample mean winnings on a Barcelona bet are 0.6 × (100) + 0.4 × (−100) = 20. The sample standard deviation is r .6 × (20 −100)2 + 0.4 × (20 −(−100))2 = 97.9795. Rel- ative to the sample mean, the sample standard deviation is 97.9795 20 = 4.89897. This is a much larger number than the previous 0.203. In this case, even after 100 trials, a bettor cannot be very confident in predicting that the expected win is the sample mean, $20. The overall intuition is as follows: If we take a sufficiently large number of samples, their average is close to the expected value. The exact definition of what constitutes a “sufficiently large” number of samples is not known. However, the larger the variance (relative to the mean), the more samples are needed. 5.9 Some famous probability distributions In this section, we introduce some probability distributions and density functions often used in deep learning. We will use PyTorch code snippets to demonstrate how to set up, sample, and compute properties like expected values, variance/covariance, and so on for each distribution. Note the following: In the code snippets, for every distribution, we evaluate the probability using – A PyTorch distributions function call – A raw evaluation from the formula (to understand the math) Both should yield the same result. In practice, you should use the PyTorch distributions function call instead of the raw formula. In the code snippets, for every distribution, – We evaluate the theoretical mean and variance using a PyTorch distributions function call. – We evaluate the sample mean and variance. When the sample set is large enough, the sample mean and theoretical mean should be close. Ditto for variance. NOTE Fully functional code for these distributions, executable via Jupyter Note- book, can be found at http://mng.bz/8NVg. 170 CHAPTER 5 Probability distributions in machine learning Another point to remember: In machine learning, we often work with the logarithm of the probability. Since the popular distributions are exponential, this leads to simpler computations. With that, let’s dive into the probability distributions. 5.9.1 Uniform random distributions Consider a continuous random variable x that can take any value from a fixed compact range, say [a, b], with equal probability, while the probability of x taking a value outside the range is zero. The corresponding p (x) is a uniform probability distribution. Formally stated, p (x) =   1 b −a if x ∈[a, b] 0 otherwise (5.17) Equation 5.17 means p (x) is constant, 1 b−a, for x between a and b and zero for other values of x. Note how the value of the constant is cleverly chosen to make the total area under the curve 1. This equation is depicted graphically in figure 5.4, and listing 5.1 shows the PyTorch code for the log probability of a univariate uniform random distribution. Uniform probability density function Figure 5.4 Univariate (single-variable) uniform random probability density function. Probability p (x) is constant, 0.05, in the interval [−10, 10] and zero everywhere outside the interval. Thus it depicts equa- tion 5.17 with b = 10, a = −10. The area under the curve is the area of the shaded rectangle of width 20 and height 0.05, 20 × 0.05 = 1. The thin rectangle depicts an infinitesimally small interval corresponding to event E = {x ≤X < x + 휹x}. If we draw a random sample x from this distribution, the probability that the value of the sample is between, say, 4 and 4 + 휹x, with 휹x −→0, is p (4) = 0.05. The probability that the value of the sample is between, say, 15 and 15 + 휹x, with 휹x −→0, is p (15) = 0. 5.9 Some famous probability distributions 171 Listing 5.1 Log probability of a univariate uniform random distribution from torch.distributions import Uniform Imports a PyTorch uniform distribution a = torch.tensor([1.0], dtype=torch.float) Sets the distribution parameters b = torch.tensor([5.0], dtype=torch.float) ufm_dist = Uniform(a, b) Instantiates a uniform distribution object X = torch.tensor([2.0], dtype=torch.float) Instantiates a single-point test dataset def raw_eval(X, a, b): return torch.log(1 / (b - a)) log_prob = ufm_dist.log_prob(X) Evaluates the probability using PyTorch raw_eval_log_prob = raw_eval(X, a, b) Evaluates the probability using the formula assert torch.isclose(log_prob, raw_eval_log_prob, atol=1e-4) Asserts that the probabilities match NOTE Fully functional code for the uniform distribution, executable via Jupyter Notebook, can be found at http://mng.bz/E2Jr. EXPECTED VALUE OF A UNIFORM DISTRIBUTION We do this for the univariate case, although the computations can be easily extended to the multivariate case. Substituting the probability density function from equation 5.17 into the expression for the expected value for a continuous variable, equation 5.10, 피uni f orm (X) = ∞ ∫ −∞ x p (x) dx = b ∫ a x  1 b −a  dx = 1 (b −a) b ∫ a x dx = 1 (b −a) ⌈x2 2 ⌉b a =  b2 −a2 2 (b −a) = (a + b) 2 (5.18) NOTE The limits of integration changed because p (x) is zero outside the interval [a, b]. Overall, equation 5.18 agrees with our intuition. The expected value is right in the middle of the uniform interval, as shown in figure 5.5. VARIANCE OF A UNIFORM DISTRIBUTION If we look at figure 5.5, it is intuitively obvious that the packing density of the samples is related to the width of the rectangle. The smaller the width, the tighter the packing and the smaller the variance, and vice versa. Let’s see if the math supports that intuition: varuni f orm (x) = ∞ ∫ x=−∞ (x −휇)2 p (x) dx = 172 CHAPTER 5 Probability distributions in machine learning = ∞ ∫ x=−∞  x −a + b 2 2 1 (b −a) dx = = (b −a)2 12 (5.19) Uniform probability density function Figure 5.5 Univariate (single-variable) uniform random probability density function. The solid line in the middle indicates the expected value. Interactive visualizations (where you can change the parameters and observe how the graph changes as a result) can be found at http://mng.bz/E2Jr. Figure 5.5 shows that the variance in equation 5.19 is proportional to the square of the width of the rectangle: that is, (b −a)2. Here is the PyTorch code for the mean and variance of a uniform random distribution. Listing 5.2 Mean and variance of a uniform random distribution num_samples = 100000 Number of sample points samples 100000 × 1 tensor = ufm_dist.sample([num_samples]) Obtains samples from ufm_dist instantiated in listing 5.1 sample_mean = samples.mean() Sample mean dist_mean = ufm_dist.mean Mean via PyTorch function assert torch.isclose(sample_mean, dist_mean, atol=0.2) sample_var = ufm_dist.sample([num_samples]).var() Sample variance dist_var = ufm_dist.variance Variance via PyTorch function assert torch.isclose(sample_var, dist_var, atol=0.2) 5.9 Some famous probability distributions 173 MULTIVARIATE UNIFORM DISTRIBUTION Uniform distributions also can be multivariate. In that case, the random variable is a vector, ®x (not a single value, but a sequence of values). Its domain is a multidimensional volume instead of the X-axis, and the graph has more than two dimensions. For example, this is a two-variable uniform random distribution: p (x, y) = ( 1 (b1−a1) (b2−a2) if (x, y) ∈[a1, b1] × [a2, b2] 0 otherwise (5.20) Here, (x, y) ∈[a1, b1] × [a2, b2] indicates a rectangular domain on the two-dimensional XY plane where x lies between a1 and b1 and y lies between a2 and b2. Equation 5.20 is shown graphically in figure 5.6. In the general multidimensional case, p  ®x = ( 1 V if ®x ∈D 0 otherwise (5.21) Figure 5.6 Bivariate uniform random probability density function. The probability p (x, y) is constant, 0.0025, in the domain (x, y) ∈ [−10, 10] × [−10, 10] and zero ev- erywhere outside the interval. The volume of the box of width 20 × 20 and height 0.0025, 20 ∗20 ∗0.0025 = 1. Here, V is the volume of the hyperdimensional box with base D. Equation 5.21 means p  ®x is constant for ®x in the domain D and zero for other values of x. When nonzero, it has a constant value, the inverse of the volume V : this makes the total volume under the density function 1. 5.9.2 Gaussian (normal) distribution This is probably the most famous distribution in the world. Let’s consider, one more time, the weights of adult residents of Statsville. If Statsville is anything like a real city, the likeliest weight is around 75 kg: the largest percentage of the population will weigh this much. Weights near this value (say 70 or 80 kg) will also be quite likely, although slightly less likely than 75 kg. Weights further away from 75 kg are still less likely, and so on. The further we go from 75 kg, the lower the percentage of the population with that 174 CHAPTER 5 Probability distributions in machine learning weight. Outlier values like 40 and 110 kg are unlikely. Informally speaking, a Gaussian probability density function looks like a bell-shaped curve. The central value has the highest probability. The probability falls gradually as we move away from the center. In theory, however, it never disappears completely (the function p (x) never becomes equal to 0), although it becomes almost zero for all practical purposes. This behavior is described in mathematics as asymptotically approaching zero. Figure 5.7 shows a Gaussian probability density function. Formally, p (x) = 1 √ 2휋휎 e −(x−휇)2 2휎2 (5.22) Gaussian distribution Figure 5.7 Univariate Gaussian random probability density function, 흁= 0 and 흈= 4. The bell-shaped curve is highest at the center and decreases more and more as we move away from the center, approach- ing zero asymptotically. The value x = 0 has the highest probability, corresponding to the center of the probability density function. Note that the curve is symmetric. Thus, for instance, the probability of a random sample being in the vicinity of −5 is the same as that of 5 (0.04): that is, p (−5) = p (5) = 0.04. An interactive visualization (where you can change the parameters and observe how the graph changes as a result) can be found at http://mng.bz/NYJX. Here, 휇and 휎are parameters; 휇corresponds to the center (for example, in figure 5.7, 휇= 0). The parameter 휎controls the width of the bell. A larger 휎implies that p (x) falls more slowly as we move away from the center. The Gaussian (normal) probability density function is so popular that we have a special symbol for it: N (x, 휇, 휎2). It can be proved (but doing so is exceedingly tedious, so we will skip the proof here) that ∞ ∫ x=−∞ N (x; 휇, 휎2)dx = ∞ ∫ x=−∞ 1 √ 2휋휎 e −(x−휇)2 2휎2 dx = 1 5.9 Some famous probability distributions 175 This establishes that N (x; 휇, 휎2) is a true probability (satisfying the sum rule in equa- tion 5.7). Listing 5.3 Log probability of a univariate normal distribution from torch.distributions import Normal Imports a PyTorch univariate normal distribution mu = torch.tensor([0.0], dtype=torch.float) Sets the distribution params sigma = torch.tensor([5.0], dtype=torch.float) uvn_dist = Normal(mu, sigma) Instantiates a univariate normal distribution object X = torch.tensor([0.0], dtype=torch.float) Instantiates a single-point test dataset def raw_eval(X, mu, sigma): K = 1 / (math.sqrt(2 * math.pi) * sigma) E = math.exp( -1 * (X - mu) ** 2 * (1 / (2 * sigma ** 2))) return math.log(K * E) log_prob = uvn_dist.log_prob(X) Evaluates the probability using PyTorch raw_eval_log_prob = raw_eval(X, mu, sigma) Evaluates the probability using the formula assert log_prob == raw_eval_log_prob Asserts that the probabilities match NOTE Fully functional code for this normal distribution, executable via Jupyter Notebook, can be found at http://mng.bz/NYJX. MULTIVARIATE GAUSSIAN A Gaussian distribution can also be multivariate. Then the random variable x is a vector ®x, as usual. The parameter 휇also becomes a vector ®휇, and the parameter 휎becomes a matrix 횺. As in the univariate case, these parameters are related to the expected value and variance. The Gaussian multivariate probability distribution function is p  ®x = N  ®x; ®휇, 횺 = 1 (2휋det 횺) 1 2 e−1 2 (®x−®휇)T 횺−1(®x−®휇) (5.23) Equation 5.23 describes the probability density function for the random vector ®x to lie within the infinitesimally small volume with dimensions 훿®x around the point ®x. (Imagine a tiny box (cuboid) whose sides are successive elements of 훿®x, with the top-left corner of the box at ®x.) The vector ®휇and the matrix 횺are parameters. As in the univariate case, ®휇corresponds to the most likely value of the random vector. Figure 5.8 shows the Gaussian (normal) distribution with two variables in three dimensions. The shape of the base of the bell is controlled by the parameter 횺. Listing 5.4 Log probability of a multivariate normal distribution from torch.distributions import MultivariateNormal Imports a PyTorch multivariate normal distribution mu = torch.tensor([0.0, 0.0], dtype=torch.float) Sets the distribution params 176 CHAPTER 5 Probability distributions in machine learning C = torch.tensor([[5.0, 0.0], [0.0, 5.0]], dtype=torch.float) mvn_dist = MultivariateNormal(mu, C) Instantiates a multivariate normal distribution object X = torch.tensor([0.0, 0.0], dtype=torch.float) Instantiates a single point test dataset def raw_eval(X, mu, C): K = (1 / (2 * math.pi * math.sqrt(C.det()))) X_minus_mu = (X - mu).reshape(-1, 1) E1 = torch.matmul(X_minus_mu.T, C.inverse()) E = math.exp(-1 / 2. * torch.matmul(E1, X_minus_mu)) return math.log(K * E) log_prob = mvn_dist.log_prob(X) Evaluates the probability using PyTorch raw_eval_log_prob = raw_eval(X, mu, C) Evaluates the probability using the formula assert log_prob == raw_eval_log_prob Asserts that the probabilities match Figure 5.8 Bivariate Gaussian random probability density function. It is a bell- shaped surface: highest at the center and decreasing as we move away from the center, approaching zero asymptot- ically. x = 0, y = 0 has the highest prob- ability, corresponding to the center of the probability density function. The bell has a circular base, and the 횺matrix is a scalar multiple of the identity matrix 핀. An interactive visualization (where you can change the parameters and observe how the graph changes as a result) can be found at http://mng.bz/NYJX. EXPECTED VALUE OF A GAUSSIAN DISTRIBUTION Substituting the probability density function from equation 5.22 into the expression for the expected value of a continuous variable, equation 5.10, we get 피gaussian (X) = ∞ ∫ −∞ x 1 √ 2휋휎 e −(x−휇)2 2휎2 dx = 1 √휋 ∞ ∫ −∞ (x −휇) √ 2휎 e −(x−휇)2 2휎2 dx + 휇 ∞ ∫ −∞ 1 √ 2휋휎 e −(x−휇)2 2휎2 dx Substituting y = −(x−휇) √ 2휎 피gaussian (X) = √ 2휎 √휋 ∞ ∫ −∞ ye−y2dy + 휇 ∞ ∫ −∞ p (x) dx 5.9 Some famous probability distributions 177 Substituting u = y2 and using equation 5.6 피gaussian (X) = √ 2휎 2√휋 ∞ ∫ ∞ e−udu + 휇 Note that the limits of the integral in the first term are identical. This is because u = y2 →∞whether y →∞or y →−∞. But an integral with the same lower and upper limits is zero. Thus the first term is zero. Hence, 피gaussian (X) = 휇 (5.24) Intuitively, this makes perfect sense. The probability density p (x) = 1 √ 2휋휎e −(x−휇)2 2휎2 peaks (maximizes) at x = 휇. At this x, the exponent becomes zero, which makes the term e −(x−휇)2 2휎2 attain its maximum possible value of 1. This is right in the middle of the bell, as shown in figure 5.9. And, of course, the expected value coincides with the middle value if the density is symmetric and peaks in the middle. Analogously, in the multivariate case, the Gaussian multidimensional random variable X that takes vector values ®x in the d-dimensional domain ℝd (that is, ®x ∈ℝd) has an expected value 피gaussian (X) = ®휇 (5.25) Gaussian distribution Figure 5.9 Univariate (single-variable) normal (Gaussian) random probability density function, 흁= 0 and 흈= 4. The solid line in the middle indicates the expected value. VARIANCE OF A GAUSSIAN DISTRIBUTION The variance of the Gaussian distribution is obtained by substituting equation 5.22 in the integral form of equation 5.13. The mathematical derivation is shown in the book’s appendix; here we only state the result. 178 CHAPTER 5 Probability distributions in machine learning The variance of a Gaussian distribution with probability density function p (x) = 1 √ 2휋휎e −(x−휇)2 2휎2 is 휎2, and the standard deviation is the square root of that (휎). This makes intuitive sense. 휎appears in the denominator of a negative exponent in the expression for the probability density function p (x) = 1 √ 2휋휎e −(x−휇)2 2휎2 . As such, p (x) is an increasing function of 휎: that is, for a given x and 휇, a larger 휎implies a larger p (x). In other words, a larger 휎implies that the probability decays more slowly as we move away from the center: a fatter bell curve, a bigger spread, and hence a larger variance. Figure 5.10 depicts this. (a) Different 휇s but the same 휎s. (b) The same 휇s but different 휎s. Figure 5.10 Gaussian densities with varying 흁s and 흈s. Changing 흁shifts the center of the curve. A larger 흈(variance) implies a fatter bell ⇒more spread. Note that fatter curves are smaller in height as the total area under the curve must be 1. Listing 5.5 Mean and variance of a univariate Gaussian num_samples = 100000 Number of sample points samples 100000 × 1 tensor = uvn_dist.sample([num_samples]) Obtains samples from uvn_dist instantiated in listing 5.3 sample_mean = samples.mean() Sample mean dist_mean = uvn_dist.mean Mean via PyTorch function assert torch.isclose(sample_mean, dist_mean, atol=0.1) sample_var = uvn_dist.sample([num_samples]).var() Sample variance dist_var = uvn_dist.variance Variance via PyTorch function assert torch.isclose(sample_var, dist_var, atol=0.1) COVARIANCE OF A MULTIVARIATE GAUSSIAN DISTRIBUTION AND GEOMETRY OF THE BELL SURFACE Comparing equation 5.22 for a univariate Gaussian probability density with equation 5.23 for a multivariate Gaussian probability density, we intuitively feel that the matrix 횺 5.9 Some famous probability distributions 179 is the multivariate peer of the univariate variance 휎2. Indeed it is. Formally, for a multi- variate Gaussian random variable with a probability distribution given in equation 5.23, the covariance matrix is given by the equation ℂgaussian (X) = 횺 (5.26) As shown in table 5.11, Σ regulates the shape of the base of the bell-shaped probability density function. It is easy to see that the exponent in equation 5.23 is a quadratic form (introduced in section 4.2). As such, it defines a hyper-ellipse, as shown in figure 5.11 and section 2.17. All the properties of quadratic forms and hyper-ellipses apply here. Listing 5.6 Mean and variance of a multivariate normal distribution num_samples = 100000 Number of sample points samples 100000 × 1 tensor = mvn_dist.sample([num_samples]) Obtains samples from mvn_dist instantiated in listing 5.4 sample_mean = samples.mean() Sample mean dist_mean = mvn_dist.mean Mean via PyTorch function assert torch.allclose(sample_mean, dist_mean, atol=1e-1) sample_var = mvn_dist.sample([num_samples]).var() Sample variance dist_var = mvn_dist.variance Variance via PyTorch function assert torch.allclose(sample_var, dist_var, atol=1e-1) Let’s look at the geometric properties of the Gaussian covariance matrix Σ. Consider a 2D version of equation 5.23. We rewrite ®x =  x y  and ®휇=  휇x 휇y  —2D vectors both. Also Σ−1 =  휎11 휎12 휎21 휎22  —a 2 × 2 matrix. The probability density function from equation 5.23 becomes p (x, y) = N  x, y; ®휇, 횺 = 1 (2휋det 횺) 1 2 e−1 2  휎11x2+(휎11+휎12)xy+휎22y2 (5.27) (Use what you learned in chapter 3 to satisfy yourself that equation 5.27 is a 2D analog of equation 5.23.) If we plot the surface p (x, y) against (x, y), it looks like a bell in 3D space. The shape of the bell’s base, on the (x, y) plane, is governed by the 2 × 2 matrix 횺. In particular, If 횺is a diagonal matrix with equal diagonal elements, the bell is symmetric in all directions, and its base is circular. If 횺is a diagonal matrix with unequal diagonal elements, the base of the bell is elliptical. The axes of the ellipse are aligned with the coordinate axes. 180 CHAPTER 5 Probability distributions in machine learning For a general 횺matrix, the base of the bell is elliptical. The axes of the ellipse are not necessarily aligned with the coordinate axes. The eigenvectors of 횺yield the axes of the elliptical base of the bell surface. Now, if we sample the distribution from equation 5.27, we get a set of points (x, y) on the base plane of the surface shown in figure 5.8. The taller the z coordinate (depicting p (x, y)) of the surface at a point (x, y), the greater its probability of being selected in the sampling. If we draw a large number of samples, the corresponding point cloud will look more or less like the base of the bell surface. Figure 5.11 shows various point clouds formed by sampling Gaussian distributions with different covariance matrices 횺. Compare it to figure 5.10. GEOMETRY OF SAMPLED POINT CLOUDS: COVARIANCE AND DIRECTION OF MAXIMUM OR MINIMUM SPREAD We have seen that if a multivariate distribution has a covariance matrix ℂ, its variance (spread) in any specific direction ˆl is ˆl Tℂˆl. What is the direction of maximum spread? Asking this is the same as asking “What direction ˆl maximizes the quadratic form ˆl Tℂˆl?” In section 4.2, we saw that a quadratic form like this is maximized or minimized when the direction ˆl is aligned with the eigenvector corresponding to the maximum or minimum eigenvalue of the matrix ℂ. Thus, the maximum spread of a distribution occurs along the eigenvector of the covariance matrix corresponding to its maximum eigenvalue. This led to the PCA technique in section 4.4. Next, we discuss the covariance of the Gaussian distribution and geometry of the point cloud formed by sampling a multivariate Gaussian a large number of times. You may want to take a look at figure 5.11, which shows various point clouds formed by sampling Gaussian distributions with different covariance matrices 횺. MULTIVARIATE GAUSSIAN POINT CLOUDS AND HYPER-ELLIPSES The numerator of the exponential term in equation 5.23,  ®x −®휇T 횺−1  ®x −®휇, is a quadratic form as we discussed in section 4.2. It should also remind you of the hyper- ellipse we looked at in section 2.17, equation 2.33, and equation 4.1. Now consider the plot of p  ®x against ®x. This is a hypersurface in n + 1-dimensional space, where the random variable ®x is n-dimensional. For instance, if the random Gaussian variable ®x is 2D, the  ®x, p  ®x plot in 3D is as shown in figure 5.8. It is a bell- shaped surface. The hyper-ellipse corresponding to the quadratic form in the numerator of the probability density function in equation 5.23 governs the shape and size of the base of this bell. If the matrix 횺is diagonal (with equal diagonal elements), the base is circular—this is the special case shown in figure 5.8. Otherwise, the base of the bell is elliptic. The eigenvectors of the covariance matrix 횺correspond to the directions of the axes of the elliptical base. The eigenvalues correspond to the lengths of the axes. 5.9.3 Binomial distribution Suppose we have a database containing photos of people. Also, suppose we know that 20% of the photos contain a celebrity and the remaining 80% do not. If we randomly 5.9 Some famous probability distributions 181 (a) Σ = " 5 0 0 5 # (b) Σ = " 2.75 2.25 2.25 2.75 # (c) Σ = " 5 0 0 0.5 # (d) Σ = " 2.75 −2.25 −2.25 2.75 # Figure 5.11 Point clouds formed by sampling multivariate Gaussians with the same ®흁= [0, 0]T but different 횺s. These point clouds correspond to the bases of the bell curves for multivariate Gaussian prob- ability densities. All the point clouds except (a) may be replaced by a univariate Gaussian after rotation to align the coordinate axes with the eigenvectors of 횺(dimensionality reduction). See sections 4.4, 4.5, and 4.6 for details. Interactive contour plots for the base of the bell curve can be found at http://mng.bz/NYJX. 182 CHAPTER 5 Probability distributions in machine learning select three photos from this database, what is the probability that two of them contain a celebrity? This is the kind of problem the binomial distribution deals with. In a computer vision-centric machine learning setting, we would probably inspect the selected photos and try to predict whether they contained a celebrity. But for now, let’s restrict ourselves to the simpler task of blindly predicting the chances from aggregate statistics. If we select a single photo, the probability of it containing a celebrity is 휋= 0.2. NOTE This has nothing to do with the natural number 휋denoting the ratio of the circumference to the diameter of a circle. We are just reusing the symbol 휋 following popular convention. The probability of the photo not containing a celebrity is 1 −휋= 0.8. From that, we can compute the probability of, say, the first two sampled photos containing a celebrity but the last one containing a non-celebrity: that is, the event {S, S, F} (where S de- notes success in finding a celebrity and F denotes failure in finding a celebrity). Using equation 5.4, the probability of the event {S, S, F} is 휋× 휋× (1 −휋) = 0.2 × 0.2 × 0.8. However, many other combinations are also possible. All the possible combinations that can occur in three trials are shown in table 5.8. In the table, event ids 3, 5, and 6 correspond to two successes and one failure. They occur with probabilities 0.8 × 0.2 × 0.2, 0.2 × 0.8 × 0.2, and 0.2 × 0.2 × 0.8, respectively. If any one of them occurs, we have two celebrity photos in three trials. Thus, using equation 5.3, the overall probability of selecting two celebrity photos in three trials is the sum of these event probabilities: 0.8 × 0.2 × 0.2 + 0.2 × 0.8 × 0.2 + 0.2 × 0.2 × 0.8 = 0.096. Table 5.8 All possible combinations of three trials Event Id Event Probability 0 {F, F, F } (1 −휋) × (1 −휋) × (1 −휋) = 0.8 × 0.8 × 0.8 1 {F, F, S} (1 −휋) × (1 −휋) × 휋= 0.8 × 0.8 × 0.2 2 {F, S, F } (1 −휋) × 휋× (1 −휋) = 0.8 × 0.2 × 0.8 3 {F, S, S} (1 −휋) × 휋× 휋= 0.8 × 0.2 × 0.2 4 {S, F, F } 휋× (1 −휋) × (1 −휋) = 0.2 × 0.8 × 0.8 5 {S, F, S} 휋× (1 −휋) × 휋= 0.2 × 0.8 × 0.2 6 {S, S, F } 휋× 휋× (1 −휋) = 0.2 × 0.2 × 0.8 7 {S, S, S} 휋× 휋× 휋= 0.2 × 0.2 × 0.2 In the general case, with more than three trials, it would be impossibly tedious to enumerate all the possible combinations of success and failure that can occur in a set of n trials. Fortunately, we can derive a formula. But before doing that, let’s state the task of a binomial distribution in more general terms: Given a process that has a binary outcome (success or failure) in any given trial, and given that the probability of success in a trial is a known constant (say, 휋), a 5.9 Some famous probability distributions 183 binomial distribution deals with the probability of observing k successes in n trials of the process. Imagine events with n successive items, where each individual item can be either S or F. Table 5.8 shows such events with n = 3. Each item has two possible values (S or F), and there are n items. Hence, altogether there can be 2 × 2 × · · · 2 = 2n possible events. We are only interested in events with k occurrences of S (and therefore (n −k) occurrences of F). How many of the n events are like that? Well, asking this is the same as asking how many ways we can choose k slots from a total of n possible slots. Another way to pose the same question is, “How many different orderings of n items exist, where each item is either S or F and the total count of S is k?” The answer, from combination theory, is n k  = n! k! (n −k)! Each of these events has a probability of 휋k × (1 −휋)n−k. Hence, the overall probability of k successes in n trials is  n k 휋k × (1 −휋)n−k. Formally, if X is a random variable denoting the number of successes in n trials, with the probability of success in any single trial being some constant value 휋, p (X = k) = n k  휋k × (1 −휋)n−k (5.28) What values can k take? Of course, we cannot have more than n successes in n trials; therefore, the maximum possible value of k is n. All integer values between 0 and n are possible: k=n Õ k=0 p (X = k) = k=n Õ k=0 n k  휋k × (1 −휋)n−k The right-hand side is an expression for the generic term in the famous binomial expansion of (a + b)n with a = 휋and b = 1 −휋. Hence, we get k=n Õ k=0 p (X = k) = k=n Õ k=0 n k  휋k × (1 −휋)n−k = (휋+ 1 −휋)n = 1n = 1 (5.29) This agrees with intuition, since given n, k can only take values 0, 1, · · · , n; the sum of the probabilities on the left-hand side of equation 5.29 corresponds to a certain event with probability 1. Also, plugging n = 3, k = 2, and 휋= 0.2 into equation 5.28 yields 3! 2! 1! (0.2)2 (0.8)3−2 = 0.096: exactly what we get from explicit enumeration. Listing 5.7 Log probability of a binomial distribution from torch.distributions import Binomial Imports a PyTorch binomial distribution num_trials = 3 Sets the distribution params p = torch.tensor([0.2], dtype=torch.float) binom_dist = Binomial(num_trials, probs=p) Instantiates a binomial distribution object 184 CHAPTER 5 Probability distributions in machine learning X = torch.tensor([1], dtype=torch.float) Instantiates a single point test dataset def nCk(n, k): f = math.factorial return f(n) * 1. / (f(k) * f(n-k)) def raw_eval(X, n, p): result = nCk(n, X) * (p ** X) * (1 - p) ** (n - X) return torch.log(result) log_prob = binom_dist.log_prob(X) Evaluates the probability using PyTorch raw_eval_log_prob = raw_eval(X, num_trials, p) Evaluates the probability using formula assert torch.isclose(log_prob, raw_eval_log_prob, atol=1e-4) Asserts that the probabilities match NOTE Fully functional code for the binomial distribution, executable via Jupyter Notebook, can be found at http://mng.bz/DRJ0. EXPECTED VALUE OF A BINOMIAL DISTRIBUTION We have seen that the binomial distribution deals with a random variable X that depicts the number of successes in n trials, where the probability of success in a given trial is a constant 휋(again, this has nothing to do with the 휋denoting the ratio of the circumference to the diameter of a circle). This X can take any integer value 0 to n. Hence, 피(X) = k=n Õ k=0 k p (X = k) = k=n Õ k=0 k n k  휋k (1 −휋)n−k = k=n Õ k=0 k n! k! (n −k)! 휋k × (1 −휋)n−k We can drop the first term, which has the multiplier k = 0. Thus we get 피(X) = k=n Õ k=1 n! (k −1)! (n −k)! 휋k × (1 −휋)n−k We can factor n! = n (n −1)! and 휋k = 휋휋k−1. Also, n −k = (n −1) −(k −1). This gives us 피(X) = k=n Õ k=1 n (n −1)! (k −1)! ((n −1) −(k −1))! 휋휋k−1 × (1 −휋)n−k Substituting j for k −1 and m for n −1, we get 피(X) = n휋 j=m Õ j=0 m! j! (m −j)! 휋j × (1 −휋)m−j (5.30) The quantity within the summation is similar to that in equation 5.29 (should sum to 1). This leaves us with 피binomial (X) = n휋 (5.31) Equation 5.31 says that if 휋is the probability of success in a single trial, then the expected number of successes in n trials is n 휋. For instance, if the probability of success in a 5.9 Some famous probability distributions 185 single trial is 0.2, then the expected number of successes in 100 trials is 20—which is almost intuitively obvious. VARIANCE OF A BINOMIAL DISTRIBUTION The variance of a binomial random variable depicting the number of successes in n trials where the probability of success in a given trial is a constant 휋is varbinomial = n휋(1 −휋) (5.32) The proof follows the same lines as that of the expected value. Listing 5.8 Mean and variance of a binomial distribution num_samples = 100000 Number of sample points samples 100000 × 1 tensor = binom_dist.sample([num_samples]) Obtains samples from the binom_dist instantiated in listing 5.7 sample_mean = samples.mean() Sample mean dist_mean = binom_dist.mean Mean via PyTorch function assert torch.isclose(sample_mean, dist_mean, atol=0.2) sample_var = binom_dist.sample([num_samples]).var() Sample variance dist_var = binom_dist.variance Variance via PyTorch function assert torch.isclose(sample_var, dist_var, atol=0.2) 5.9.4 Multinomial distribution Consider again the example problem we discussed in section 5.9.3. We have a database of photos of people. But instead of two classes, celebrity and non-celebrity, we have four classes: Photos of Albert Einstein (class 1): 10% of the photos Photos of Marie Curie (class 2): 42% of the photos Photos of Carl Friedrich Gauss (class 3): 4% of the photos Other photos (class 4): 44% of the photos If we randomly select a photo from the database (that is, perform a random trial), The probability of selecting class 1 (picking an Einstein photo) is 휋1 = 0.1. The probability of selecting class 2 (picking a Marie Curie photo) is 휋2 = 0.42. The probability of selecting class 3 (picking a Gauss photo) is 휋3 = 0.04. The probability of selecting class 4 (picking a photo of none of the above) is 휋4 = 0.44. Notice that 휋1 + 휋2 + 휋3 + 휋4 = 1. This is because the classes are mutually exclusive and exhaustive, so exactly one of these classes must occur in every trial. Given all this, let’s ask the question: “What is the probability that in a set of 10 random trials, class 1 occurs 1 time, class 2 occurs 2 times, class 3 occurs 1 time, and class 4 occurs the remaining 6 times?” This is the kind of problem multinomial distributions deal with. 186 CHAPTER 5 Probability distributions in machine learning Formally, Let C1, C2, · · · , Cm be a set of m classes such that in any random trial, exactly one of these classes will be selected with the respective probabilities 휋1, 휋2, · · · , 휋m. Let X1, X2, · · · , Xm be a set of random variables. Xi corresponds to the number of occurrences of class Ci in a set of n trials. Then the multinomial probability function depicting the probability that class C1 is selected k1 times, class C2 is selected k2 times, and class C3 is selected km times is p (X1 = k1, X2 = k2, · · · , Xm = km) = n! k1! k2! · · · , km! 휋k1 1 휋k2 2 · · · 휋km m (5.33) where m Õ i=1 ki = n m Õ i=1 휋i = 1 We can verify that for m = 2, this becomes the binomial distribution (equation 5.28). A noteworthy point is that if we look at any one of the m variables X1, X2, · · · , Xm individually, its distribution is binomial. Let’s work out the final probability for the example we started with: the probability that in a set of 10 random trials, class 1 occurs 1 time, class 2 occurs 2 times, class 3 occurs 1 time, and class 4 occurs the remaining 6 times. This is p (X1 = 1, X2 = 2, X3 = 1, X4 = 6) = 10! 1! 2! 1! 6! (0.1)1 (0.42)2 (0.04)1 (0.44)6 = 0.0129 Listing 5.9 Log probability of a multinomial distribution from torch.distributions import Multinomial Imports a PyTorch multinomial distribution num_trials = 10 Sets the distribution params P = torch.tensor([0.1, 0.42, 0.04, 0.44], dtype=torch.float) multinom_dist = Multinomial(num_trials, probs=P) Instantiates a multinomial dist object X = torch.tensor([1, 2, 1, 6], dtype=torch.float) Instantiates a single-point test dataset def raw_eval(X, n, P): f = math.factorial result = f(n) for p, x in zip(P, X): result *= (p ** x) / f(x) return math.log(result) log_prob = multinom_dist.log_prob(X) Evaluates the probability using PyTorch raw_eval_log_prob = raw_eval(X, num_trials, P) Evaluates the probability using formula assert torch.isclose(log_prob, raw_eval_log_prob, atol=1e-4) Asserts that the probabilities match 5.9 Some famous probability distributions 187 NOTE Fully functional code for the multinomial distribution, executable via Jupyter Notebook, can be found at http://mng.bz/l1gz. EXPECTED VALUE OF A MULTINOMIAL DISTRIBUTION Each of the random variables X1, X2, · · · , Xm individually subscribes to a binomial distribution. Accordingly, following the binomial distribution expected value formula from equation 5.31, 피multinomial (Xi) = n휋i (5.34) VARIANCE OF A MULTINOMIAL DISTRIBUTION The variation of the random variables X1, X2, · · · , Xm, following the binomial distribution variance formula from equation 5.32, is varmultinomial (Xi) = n휋i (1 −휋i) (5.35) If each of the X1, X2, · · · , Xm is a scalar, then we can think of a random vector X =  X1 X2 ... Xm  . The expected value of such a random variable is 피multinomial (X) =  n휋1 n휋1 ... n휋m  and the covariance is ℂmultinomial (X) =  휎11 휎12 · · · 휎1m 휎21 휎22 · · · 휎2m · · · 휎m1 휎m2 · · · 휎mm  (5.36) where the diagonal terms are like the binomial variance 휎ii = n휋i (1 −휋i) ∀i ∈[1, m] and the off-diagonal terms are 휎i j = −n휋i휋j ∀(i, j) ∈[1, m] × [1, m]. The cross-covariance terms in the diagonal are negative because an increase in one element implies a decrease in the others. Listing 5.10 Mean and variance of a multinomial distribution num_samples = 100000 Number of sample points samples 100000 × 1 tensor = multinom_dist.sample([num_samples]) Obtains samples from the multinom_dist instantiated in listing 5.9 sample_mean = samples.mean(axis=0) Sample mean 188 CHAPTER 5 Probability distributions in machine learning dist_mean = multinom_dist.mean Mean via PyTorch function assert torch.allclose(sample_mean, dist_mean, atol=0.2) sample_var = multinom_dist.sample([num_samples]).var(axis=0) Sample variance dist_var = multinom_dist.variance Variance via PyTorch function assert torch.allclose(sample_var, dist_var, atol=0.2) 5.9.5 Bernoulli distribution A Bernoulli distribution is a special case of a binomial distribution where n = 1: that is, a single success-or-failure trial is performed. The probability of success is 휋, and the probability of failure is 1 −휋. In other words, let X be a discrete random variable that takes the value 1 (success) with probability 휋and the value 0 (failure) with probability 1 −휋. The distribution of X is the Bernoulli distribution: p (X = 1) = 휋 p (X = 0) = 1 −휋 Listing 5.11 Log probability of a Bernoulli distribution from torch.distributions import Bernoulli Imports a PyTorch Bernoulli distribution p = torch.tensor([0.3], dtype=torch.float) Sets the distribution params bern_dist = Bernoulli(p) Instantiates a Bernoulli distribution object X = torch.tensor([1], dtype=torch.float) Instantiates a single-point test dataset def raw_eval(X, p): prob = p if X == 1 else 1-p return math.log(prob) log_prob = bern_dist.log_prob(X) Evaluates the probability using PyTorch raw_eval_log_prob = raw_eval(X, p) Evaluates the probability using the formula assert torch.isclose(log_prob, raw_eval_log_prob, atol=1e-4) Asserts that the probabilities match NOTE Fully functional code for the Bernoulli distribution, executable via Jupyter Notebook, can be found at http://mng.bz/BRwq. EXPECTED VALUE OF A BERNOULLI DISTRIBUTION If there are only two classes, success and failure, we cannot speak directly of an expected value. If we run, say, 100 trials and get 30 successes and 70 failures, the average is 0.3 success, which is not a valid outcome. We cannot have fractional success or failure in this binary system. We can, however, talk about the expected value of a Bernoulli distribution if we introduce an artificial construct. We assign numerical values to these binary entities: 5.9 Some famous probability distributions 189 success = 1 and failure = 0. Then the expected value of X is 피(X) = Õ x∈{0,1} xp (x) = 1 · 휋+ (1 −휋) · 0 = 휋 (5.37) VARIANCE OF A BERNOULLI DISTRIBUTION Similarly, if we assign numerical values to these binary entities—success = 1 and failure = 0—the variance of the Bernoulli distribution is var (X) = Õ x∈{0,1} (x −피(X))2 p (x) = (1 −휋)2 휋+ (0 −휋)2 (1 −휋) = 휋(1 −휋) (5.38) Listing 5.12 Mean and variance of a Bernoulli distribution num_samples = 100000 Number of sample points samples 100000 × 1 tensor = bern_dist.sample([num_samples]) Obtains samples from the bern_dist instantiated in listing 5.11 sample_mean = samples.mean() Sample mean dist_mean = bern_dist.mean Mean via PyTorch function assert torch.isclose(sample_mean, dist_mean, atol=0.2) sample_var = bern_dist.sample([num_samples]).var() Sample variance dist_var = bern_dist.variance Variance via PyTorch function assert torch.isclose(sample_var, dist_var, atol=0.2) 5.9.6 Categorical distribution and one-hot vectors Consider again the example problem introduced in section 5.9.4. We have a database with four classes of photos: Photos of Albert Einstein (class 1): 10% Photos of Marie Curie (class 2): 42% Photos of Carl Friedrich Gauss (class 3): 4% Other photos (class 4): 44% If we randomly select a photo from the database, The probability of selecting class 1 is 휋1 = 0.1. The probability of selecting class 2 is 휋2 = 0.42. The probability of selecting class 3 is 휋3 = 0.04. The probability of selecting class 4 is 휋4 = 0.44. As before, 휋1 + 휋2 + 휋3 + 휋4 = 1 because the classes are mutually exclusive and exhaustive so exactly one class must occur in each trial. In multinomial distribution, we performed n trials and asked how many times each specific class would occur. What if we perform only one trial? Then we get categorical distribution. Categorical distribution is a special case of multinomial distribution (with the number of trials n = 1). It is also an extension of the Bernoulli distribution where instead of just two classes, success and failure, we can have an arbitrary number of classes. 190 CHAPTER 5 Probability distributions in machine learning Formally, Let C1, C2, · · · , Cm be a set of m classes such that in any random trial, exactly one of these classes will be selected, with the respective probabilities 휋1, 휋2, · · · , 휋m. We sometimes refer to the probabilities of all the classes together as a vector ®휋=  휋1 휋2 ... 휋m  Let X1, X2, · · · , Xm be a set of random variables. Xi corresponds to the number of occurrences of class Ci in a set of n trials. Then the categorical probability function depicts the probability of each of the classes C1, C2, and so on, in a single trial. ONE-HOT VECTOR We can use a one-hot vector to compactly express the outcome of a single trial of categorical distribution. This is a vector with m elements. Exactly a single element is 1; all other elements are 0. The 1 indicates which of the m possible classes occurred in that specific trial. For instance, in the example with the database of photos, if a Marie Curie photo comes up in a given trial, the corresponding one-hot vector is ®x =  0 1 0 0  . PROBABILITY OF A CATEGORICAL DISTRIBUTION We can think of a one-hot vector X as a random variable with a categorical distribution. Note that each individual class follows a Bernoulli distribution. The probability of class Ci occurring in any given trial is p (Ci) = 휋i We can express the probability distribution of all the classes together compactly p  X = ®x = 휋x1 1 휋x2 2 · · · 휋xm m = i=m Ö i=1 휋xi i (5.39) where ®x is a one-hot vector. Note that all but one of the powers in equation 5.39 is 0; hence the corresponding factor evaluates to 1. The remaining power is 1. Hence the overall probability always evaluates to 휋i, where i is the index of the class that occurred in the trial. EXPECTED VALUE OF A CATEGORICAL DISTRIBUTION Since we are talking about classes, expected value and variance do not make sense in this context. We encountered a similar situation with the Bernoulli distribution. We assigned numerical values to each class and somewhat artificially defined the expected value and Summary 191 variance. A similar idea can also be applied here: we can talk about the expected value and variance of the one-hot vector (which consists of numerical values 0 and 1). But it remains an artificial construct. Given a random variable X whose instances are one-hot vectors ®x following a cate- gorical distribution with m classes with respective probabilities 휋1, 휋2, · · · , 휋m, 피(X) = ®휋=  휋1 휋2 ... 휋m  (5.40) We skip the variance of a categorical distribution. Summary In this chapter, we first looked at probability and statistics from a machine learning point of view. We also introduced the PyTorch distributions package and illustrated each concept with PyTorch distributions code samples immediately following the math. The probability of a specific event type is defined as the fraction of the total population of all possible events occupied by events of that specific type. A random variable is a variable that can assume any value from a predefined range of possible values. Random variables can be discrete or continuous. A probability is associated with a discrete random variable taking a specific value. A probability is also associated with a continuous random variable taking a value in an infinites- imally small range around a specific value, called its probability density at that value. The sum rule of probabilities states that the sum of the probabilities of a set of mutually exclusive events is the probability of one or another of them occurring. If the set of events is exhaustive (that is, among them, they cover the entire space of possible events), then their sum is 1 because one or another of them must occur. For continuous random variables, integrating the probability density function over the domain of possible values yields 1. The joint probability of a set of events is the probability of all those events occurr- ing together. If the events are independent, the joint probability is the product of their individual probabilities. Drawing a sample from the probability distribution of a random variable returns an arbitrary value from the set of possible values. If we draw many samples, the higher-probability values show up more often than the lower-probability values. The sampled points occupy a region (called the sample point cloud) in the domain of possible values. In a sample point cloud, the region where the probabilities are higher is more densely populated than lower-probability regions. 192 CHAPTER 5 Probability distributions in machine learning The expected value of a random variable is the average of the values of points in a very large (approaching infinity) sample cloud. It is equal to the weighted sum of all possible values of the random variable, where the weight for each value is its probability of occurrence. For continuous random variables, this boils down to integration—over the domain of possible values—of the product of the random variable’s value and the probability density. The physical significance of the expected value is that it is a single-point representation of the entire distribution. The variance of a random variable is the square root of the average squared distances of the sample point values from the mean in a very large (approaching infinity) sample cloud. It is equal to the weighted sum of the squared distances of all possible values of the random variable from the mean. The weight for each value is its probability of occurrence. For continuous random variables, this boils down to integration—over the domain of possible values—of the product of the squared distance of the random variable’s value from the mean and the probability density. Physically, the variance is a measure of the spread of the points in the distribution around its mean. In the multivariate case, this spread depends on the direction. Since there are infinite possible directions in a space with two or more dimensions, we cannot speak of a single variance value. Instead, we compute a covariance matrix with which to compute the spread along any specified direction. The eigenvector corresponding to the largest eigenvalue of this covariance matrix yields the direction of maximum spread. That eigenvalue yields the maximum spread. The eigenvector corresponding to the next-largest eigenvalue yields the orthogonal direction with the next-highest spread, and so forth. Principal component analysis (PCA) is a technique in multivariate statistics to identify the directions of the maximum spread of data. It uses the eigenvectors and eigenvalues of the covariance matrix. The Gaussian distribution is the most important probability distribution. The Gaussian random variable has one value with the highest probability of occurrence. The probability decreases smoothly with increasing distance from that highest probability value. The probability density function is continuous and looks like a bell-shaped surface. The center of the bell is the highest probability value, which also happens to be the expected value of the Gaussian random variable. The covariance matrix determines the shape of the base of the bell surface. It is circular when the covariance matrix is diagonal, with equal values on the diagonal; it is elliptical in general, with the axes of the ellipse along the eigenvectors of the covariance matrix. The sample point cloud of a Gaussian distribution is elliptical. It corresponds to the base of the bell-shaped probability density function. The longest spread corresponds to the ellipse’s major axis, which corresponds to the eigenvector corresponding to the largest eigenvalue of the covariance matrix. In the GitHub repository, we have provided an interactive visualizer for observing the shapes of Gaussian distributions in one and two dimensions as you change the parameter values. Take a look at the interactive visualization section at http://mng.bz/NYJX. 6 Bayesian tools for machine learning This chapter covers Unsupervised machine learning models Bayes’ theorem, conditional probability, entropy, cross-entropy, and conditional entropy Maximum likelihood estimation (MLE) and maximum a posteriori (MAP) estimation of model parameters Evidence maximization KLD Gaussian mixture models (GMM) and MLE estimation of GMM parameters The Bayesian approach to statistics tries to model the world by modeling the overall uncertainties and prevailing beliefs and knowledge about the system. This is in con- trast to the frequentist paradigm, where probability is strictly measured by observing a phenomenon repeatedly and measuring the fraction of time an event occurs. Ma- chine learning, in particular unsupervised machine learning, is a lot closer to the Bayesian paradigm of statistics—the subject of this chapter. 193 194 CHAPTER 6 Bayesian tools for machine learning In chapter 1, we primarily discussed supervised machine learning, where the training data is labeled: each input value is accompanied by a manually created desired output value. Labeling training inputs is a manual, labor-intensive process and often the worst pain point in building a machine learning–based system. This has led to considerable recent interest in unsupervised machine learning, where we build a model from unlabeled training data. How is this done? The general approach is best visualized geometrically. Each input data instance is a point in a high-dimensional space. These points form an overall pattern in the space of all possible inputs. If the inputs all have a common property, the points are not distributed randomly over the input space. Rather, they occupy a region in the input space with a definite shape. If the inputs have multiple classes, each class occupies a separate cluster in the space. Sometimes we apply a transformation to the input first— the transform is chosen or learned so that the transformed points exhibit a pattern more clearly than raw input points. We then identify a probability distribution whose sample point cloud matches the shape of the (potentially transformed) training data point cloud. We can generate faux input by sampling from this distribution. We can also classify an arbitrary input by observing which cluster it falls into. NOTE The complete PyTorch code for this chapter is available at http://mng.bz /WdZa in the form of fully functional and executable Jupyter notebooks. 6.1 Conditional probability and Bayes’ theorem As usual, the discussion is accompanied by examples. In this context, we first offer a refresher on the concepts of joint and marginal probability from section 5.4 (you may want to revisit the topic of joint probability in sections 5.4, 5.4.1, and 5.4.2). Consider two random variables: the height and weight of adult Statsville residents. Weight (denoted W ) can take three quantized values: E1, E2, E3. Height (H) can also take three quantized values: F1, F2, F3. Table 6.1 shows their joint probability. 6.1.1 Joint and marginal probability revisited One glance at table 6.1 tells us that the probabilities are concentrated along the main diagonal, which indicates dependent events. This can be validated by inspecting one joint probability—say, p (E1, F1)—and the corresponding marginal probabilities p (F1) and p (E1). We can see that p (E1, F1) = 0.2 ≠p (F1) × p (E1) = 0.26 × 0.26, establishing that the random variables weight W and height H are not independent. For contrast, look at table 5.6. In that case, for any valid i, j pair, p  Ei, Gj  = p (Gi) × p  Ej : the two events (weight and distance of a resident’s home from the city center) are independent. Note the following: Joint probability—This is the probability of a specific combination of values occur- ring together. Each cell in table 6.1 depicts one joint probability: for example, the probability that a resident’s weight is between 60 and 90 kg and that their height is greater than 183 cm is p (E2, F3) = 0.04. 6.1 Conditional probability and Bayes’ theorem 195 Table 6.1 Example population sizes and joint probability distribution for variables W = {E1, E2, E3} and H = {F1, F2, F3} (weights and heights of adult Statsville residents), showing marginal probabilities Less than Between More than Marginals 60 kg (E1) 60 and 90 kg (E2) 90 kg (E3) for Fs Less than pop. = 20,000 pop. = 4,000 pop. = 2,000 pop. = 26,000; 160 cm (F1) p (E1, F1) p (E2, F1) p (E3, F1) p (F1) = 0.2 = 0.2 = 0.04 = 0.02 + 0.04 + 0.02 = 0.26 Between pop. = 4,000 pop. = 40,000 pop. = 4,000 pop. = 48,000; 160 cm and p (E1, F2) p (E2, F2) p (E3, F2) p (F2) = 0.04 183 cm = 0.04 = 0.4 = 0.04 + 0.4 + 0.04 (F2) = 0.48 More than pop. = 2,000 pop. = 4,000 pop. = 20,000 pop. = 26,000; 183 cm (F3) p (E1, F3) p (E2, F3) p (E3, F3) p (F3) = 0.02 = 0.02 = 0.04 = 0.2 + 0.04 + 0.2 = 0.26 Marginals p (E1) p (E2) p (E3) Total pop. for Es = 0.2 + 0.04 + 0.02 = 0.04 + 0.4 + 0.04 = 0.02 + 0.04 + 0.2 = 100,000; = 0.26 = 0.48 = 0.26 Total prob = 1 Sum rule—The joint probabilities of all possible variable combinations sum to 1 (bottom right cell in table 6.1): 3 Õ i=1 3 Õ j=1 p  Fi, Ej  = 1 The sum of probabilities is the probability of one or another of the corresponding events occurring. Here we are adding all possible event combinations—one or another of these combinations will certainly occur. Hence the sum is 1, which matches our intuition. Marginal probability for a variable—This is obtained by “summing away” the other variables (right-most column and bottom-most row in table 6.1): p  Ej  = 3 Õ i=1 p  Fi, Ej  p (Fi) = 3 Õ j=1 p  Fi, Ej  We have added all possible combinations of other variables, so the sum represents the probability of this one variable. 196 CHAPTER 6 Bayesian tools for machine learning Marginal probabilities—These sum to 1: 3 Õ j=1 p  Ej  = 3 Õ i=1 p (Fi) = 1 The sum of the marginal probabilities is the sum of all possible joint probabilities. Dependent vs. independent variables—If and only if the variables are independent, the product of the marginal probabilities is the same as the joint probability: p  Fi, Ej  ≠p (Fi) × p  Ej  ⇐⇒for dependent variables in table 5.6 p  Gi, Ej  = p (Gi) × p  Ej  ⇐⇒for independent variables in table 6.1 You should verify that this condition is not satisfied in table 6.1 for the weight and height variables. It is satisfied in table 5.6 for the weight and distance-of-home-from- city-center variables. 6.1.2 Conditional probability Suppose we know that the height of a subject is between 160 and 183 cm (H = F2). What is the probability of the subject’s weight being more than 90 kg (W = E3)? In statistical parlance, this probability is denoted p (W = E3|H = F2). It is read “probability of W = E3 given H = F2,” aka “probability of W = E3 subject to the condition H = F2.” This is an example of conditional probability. Note that if we are given that the height is between 160 and 183 cm (H = F2), our universe is restricted to the second row of table 6.1. In particular, our population size is not 100,000 (that is, the entire population of Statsville). Rather, it is 48,000: the size of the population satisfying the given condition H = F2. Using the frequentist definition, p (W = E3|H = F2) = population satisfying W = E3 and H = F2 population satisfying H = F2 = 4K 48K = 0.083 or p (W = E3|H = F2) = p (W = E3, H = F2) p (H = F2) Table 6.2 shows table 6.1 with conditional probabilities added. 6.1.3 Bayes’ theorem As demonstrated in table 6.2, in general, p  W = Ej H = Fi  = p  W = Ej, H = Fi  p (H = Fi) p  H = Fi W = Ej  = p  W = Ej, H = Fi  p  W = Ej  This is the essence of Bayes’ theorem. We can generalize and say the following: given two random variables X andY, the conditional probability of X taking the value x given the condition that Y has value y is given by the ratio of the joint probability of the two 6.1 Conditional probability and Bayes’ theorem 197 Table 6.2 Example population sizes and joint, marginal, and conditional probabilities for variables W = {E1, E2, E3} and H = {F1, F2, F3} (weights and heights of adult Statsville residents). (This is table 6.1 with conditional probabilities added.) Less than Between 60 More than Marginals 60 kg (E1) and 90 kg (E2) 90 kg (E3) for Fs Less than pop. = 20,000 pop. = 4,000 pop. = 2,000 pop. = 26,000; 160 cm (F1) p (E1, F1) = 0.2 p (E2, F1) = 0.04 p (E3, F1) = 0.02 p (F1) = 0.2 p (E1|F1) = p E1,F1  p F1  p (E2|F1) = p E2,F1  p F1  p (E3|F1) = p E3,F1  p F1  + 0.04 + 0.02 = 0.77 = 0.154 = 0.077 = 0.26 p (F1|E1) = p E1,F1  p E1  p (F1|E2) = p E2,F1  p E2  p (F1|E3) = p E3,F1  p E3  = 0.77 = 0.083 = 0.077 Between pop. = 4,000 pop. = 40,000 pop. = 4,000 pop. = 48,000; 160 cm and p (E1, F2) = 0.04 p (E2, F2) = 0.4 p (E3, F2) = 0.04 p (F2) = 0.04 183 cm (F2) p (E1|F2) = p E1,F2  p F2  p (E2|F2) = p E2,F2  p F2  p (E3|F2) = p E3,F2  p F2  + 0.4 + 0.04 = 0.083 = 0.83 = 0.083 = 0.48 p (F2|E1) = p E1,F2  p E1  p (F2|E2) = p E2,F2  p E2  p (F2|E3) = p E3,F2  p E3  = 0.154 = 0.83 = 0.154 More than pop. = 2,000 pop. = 4,000 pop. = 20,000 pop. = 26,000; 183 cm (F3) p (E1, F3) = 0.02 p (E2, F3) = 0.04 p (E3, F3) = 0.2 p (F3) = 0.02 p (E1|F3) = p E1,F3  p F3  p (E2|F3) = p E2,F3  p F3  p (E3|F3) = p E3,F3  p F3  + 0.04 + 0.2 = 0.077 = 0.154 = 0.77 = 0.26 p (F3|E1) = p E1,F3  p E1  p (F3|E2) = p E2,F3  p E2  p (F3|E3) = p E3,F3  p E3  = 0.077 = 0.083 = 0.77 Marginals p (E1) p (E2) p (E3) Total pop. for Es = 0.2 + 0.04 + 0.02 = 0.04 + 0.4 + 0.04 = 0.02 + 0.04 + 0.2 = 100,000; = 0.26 = 0.48 = 0.26 Total prob = 1 and the marginal probability of the condition p (X = x|Y = y) = p (X = x,Y = y) p (Y = y) (6.1) Sometimes we drop the names of the random variable and just use the values. Using such notation, Bayes’ theorem can be stated as p (x|y) = p (x, y) p (y) Note that the denominator is the marginal probability, which can be obtained by summing over the joint probabilities. For instance, for continuous variables, Bayes’ theorem can be written as p (x|y) = p (x, y) ∫∞ −∞p (x, y) dx 198 CHAPTER 6 Bayesian tools for machine learning Bayes’ theorem can be generalized further to more than two variables and multiple dimensions: p  X1 = ®x1 X2 = ®x2, X3 = ®x3, · · · , Xn = ®xn  = p  X1 = ®x1, X2 = ®x2, X3 = ®x3 · · · , Xn = ®xn  p  X2 = ®x2, · · · , Xn = ®xn  (6.2) p  X1 = ®x1, X2 = ®x2 X3 = ®x3 · · · , Xn = ®xn  = p  X1 = ®x1, X2 = ®x2, X3 = ®x3 · · · , Xn = ®xn  p  X3 = ®x3, · · · , Xn = ®xn  (6.3) It is common practice to drop the name of the random variable (uppercase), retain only the value (lowercase), and state these equations informally as p  ®x1 ®x2, ®x3, · · · , ®xn  = p  ®x1, ®x2, ®x3 · · · , ®xn  p  ®x2, · · · , ®xn  p  ®x1, ®x2 ®x3 · · · , ®xn  = p  ®x1, ®x2, ®x3 · · · , ®xn  p  ®x3, · · · , ®xn  What happens if the random variables are independent? Well, let’s check out equa- tion 6.1. If X and Y are independent, p (x, y) = p (x) p (y) and hence p (x|y) = p (x, y) p (y) = p (x) This makes intuitive sense: if X and Y are independent, knowing Y does not make any difference to p (X = x), so the probability of X given Y is the same as the probability of X. 6.2 Entropy Suppose a daily meteorological bulletin informs the folks in the United States whether it rained in the Sahara desert yesterday. Is there much overall information in that bulletin? Not really—it almost always reports the obvious. The probability of “no rain” is overwhelmingly high (it is almost certain that there will be no rain), and the uncertainty associated with the outcome is very low. Even without the bulletin, if we guess the outcome “no rain,” we will be right almost every time. Similarly, a daily news bulletin telling us whether it rained yesterday in Cherapunji, India—a place where it pretty much rains all the time—has little informational content because we can guess the results with high certainty even without the bulletin. Stated another way, the uncertainty associated with the probability distributions of “rain vs. no rain in the Sahara” and or “rain vs. no rain in Cherapunji” is low. This is a direct consequence of the fact that the probability of one of the events is close to 1 and the probabilities of the other events are near 0: the probability density function (PDF) has a very tall peak at one location and very low heights elsewhere. 6.2 Entropy 199 On the other hand, a daily bulletin reporting whether it rained in San Francisco is of considerable interest because the probability of “rain” and “no rain” are comparable. Without the bulletin, we cannot guess the result with much certainty. The concept of entropy attempts to quantify the uncertainty associated with a chancy event. If the probability for any one event is overwhelmingly high (meaning the proba- bilities of other events are very low since the sum is 1), the uncertainty is low—we pretty much know that the high-probability event will occur. On the other hand, if there are multiple events with comparable high probabilities, uncertainty is high—we cannot predict which event will occur. Entropy captures this notion of uncertainty in a system. Let’s look at another example. Suppose we have tiny images, four pixels wide by four pixels high, and each pixel is one of four possible colors: G(reen), R(ed), B(lue), or Y(ellow). Two such images are shown in figure 6.1. We want to encode such images. The simplest thing to do is to use a two-bit representation for each color: G(reen) = 00 R(ed) = 01 B(lue) = 10 Y (ellow) = 11 R G G G G R R R B B B B Y Y Y Y G G G G G G G G G G G G R R B Y Figure 6.1 Two 4 × 4 images with different pixel color distributions. In the left image, the four colors R, G, B, and Y are equally probable. In the right image, one color (green) is much likelier than the others. The left image has higher entropy (uncertainty): we cannot predict any color with much certainty. In the right image, we can predict green with relative certainty. The entire 16-pixel image on the left can be represented by the string 00 00 00 00 01 01 01 01 10 10 10 10 11 11 11 11. Here, we have iterated over the pixels in raster scan order, left to right and top to bottom. The total number of bits needed to store the 16-pixel image is 16 × 2 = 32 bits. The right image can be represented as 00 00 00 00 00 00 00 00 00 00 00 00 01 01 10 11. The total number of bits needed is 16 × 2 = 32 bits. Both images need the same amount of storage. But is this optimal? Consider the right-hand image. The color G appears much more frequently than the others. We can use this fact to reduce the total number of bits required to store 200 CHAPTER 6 Bayesian tools for machine learning the image. It is not mandatory to use the same number of bits to represent each color. How about using shorter representations for the more frequently occurring (higher- probability) colors and longer representations for the infrequent (lower-probability) colors? This is the core principle behind the technique of variable bit-rate coding. For instance, we can use the following representation: G(reen) = 0 R(ed) = 10 B(lue) = 110 Y (ellow) = 111 The right-hand image can thus be represented as 0 0 0 0 0 0 0 0 0 0 0 0 10 10 110 111. NOTE This is an example of what is known as prefix coding: no two colors share the same prefix. It enables us to identify the color as soon as we see its code. For instance, if we see a 0 bit at the beginning, we immediately know the color is green since no other color code starts with 0. If we see 10, we immediately know the color is red since no other color code starts with 10, and so on. With this new color code, we need 12 × 1 = 12 bits to store the 12 green pixels, 2 × 2 = 4 bits to store the 2 red pixels, 1 × 3 = 3 bits to store the single blue pixel, and 1 × 3 = 3 bits to store the single yellow pixel—a total of 22 pixels. Equivalently, we need 22 16 = 1.375 bits per pixel. This is less than the 32 pixels at 2 bits per pixel we needed with the simple fixed bit-rate coding. NOTE You have just learned about Huffman encoding, an important technique in image compression. Does the new representation result in smaller storage for the left-hand image? There, we need 4 × 1 = 4 bits to store the four green pixels, 4 × 2 = 8 pixels to store the four red pixels, 4 × 3 = 12 bits to store the four blue pixels, and 4 × 3 = 12 bits to store the single yellow pixel: a total of 36 pixels at 36 16 = 2.25 bits per pixel. Here, variable bit-rate coding does worse than fixed bit-rate coding. So, the probability distribution of the various pixel colors in the image affects how much compression can be achieved. If the distribution of pixel colors is such that a few colors are much more probable than others, we can assign shorter codes to them to reduce storage for the whole image. Viewed another way, if low uncertainty is associated with the system—certain colors are more or less certain to occur—we can achieve high compression. We assign shorter codes to nearly certain colors, resulting in compression. On the other hand, if high uncertainty is associated with the system—all colors are more or less equally probable, and no color occurs with high certainty—variable bit-rate coding will not be very effective. How do we quantify this notion? In other words, can we examine the pixel color distribution in an image and estimate whether variable bit-rate coding will be effective? The answer again is entropy. Formally, Entropy measures the overall uncertainty associated with a probability distribution. 6.2 Entropy 201 Entropy is a measure that is high if everything is more or less equally probable and low if a few items have a much higher probability than the others. It measures the uncertainty in the system. If everything is equally probable, we cannot predict any one item with any extra certainty. Such a system has high entropy. On the other hand, if some items are much more probable than others, we can predict them with relative certainty. Such a system has low entropy. In the discrete univariate case, for a random variable X that can take any one of the discrete values x1, x2, x3, · · · , xn with probabilities p (x1), p (x2), p (x3), · · · , p (xn), entropy is defined as ℍ(X) = − n Õ i=1 p (xi) log p (xi) (6.4) The logarithm is taken with respect to the natural base e. Let’s apply equation 6.4 to the images in figure 6.1 to see if the results agree with our intuition. The computations are shown in table 6.3. The notion of entropy applies to continuous and multidimensional random variables equally well. Table 6.3 Entropy computation for the pair of images in figure 6.1. The right-hand image has lower entropy and can be compressed more. Left image Right image x1 =G, p (x1) = 4 16 = 0.25 x1 =G, p (x1) = 12 16 = 0.75 x2 = R , p (x2) = 4 16 = 0.25 x2 = R , p (x2) = 2 16 = 0.125 x3 = B, p (x3) = 4 16 = 0.25 x3 = B, p (x3) = 1 16 = 0.0625 x4 =Y, p (x4) = 4 16 = 0.25 x4 =Y, p (x4) = 1 16 = 0.0625 ℍ= −(0.25 log (0.25) + 0.25 log (0.25) ℍ= −(0.75 log (0.75) + 0.125 log (0.125) + 0.25 log (0.25) + 0.25 log (0.25)) + 0.0625 log (0.0625) + 0.0625 log (0.0625)) = 1.386294 = 0.822265 For a univariate continuous random variable X that takes values x ∈{−∞, ∞} with probabilities p (x), ℍ(X) = − ∞ ∫ x=−∞ p (x) log p (x) dx (6.5) For a continuous multidimensional random variable X that takes values ®x in the domain D, (®x ∈D) with probabilities p  ®x, ℍ(X) = − ∫ ®x∈D p  ®x log p  ®x d®x (6.6) 6.2.1 Geometrical intuition for entropy Geometrically speaking, entropy is a function of how lopsided the PDF is (see figure 6.2). If all inputs are more or less equally probable, the density function is more or less flat and uniform in height everywhere (see figure 6.2a). The corresponding sample 202 CHAPTER 6 Bayesian tools for machine learning (a) Flatter, wider PDFs correspond to higher entropy. Entropy = 12.04. 100 75 50 25 0 -25 -50 -75 -100 -100 -75 -50 -25 0 25 50 75 100 X Y (b) Diffused sample point clouds correspond to higher entropy. (c) Taller, narrower peaks in probability density functions correspond to lower entropy. Entropy = 7.44. 100 75 50 25 0 -25 -50 -75 -100 -100 -75 -50 -25 0 25 50 75 100 X Y (d) Concentrated sample point clouds correspond to lower entropy. Figure 6.2 Entropies of peaked and flat distributions point cloud has a diffused mass: there are no regions with a high concentration of points. Such a system has high uncertainty or high entropy (see figure 6.2b). On the other hand, if a few of all the possible inputs have disproportionately high probabilities, the PDF has tall peaks in some regions and low heights elsewhere (see figure 6.2c). The corresponding sample point cloud has regions of high concentration matching the peaks in the density function and low concentration elsewhere (see figure 6.2d). Such a system has low uncertainty and low entropy. 6.2 Entropy 203 NOTE Since the sum of all the probabilities is 1, if a few are high, the others have to be low. We cannot have all high or all low probabilities. 6.2.2 Entropy of Gaussians The wider a Gaussian is, the less peaked it is, and the closer it is to being a uniform distribution. A univariate Gaussian’s variance, 휎, determines its fatness (see figure 5.10b). Consequently, we expect a Gaussian’s entropy to be an increasing function of 휎. Indeed, that is the case. In this section, we derive the entropy of a Gaussian in the univariate case and simply state the result for the multivariate case. For a random variable x whose PDF is given by equation 5.22 (repeated here for convenience), p (x) = 1 √ 2휋휎 e −(x−휇)2 2휎2 From that, we get log p (x) = −1 2log (2휋) −log 휎−(x −휇)2 2휎2 Using equation 6.6, the entropy is ℍ(X) = − ∞ ∫ x=−∞ p (x)  −1 2log (2휋) −log 휎−(x −휇)2 2휎2  dx = 1 2log (2휋) ∞ ∫ x=−∞ p (x) dx + log 휎 ∞ ∫ x=−∞ p (x) dx + 1 2휎2 ∞ ∫ x=−∞ p (x) (x −휇)2 dx Remembering the probability sum rule from equation 5.6, ∫∞ x=−∞p (x) dx = 1, we get ℍ(X) = 1 2log (2휋) + log 휎+ 1 2휎2 ∞ ∫ x=−∞ p (x) (x −휇)2 dx Now, by definition (see section 5.7.2), ∞ ∫ x=−∞ p (x) (x −휇)2 dx = 피  (x −휇)2 = 휎2 Hence, ℍ(X) = 1 2log (2휋) + log 휎+ 휎2 2휎2 = 1 2log (2휋) + log 휎+ 1 2 = 1 2log  2휋e휎2 (6.7) Entropy for multivariate Gaussians is as follows: ℍ(X) = 1 2log (2휋) + log (det (횺)) + 1 2 = 1 2log (2휋e det (횺)) (6.8) Listing 6.1 shows the Python PyTorch code to compute the entropy of a Gaussian. NOTE Fully functional code to compute the entropy of a Gaussian distribution, executable via Jupyter Notebook, can be found at http://mng.bz/zx7B. 204 CHAPTER 6 Bayesian tools for machine learning Listing 6.1 Computing the entropy of a Gaussian distribution def entropy_gaussian_formula(sigma): return 0.5 * torch.log(2 * math.pi * math.e * sigma * sigma) Equation 6.7 p = Normal(0, 10) Instantiates a Gaussian distribution H_formula = entropy_gaussian_formula(p.stddev) Computes the entropy using the direct formula H = p.entropy() Computes the entropy using the PyTorch interface assert torch.isclose(H_formula, H) Asserts that the entropies computed two different ways match 6.3 Cross-entropy Consider a supervised classification problem where we have to analyze an image and identify which of the following objects is present: cat, dog, airplane, or automobile. We assume that one of these will always be present in our universe of images. Given an input image, our machine emits four probabilities: p (cat), p  dog, p  airplane, and p (automobile). During training, for each training data instance, we have a ground truth (GT): a known class to which that training data instance belongs. We have to estimate how different the network output is from the GT—this is the loss for that data instance. We adjust the machine parameters to minimize the loss and continue doing so until the loss stops decreasing. How do we quantitatively estimate the loss—the difference between the known GT and the probabilities of various classes emitted by the network? One principled approach is to use the cross-entropy loss. Here is how it works. Consider a random variable X that can take four possible values: X = 1 signifying cat, X = 2 signifying dog, X = 3 signifying airplane, and X = 4 signifying automobile. The ran- dom variable has the PDF p (X = 1) ≡p (cat), p (X = 2) ≡p  dog, p (X = 3) ≡p  airplane, p (X = 4) ≡p (automobile). The PDF for a GT, which selects one from the set of four possible classes, is a one-hot vector (one of the elements is 1, and the others are 0). Such random variables and corresponding PDFs can be associated with every GT and machine output. Here are some examples, which are also shown graphically in figure 6.3. A PDF for GT cat (one-hot vector) is shown figure 6.3a: pgt_cat =  p(cat) z}|{ 1 , p(dog) z}|{ 0 , p(airplane) z}|{ 0 , (automobile) z}|{ 0  A PDF for a good prediction is shown figure 6.3b: pgood_pred =  p(cat) z}|{ 0.8 , p(dog) z}|{ 0.15 , p(airplane) z}|{ 0.04 , (automobile) z}|{ 0.01  6.3 Cross-entropy 205 1.0 0.8 0.6 0.4 0.2 0.0 p(X) Cat Dog Airplane Automobile Class (a) Ground truth probability 1.0 0.8 0.6 0.4 0.2 0.0 p(X) Cat Dog Airplane Automobile Class (b) Good prediction: probabilities similar to ground truth. Cross-entropy loss = 0.22. 1.0 0.8 0.6 0.4 0.2 0.0 p(X) Cat Dog Airplane Automobile Class (c) Bad prediction: probabilities dissimilar to ground truth. Cross-entropy loss = 1.38. Figure 6.3 Cross-entropy loss A PDF for a bad prediction is shown figure 6.3c: pbad_pred =  p(cat) z}|{ 0.25 , p(dog) z}|{ 0.25 , p(airplane) z}|{ 0.25 , (automobile) z}|{ 0.25  Let Xgt denote such a random variable for a specific GT and pgt denote the correspond- ing PDF. Similarly, let Xpred and ppred denote the random variable and PDF for the machine prediction. Consider the following expression: ℍc  Xgt, Xpred  = − 4 Õ i=1 pgt (i) log  ppred (i) (6.9) This is the expression for cross-entropy. It is a quantitative measure for how dissimilar the two PDFs pgt and ppred are: that is, how much error will be caused by approximating the PDF pgt with ppred. Equivalently, cross-entropy measures how well the machine is doing that output the prediction ppred when the correct PDF is pgt. 206 CHAPTER 6 Bayesian tools for machine learning To gain insight into how ℍc  Xgt, Xpred  measures dissimilarity between PDFs, exam- ine the expression carefully. Remember that Í4 i=1 pgt (i) = Í4 i=1 ppred (i) = 1 (using the probability sum rule from equation 5.3): Case 1: The i values where pgt (i) is high (close to 1). Case 1a: If ppred (i) is also close to 1, then log  ppred (i) will be close to zero (since log 1 = 0). Hence the term pgt (i) log  ppred (i) will be close to zero since the product of anything with a near-zero number is near zero. These terms will contribute little to ℍc  Xgt, Xpred . Case 1b: On the other hand, at the i values where pgt (i) is high, if ppred (i) is low (close to zero), then −log  ppred (i) will be very high (since log 0 →−∞). Case 2: The i values where pgt (i) is low (close to 0). These will have low values and will contribute little to ℍc  Xgt, Xpred  since the product of anything with a near zero number is near zero. Thus, overall, large contributions can happen only in case 1b, where pgt (i) is high and ppred (i) is low—that is, pgt and ppred are very dissimilar. What if pgt (i) is low and ppred (i) is high? They are also dissimilar, so those terms will not contribute much! True, but if such terms exist, there must be other terms where pgt (i) is high and ppred (i) is low. This is because the sums of all pgt (i) and ppred (i) must be both 1. Either way, if there is dissimilarity, the cross-entropy is high. For instance, consider the case where Xgt = Xgt_cat and Xpred = Xgood_pred or Xpred = Xbad_pred. We know pgt_cat is a one-hot selector vector, meaning it has 1 as one element and 0s elsewhere. Only a single term survives, corresponding to i = 0, and ℍc  Xgt_cat, Xpred  =   −Í4 i=1 pgt_cat (i) log  pgood_pred (i) = −log (0.8) = 0.22 −Í4 i=1 pgt_cat (i) log  pbad_pred (i) = −log (0.25) = 1.38 We see that cross-entropy is higher where similarity is lower (the prediction is bad). Finally, we are ready to formally define the cross-entropy of two arbitrary random variables. Let X1, X2 be a pair of random variables that take values x from the same input domain D (that is, x ∈D), with probabilities p1 (x), p2 (x), respectively: ℍc (X1, X2) =   −Í x∈D p1 (x) log (p2 (x)) discrete − ∫ x∈D p1 (x) log (p2 (x)) dx continuous univariate − ∫ ®x∈D p1  ®x log  p2  ®x d®x continuous multivariate (6.10) Note that cross-entropy in equation 6.10 reduces to entropy (equations 6.5, 6.6) ifY = X. Listing 6.2 shows the Python PyTorch code to compute the entropy of a Gaussian. NOTE Fully functional code to compute cross-entropy, executable via Jupyter Notebook, can be found at http://mng.bz/0mjN. 6.4 KL divergence 207 Listing 6.2 Computing cross-entropy def cross_entropy(X_gt, X_pred): H_c = 0 for x_gt, x_pred in zip(X_gt, X_pred): H_c += -1 * (x_gt * torch.log (x_pred)) Direct computation of cross-entropy from equation 6.9 return H_c X_gt = torch.Tensor([1., 0., 0., 0.]) Probability density function for the ground truth (one-hot vector) X_good_pred = torch.Tensor([0.8, 0.15, 0.04, 0.01]) Probability density function for a good prediction X_bad_pred = torch.Tensor([0.25, 0.25, 0.25, 0.25]) Probability density function for a bad prediction H_c_good = cross_entropy(X_gt, X_good_pred) Cross-entropy between Xgt and Xgood_pred (a low value) H_c_bad = cross_entropy(X_gt, X_bad_pred) Cross-entropy between Xgt and Xbad_pred (a high value) 6.4 KL divergence In section 6.3, we saw that cross-entropy, ℍc (X1, X2), measures the dissimilarity between the distributions of two random variables X1 and X2 with probabilities p1 (x) and p2 (x). But cross-entropy has a curious property for a dissimilarity measure. If X1 = X2, the cross- entropy ℍc (X1, X2) reduces to the entropy ℍ(X1). This is somewhat counterintuitive: we expect the dissimilarity between two copies of the same thing to be zero. We should look at cross-entropy as a dissimilarity with an offset. Let’s denote the pure dissimilarity measure as D (X1, X2). Then ℍc (X1, X2) = of f set z }| { ℍ(X1) + pure dissimilarity z }| { D (X1, X2) This means the pure dissimilarity measure D (X1, X2) = ℍc (X1, X2) −ℍ(X1) = − Õ x∈D p1 (x) log (p2 (x)) + Õ x∈D p1 (x) log (p1 (x)) = Õ x∈D p1 (x) log (p1 (x) −p2 (x)) = Õ x∈D p1 (x) log  p1 (x) p2 (x)  This pure dissimilarity measure, D (X1, X2), is called Kullback–Leibler divergence (KL divergence or KLD). As expected, it is 0 when the two random variables are identical. Formally, KLD is as follows: D (X1, X2) = Õ x∈D p1 (x) log  p1 (x) p2 (x)  (6.11) 208 CHAPTER 6 Bayesian tools for machine learning For continuous univariate randoms, D (X1, X2) = ∫ x∈D p1 (x) log  p1 (x) p2 (x)  dx (6.12) For continuous multivariate randoms, D (X1, X2) = ∫ ®x∈D p1  ®x log  p1  ®x p2  ®x  d®x (6.13) Let’s examine some properties of KLD: The KLD between identical random variables is zero. If X1 = X2, p1 (x) = p2 (x) ∀x ∈ D. Then the log term vanishes at every x, and KLD is zero. The KLD between non-identical probability distributions is always positive. We can see this by examining equation 6.11. At all values of x where p1 (x) > p2 (x), the log term is positive (since the logarithm of a number greater than 1 is positive). On the other hand, at all values of x where p1 (x) < p2 (x), the log term is negative (since the logarithm of a number less than 1 is negative). But the positive terms get higher weights because p1 (x) are higher at these points. In this context, it is worth noting that given any pair of PDFs, one cannot be uniformly higher than the other at all points. This is because both of them must sum to 1. If one PDF is higher somewhere, it must be lower somewhere else to compensate. Given a GT PDF pgt for a classification problem and a machine prediction ppred, minimizing the cross-entropy ℍ(gt, pred) is logically equivalent to minimizing the KLD D (gt, pred). This is because the entropy ℍ(gt) is a constant, independent of the machine parameters. The KLD is not symmetric: D (X1, X2) ≠D (X2, X1). 6.4.1 KLD between Gaussians Since the Gaussian probability distribution is so important, in this subsection we look at the KLD between two Gaussian random variables X1 and X2 having PDFs p1 (x) = N (x; 휇1, 휎1) and p2 (x) = N (x; 휇2, 휎2). We derive the expression for the univariate case and simply state the expression for the multivariate case: D (X1, X2) = ∞ ∫ −∞ N(x; 휇1,휎1) z }| { 1 √ 2휋휎1 e − x−휇1 2 2휎2 1 log ©­­­­­ « 1 √ 2휋휎1 e − x−휇1 2 2휎2 1 1 √ 2휋휎2 e −  x−휇2 2 2휎2 2 ª®®®®® ¬ dx = ∞ ∫ −∞ N (x; 휇1, 휎1) log 휎2 휎1 + (x −휇2)2 2휎2 2 −(x −휇1)2 2휎2 1 ! dx 6.4 KL divergence 209 Opening the parentheses, we get log 휎2 휎1 =1, by equation 5.6 z }| { ∞ ∫ −∞ N (x; 휇1, 휎1) dx + 1 2휎2 2 ∞ ∫ −∞ (x −휇2)2 N (x; 휇1, 휎1) dx − 1 2휎2 1 =휎2 1 , by equation 5.13 z }| { ∞ ∫ −∞ (x −휇1)2 N (x; 휇1, 휎1) dx = log 휎2 휎1 + 1 2휎2 2 ∞ ∫ −∞ (x −휇1 + 휇1 −휇2)2 N (x; 휇1, 휎1) dx −1 2 Expanding the square term, we get D (X1, X2) = log 휎2 휎1 + 1 2휎2 2 ∞ ∫ −∞  (x −휇1)2 + ( 휇1 −휇2)2 + 2 (x −휇1) ( 휇1 −휇2)  N (x; 휇1, 휎1) dx −1 2 Since ∞ ∫ −∞ (x −휇1) N (x; 휇1, 휎1) dx = 휇1 −휇1 = 0 the final equation for the KLD between two univariate Gaussian random variables X1, X2 with PDFs N (x; 휇1, 휎1) and N (x; 휇2, 휎2) becomes D (X1, X2) = log 휎2 휎1 + 휎2 1 + ( 휇1 −휇2)2 2휎2 2 −1 2 (6.14) The KLD between two d-dimensional Gaussian random variables X1, X2 with PDFs N  ®x; 휇1, 횺1  and N  ®x; 휇2, 횺2  is D (X1, X2) = 1 2  tr  횺−1 2 횺1  +   ®휇2 −®휇1 T 횺−1 2   ®휇2 −®휇1  −d + log det 횺2 det 횺1  (6.15) where the operator tr denotes the trace of a matrix (sum of diagonal elements) and the operator det denotes the determinant. Listing 6.3 shows the Python PyTorch code to compute the KLD. 210 CHAPTER 6 Bayesian tools for machine learning NOTE Fully functional code to compute the KLD, executable via Jupyter Notebook, can be found at http://mng.bz/KMyj. Listing 6.3 Computing the KLD from torch.distributions import kl_divergence p = Normal(0, 5) q = Normal(0, 10) Instantiates three Gaussian distributions with the same means but different standard deviations r = Normal(0, 20) kld_p_p = kl_divergence(p, p) kld_p_q = kl_divergence(p, q) kld_q_p = kl_divergence(q, p) Computes the KLD between various pairs of distributions kld_p_r = kl_divergence(p, r) assert kld_p_p == 0 The KLD between a distribution and itself is 0. assert kld_p_q != kld_q_p The KLD is not symmetric. assert kld_p_q < kld_p_r KLD(p, q) < KLD(p, r). See figure 6.4a. In figure 6.4a, we compare three Gaussian distributions p, q, and r with the same 휇s but different 휎s. KLD(p, q) < KLD(p, r) because 휎p is closer to 휎q than 휎r. In figure 6.4b, we compare a uniform distribution p with two Gaussian distributions q and r that have different 휇s but the same 휎s. KLD(p, q) < KLD(p, r) because 휇p is closer to 휇q than 휇r. 6.5 Conditional entropy In section 6.2, we learned that entropy measures the uncertainty in a system. Earlier, in section 6.1.2, we studied conditional probability, which measures the probability of occurrence of one set of random variables under the condition that another set has known fixed values. In this section, we combine the two concepts into a new concept called conditional entropy. Consider the following question from table 6.2. What is the entropy of the weight variableW under the condition that the value of the height variable H is F1? As observed in section 6.1.1, the condition effectively restricts our universe to a single row (in this case, the top row) of the table. We can compute the entropy of the elements of that row mathematically, using equation 6.5, as conditional entropy of W given H=F1 z }| { ℍ(W |H = F1) = − 3 Õ j=1 p  Ej F1  log  p  Ej F1  = −(0.77 × log (0.77) + 0.154 × log (0.154) + 0.077 × log (0.077)) = 0.6868 6.5 Conditional entropy 211 (a) p ≡N( 휇= 0, 휎= 5), q ≡N( 휇= 0, 휎= 10), r ≡N( 휇= 0, 휎= 20) (b) p ≡U (a = −20, b = 20), q ≡N( 휇= 0, 휎= 20), r ≡N( 휇= −50, 휎= 20) Figure 6.4 KLD between example distributions Similarly, ℍ(W |H = F2) = − 3 Õ j=1 p  Ej F2  log  p  Ej F2  = −(0.083 × log (0.083) + 0.83 × log (0.83) + 0.083 × log (0.083)) = 0.5678 ℍ(W |H = F3) = − 3 Õ j=1 p  Ej F3  log  p  Ej F3  = −(0.077 × log (0.077) + 0.154 × log (0.154) + 0.77 × log (0.77)) = 0.6868 212 CHAPTER 6 Bayesian tools for machine learning ℍ(W |H = Fi) is the entropy of W given H = Fi for i = 1 or 2 or 3. What is the overall conditional entropy of W given H: that is, ℍ(W |H)? To compute this, we take the expected value (that is, the probability-weighted average; see equation 5.8) of the conditional entropy ℍ(W |H = Fi) over all possible values of i: conditional entropy of W given H z }| { ℍ(W |H) = 3 Õ i=1 p (Fi) ©­ « − 3 Õ j=1 p  Ej Fi  log  p  Ej Fi ª® ¬ = (0.6868 ∗0.26 + 0.5678 ∗0.48 + 0.6868 ∗0.26) = 0.6297 This idea can be generalized. Formally, given two random variables X and Y that can take values x ∈Dx, y ∈Dy, respectively, ℍ(X|Y) = 피yℍ(X |Y=y) z }| { Õ y∈Dy p (y) ℍ(X |Y=y) z }| { − Õ x∈Dx p (x|y) log (p (x|y)) ! ⇔discrete (6.16) ℍ(X|Y) = ∫ y∈Dy p (y) ©­­ « − ∫ x∈Dx p (x|y) log (p (x|y)) dx ª®® ¬ dy ⇔continuous (6.17) 6.5.1 Chain rule of conditional entropy This rule states: ℍ(X|Y) = ℍ(X,Y) −ℍ(Y) (6.18) This can be derived from equation 6.17. ℍ(X|Y) = ∫ y∈Dy p (y) ©­­ « − ∫ x∈Dx p (x|y) log (p (x|y)) dx ª®® ¬ dy Applying Bayes’ theorem (equation 6.1), ℍ(X|Y) = − ∫ y∈Dy ∫ x∈Dx p(x,y) z }| { p (y) p (x|y) log log  p(x,y) p(y)  z }| { (p (x|y)) dx dy = ℍ(X,Y ) z }| { − ∫ y∈Dy ∫ x∈Dx p (x, y) log (p (x, y)) dx dy + ∫ y∈Dy log (p (y)) marginal probability p(y) z }| { ∫ x∈Dx p (x, y) dx dy = ℍ(X,Y) −ℍ(Y) (6.19) 6.6 Model parameter estimation 213 6.6 Model parameter estimation Suppose we have a set of sampled input data points X =  ®x(1), ®x(2), · · · , ®x(n) from a distribution. We refer to the set collectively as training data. Note that we are not assum- ing it is labeled training data—we do not know the outputs corresponding to the inputs ®x(i). Also, suppose that based on our knowledge of the problem, we have decided which model family to use. Of course, simply knowing the family is not enough; we need to know (or estimate) the model parameters before we can use the model. For instance, our model family might be Gaussian, N  x; ®휇, 횺. Until we know the actual value of the parameters ®휇and 횺, we do not fully know the model and cannot use it. How do we estimate the model parameters from the unlabeled training data? This is what we cover in this section. At the moment, we are discussing it without referring to any specific model architecture, so let’s denote model parameters with a generic symbol 휃. For instance, when dealing with Gaussian models, 휃=  ®휇, 횺 . 6.6.1 Likelihood, evidence, and posterior and prior probabilities Before tackling the problem of parameter estimation, it is important to have a clear understanding of the terms likelihood, evidence, posterior probability, and prior probability in the current context. Equation 6.20 illustrates them. Using Bayes’ theorem, posterior probability z }| { p (휃|X) = p (X, 휃) p (X) = likelihood z }| { p (X|휃) prior probability z}|{ p (휃) p (X) |{z} evidence (6.20) Let’s first examine the likelihood term. Using the fact that data instances are indepen- dent of each other, p (X|휃) = p  ®x(1), ®x(2), · · · , ®x(n) 휃  = n Ö i=1 p  ®x(i) 휃  Now, p(®x(i)|휃) is essentially the probability density of the distribution family we have chosen. For instance, if the model in question in Gaussian, then given 휃=  ®휇, 횺 , this will be p  ®x(i) 휃  = N  ®x(i); ®휇, 횺  = 1 (2휋det 횺) 1 2 e−1 2  ®x(i) −®휇 T 횺−1 ®x(i) −®휇  which is basically an expression for the Gaussian PDF: a restatement of equation 5.23 (but in equation 5.23, we dropped the “given 휃,” part in the notation and expressed p  ®x 휃 simply as p  ®x). Thus, we can always express the likelihood from the PDF of the chosen model family using the independence of individual training data instances. Now let’s examine the prior probability, p (휃). It typically comes from some physical constraint—without referring to the input. A very popular approach is to say that, all other things being equal, we prefer parameters with smaller magnitudes. By this token, the larger the total magnitude ∥휃∥2, the lower the prior probability. For instance, we may use p (휃) ∝e−∥휃∥2 (6.21) 214 CHAPTER 6 Bayesian tools for machine learning An indirect justification for favoring parameter vectors with the smallest length (mag- nitude) can be found in the principle of Occam’s razor. It states, Entia non sunt mul- tiplicanda praeter necessitatem, which roughly translates to “One should not multiply unnecessarily.” This is often interpreted in machine learning and other disciplines as “favor the briefest representation.” As shown previously, we can always express the likelihood and prior terms. Using them, we can formulate different paradigms, each with a different quantity, to optimize in order to estimate the unknown probability distribution parameters from training data. These techniques can be broadly classified into the following categories: Maximum likelihood parameter estimation (MLE) Maximum a posteriori (MAP) parameter estimation We provide an overview of them next. You will notice that, in all the methods, we typically preselect a distribution family as a model and then estimate the parameter values by maximizing one probability or another. Later in the chapter, we look at MLE in the special case of the Gaussian family of distributions. Further down the line, we look at MLE with respect to Gaussian mixture models. Another technique outlined later is evidence maximization: we will visit it in the context of variational autoencoders. The log-likelihood trick If we choose a distribution family whose PDF is exponential (the most obvious example is Gaussian), instead of maximizing the likelihood, we usually maximize its logarithm, aka the log-likelihood. We can do this because whatever maximizes a quantity also maximizes its logarithm and vice versa. But the logarithm simplifies expressions in the case of exponential probability functions. This becomes obvious if we note that log (ex) = x log Ö ex(i)  = Õ x(i) 6.6.2 Maximum likelihood parameter estimation (MLE) In MLE of parameters, we ask, “What parameter values will maximize the joint likeli- hood of the training data instances?” In this context, remember that likelihood is the probability of a data instance occurring given specific parameter values (equation 6.20). Expressed mathematically, MLE estimates what value of 휃maximizes p (X|휃). The geometric mental picture is as follows: we want to estimate the unknown parameters for our model probability distribution such that if we draw many samples from that distribution, the sample point cloud will largely overlap the training data. Often we employ the log-likelihood trick and maximize the log-likelihood instead of the actual likelihood. For some models, such as Gaussians, this maximization problem can be solved analytically, and a closed-form solution can be obtained (as shown in section 6.8). 6.7 Latent variables and evidence maximization 215 For others, such as Gaussian mixture models (GMMs), the maximization problem yields no closed-form solution, and we go for an iterative solution (as shown in sec- tion 6.9.4). 6.6.3 Maximum a posteriori (MAP) parameter estimation and regularization Instead of asking what parameter value maximizes the probability of occurrence of the training data instances, we can ask, “What are the most probable parameter values, given the training data?” Expressed mathematically, in MAP, we directly estimate the 휃 that maximizes p (휃|X). Using equation 6.20, p (휃|X) = p (X|휃) p (휃) p (X) (6.22) Since the denominator is independent of 휃, maximizing the numerator with respect to 휃maximizes the fraction. Thus In MAP parameter estimation, we look for parameters 휃that maximize p (X|휃) p (휃). The first factor, p (X|휃), is what we optimized in MLE and comes from the model definition (such as equation 5.23 for multivariate Gaussian models). The second factor, p (휃), is the prior term, which usually incentivizes the opti- mization system to choose a solution with predefined properties like smaller parameter magnitudes (equation 6.21). Viewed this way, MAP estimation is equivalent to MLE parameter estimation with reg- ularization. Regularization is a technique often used in optimization. In regularized optimization, we add a term to the expression being maximized or minimized. This term effectively incentivizes the system to choose the solution with the smallest magnitudes of the unknown from the set of possible solutions. It is easy to see that MAP estimation essentially imposes the prior probability term on top of MLE. This extra term acts as a regularizer, incentivizing the system to choose the lowest magnitude parameters while still trying to maximize the likelihood of the training data. Equation 6.22 can be interpreted another way. When we have no training data, all we can do is estimate the parameters from our prior beliefs about the system: the prior term p (휃). When the training data set X arrives, it influences the system through the likelihood term p (X|휃). As more and more training data arrives, the prior term (whose magnitude does not change with training data) dominates less and less, and the posterior probability p (휃|X) is dominated more by the likelihood. 6.7 Latent variables and evidence maximization Suppose we have the height and weight data for a population (say, for the adult residents of our favorite town, Statsville). A single data instance looks like this: ®x =  height weight  Although the data is not explicitly labeled or classified, we know the data points can be clustered into two distinct classes, male and female. It is reasonable to expect that the 216 CHAPTER 6 Bayesian tools for machine learning distribution of each class is much simpler than the overall distribution. For instance, here, the distributions for males and females may be Gaussians individually (presumably, the means for females will occur at smaller height and weight values). The combined distribution does not fit any of the distributions we have discussed so far (later, we see it is a Gaussian mixture). We look at such situations in more detail in connection to Gaussian mixture modeling and variational autoencoders. Here we only note that in these cases, it is often beneficial to introduce a variable for the class, say Z. In this example, Z is discrete: it can take one of two values, male or female. Then we can model the overall distribution as a combination of simple distributions, each corresponding to a specific value of Z. Such variables Z that are not part of the observed data X but are introduced to facilitate modeling are called latent or hidden variables/parameters. Latent variables are connected to observed variables through the usual Bayesian expression: p(®x, ®z) = p  ®x ®z p  ®z p  ®z ®x = p  ®x ®z p  ®z p  ®x How do we estimate the distribution of Z? One way is to ask, “What distribution of the hidden variables would maximize the probability of exactly these training data points being returned if we drew random samples from the distribution?” The philosophy behind this is as follows: we assume that the training data points are fairly typical and have a high probability of occurrence in the unknown data distribution. Hence, we try to find a distribution under which the training data points will have the highest probabilities. Geometrically speaking, each data point (vector) can be viewed as a point in some d-dimensional space, where d is the number of elements in the vector ®xi. The training data points typically occupy a region within that space. We are looking for a distribution whose mass is largely aligned with the training data region. In other words, the probability associated with the training data points is as high as possible—the sample distribution cloud largely overlaps the training data cloud. Expressed mathematically, we want to identify p  ®x ®z and p  ®z that maximize the quantity p (X) = ∫ p (X, z) dz = ∫ N Ö i=1 p  ®x(i), ®z  d®z = ∫ N Ö i=1 p  ®x(i) ®z  p  ®z d®z (6.23) As usual, we get p  ®x(i) ®z  from the PDF of our chosen model family and p  ®z through some physical constraint. 6.8 Maximum likelihood parameter estimation for Gaussians We look at this with a one-dimensional example, but the results derived apply to higher dimensions. Suppose we are trying to predict whether an adult Statsville resident is 6.8 Maximum likelihood parameter estimation for Gaussians 217 female, given that the resident’s height lies in a specified range [a, b]. For this purpose, we have collected a set of height samples of adult female Statsville residents. These height samples constitute our training data. Let’s denote them as x(1), x(2), · · · , x(n). Based on physical considerations, we expect the distribution of heights of adult Statsville females to be a Gaussian distribution with unknown mean and variance. Our goal is to determine them from the training data via MLE, which effectively estimates a distribution whose sample cloud maximally matches the distribution of the training data points. Let’s denote the (as yet unknown) mean and variance of the distribution as 휇and 휎. Then, from equation 5.22, we get p  x(i) 휇, 휎  = 1 √ 2휋휎 e −  x(i) −휇 2 2휎2 n Ö i=1 p  x(1), x(2), · · · , x(n) 휇, 휎  = 1 √ 2휋휎 n e −Ín i=1  x(i) −휇 2 2휎2 Employing the log-likelihood trick, log n Ö i=1 p  x(1), x(2), · · · , x(n) 휇, 휎  = log ©­­ « 1 √ 2휋휎 n e −Ín i=1  x(i) −휇 2 2휎2 ª®® ¬ = −nlog √ 2휋  −nlog휎− Ín i=1  x(i) −휇 2 2휎2 To maximize with respect to 휇, we solve 휕 휕휇log n Ö i=1 p  x(1), x(2), · · · , x(n) 휇, 휎  = 0 or 2 Ín i=1  x(i) −휇  2휎2 = 0 or n Õ i=1  x(i) −휇  = 0 Finally, we get a closed-form expression for the unknown 휇in terms of the training data: 휇= 1 n n Õ i=1 x(i) Similarly, to maximize with respect to 휎, we solve 휕 휕휎log n Ö i=1 p  x(1), x(2), · · · , x(n) 휇, 휎  = 0 or n 휎− 2 Ín i=1  x(i) −휇 2 2휎3 = 0 218 CHAPTER 6 Bayesian tools for machine learning or n휎2 = n Õ i=1  x(i) −휇 2 Finally, we get a closed-form expression for the unknown 휎in terms of the training data: 휎2 = 1 n n Õ i=1  x(i) −휇 2 Thus we see that for a Gaussian, the maximum-likelihood solutions coincide with the sample mean and variance of the training data. Once we have the mean and standard deviation, we can calculate the probability that a female resident’s height belongs to a specified range [a, b] by using the following equation: prob(a < X <= b) = ∫b a p(X)dX (6.24) In the multidimensional case: Given a training dataset, n ®x(1) , ®x(2) , · · · , ®x(n)o , the best fit Gaussian has the mean ®휇= 1 n n Õ i=1 ®x(i) =⇒mean of the training data samples. (6.25) and the covariance matrix 횺= 1 n n Õ i=1  ®x(i) −®휇   ®x(i) −®휇 T =⇒covariance of the training data samples. (6.26) We began this section by stating the problem of estimating the probability of an adult Statsville resident being female, given that their height lies in a specified range [a, b], when we are provided a training dataset of n height values of adult Statsville female residents. Let’s now revisit that problem. Using (scalar versions of) equations 6.25 and 6.26, we can estimate 휇and 휎and thereby define a Gaussian probability distribution p (x) = N (x; 휇, 휎) Using this, given any height x, we can compute the probability p (x) that the resident is female. Let’s see this using PyTorch. 6.8.1 Python PyTorch code for maximum likelihood estimation Suppose we assume that the height values of adult female residents of Statsville follow a Gaussian distribution. If we know the parameters of this Gaussian (휇and 휎), we know the Gaussian distribution fully. That allows us to estimate many interesting things: for instance, the expected height of an adult female resident of Statsville, or the probability that the height of an adult female Statsville resident lies in a certain range such as between 160 and 170 cm. The problem is, in a typical real-life situation, we do not know the parameters 휇cm and 휎. All we have is a large dataset X of height values of adult Statsville female residents—training data. We have to use this data to estimate the 6.8 Maximum likelihood parameter estimation for Gaussians 219 unknown parameters 휇cm and 휎. Once we have these, we have an estimated distribution (aka model) from which we can predict the probabilities of events of interest. As we saw in section 6.6.2, MLE is a technique to estimate the parameters from given training data when the family to which the distribution belongs is known but the exact values of the parameters are not known. Listing 6.4 shows the PyTorch implementation of MLE for the Gaussian family. NOTE Fully functional code for model parameter estimation using MLE and MAP, executable via Jupyter Notebook, can be found at http://mng.bz/9Mv7. Listing 6.4 Maximum likelihood estimate for a Gaussian sample_mean = X.mean() Estimates Gaussian MLE parameters ®휇and 횺. They equal the sample mean and sample covariance of the training data. See equations 6.25 and 6.26. sample_std = X.std() gaussian_mle = Normal(sample_mean, sample_std) Defines a Gaussian with the estimated parameters a, b = torch.Tensor([160]), torch.Tensor([170]) Once the Gaussian is estimated, we can use it to predict probabilities. prob = gaussian_mle.cdf(b) - gaussian_mle.cdf(a) 6.8.2 Python PyTorch code for maximum likelihood estimation using gradient descent In listing 6.4, we computed the MLE using the closed-form solution. Now, let’s try to compute the MLE using a different method: gradient descent. In real-life scenarios, we do not use gradient descent to compute the MLE because the closed-form solution is available. However, we discuss this method here to highlight some of the challenges of using gradient descent and how MAP estimation addresses these challenges. Our goal is to maximize the likelihood function using gradient descent. This can alternatively be viewed as minimizing the negative log-likelihood function. We choose to use the logarithm of the likelihood function since that leads to simpler computation without any loss of generalization. (If you want a quick refresher on gradient descent, see section 3.5.) Following is the equation for negative log-likelihood: −log p(X|휃) = n 2 log 2휋휎2 + Ín i=1(x(i) −휇)2 2휎2 (6.27) Listings 6.5 and 6.6 show the PyTorch code for the minimization process. Listing 6.5 Gaussian negative log-likelihood for training data def neg_log_likelihood(X, mu, sigma): Equation 6.27 N = X.shape[0] X_minus_mu = torch.sub(X, mu) 220 CHAPTER 6 Bayesian tools for machine learning t1 = torch.mul(0.5 * N, torch.log(2 * np.pi * torch.pow(sigma, 2))) n 2 log 2흅흈2 t2 = torch.div(torch.matmul(X_minus_mu.T, X_minus_mu), 2 * torch.pow(sigma, 2)) Ín i=1 (xi−흁)2 2흈2 return t1 + t2 Note how all the training data X is crunched in a single operation. Such vector operations are parallel and very efficient in PyTorch. Listing 6.6 Minimizing MLE loss via gradient descent def minimize(X, mu, sigma, loss_fn, num_iters=100, lr = 0.001): Negative log-likelihood (listing 6.5) Iterates to train for i in range(num_iters): loss = loss_fn(X, mu, sigma) Computes the loss loss.backward() Computes the gradients of the loss with regard to 흁and 흈. PyTorch stores the gradients in 흁.grad and 흈.grad. mu.data -= lr * mu.grad sigma.data -= lr * sigma.grad Scales the gradients by learning the rate and update parameters mu.grad.data.zero_() sigma.grad.data.zero_() Resets the gradients to zero post-update mu = Variable(torch.Tensor([5]).type(dtype), requires_grad=True) sigma = Variable(torch.Tensor([5]).type(dtype), requires_grad=True) minimize(X, mu, sigma, neg_log_likelihood) Figure 6.5 shows how 휇and 휎change with each iteration of gradient descent. We expect 휇and 휎to end up close to 휇expected and 휎expected, respectively. However, when 휇 and 휎start off far from 휇expected and 휎expected (as in figure 6.5a), they do not converge to the expected values and instead become very large numbers. On the other hand, when they are instantiated with values closer to 휇expected and 휎expected (as in figure 6.5b), they converge to the expected values. MLE is very sensitive to the initial values and has no mechanism to prevent the parameters from exploding. This is why MAP estimation is preferred. The prior p(휃) acts as a regularizer and prevents the parameters from becoming too large. Figure 6.5c shows how 휇and 휎converge to the expected values using MAP even though they started far away. The MAP loss function is as follows. Note that it is the same equation as the negative log-likelihood, but with two additional terms—휇2 and 휎2—that act as regularizers: −log p(휃|X) = N 2 log 2휋휎2 + 1 2휎2 n Õ i=1 (x(i) −휇)2 + 휇2 + 휎2 | {z } Regularizer (6.28) 6.8 Maximum likelihood parameter estimation for Gaussians 221 (a) MLE explodes: 휇init = 1, 휎init = 1. (b) MLE converges: 휇init = 100, 휎init = 10. (c) MAP converges: 휇init = 1, 휎init = 1. Figure 6.5 Gaussian parameter estimation using maximum likelihood estimate and maximum a post- eriori estimation. In figure 6.5a, the MLE explodes because 흁and 흈are initialized far from 흁expected and 흈expected. However, the MLE converges in figure 6.5b because 흁and 흈are initialized closed to 흁expected and 흈expected. Figure 6.5c shows how, for MAP, 흁and 흈are able to converge to 흁.expected and 흈expected even though they are initialized far away. Listing 6.7 Gaussian negative log-likelihood with regularization def neg_log_likelihood_reg(X, mu, sigma, k=0.2): Equation 6.28 N = X.shape[0] X_minus_mu = torch.sub(X, mu) t1 = torch.mul(0.5 * N, torch.log(2 * np.pi * torch.pow(sigma, 2))) n 2 log 2흅흈2 222 CHAPTER 6 Bayesian tools for machine learning t2 = torch.div(torch.matmul(X_minus_mu.T, X_minus_mu), 2 * torch.pow(sigma, 2)) Ín i=1 (xi−흁)2 2흈2 loss_likelihood = t1 + t2 Negative log-likelihood loss_reg = k * (torch.pow(mu, 2) + torch.pow(sigma, 2)) Regularization return loss_likelihood + loss_reg Note how all the training data X is crunched in a single operation. Such vector operations are parallel and very efficient in PyTorch. 6.9 Gaussian mixture models In many real-life problems, the simple unimodal (single-peak) probability distributions we learned about in chapter 5 fail to model the true underlying distribution of the data. For instance, consider a situation where we are given the heights of many adult Statsville residents. Say there are two classes of adults in Statsville: male and female. The height data we have is unlabeled, meaning we do not know whether a given instance of height data is associated with a male or a female. Thus the data is one-dimensional, and there are two classes. Figure 6.6 depicts the situation. None of the simple probability distributions we discussed in chapter 5 can be fitted to figure 6.6. But the two partial bells in figure 6.6a suggest that we should be able to mix a pair of Gaussians (each of which looks like a bell) to mimic this distribution. This is also consistent with our knowledge that the distribution represents not one but two classes, each of which can be reasonably represented individually by Gaussians. The point cloud also indicates two separate clusters of points. While a single Gaussian will not work, a mixture of two separate 1D Gaussians can (and, as we shall shortly see, will) work. 0.07 0.06 0.05 0.04 0.03 0.02 0.01 0.00 p(x) 50 100 150 200 250 300 x (height in cms) GMM Gaussian distribution (a) PDF 0.04 0.02 0.00 -0.02 -0.04 145 150 155 160 165 170 175 180 (b) Sample point distribution Figure 6.6 Probability density functions (PDFs) and sample point distributions for 1D height data of adult male and female residents of Statsville 6.9 Gaussian mixture models 223 Let’s now discuss a slightly more complex problem in which the data is two-dimensional and has three classes. Here we are given the weights and heights of three classes of Statsville residents: adult females, adult males, and children. Again, the data is unlabeled, meaning we do not know whether a given instance of (height, weight) data is associated with a man, woman, or child. This is depicted in figure 6.7. Once again, none of the simple probability distributions we studied in chapter 5 can be fitted to this situation. But the PDF shows three bell-shaped peaks, the point cloud shows three clusters, and the physical nature of the problem indicates three separate classes, each of which can be reasonably represented by Gaussian. While a single Gaussian will not work, a mixture of three separate 2D Gaussians can (and, as we shall shortly see, will) work. A Gaussian mixture model (GMM) is a weighted combination of a specific number of Gaussian components. (a) PDF 90 85 80 75 70 65 60 55 50 45 40 35 30 Y X (b) Sample point distributions Figure 6.7 Probability density functions (PDFs) and sample point distributions for 2D (height, weight) data of children, adult males, and adult females of Statsville For instance, in our first problem with one dimension and two classes, we choose a mixture of two 1D Gaussians. For the second problem, we take a mixture of three 2D Gaussians. Each individual Gaussian component corresponds to a specific class. 6.9.1 Probability density function of the GMM Formally, The PDF for a GMM is p  ®x = K Õ k=1 휋k N  ®x; ®휇k, 횺k  (6.29) 224 CHAPTER 6 Bayesian tools for machine learning where 휋k is the weight of the kth Gaussian component, satisfying k=K Õ k=1 휋k = 1 K is the number of classes or Gaussian components, and N  ®x; ®휇k, 횺k  (defined in equation 5.23) is the PDF for the kth Gaussian component. Such a GMM models a K-peaked PDF or, equivalently, a K-clustered sample point cloud. For instance, the PDF and sample point clouds shown in figure 6.6 correspond to the following Gaussian mixture: p (x) = 0.7 z}|{ 휋1 N ©­­­ « x; 휇1 |{z} 152.0 , 4.0 z}|{ 휎1 ª®®® ¬ + 0.3 z}|{ 휋2 N ©­­­ « x; 휇2 |{z} 175.0 , 7.0 z}|{ 휎2 ª®®® ¬ The 2D three-class problem, PDF, and sample point clouds shown in figure 6.7 corre- spond to the following Gaussian mixture: p (x) = 0.33 z}|{ 휋1 N ©­­­­­ «  152 55   20 0 0 28  ®x; z}|{ ®휇1 z}|{ 횺1 ª®®®®® ¬ + 0.33 z}|{ 휋2 N ©­­­­­ «  175 70   35 39 39 51  ®x; z}|{ ®휇2 , z}|{ 횺2 ª®®®®® ¬ + 0.33 z}|{ 휋3 N ©­­­­­ «  135 40   10 0 0 10  ®x; z}|{ ®휇3 , z}|{ 횺3 ª®®®®® ¬ The PDF and sample point distribution of the GMM depend on the values of 휋ks, 휇ks, 횺ks, and K. In particular, K influences the number of peaks in the PDF (although if two peaks are very close, sometimes they merge). It also influences the number of clusters in the sample point cloud (again, if two clusters are too close, they may not be visually distinct). The 휋ks regulate the relative heights of the hills. The 휇ks and 횺ks influence the individual hills in the PDF as well as the individual clusters in the sample point cloud. Specifically, 휇k regulates the locations of the kth peak in the PDF and the centroid of the kth cluster in the sample point cloud. The 횺ks regulate the shape of the kth individual hill and the kth cluster in the sample point cloud. Figures 6.8, 6.9, 6.10, and 6.11 show some example GMMs with various values of these parameters. 6.9 Gaussian mixture models 225 Figure 6.8 shows a pair of Gaussian distributions and various GMMs with those as components, with different values for the parameters. Figure 6.9 depicts 2D GMMs with various 휋ks. Figure 6.10 shows GMMs with non-circular bases (non-symmetric Σs) and various 휇s). 0.05 0.04 0.03 0.02 0.01 0.00 p(x) 50 100 150 200 250 x (height in cms) female: N(152.4, 7.6) male: N(175.3, 7.6) Gaussian distribution (a) Gaussian components 휇1 = 152, 휇2 = 175, 휎1 = 휎2 = 9 0.05 0.04 0.03 0.02 0.01 0.00 p(x) 50 100 150 200 250 x (height in cms) female: N(152.4, 7.6) male: N(175.3, 7.6) GMM: (mu1 = 152.4 sigma1 = 7.6), (mu2 = 175.3 sigma2 = 7.6) pi1 = 0.5 pi2 = 0.5 female: N(152.4, 7.6) male: N(175.3, 7.6) GMM: (mu1 = 152.4 sigma1 = 7.6), (mu2 = 175.3 sigma2 = 7.6) pi1 = 0.5 pi2 = 0.5 Gaussian distribution (b) GMM with 휋1 = 0.5, 휋2 = 0.5 0.05 0.04 0.03 0.02 0.01 0.00 p(x) 50 100 150 200 250 x (height in cms) female: N(152.4, 7.6) male: N(175.3, 7.6) GMM: (mu1 = 152.4 sigma1 = 7.6), (mu2 = 175.3 sigma2 = 7.6) pi1 = 0.7 pi2 = 0.3 female: N(152.4, 7.6) male: N(175.3, 7.6) GMM: (mu1 = 152.4 sigma1 = 7.6), (mu2 = 175.3 sigma2 = 7.6) pi1 = 0.7 pi2 = 0.3 Gaussian distribution (c) GMM with 휋1 = 0.7, 휋2 = 0.3 0.05 0.04 0.03 0.02 0.01 0.00 p(x) 50 100 150 200 250 x (height in cms) female: N(152.4, 7.6) male: N(175.3, 7.6) GMM: (mu1 = 152.4 sigma1 = 7.6), (mu2 = 175.3 sigma2 = 7.6) pi1 = 0.3 pi2 = 0.7 female: N(152.4, 7.6) male: N(175.3, 7.6) GMM: (mu1 = 152.4 sigma1 = 7.6), (mu2 = 175.3 sigma2 = 7.6) pi1 = 0.3 pi2 = 0.7 Gaussian distribution (d) GMM with 휋1 = 0.3, 휋2 = 0.7 Figure 6.8 Various GMMs (solid curves) with the same Gaussian components (dotted and dashed curves, respectively) but different 흅1 and 흅2 values Another way to visualize GMMs is via sample point distributions. Figure 6.11 shows the sample points from a pair of 2D Gaussians and the points sampled from a GMM having those Gaussians as components and various mixture-selections probabilities. (a) 휋1 = 0.5, 휋2 = 0.5 (b) 휋1 = 0.4, 휋2 = 0.6 (c) 휋1 = 0.7, 휋2 = 0.3 (d) 휋1 = 0.3, 휋2 = 0.7 Figure 6.9 Two-dimensional GMMs with circular bases, (횺1 = 횺2 =  5 0 0 5  ), ®흁1 =  −3 −3  , ®흁2 =  3 3  . Note how the relative heights of the hills depend on 흅s. (a) ®휇1 = " −3 −3 # , Σ1 = " 2.75 2.25 2.25 2.75 # , ®휇2 = " 3 3 # , Σ2 = " 2.75 −2.25 −2.25 2.75 # (b) ®휇1 = " −4 −4 # , Σ1 = " 2.75 2.25 2.25 2.75 # , ®휇2 = " 4 4 # , Σ2 = " 2.75 −2.25 −2.25 2.75 # Figure 6.10 Two-dimensional GMMs with elliptical bases, 흅1 = 0.3, 흅2 = 0.7. Note how the shape of the hill base depends on 횺and how the hill positions depend on the ®흁s. 6.9 Gaussian mixture models 227 80 70 60 50 40 Weight in kg 130 140 150 160 170 180 Height in cm (a) 80 70 60 50 40 Weight in kg 130 140 150 160 170 180 Height in cm Step 15 (b) Figure 6.11 (a) 1, 000 random samples from three Gaussians with ®흁woman =  152 55  , 횺woman =  7 0 0 15  , ®흁man =  175 70  , 횺man =  9 10 10 25  , ®흁child =  135 40  , 횺child =  5 0 0 5  . (b) 1, 000 random samples from a GMM with the same three component Gaussians as in (a) and 흅1 = 흅2 = 0.4, 흅3 = 0.2. Note how the GMM sample distribution shape mimics the combined sample distribution shape of the component Gaussians. It can be proved that equation 6.29 is a proper probability: that is, it sums to 1 over the space of all possible inputs (all possible values of ®x in the d-dimensional space). Here is the proof outline: ∫ ®x∈ℜd p  ®x d®x = ∫ ®x∈ℜd K Õ k=1 휋kN  ®x; ®휇k, 횺k  ! d®x = K Õ k=1 휋k ©­­­­­ « equals 1, N being a PDF z }| { ∫ ®x∈ℜd N  ®x; ®휇k, 횺k  d®x ª®®®®® ¬ = K Õ i=1 휋k = 1 6.9.2 Latent variables for class selection Let’s discuss GMMs in more detail. In particular, we look at the physical meaning of the various terms in equation 6.29. Before diving in, let’s introduce an auxiliary random variable Z, which effectively is a class selector. In the context of equation 6.29, Z can take discrete values in the range [1 · · · K]. It thus follows a categorical distribution (see section 5.9.6). Physically, Z = k means the kth class—that is, the kth component of the Gaussian mixture—has been selected. 228 CHAPTER 6 Bayesian tools for machine learning NOTE As usual, we are denoting the random variable with uppercase and the specific value it takes in a given instance with lowercase. For instance, in the two-class problem shown in figure 6.6, Z can take one of two values: 1 (implying adult female) or 2 (implying adult male). For the three-class problem shown in figure 6.7, Z can take one of three values: 1 (adult female), 2 (adult male), or 3 (child). Z is called a latent (hidden) random variable because its values are not directly observed. Contrast this with the input random variable ®x whose values are explicitly observed. You may recognize Z as a latent variable in the GMM (latent variables were introduced in section 6.7). Consider the joint probability p  X = ®x, Z = k, which we sometimes informally denote as p  ®x, k. This is the probability of the input variable ®x occurring together with the class k. Using Bayes’ theorem, p  ®x, k = p  ®x k p (k) The conditional probability term p  ®x k is the probability of ®x when the kth class has been selected. This means it is the PDF for the kth Gaussian component, which is a Gaussian distribution by assumption. As such, using equation 5.23, p  ®x k = N  ®x; ®휇k, 횺k  k ∈[1, K] On the other hand, p (Z = k), which we sometimes informally refer to as p (k), is the prior probability (that is, without reference to the input) of the input belonging to one of the classes. Let’s denote it as follows: p (k) = 휋k, ∀k ∈{1, K} This is often modeled as the fraction of training data points belonging to class k: 휋k ≈Nk N k ∈{1, K} where Nk is the number of training data instances belonging to class k, and N is the total number of training data instances. From this, we get p  ®x, k = p (k) p  ®x k = 휋k N  ®x; ®휇k, 횺k  k ∈[1, K] From equation 5.5, we get the marginal probability p (x) p (x) = Õ k∈{1,K} p (x, k) = K Õ k=1 휋k N  ®x; ®휇k, 횺k  which is the same as equation 6.29. 6.9 Gaussian mixture models 229 This leads to the following physical interpretations: A GMM can be viewed as a weighted sum of K Gaussian components. Equation 6.29 depicts the PDF of the overall GMM. The weights 휋k are component selection probabilities. Specifically, 휋k can be interpreted as the prior probability p (Z = k), aka p (k), of selecting the kth subclass—modeled as the fraction of the population belonging to the kth subclass. The 휋k are probabilities in a categorical distribution with K classes. The 휋ks sum up to 1. Sampling from the GMM can be viewed as a two-step process: 1 Randomly select a component. The probability of the kth component being selected is 휋k. The sum of all 휋ks is 1, which signifies that one or another com- ponent must be selected. 2 Random sample from the selected Gaussian component. The probability of generating vector ®x is N  ®x; ®휇k, 횺k . Each of the K Gaussian components models an individual class. Geometrically speaking, the components correspond to the clusters in the sample point cloud or the peaks in the PDF of the GMM. The kth Gaussian component, N  ®x; ®휇k, 횺k , can be interpreted as the conditional probability, p  ®x k. This is the likelihood—the probability of data value ®x occurring, given that the kth subclass has been selected. The product 휋k N  ®x; ®휇k, 횺k  then represents the joint probability p  ®x, k = p  ®x k p (k). The sum of all the joint subclass probabilities is the marginal probability p  ®x of the data value ®x. Listing 6.8 Gaussian mixture model distribution Pytorch supports distributions that are mixtures of the same family (here, Gaussian) from torch.distributions.mixture_same_family import MixtureSameFamily pi = Categorical(torch.tensor([0.4, 0.4, 0.2])) Prior probabilities over the three classes (male, female, child): categorical distribution mu = torch.tensor([[175.0, 70.0], [152.0, 55.0], [135.0, 40.0]]) Mean height, weight for the three classes (male, female, child) sigma = torch.tensor([[[30.0, 20.0], [20.0, 30.0]], Covariance matrices for the three classes (male, female, child) [[50.0, 0.0], [0.0, 10.0]], [[20.0, 0.0], [0.0, 20.0]]]) gaussian_components = MultivariateNormal(mu, sigma) Creates the component Gaussians gmm = MixtureSameFamily(pi, gaussian_components) Creates the GMM 230 CHAPTER 6 Bayesian tools for machine learning 6.9.3 Classification via GMM A typical practical problem involving GMMs goes as follows. A set of unlabeled input data X (training data) is provided. It is important to note that this is unsupervised machine learning—the training data does not come with known output classes. The physical nature of the problem indicates the subclasses in the data (denoted by indices [1 · · · K]). The goal is to classify any arbitrary input ®x: that is, map it to one of the K classes. To do this, we have to fit a GMM (that is, derive the values of 휋k, ®휇k, 횺k for all k ∈[1 · · · K]). Given an arbitrary ®x, we compute p  k ®x for all the classes (all values of k). The value of k yielding the max value for p  k ®x is the class corresponding to ®x. How do we compute p  k ®x? Using Bayes’ theorem, p  k ®x = p  ®x, k p  ®x = p  ®x k p (k) ÍK i=1 p  ®x, k = 휋k N  ®x; ®휇k, 횺k  ÍK i=1 휋i N  ®x; ®휇i, 횺i  (6.30) If we know all the GMM parameters, evaluating equation 6.30 is straightforward. We classify the input ®x by assigning it to the cluster k that yields the highest value of p (Z = k|X = x). Geometrically, this assigns the input to the cluster with the “closest” mean—with distance normalized by the variance of the respective distribution. Basically, we are measuring the distance from the mean, but in clusters of high variance, we are more tolerant of distance from the mean. This makes intuitive sense: if the cluster is widely spread (has high variance), a point relatively far from the cluster mean can be said to belong to the cluster. On the other hand, a point the same distance from the mean of a tightly packed cluster may be deemed to be outside the cluster. 6.9.4 Maximum likelihood estimation of GMM parameters (GMM fit) A GMM is fully described in terms of its parameter set 휃=  휋k, ®휇k, 횺k ∀k ∈[1 · · · K] . But how do we estimate these parameter values? In typical real-life situations, they are not given to us. We only have a set of observed unlabeled training data points X =  ®x(i) , such as (weight, height) values for Statsville residents. Geometrically speaking, each data instance in the training dataset corresponds to a single point in the multidimensional feature space. The training dataset is a point cloud that naturally clusters into Gaussian subclouds (otherwise, we should not be trying GMMs). Our GMM mimicking this dataset should have as many components as there are natural clusters in the data. The parameter values 휋k, ®휇k, 횺k for k ∈[1 · · · K] should be estimated such that the GMM’s sample point cloud overlaps the training data point cloud as much as possible. That is the basic problem we try to solve in this section. NOTE We do not estimate K, the number of classes; rather, we use a fixed value of K, usually estimated from the physical conditions of the problem. For example, in the problem with men, women, and children, it is pretty obvious that K = 3. In section 6.8, we did MLE for a simple Gaussian. We computed an expression for the joint log-likelihood of all the training data given a Gaussian probability distribution. Then we took the gradient of that expression with respect to the parameters and equated 6.9 Gaussian mixture models 231 it to zero. We were able to solve that equation to derive a closed-form solution for the parameters, ®휇and 횺(equations 6.25 and 6.26 ). This means we simplified the equation into a form where the unknown (to be solved) appeared alone on the left-hand side and there were only known entities on the right-hand side. Unfortunately, with GMMs, equating the gradient of the log-likelihood to zero leads to an equation that has no closed-form solution. So, we cannot reduce the equation to a form where the unknowns 휋ks, 휇ks, and 횺k appear alone on the left-hand sides and only known entities (®xis) appear on the right-hand side. Consequently, we have to go for an iterative approximation. We rewrite the equation we get by equating the gradient of the log-likelihood to zero such that the unknowns 휇s and 휎s appear alone on the right-hand side. It looks something like 휋k = f1 (X, 휃) ®휇k = f2 (X, 휃) 횺k = f3 (X, 휃) where f1, f2, f3 are some functions whose exact nature is unimportant at the moment. Note that the right-hand side also contains the unknowns: 휃contains 휋ks, 휇ks, and 횺k. We cannot directly solve such equations, but we can use iterative relaxation, which works roughly as follows: 1 Start with random values of 휋ks, ®휇ks, and 횺ks. 2 Evaluate the right-hand side by plugging current values of 휋ks, ®휇ks, and 횺ks into functions f1, f2, and f3. 3 Use the values estimated in step 2 to set new values of 휋ks, ®휇ks, and 횺ks. 4 Repeat steps 1–3 until the parameter values stop changing appreciably. The actual functions f1, f2, f3 are worked out in (equations 6.36, 6.37, and 6.38). As iteration progresses, the values of 휋ks, ®휇ks, and 횺ks start to converge to their true values. This is not a lucky coincidence. If we follow algorithm 6.1, it can be proved that every iteration improves the approximation, even if by a minuscule amount. Eventually, we reach a point when the approximation is no longer improving appreciably. This is called the fixed point, and we should stop iterating and declare the current values final. Figure 6.12 shows the progression of an iterative GMM fit algorithm. Figure 6.12a shows the sampled training data distribution. Figure 6.12b shows the fitted GMM at the beginning: the parameters are essentially random, and the GMM looks nothing like the target training data distribution. It improves slowly until at iteration 15, it matches the target distribution snugly (figure 6.12d). Now let’s discuss the details. We already know the dataset X that has been observed. What parameter set 휃will maximize the conditional probability, p (X|휃), of exactly these data points, given the parameter set? In other words, what model parameters will maximize the overall likelihood of the training data? Those will be our best guesses for the unknown model parameters. This is MLE, which we encountered in section 6.6.2. 232 CHAPTER 6 Bayesian tools for machine learning 90 85 80 75 70 65 60 55 50 45 40 35 30 (a) Training data point cloud (target for fitting) 120 100 80 60 40 20 Weight in kg 120 140 160 180 200 220 Height in cm Step 0 100 (b) Fitted GMM’s sample point cloud at step 0 80 60 40 20 Weight in kg Step 5 120 140 160 180 200 Height in cm 100 (c) Fitted GMM’s sample point cloud at step 5 Height in cm 140 150 160 170 180 130 80 70 60 50 40 Weight in kg (d) Fitted GMM’s sample point cloud at step 15. It almost matches the target. Figure 6.12 Progression of maximum likelihood estimation for GMM parameters Let  ®x(1), ®x(n), · · · ®x(n) be the set of observed data points, aka training data. From equation 6.29, likelihood z }| { p  ®x(i) 휃  = K Õ k=1 휋k N  ®x(i); ®휇k, 횺k  ∀i ∈[1, n] Henceforth, for simplicity, we drop the “given 휃” part and refer to p  ®x(i) 휃  simply as p  ®x(i) . As usual, instead of maximizing the likelihood directly, we maximize its logarithm, the log-likelihood. This will yield the same parameters as maximizing the likelihood directly. 6.9 Gaussian mixture models 233 Since the x(i)s are independent, their joint probability, as per equation 5.4, is p  ®x(1) p  ®x(2) · · · p  ®x(n) The corresponding log joint probability is joint log-likelihood z }| { log  p  ®x(1) p  ®x(2) · · · p  ®x(n) = n Õ i=1 log  p  ®x(i) = n Õ i=1 log K Õ k=1 휋k N  ®x(i); ®휇k, 횺k ! (6.31) At this point, we begin to see a difficulty peculiar to GMMs. We have a logarithm of a sum, which is not a very friendly expression to handle; the logarithm of products is much nicer to deal with. But let’s soldier on. To identify the parameters ®휇1, Σ1, ®휇2, Σ2, · · · that will maximize the log joint proba- bility, we take the gradient of the log joint probability with respect to these parameters, equate them to zero, and solve for the parameter value (as discussed in section 3.3.1). Here we demonstrate the process with respect to ®휇1: ∇®휇1log  p  ®x(1) p  ®x(2) · · · p  ®x(n) = 0 Since the log of products is the sum of logs, we get ∇®휇1 n Õ i=1 log  p  ®x(i) = 0 Applying equation 6.29, we get ∇®휇1 n Õ i=1 log K Õ k=1 휋k N  ®x(i); ®휇k, 횺k ! = 0 Since the gradient is a linear operator, we can move it inside the summation: n Õ i=1 ∇®휇1log K Õ k=1 휋k N  ®x(i); ®휇k, 횺k ! = 0 Since d dxlog (f (x)) = 1 f (x) df dx , we get n Õ i=1 ∇®휇1 ÍK k=1 휋kN  ®x(i); ®휇k, 횺k  ÍK k=1 휋k N  ®x(i); ®휇k, 횺k  = 0 Now, if x1 and x2 are independent variables, dx2 dx1 = 0. Consequently, ∇®휇1N  ®x(i); ®휇k, 횺k  = 0 for k ≠1 234 CHAPTER 6 Bayesian tools for machine learning Only a single term corresponding to k = 1 survives the differentiation (gradient) in the numerator. So, n Õ i=1 휋1∇®휇1N  ®x(i); ®휇1, 횺1  ÍK k=1 휋k N  ®x(i); ®휇k, 횺k  = 0 Now d dx e−(x−휇)2 = −2 (x −휇) e−(x−휇)2, and in multiple dimensions, ∇®휇e−1 2 (®x−®휇)T Σ−1(®x−®휇) = −∇®휇  ®x −®휇T Σ−1  ®x −®휇 e−1 2 (®x−휇)T Σ−1(®x−휇) Plugging equation 5.23 into our maximization problem, we get n Õ i=1 ∇®휇1  ®x(i) −®휇1 T 횺−1 1  ®x(i) −®휇1  휋1 N  ®x(i); ®휇1, 횺1  ÍK k=1 휋k N  ®x(i); ®휇k, 횺k  = 0 Furthermore, with a little effort, you can prove the following about the gradient of a quadratic form: ∇®x  ®xT A®x  = A®x (6.32) Applying equation 6.32 to our problem, we get n Õ i=1 횺−1 1  ®x(i) −®휇1  휋1 N  ®x(i); ®휇1, 횺1  ÍK k=1 휋k N  ®x(i); ®휇k, 횺k  = 0 Multiplying both sides by the constant 횺1, we get n Õ i=1  ®x(i) −®휇1  휋1 N  ®x(i); ®휇1, 횺1  ÍK k=1 휋k N  ®x(i); ®휇k, 횺k  = 0 Substituting 훾i1 = 휋1 N  ®x(i); ®휇1, 횺1  ÍK k=1 휋k N  ®x(i); ®휇k, 횺k  (6.33) we get n Õ i=1  ®x(i) −®휇1  훾i1 = 0 This expression has 휇1 inside 훾i1 as well. It is impossible to extract 휇1 alone on the left side of the equation. In other words, we cannot create a closed-form solution for 휇1. Hence, we have to solve it iteratively. We can rewrite the previous equation as ®휇1 = 1 N1 n Õ i=1 훾i1®x(i) 6.9 Gaussian mixture models 235 where N1 = n Õ i=1 훾i1 (6.34) Proceeding similarly, we can derive the corresponding expressions for 휋1 and 횺1. Let’s collect all the equations for updating the GMM parameters: N1 = n Õ i=1 훾i1 (6.35) 휋1 = N1 n (6.36) ®휇1 = 1 N1 n Õ i=1 훾i1®x(i) (6.37) 횺1 = 1 N1 n Õ i=1 훾i1  ®x(i) −®휇1   ®x(i) −®휇1 T (6.38) Equations 6.36, 6.37, and 6.38 provide the definitions for functions f1, f2, and f3 that we saw at the beginning of this section in the context of iterative relaxation. We can deal similarly with k = 2 · · · K. Physical significance of 휸ik We encountered the entity 훾ik while computing the gradient of the log-likelihood. It appeared as a multiplicative weight in the final iterative expression for computing 휇k and Σk in equations 6.37 and 6.38. It is not an arbitrary entity. By comparing equations 6.33 and 6.30, we can see that 훾ik = p  k ®x(i) In other words, the quantity 훾ik is really the posterior probability: the conditional prob- ability of the class k given the ith data point. This gives us a new way to look at equations 6.35, 6.36, 6.37, and 6.38: Equation 6.35 essentially assigns to N1 the probability mass concentrated in class 1 as per the current parameter values. Equation 6.36 assigns to 휋1 the fractional mass in class 1 as per the current param- eter values. Equation 6.37 assigns to 휇1 the centroid of all the training data points. Each data point’s contribution is weighted by the posterior probability, as per the current parameter values, of that data point belonging to class 1. Equation 6.38 assigns to 횺1 the covariance of the training data points. Each data point’s contribution is weighted by the posterior probability, as per the current parameter values, of that data point belonging to class 1. 236 CHAPTER 6 Bayesian tools for machine learning Algorithm 6.1 ties together equations 6.33, 6.36, 6.37, and 6.38 into a complete approach for iterative MLE of GMM parameters. It is an example of a general class of algorithms called expectation maximization. Algorithm 6.1 GMM fit (MLE of GMM parameters from unlabeled training data) Input: X = ®x(i), ®x(2), · · · , ®x(n) Initialize parameters 휃= {휋k, 휇k, 횺k k ∈[1, K]} with random values ⊲repeat E-step and M-step until likelihood stops increasing while (likelihood is increasing ) do ⊲E-step 훾ik = 휋k N  ®x(i); ®휇k, 횺k  ÍK k=1 휋k N  ®x(i); ®휇k, 횺k  ∀i, k ∈[1, n] × [1, K] ⊲M-step Nk = Ín i=1 훾ik 휋k = Nk n ®휇k = 1 Nk Ín i=1 훾ik ®x(i) 횺k = 1 Nk Ín i=1 훾ik  ®x(i) −®휇k   ®x(i) −®휇k T   ∀k ∈[1, K] end while return {휋1, 휇1, 횺1, 휋2, 휇2, 횺2, · · · , 휋K, 휇K, 횺K} NOTE Fully functional code for Gaussian mixture modeling, executable via Jupyter Notebook, can be found at http://mng.bz/j4er. Listing 6.9 GMM fit while (curr_likelihood - prev_likelihood) < 1e-4: Repeats until the likelihood increase is negligible # E Step Computes the posterior probabilities 휸i,k = p (Z = k|X = xi ) using current ®흁ks and 횺ks, equation 6.33 pi = gmm.mixture_distribution.probs Tensor of shape [K] holding 흅ks for all k components = gmm.component_distribution Gaussian objects N   ®x; ®흁k, 횺k  for all k Summary 237 Vector computation of log of 휸i,k numerators for all i, k, equation 6.33 log_gamma_numerators = components.log_prob( X.unsqueeze(1)) + torch.log(pi).repeat(n, 1) In practice, the probability involving an exponential goes to 0. So we use the log probability. Vector computation of the log of 휸i,k denominators for all i, k, equation 6.33 log_gamma_denominators = torch.logsumexp( log_gamma_numerators, dim=1, keepdim=True). Vector computation of the [n × K] tensor 휸i,k, equation 6.33 for all i, k log_gamma = log_gamma_numerators - log_gamma_denominators self.gamma = torch.exp(log_gamma) # M Step Updates ®흁k and 횺k for all k using 휸i,k = p(Z = k|X) from the E-step via equations 6.36, 6.37, and 6.38 n = X.shape[0] Number of data points N = torch.sum(gamma, 0) pi = N / n Vector update of 흅k for all k, equation 6.36 mu = ((X.T @ gamma)/N).T Vector update of [K × d] tensor, ®흁k for all k, equation 6.37 x_minus_mu = (X.repeat(K, 1, 1) - gmm.component_distribution.unsqueeze(1). repeat(1, n, 1)) Vector computation of   ®xi −®흁k    ®xi −®흁k T for all i, k x_minus_mu_squared = x_minus_mu.unsqueeze(3) @ x_minus_mu.unsqueeze(2) Vector update of K × d × d] tensor 횺k for all k, equation 6.38 sigma = torch.sum(gamma.T.unsqueeze(2).unsqueeze(3) * x_minus_mu_squared, axis=1) / N.unsqueeze(1).unsqueeze(1).repeat(1, d, d) prev_likelihood = curr_likelihood curr_likelihood = torch.sum(gmm.log_prob(X)) log likelihood, equation 6.31 Summary In this chapter, we looked at the Bayesian tools for decision-making in uncertain systems. We discussed conditional probability and Bayes’ theorem, which connects conditional probabilities to joint and marginal probabilities. Conditional probability is the probability of an event occurring subject to the condition that another event has already occurred. In machine learning, we are often interested in the conditional probability p  ®x 휃 of an input ®x given that the parameters of the model predicting the input are 휃. This conditional probability is known as the likelihood of the input. We are also interested in the conditional probability p  휃 ®x, known as the posterior probability. 238 CHAPTER 6 Bayesian tools for machine learning Joint probability is the probability of a set of events occurring together. If the events are independent, the joint probability is the product of their individual probabili- ties. Whether events are independent or not, Bayes’ theorem connects joint and conditional probabilities. Of particular interest in machine learning is the Bayes’ theorem expression connecting the likelihood and joint and posterior probabili- ties of inputs and parameters: p  ®x, 휃 = p  ®x 휃 p (휃) and p  휃 ®x = p(®x|휃)p(휃) p(®x) . p  ®x 휃 is the probability distribution function of the chosen distribution family. p (휃) is the prior probability that codifies our belief, sans data, about the system. A pop- ular choice is p (휃) ∝e−∥휃∥2, implying smaller probabilities for higher-magnitude parameters and vice versa. Entropy models the uncertainty in a system. Systems where all events have more or less similar probabilities tend to be high-entropy. Systems where a particular subset of possible events have significantly high probabilities and others have significantly low probabilities tend to be low-entropy. Equivalently, the probability density functions of low-entropy systems tend to have tall peaks, and their sample point clouds have a high concentration of points in some regions. High-entropy systems tend to have flat probability density functions and diffused sample point clouds. Cross-entropy allows us to quantify how good our modeling is against a known ground truth. Kullback–Leibler divergence gives us a measure of the dissimilarity between two probability distributions. Maximum likelihood estimation (MLE) and maximum a posteriori (MAP) estima- tion are two paradigms for estimating model parameters. MLE maximizes p (X|휃), and MAP maximizes p (X|휃) p (휃). MLE essentially tries to estimate probability distribution parameters that maximize the overlap between the sample point cloud of the probability distribution and the training data point cloud. MAP is MLE with a regularization condition. The regularization condition is injected via the prior probability term p (휃), which favors solutions with a certain property (such as small parameter magnitudes) that we believe to be true from empirical knowledge without data. MLE for Gaussian distributions has a closed-form solution. The mean and variance (covariance in the multidimensional case) of the optimal probability dis- tribution that best fits the training data are the sample mean and sample variance or covariance on the training dataset. Latent variables in a machine learning system are auxiliary variables that are not directly observed but can be derived from the input. They facilitate the expression of the goal of optimization or the loss to be minimized. Gaussian mixture models (GMM) are unsupervised probability models that fit multiclass data distributions having multiple clusters in the training dataset, each corresponding to a different class. Here, MLE does not yield a closed-form solution but instead yields an iterative solution to estimate the mixture weights, means, and variances of the individual Gaussians in the mixture. 7 Function approximation: How neural networks model the world This chapter covers Expressing real-world problems as mathematical functions Understandingthebuildingblocks ofaneural network Approximatingfunctionsvianeuralnetworks Computing to date has been dominated by the von Neumann architecture in which the processor and the program are separate. The program sits in memory and is fetched and executed by the processor. The advantage of this approach is that different programs solving unrelated problems can be loaded into memory, and the same processor can execute them. But neural networks have a fundamentally different architecture. There are no separate processors and programs; instead, there is a single entity called, well, the neural network, which can run on dedicated hardware or a Von Neumann computer. In this chapter, we discuss this paradigm in detail. 239 240 CHAPTER 7 Function approximation: How neural networks model the world NOTE The complete PyTorch code for this chapter is available at http://mng.bz/K4zj in the form of fully functional and executable Jupyter notebooks. 7.1 Neural networks: A 10,000-foot view In section 1.7, we provided an overview of neural networks. (You may want to do a quick refresher on chapter 1 at this point.) There we indicated that most intelligent tasks performed by humans can be expressed in terms of mathematical functions that we will refer to as target functions. So, to develop machines that perform intelligent tasks, we need to have machines that model target functions. While that gives us hope of developing automated solutions, we are hobbled by two serious difficulties: In addition to being arbitrarily complicated, the target functions underlying various real-life problems are completely different from one another. There is hardly any common pattern. For most problems, we donot know the underlying target function. Despite all this, we want to come up with a mechanized repeatable solution for perform- ing real-life intelligent tasks. And we do not want to start from scratch and design the underlying function for each such problem. This is where neural networks help: Neural networks provide a unified framework to model an extremely wide variety of arbitrarily complicated functions. While the overall neural network models a complicated function, its building block is a fairly basic unit called a neuron. The neuron represents a relatively simple function. The full neural network is made up of many neurons with weighted connections between them. It can be made to approximate any arbitrary target function underlying a particular problem of interest by manipulating the number of neurons, the connectivity between them, and the connection weights. The variety and complexity of the functions a neural network can represent are known as its expressive power. Expressive power increases with the number of neurons in the neural network and the number of connections between them. The more complex the target function, the more expressive power will be needed in the neural network modeling it. How can we make a neural network model/approximate/express a specific target function corresponding to a particular problem of interest? Answer: we can adjust the following two aspects of the neural network: Architecture—The number of neurons and the connections between them Parameter values—The weights of the connection between neurons The architecture is typically chosen based on the nature of the problem. Some popular architectures are reused frequently, and a neural network engineer typically chooses an architecture that is historically known to be effective for a problem similar to the problem at hand. We look at several popular architectures later in this book—for instance, in chapter 11. Once the architecture is set, we determine the parameter values through a process called training. 7.2 Expressing real-world problems: Target functions 241 Neural networks can be classified into two major classes: supervised and unsupervised. In supervised neural networks, we identify the desired output values corresponding to a set of sampled input values for the problem we are trying to solve. The desired output for these sampled inputs is typically chosen manually using a process called labeling (aka manual annotation or manual curation). The overall set of <sampled input, desired output> pairs constitutes the supervised training data. The set of desired outputs for training data inputs is sometimes collectively referred to as the ground truth or target output. During training, the parameter values (aka weights) are adjusted such that the network’s outputs on training inputs match the corresponding ground truth as closely as possible. If all goes well, at the end of training, we are left with a neural network whose outputs on training inputs are close to the ground truth. This trained neural network is then deployed to the real world, where it performs inferencing—it generates output on inputs it has never seen before. If we have chosen the architecture with enough expressive power and properly trained the network with adequate training data, it should emit accurate results during inferencing. Note that we cannot guarantee correct results during inferencing; we can only make a probabilistic statement that our output has a p probability of being correct. Unsupervised neural networks do not need the manually labeled ground truth—they just work on the training inputs. The manual labor involved in labeling the training data is expensive and bothersome. Consequently, considerable research effort is going into neural networks that are unsupervised, semi-supervised (a fraction of the training data is labeled manually), or self-supervised (labeled training data is created programmatically rather than manually). However, unsupervised and semi-supervised neural network technology is less mature at the time of this writing, and it is harder to achieve desired accuracy levels with them. Later in this book, we examine unsupervised approaches, in- cluding variational autoencoders (chapter 14). But for now, we mostly talk about supervised approaches. It is important to note that nowhere in the architecture selection or training process do we need a closed-form representation of either the function being approximated (the target function) or the approximator function (the modeling function). This is important. In most cases, it is impossible to know the target function—all we know are sample input and ground-truth pairs (training data). As for the modeling function, even when we know the architecture of the modeling neural network, the overall function it represents is so complicated that it is virtually intractable. Thus, the fact that we do not need to know the target or modeling function in closed form is what makes the technology practical. 7.2 Expressing real-world problems: Target functions Consider the classic investor’s problem: to sell or not sell a stock. The problem inputs could be the purchase price, current price, whether the investor’s favorite expert is advising to sell or not, and so on. The problem can be solved by a function that takes 242 CHAPTER 7 Function approximation: How neural networks model the world these inputs and outputs as 0 (do not sell) or 1 (sell). If we could model this function, we would have a mechanical solution to this real-world problem. Like this example, most real-world problems can be expressed as target functions. We collect all quantifiable variables that can have a bearing on the outcome: these constitute the input variables. The input variables are expressed as numeric entities: scalars, vectors, matrices, and so on. The outputs are also expressed as numeric variables called the output variables. Given a specific input (say, specific values for purchase price and current price), our model function emits an output (0 or 1 indicating do not sell or sell) that is a solution to the problem for that specific input. We usually denote input variables with the symbol x; a sequence of input variables is often expressed as a vector ®x. Output variables are denoted with the symbol y. The overall target function is usually expressed as y = f  ®x. We will often use subscripts to denote various elements of a vector (x0, x1, · · · , xi) and superscripts to denote input instances, as in y(0) = f  ®x(0) , y(1) = f  ®x(1) , · · · , y(i) = f  ®x(i) . But in some cases, we will use subscripts to denote different items of the training data. The usage should be obvious from the context. Numeric quantities can occur in two distinct forms: continuous and categorical. Continuous variables can take any of the infinitely many real number values in a given range. For instance, the stock price in our “to sell or not sell a stock” problem can take any value greater than zero. Categorical variables can take one of a finite set of allowed values, where the value represents a category. A special categorical case is a binary variable, where there are only two categories. For instance, expert advice in our stock-selling problem can take only two values: 0 or 1, corresponding to the two categories of advice, “do not sell” and “sell,” respectively. In this section, we discuss three distinct families of target functions: logical functions, general functions, and classifiers. 7.2.1 Logical functions in real-world problems These are functions whose inputs and outputs are binary variables: variables that can take only two values, 0 (aka “no” or “don’t fire”) and 1 (aka “yes” or “fire”). Machines emulating logical functions are often added on top of separate machines performing other tasks, as will be evident from the following examples: Logical OR—To look at logical OR functions, let’s bring back the mythical cat whose brain we discussed in chapter 1. Say we are trying to build a machine that helps the poor creature make the binary decision whether to run away from the object in front of it or approach the object and purr. Being very timid, this cat runs away from anything that looks hard or sharp. The only time it will approach and purr is when the object in front of it looks neither hard nor sharp. Let’s assume a separate machine outputs a binary decision 0 (not hard) or 1 (hard). Another machine outputs a binary decision 0 (not sharp) or 1 (sharp). The logical OR machine combines the binary decisions from the two separate machines, as shown in figure 7.1a. 7.2 Expressing real-world problems: Target functions 243 Threat classifier model Hardness estimator model Sharpness estimator model 0 0 0 0 1 1 1 0 1 1 1 1 Truth table 0 = not hard 1 = hard 0 = not sharp 1 = sharp 0 = approach and purr 1 = run away Input image hardness sharpness (a) Logical OR: a timid cat that runs away from things whose hardness exceeds threshold t0 OR whose sharpness exceeds threshold t1 Threat classifier model Hardness estimator model Sharpness estimator model 0 0 0 0 1 0 1 0 0 1 1 1 Truth table 0 = not hard 1 = hard 0 = not sharp 1 = sharp 0 = approach and purr 1 = run away Input image hardness sharpness (b) Logical AND: a less timid cat that runs away from things whose hardness exceeds threshold t0 AND whose sharpness exceeds threshold t1 0 0 0 0 0 0 1 1 0 1 0 1 0 1 1 1 1 0 0 1 1 0 1 1 1 1 0 1 1 1 1 1 Truth table To-Brake or Not-To-Brake Person detector model 0 = person not detected 1 = person detected 0 = do not apply brakes 1 = apply brakes Vehicle detector model Road bend detector model 0 = vehicle not detected 1 = vehicle detected 0 = road bend not detected 1 = road bend detected Input image (c) Multi-input logical OR: A self-driving car that applies the brake if it sees a person, vehicle, or bend in the road in front of it Figure 7.1 Examples of logical operators (OR, AND) in real-life problems 244 CHAPTER 7 Function approximation: How neural networks model the world Logical AND—We also exemplify this in terms of the cat brain. Imagine a slightly less timid cat that runs away from things that are both hard and sharp. But it is not scared by hardness and sharpness alone. Its brain can be modeled by the system of machines shown in figure 7.1b. Logical NOT—Consider a machine that sounds an alarm if it sees any unauthorized person in a restricted access area. Let’s assume that we also have a separate machine: a face detector that can recognize the faces of all authorized personnel. It emits a binary decision 1 (recognized face) or 0 (unrecognized face). The overall system takes the output of the face detector and performs a logical NOT operation on it. Multi-input logical OR—Imagine a machine that decides whether a self-driving car needs to brake. Assume that three separate detectors emit 1 if a person, vehicle, or bend in the road, respectively, is seen in front of the car. A brake must be applied if any of these separate detectors emits a 1. This is shown in figure 7.1c. Multi-input logical AND—Consider a machine that helps a venture capitalist decide whether to invest in a startup. Assume that three separate machines emit 1 when the following conditions are met: (1) the CEO has a track record of success, (2) the product elicits interest from targeted customers, and (3) the product is sufficiently novel, respectively. The machine will decide to invest if all three separate machines emit 1. Thus, the machine outputs 1 when condition (1) is met AND condition (2) is met AND condition (3) is met. This is an example of a three-input AND. Logical XOR—Suppose we are building a social media site. Assume we have a separate detector that, for any person, emits 1 if they like rock music and 0 otherwise. Using this information about two people, the problem is to decide whether they should be recommended as friends to each other. Friendship potential is high if they both like rock music or both dislike it. But if one person likes rock and the other dislikes it, they will probably not be good friends. Thus condition 1 is high rock-music affinity for person 1, and condition 2 is high rock-music affinity for person 2. The exclusive OR of the two conditions is 1 when one is true but the other is not. This machine outputs 1 if the NOT of the exclusive OR is true, meaning neither person likes rock music or both people like rock music. Figure 7.2 depicts this. m-out-of-n trigger—Imagine we are trying to create a face detector. We have already created separate part detectors for noses, eyes, lips, and ears. If we detect, say, any two of these together, we feel confident enough to declare a face. In computer vision, we often have a problem called occlusion, where an important object becomes invisible to the camera because another object blocks the camera’s line of sight. Computer vision algorithms always try to be robust against occlusion, meaning they want to emit the right output even when occlusion occurs. This is why we do not want to mandate a positive signal from all the part detectors; we want to detect the face even when a few of the parts are occluded. Hence, our machine emits 1 when, say, two of the n parts (such as eyes and lips) are detected. 7.2 Expressing real-world problems: Target functions 245 0 0 1 0 1 0 1 0 0 1 1 1 Truth table Recommendation engine Rock music affinity estimator model Rock music affinity estimator model 1 = likes rock music 0 = dislikes rock music 1 = recommend 0 = do not recommend 1 = likes rock music 0 = dislikes rock music Person A’s music playing history Person B’s music playing history (person A’s rock music afﬁnity) (person B’s rock music afﬁnity) Figure 7.2 Example of logical NOT and XOR in a real-life problem. A social media system makes a friendship recommendation between persons A and B if and only if they both like or both dislike rock music. friendship = ¬ (rock-music-affinity-of-A ⊕rock-music-affinity-of-B) where ¬ =⇒logical NOT and ⊕=⇒logical XOR. 7.2.2 Classifier functions in real-world problems A classifier is a function whose output is categorical. Inputs can be either continuous or categorical. Thus, given an input, the function chooses one category (aka class) or another. For instance, a face detector can be a classifier. Its input is an image, and its output is a categorical (binary) variable that takes one of two possible values: 1 (face) or 0 (not face). This is shown in figure 7.3a. As we saw in section 2.3, any image can be represented by a vector ®x. Accordingly, the classifier function for the face detector Discriminative face detector Discriminative face detector Discriminative face detector (a) Discriminative face detector (classifier). The output is categorical (face or not face). Generative face detector Generative face detector Generative face detector (b) Generative face detector. The output is continuous (the probability of an image containing a face). Figure 7.3 The face detector takes an image as input and outputs a categorical or continuous variable. 246 CHAPTER 7 Function approximation: How neural networks model the world can be written as the function 휙 ®x = ( 0 not a face 1 face How to design the function 휙 ®x is one of the primary topics of this chapter. Geometrically, each scalar input variable forms a separate dimension in the input space. All possible combinations of these scalar input variables together form a multi- dimensional space called the input space (or feature space). Each specific combination of input values is a point (represented by the input vector ®x) in this space. For instance, in an image, each pixel can be taken as a separate input scalar variable that can take any 3-byte pixel color value between RGB = 00 00 00 (hex) (black) and RGB = FF FF FF (hex) (white), with successive bytes (demarcated by an overline) representing red, green, and blue components of the pixel, respectively. The input has as many dimensions as the number of pixels in the image. For instance, a 224 × 224 image forms a 50,176-dimensional input space. Each specific image is a single point in this space. The face-classifier function 휙 ®x maps that point to either 0 (not face) or 1 (face). For a simpler instance, consider our familiar cat brain example from section 1.4. There are two input variables: x0, indicating hardness; and x1, indicating sharpness. The overall input space is two-dimensional, in which a specific input combination is denoted by the 2D vector ®x =  x0 x1  . Our goal is to construct a machine that, given any input combination of hardness and sharpness, classifies it as either threatening or nonthreatening. This is equivalent to designing a function that maps arbitrary input vectors ®x ∈ℝ2 to 0 or 1: 휙 ®x = ( 0 not a threat 1 threat GEOMETRICAL VIEW OF CLASSIFIERS: DECISION BOUNDARIES Geometrically speaking, a classifier partitions the feature space into separate regions, each corresponding to a class. For instance, consider the simple cat-brain model from section 1.4. There are two input variables, hardness (x0) and sharpness (x1). Hence we have a two-dimensional input feature space that geometrically corresponds to a plane. Each combination of hardness and sharpness is represented by a specific vector ®x = [x0, x1] corresponding to a point on the plane. Note that unlike the machines shown in figures 7.1a and 7.1b, here we are talking about a machine that takes as input a pair of continuous values (hardness and sharpness)—that is, a point on the two-dimensional feature plane—and maps it to a discrete space corresponding to the threat versus not a threat categorical decision. This is illustrated in figure 7.4a. The solid curve separates the threat and not-threat regions. Such curves that separate regions in input space belonging to different classes are known as decision boundaries. Estimating the decision boundary is effectively the same as building the classifier. 7.2 Expressing real-world problems: Target functions 247 The dashed line represents an approximate linear decision boundary that does the job crudely but misclassifies the points between the solid and dashed curves. (Linear decision boundaries are easier to represent with neural networks, but they are inadequate for complex problems.) Let’s look briefly at the solid curve in figure 7.4a. At low hardness values, the sharpness threshold is high (if the object in front of the cat is not very hard, it must be very sharp to qualify as a threat). As hardness increases from x0 = 0 to x0 = 20, this threshold (the sharpness required to qualify as a threat) drops more or less linearly. Beyond x0 = 20, the threshold drops at a much faster pace—if the object in front of the cat is sufficiently Non-threat region misclassified by linear decision boundary Threat region Non-threat region True nonlinear decision boundary Approximate linear decision boundary Cat brain threat/non-threat decision boundaries x0 (hardness) x1 (sharpness) (a) Cat brain threat model decision boundary. The solid curve corresponds to the true decision boundary separating the threat and non-threat regions. The dashed line represents an approximate linear decision boundary: it classifies most points correctly but misclassifies points in the region between itself and the true decision boundary. In practice, we get some sample points from each region through manual labeling. Figure 7.4 Classifiers, decision boundaries, and training data. Data points from different classes are marked with different symbols (plus and dot). (Figure continued on next page) 248 CHAPTER 7 Function approximation: How neural networks model the world Cat brain threat/non-threat decision boundaries x0 (hardness) x1 (sharpness) True nonlinear decision boundary Approximate linear decision boundary (b) Good training data. Sample points from each class roughly span the region of input space belonging to the class. This yields a good decision boundary (solid line). Cat brain threat/non-threat decision boundaries x0 (hardness) x1 (sharpness) True nonlinear decision boundary Bad decision boundary (c) Bad training data. Sample points from individual classes do not span the region of input space belonging to the class. This yields a bad decision boundary (dashed line). Figure 7.2 Expressing real-world problems: Target functions 249 hard, it need not be very sharp to pose a threat. Beyond x0 = 52 or so, sharpness ceases to matter: sufficiently hard objects are threats even if they are not sharp. This is inherently a nonlinear situation. To simplify neural network implementation, we might want to approximate the solid curve with a straight line—the dashed line is not too bad an approximation—but doing so entails errors. As shown in figure 7.4a, the region between the true and approximate curves will be wrongly classified. Figure 7.4a is only a schematic. In reality, we do not know the exact regions in the input space that correspond to the classes of interest. We identify—via human labeling—some sample points on the input space, along with their correct class (the ground truth). Such a sampled set of <input point, correct output aka ground truth> pairs is called training data. An example training data set for the cat brain problem is shown in figure 7.4b (ground-truth training data points from different classes are marked with separate symbols: plus and dot, respectively). The decision boundary we create by training a neural network is optimized to classify the training data points (and nothing more) as nicely as possible. If the training data points’ distribution is a reasonable reflection of the true distribution—that is, the sample points from each class more or less span the entire region in the input space corresponding to that class—the decision boundary obtained by training on that data set will be good. But if, as illustrated in figure 7.4c, the training data does not reflect the true distribution of the classes in the input space, the decision boundary learned by training on that data may be bad. Unlike the cat brain example, most real-life input spaces have hundreds or even thousands of dimensions. The idea of a decision boundary as a hypersurface continues to hold in higher dimensions. For higher-dimensional input spaces, hyperplanes func- tion as linear separators. In simpler problems with higher-dimensional input spaces, such linear separators suffice. In more complicated cases, we can have other curved hypersurfaces as nonlinear separators. We may not be able to visualize hyperspaces in our head, but we can form mental pictures with 3D analogs. Figure 7.5 shows some planar decision boundaries in 3D input space. Figure 7.5a shows a 3D space of input vectors along with a set of training data points. The task is to classify them into two classes. In this simple situation, the training points can be partitioned with a hyper- planar decision boundary. Figures 7.5b and 7.5c show some planes that partition the training data poorly, and figure 7.5d shows an optimal planar separator. The only differences between these planes are the values of ®w and b. This indicates that there are values of ®w and b that optimally partition the training data. These optimal values are determined by a process called training, which we discuss in detail in the next chapter. SIGNIFICANCE OF SIGN: MATHEMATICAL EXPRESSIONS FOR DECISION BOUNDARIES In a space with input vectors ®x, the equation 휙 ®x = 0 represents a surface. If the space is 2D, the surface becomes a curve. For instance, the straight dashed line in figure 7.4b can be viewed as a special case of 휙 ®x ≡®wT ®x + b = 0, which in this case becomes 250 CHAPTER 7 Function approximation: How neural networks model the world 0.62x0 + x1 −26.14 =  0.62 1  T  x0 x1  −26.14 = 0 That is, ®w =  0.62 1  b = −26.14 (a) Training data. Sample points from regions on the input space for each class (b) Bad decision boundary. The plane has the wrong orientation. ®w needs to be fixed. (c) Bad decision boundary. The plane has the correct orientation but the wrong position. b needs to be fixed. (d) Optimal decision boundary. The plane has correct ®w, b. Properly trained. Figure 7.5 Classifiers with a linear decision boundary (hyperplane). Such decision boundaries are created by perceptrons (introduced in section 7.3.3). Data points from different classes are marked with different symbols (plus and dot). 7.2 Expressing real-world problems: Target functions 251 In 3D, we have surfaces like planes and spheres. In more than three dimensions, we have hyperplanes, hyperspheres, and so on. For instance, the plane in figure 7.6 corres- ponds to 휙 ®x ≡x0 + x1 + x2 =  1 1 1  T  x0 x1 x1  + 0 = 0 (7.1) That is, ®w =  1 1 1  b = 0 In section 3.1.4, we saw that given any point ®x in the space, the sign of 휙 ®x tells us which side of the surface 휙 ®x = 0 the point ®x belongs to. Thus, if we estimate the surface corresponding to the decision boundary, given any point, we can determine the partition to which that point belongs. In other words, we can classify the point. Estimating the decision boundary is equivalent to building the classifier. For instance, the line in figure 7.4b corresponds to 0.62x0 + x1 −26.14 = 0. The points with 0.62x0 + x1 −26.14 < 0 are on one side and are indicated with dots. The points with 0.62x0 + x1 −26.14 > 0 are on the other side and indicated with plus signs (+). The idea extends to higher dimensions. Figure 7.6 shows the same idea in a 3D input space. The plane corresponds to the equation x0 + x1 + x2 = 0. The points with x0 + x1 + x2 < 0 are on one side (indicated by – in figure 7.6), and points with x0 + x1 + x2 > 0 are on the other side (indicated by + in figure 7.6). Figure 7.6 Significance of sign for 흓  ®x ≡®wT ®x + b. Note that 흓  ®x = 0 implies that ®x lies on the plane, 흓  ®x negative implies one side of the plane, and 흓  ®x positive implies the other side of the plane. 252 CHAPTER 7 Function approximation: How neural networks model the world 7.2.3 General functions in real-world problems There are problems where a categorical output variable will not do and a continuous output variable is called for: for instance, estimating the speed at which a self-driving vehicle should run. Using inputs like the speed limit for the road being traversed, speeds of neighboring vehicles, and so on, we need to estimate how fast the self-driving vehicle should go. Another noteworthy situation where the output needs to be a continuous rather than a categorical variable is when we are modeling the probability of some event occurring. For instance, let’s again consider the face detector. Given an image as input, the face classification function emits 0 to indicate not a face and 1 to indicate face. Such functions are called discriminative. We could also have a function that outputs the probability of the image containing a face. Such functions are called generative, and an example is shown in figure 7.3b. 7.3 The basic building block or neuron: The perceptron In section 7.2, we saw that most real-world problems can be expressed as functions. This is good news, but the bad news is that these functions are usually unknown, and the functions underlying various problems are wildly different from each other. It may be possible to estimate them, but if we attack them individually without adopting a generic framework, there is little hope of developing a repeatable solution. Neural networks provide an effective framework that can mechanically model an extremely wide variety of complicated functions. Furthermore, the target function need not be known in a closed form—sample input-output pairs are enough. They can represent (model) very complicated functions by connecting instances of a fairly simple building block, unsurprisingly called a neuron. In other words, the complete neural network can have huge expressive power even though a single neuron is very simple. Later, in sections 7.3.4, 7.4, 7.5, and so on, we discuss how neural networks model functions of increasing complexity. But first, in this section, we examine the building block: the neuron. 7.3.1 The Heaviside step function The Heaviside step function, often referred to as simply the step function, is a function that takes the value 0 for negative arguments and the value 1 for positive arguments: 휙(x) = ( 0 if x < 0 1 if x >= 0 (7.2) Equation 7.2 is equivalent to the following algorithm. Figure 7.7 shows the graph of this equation. 7.3 The basic building block or neuron: The perceptron 253 Algorithm 7.1 Heaviside step function as an algorithm if x < 0 then return 0 else return 1 end if Heaviside step function Figure 7.7 Heaviside step function graph 7.3.2 Hyperplanes In section 2.8, we discussed hyperplanes. They are represented by equation 2.14. In figure 2.9, we saw the role that hyperplanes play in classifiers; we briefly revisit the idea here. In section 2.1.1, we saw that d-element vectors are geometrical analogs of points in a d-dimensional space. Let ®x denote the vectors (or, equivalently, points) in the space of input vectors. For a fixed vector ®w and fixed scalar b, the equation for a hyperplane in that space is ®wT ®x + b = 0 (meaning all points ®x satisfying this equation lie on the plane). The vector ®w is the normal to the plane. This becomes intuitively obvious when we observe that if we take any two points on the plane, say ®x0 and ®x1, then ®wT ®x0 + b = 0 ®wT ®x1 + b = 0 254 CHAPTER 7 Function approximation: How neural networks model the world Subtracting, we get ®wT   ®x1 −®x0  = 0 But   ®x1 −®x0  is the vector joining two arbitrary points on the plane. This means the line joining any pair of points on the plane is perpendicular to ®w (in section 2.5.2, we discussed dot products, and in section 2.6, we saw that if the dot product between two vectors is zero, the vectors are orthogonal—perpendicular to each other). Hence, ®w is orthogonal to all lines lying on the plane. In other words, ®w is normal to the plane. The hyperplane ®wT ®x + b = 0 partitions the space into two regions with distinct signs for the expression ®wT ®x + b. That is to say, the hyperplane can serve as a decision boundary, as shown in figure 7.6. Not just a hyperplane but any hypersurface can partition space in this fashion. This is true for any dimensionality of ®x: If the expression ®wT ®x + b evaluates to zero, the point ®x lies on the hyperplane ®wT ®x + b = 0. If the sign of the expression ®wT ®x + b is negative, the point ®x lies on one side of the hyperplane. If the sign of the expression ®wT ®x + b is positive, the point ®x lies on the other side of the hyperplane. 7.3.3 Perceptrons and classification The perceptron combines the step function and a hyperplane into a single function. It represents the function P  ®x ≡휙  ®wT ®x + b  (7.3) where 휙is the Heaviside step function from equation 7.2. Combining our insights from sections 7.3.1 and 7.3.2, we can see that the perceptron function maps all points on one side of the   ®w, b plane to zero and all points on the other side of the same plane to 1. In other words, it performs as a linear classifier, with the ( ®w, b) plane as the decision boundary. Figure 7.8 graphs the perceptron function for a 2D input space (the graph itself is in 3D space: it maps points on one side of the decision boundary to the plane Z = 0 and points on the other side to the plane Z = 1). Of course, in real life, we do not know the exact regions corresponding to classes. Rather, we have sampled input points with their manually labeled classes as training data. The decision surface must be constructed based on this training data only. In figure 7.4, we saw an example decision boundary in 2D along with some good and bad training data sets. To get a mental picture of sampled training data sets in higher dimensions, look at figure 7.5a again. In this simple situation, a single hyperplanar decision boundary is sufficient to partition the training points. This means a single perceptron-based neural network will suffice as a classifier. Figures 7.5b and 7.5c depict some planes (perceptrons with specific   ®w, b values) that poorly partition the training data. Figure 7.5d shows an optimal perceptron (planar separator). This tells us that there are good values of ®w and b that optimally partition the training data, as well as 7.3 The basic building block or neuron: The perceptron 255 X Y Z plane plane Figure 7.8 Graph of the perceptron function (equation 7.3) for a two-variable input space. Note that although the input space is 2D, the perceptron graph is 3D. The decision boundary is indicated by the long diagonal straight line. It maps points on one side of the decision boundary to the Z = 0 plane and the points on the other side to the Z = 1 plane. bad values. As mentioned earlier, good values are determined through training, which we cover in chapter 8. The perceptron effectively partitions with a planar decision surface. This works only in simple problems. For an instance of a situation where a planar decision surface will not work, see figure 7.9. It depicts a problem where one class maps to the set of points sandwiched between two planes and the other class to the rest of the points in the input space. It is impossible to achieve the required partitioning with a single plane, so such a decision boundary cannot be modeled with a single perceptron. Later we will see how to model such complex decision boundaries with multiple perceptrons. Figure 7.9 Multiplane decision boundary. One class corresponds to the points in the region sandwiched between the planes (marked +). The remaining points correspond to the other class (marked –). This decision boundary cannot be represented with a single plane. 256 CHAPTER 7 Function approximation: How neural networks model the world NOTE Fully functional code for perceptrons, executable via Jupyter Notebook, can be found at http://mng.bz/9Ne7. The following listing shows the PyTorch code for a perceptron. Listing 7.1 Perceptron def fully_connected_layer(X, w, b): X : n × d tensor; each row is an input vector of size d. w : m × d tensor. X = torch.cat((X, torch.ones( [X.shape[0], 1], dtype=torch.float32)), dim=1) Adds a column of 1s. X →n × (d + 1) tensor. W = torch.cat((W, b.unsqueeze(dim=1)), dim=1) Combines weights and biases y = torch.matmul(W, X.transpose(0, 1)) Matrix multiplication of X and W y = torch.heaviside(y, torch.tensor(1.0)) Applies the Heaviside step function return y.transpose(0, 1) def Perceptron(X, W, b): A single perceptron return fully_connected_layer(X, W, b) 7.3.4 Modeling common logic gates with perceptrons Neural networks provide a structured way of modeling complex functions by connect- ing—via weighted edges—repeated instances of a simple building block called the perceptron. In this section, we explore the idea of function modeling via perceptrons. We start with modeling extremely simple logical functions (AND, OR, NOT, voting) that can be represented with single perceptrons. Then we look at the XOR function, one of the simplest functions that cannot be represented with a single perceptron; we see that it can be modeled with multiple perceptrons. Next we discuss Cybenko’s theorem, which states that most functions of interest can be modeled with as much accuracy as we want via perceptrons, with a single hidden layer between inputs and outputs. Unfortunately, this is less practical than it sounds: the catch is that although any function can be modeled to any accuracy, there is no limit on how many perceptrons are required to do the modeling. The more complicated the target function is, the more perceptrons are required. In practice, we often use many layers instead of one. A PERCEPTRON FOR LOGICAL AND Figure 7.10a depicts this perceptron. It takes two inputs, x0 and x1, which are weighted with w0 = 1 and w1 = 1, respectively; the bias is −1.5 (actually, a wide range of bias values will do). Overall, the perceptron implements the function 휙(x0 + x1 −1.5). This function emits 1 when x0 + x1 −1.5 ≥0: that is, x0 + x1 ≥1.5. Since the variables are binary (meaning they can only take the value 0 or 1), this can happen only when both inputs are 1. If either is zero, their sum is less than 1.5, and y is 0. The situation is depicted geometrically in figure 7.11a. The thick diagonal line corre- sponds to x0 + x1 ≥1.5. It partitions the plane into unshaded and shaded half-planes. 7.3 The basic building block or neuron: The perceptron 257 (a) Perceptron for a logical AND function: y = x0 ∨x1 (b) Perceptron for a logical OR function: y = x0 ∧x1 (c) Perceptron for a logical NOT function: y = ¬x0 Figure 7.10 Perceptrons for simple logical functions. A perceptron is depicted by a circle with summation followed by a step function to remind us of equation 7.3. Inputs and outputs for logical functions are binary, meaning they can be either 0 or 1. (a) Logical AND decision boundary: x0 + x1 = 1.5 (b) Logical OR decision boundary: x0 + x1 = 0.5 (c) Logical NOT decision boundary: x0 = 0.5 Figure 7.11 Geometrical views for the perceptrons in figure 7.10. Each dot represents an input point (a [x0 x1] vector). The shaded dots map to an output value of 1, and the unshaded dots map to an output value of 0. The thick line indicates the decision boundary. All points on the unshaded half-plane have an output value y = 0, and all points on the shaded half-plane have the output value y = 1. There are only four possible input points: (x0 = 1, x1 = 1), (x0 = 1, x1 = 1), (x0 = 1, x1 = 1), (x0 = 1, x1 = 1). The point (x0 = 1, x1 = 1) falls on the shaded side and the others on the unshaded side—which is exactly the logical AND function. 258 CHAPTER 7 Function approximation: How neural networks model the world A PERCEPTRON FOR LOGICAL OR Figure 7.10b depicts this perceptron. It takes two inputs, x0 and x1, which are weighted with w0 = 1 and w1 = 1, respectively; the bias is −0.5. Overall, the perceptron implements the function 휙(x0 + x1 −0.5). This function emits 1 when x0 + x1 −0.5 ≥0: that is, x0 + x1 ≥0.5. Since the variables are binary (0 or 1), this can happen when either or both inputs are 1. If and only if both of them are zero, their sum is zero (less than 0.5), and y is 0. The situation is shown geometrically in figure 7.11b. The thick line corresponds to x0 + x1 ≥0.5, partitioning the input plane into an unshaded half-plane (all points on this half-plane have output y = 0) and a shaded half-plane (output y = 1). Of the four possible input points, (0, 0) falls on the unshaded side (y = 1) and the remaining three on the shaded side (y = 1)—which is exactly the logical OR function. A PERCEPTRON FOR LOGICAL NOT This perceptron is shown in figures 7.10c and 7.11c, which should be self-explanatory by now. NOTE Fully functional code for modeling various logical gates using perceptrons, executable via Jupyter Notebook, can be found at http://mng.bz/jBRr. Listing 7.2 Modeling logical gates using perceptrons # Logical AND X = torch.tensor([[0., 0.], Input data points [0., 1.], [1., 0.], [1., 1.]], dtype=torch.float32) W = torch.tensor([[1.0, 1.0]], dtype=torch.float32) Instantiates the weights b = torch.tensor([-1.5]) Instantiates the bias Y = Perceptron(X=X, W=W, b=b, activation=torch.heaviside) Output # Logical OR X = torch.tensor([[0., 0.], [0., 1.], [1., 0.], [1., 1.]], dtype=torch.float32) W = torch.tensor([[1.0, 1.0]], dtype=torch.float32) b = torch.tensor([-1.5]) Y = Perceptron(X=X, W=W, b=b, activation=torch.heaviside) # Logical NOT X = torch.tensor([[0], [1.] ], dtype=torch.float32) W = torch.tensor([[-1.0]], dtype=torch.float32) b = torch.tensor([0.5]) Y = Perceptron(X=X, W=W, b=b, activation=torch.heaviside) 7.4 Toward more expressive power: Multilayer perceptrons (MLPs) 259 7.4 Toward more expressive power: Multilayer perceptrons (MLPs) There is a remarkably simple logical function that, somewhat surprisingly, cannot be modeled with a single perceptron: XOR. We discuss it now. 7.4.1 MLP for logical XOR Figure 7.12a shows the four possible input points on the plane and how the plane needs to be partitioned to model the XOR function. The points (0, 0), (1, 1) (unshaded) both map to output 0 and should be on the same side of the decision boundary, while the (a) Geometric view of the logical XOR perceptron. The decision boundary has two lines, so using a single perceptron is impossible. (b) MLP for the logical XOR function. Note that weights and biases have superscripts in parentheses indicating the layer index. This is a two-layered network. Layer 0 is hidden. Figure 7.12 Logical XOR: Geometric and perceptron view 260 CHAPTER 7 Function approximation: How neural networks model the world points (0, 1), (1, 0) (shaded) map to output value 1 and should be on the other side of the decision boundary. It is easy to see that it is impossible to draw a single straight line in this plane such that the shaded points are on one side and the unshaded points are on the other. Remember, a perceptron essentially introduces a linear decision boundary. Hence it is impossible to have a single perceptron modeling this function. However, it is possible to model the logical XOR function via multiple perceptrons. One such model is shown in figure 7.12b. 7.5 Layered networks of perceptrons: MLPs or neural networks The XOR example tells us that we cannot do much with single perceptrons. We must connect more than one perceptron into a network to solve practical problems. This is a neural network. How do we organize such a network of connected perceptrons? 7.5.1 Layering Layering is the most popular way to organize perceptrons into a neural network. Fig- ure 7.12b is our first example of an MLP—most of the remainder of the book talks about MLPs. Note how the perceptrons in the XOR network (figure 7.12b) are organized: The layers are numbered with increasing integers from input to output. The output of a perceptron in layer i is only fed as input to perceptrons in layer i + 1. No other connections are allowed. This keeps the network manageable and facilitates updating the weights during training via a technique called backpropaga- tion, which we discuss in the next chapter. Outputs of all layers but the last are invisible (do not directly contribute to the output). Such layers are called hidden layers. In figure 7.12b, layer 0 is hidden. Each weight and bias element belongs to one and only one layer. Throughout this book, we indicate the layer index for a weight or bias element as a superscript within parentheses. MLPs with two or more hidden layers can be called deep neural networks. This is the origin of the word deep in deep learning. 7.5.2 Modeling logical functions with MLPs Any logical function can be expressed as a truth table. Hence, if we can prove that all truth tables can be implemented via MLPs, we are done. This is the approach we take here. NOTE In the following discussion, no symbol (à la multiplication) between two variables indicates logical AND, and a + symbol indicates logical OR. 7.5 Layered networks of perceptrons: MLPs or neural networks 261 Table 7.1 Truth table for the logical function y = ¯x0x1 + x0 ¯x1 x0 x1 y 0 0 0 0 1 1 1 0 1 1 1 0 Let’s start with a simple two-variable logical functions y = ¯x0x1 + x0¯x1. Table 7.1 shows the corresponding truth table. To create the equivalent MLP, we must pick the rows corresponding to y = 1. Each row can be expressed as an AND of the input variables or their complements. For instance, the row x0 = 0 and x1 = 1, y = 1 corresponds to ¯x0x1—the first term of the function we are trying to implement—and can be implemented by the perceptron shown in figure 7.13a. The row x0 = 1 and x1 = 0, y = 1 corresponds to x0¯x1—the second term of the function we are trying to implement—and can be implemented by the perceptron shown in figure 7.13b. We have implemented the individual terms of the function; all that remains is to OR them together into a final MLP, as shown in fig- ure 7.13c. This logical function is our old friend, logical XOR, and the overall function in figure 7.13 is the same as figure 7.12. In this fashion, arbitrary logical expressions in any number of variables can be modeled using MLPs. (a) Perceptron for ¯x0x1 (b) Perceptron for x0¯x1 (c) MLP for ¯x0x1 + x0¯x1 Figure 7.13 MLP for the logical function corresponding to table 7.1 Listing 7.3 Multilayered perceptron (MLP) def MLP(X, W0, W1, b0, b1): MLP y0 = fully_connected_layer(X=X, W=W0, b=b0) return fully_connected_layer(X=y0, W=W1, b=b1) 7.5.3 Cybenko’s universal approximation theorem Any function y = f (x) that is continuous in an interval x ∈(a, b) can be approximated with a set of towers (vertical rectangles) in that interval. This is a direct consequence of the mean value for integrals theorem in calculus. The idea is depicted in figure 7.14, where a complicated function (depicted by the curve) is approximated by a sequence of towers of various heights. The thinner the towers, the greater the number of towers, and the more accurate the approximation. 262 CHAPTER 7 Function approximation: How neural networks model the world Figure 7.14 Approximating a complicated function with towers In section 7.5.3, we show that any tower (with arbitrary height and location) can be con- structed with MLPs. By summing these MLPs for individual towers, we can approximate the entire function. This is Cybenko’s theorem in a nutshell. NOTE Although Cybenko’s theorem guarantees that any continuous function can be modeled using an MLP with a single hidden layer, the number of perceptrons in that MLP can become arbitrarily impracticably large. This is why, in real life, we rarely try to approximate complicated functions with a single hidden layer. We see later that additional layers help us cut down the number of perceptrons required. In particular, any decision boundary can be modeled in this fashion. Of course, the number of perceptrons needed may become impossibly large for many problems, making such a model practically unattainable. GENERATING TOWERS WITH MLPS The basic idea is depicted in figure 7.15. We can obviously generate a regular step with a perceptron implementing y = 휙(x). The corresponding graph is shown in figure 7.15a. By imparting a bias of 5, we can shift this step leftwards. The corresponding function is y = 휙(x + 5). Furthermore, using negative weight laterally flips the step. The corresponding function is y = 휙(−x), whose graph is shown in figure 7.15c. By imparting a bias of 5, we can shift the flipped step rightwards. Figure 7.15e shows a flipped and right-shifted step corresponding to the function y = 휙(−x + 5), whose perceptron is shown in figure 7.15f. Logically ANDing a left-shifted step with a flipped and right-shifted step yields a tower 7.5 Layered networks of perceptrons: MLPs or neural networks 263 1.0 0.8 0.6 0.4 0.2 0.0 -10.0 -7.5 -5.0 -2.5 0.0 2.5 5.0 7.5 10.0 w = 1 b = 0 Positive weight yields S curve (a) 휙(x): positive w yields a regular step 1D step (b) Perceptron for a regular step 1.0 0.8 0.6 0.4 0.2 0.0 -10.0 -7.5 -5.0 -2.5 0.0 2.5 5.0 7.5 10.0 w = -1 b = 0 Negative weight yields laterally flipped S curve (c) 휙(−x): negative w yields a laterally flipped step 1D ﬂipped step (d) Perceptron for a laterally flipped step 1.0 0.8 0.6 0.4 0.2 0.0 -10.0 -7.5 -5.0 -2.5 0.0 2.5 5.0 7.5 10.0 Change bias to shift curve w = -1 b = 5 (e) 휙(−x + 5): changing the bias shifts the step left or right 1D ﬂipped + shifted step (f) Perceptron for a laterally shifted step Figure 7.15 Generating a 1D tower with perceptrons. (Figure continued on next page) 264 CHAPTER 7 Function approximation: How neural networks model the world MLP-generated rectangle 1.0 0.8 0.6 0.4 0.2 0.0 -10.0 -7.5 -5.0 -2.5 0.0 2.5 5.0 7.5 10.0 w0 = [1, -1] b0 = [5, 5] w1 = [1,1] b1 = -1.5 (g) (휙(x + 5) + 휙(−x + 5) −1.5): ANDing a left- shifted step with a flipped, right-shifted step yields a tower 1D tower (h) MLP for a 1D tower Figure 7.15 (Continued from previous page) in 1D. This corresponds to the function y = 휙(휙(x + 5) + 휙(−x + 5) −1.5), whose graph is shown in figure 7.15g. The same idea also works for higher-dimensional inputs. We can generate a step in two variables (a 2D step) aligned to the x0 direction using equation 7.4. This equation’s graph is shown in figure 7.16a, and the perceptron implementing the equation is shown in figure 7.16b. The flipped version of the same step can be generated via equation 7.5. This equation’s graph is shown in figure 7.16d, and the perceptron implementation is shown in figure 7.16e. In the 1D case, we combine a regular step with its flipped and shifted version to generate a tower. The process is slightly more complicated in 2D. Here, combining a step along a specific coordinate axis with its flipped and shifted version generates a wave function along that axis. Thus, we have separate waves in each dimension. The Step function along X0 direction (a) 2D step function along the x0 (x) direction 2D step (b) Perceptron for a 2D step function along the x0 (x) direction y = 휙©­ « h 1 0 i  x0 x1  ª® ¬ (c) Equation for a 2D step function along the x0 (x) direction Figure 7.16 Generating 2D steps and waves with perceptrons. (Figure continued on next page) 7.5 Layered networks of perceptrons: MLPs or neural networks 265 Flipped step function along X0 direction (d) Flipped 2D step along the x0 (x) direction ﬂipped 2D step (e) Perceptron for a flipped 2D step along the x0 (x) direction y = 휙©­ « h −1 0 i  x0 x1  ª® ¬ (f) Equation for a flipped 2D step along the x0 (x) direction 2D wave along X0 direction (g) 2D wave along the x0 (x) direction 2D wave (h) MLP for a 2D wave along the x0 (x) direction y = 휙 h 1 1 i 휙 " 1 0 −1 0 # " x0 x1 # + " 0.5 0.5 #! −1.5 ! (i) Equation for a 2D wave along the x0 (x) direction 2D wave along X1 direction (j) 2D wave along the x1 (y) direction 2D wave (k) MLP for a 2D wave along the x1 (y) direction y = 휙 h 1 1 i 휙 " 0 1 0 −1 # " x0 x1 # + " 0.5 0.5 #! −1.5 ! (l) MLP for a 2D wave along the x1 (y) direction Figure 7.16 (Continued from previous page) 266 CHAPTER 7 Function approximation: How neural networks model the world wave along the x0 axis corresponds to equation 7.6; its graph is shown in figure 7.16g. It is implemented by the MLP in figure 7.16h. Similarly, a 2D wave along the x1 axis can be generated via equation 7.7 and is graphed in figure 7.16j. The corresponding MLP is shown in figure 7.16k. To create a tower, we have to AND a pair of waves along the two separate dimensions. The final tower function is shown in equation 7.8; the corresponding tower graph is shown in figure 7.17a; the MLP is shown in figure 7.17b. Any continuous 2D surface can be approximated to arbitrary levels of accuracy by combining such 2D towers: y = 휙©­ « h 1 0 i  x0 x1  ª® ¬ ⇒2D step along x0 (7.4) y = 휙©­ « h −1 0 i  x0 x1  ª® ¬ ⇒Flipped 2D step along x0 (7.5) y = 휙©­ « h 1 1 i 휙©­ «  1 0 −1 0   x0 x1  +  0.5 0.5  ª® ¬ −1.5ª® ¬ ⇒2D wave along x0 (7.6) 2D tower (a) 2D tower 2D tower (b) MLP for a 2D tower (equation 7.8) Figure 7.17 Generating a 2D tower with perceptrons 7.5 Layered networks of perceptrons: MLPs or neural networks 267 y = 휙©­ « h 1 1 i 휙©­ «  0 1 0 −1   x0 x1  +  0.5 0.5  ª® ¬ −1.5ª® ¬ ⇒2D wave along x1 (7.7) y = 휙 ©­­­­­­­ « h 1 1 1 1 i 휙 ©­­­­­­­ «  1 0 −1 0 0 1 0 −1   x0 x1  +  0.5 0.5 0.5 0.5  ª®®®®®®® ¬ −3.5 ª®®®®®®® ¬ ⇒2D tower (7.8) NOTE Fully functional code for approximating surfaces using perceptrons, exe- cutable via Jupyter Notebook, can be found at http://mng.bz/WrKa. Listing 7.4 Perceptrons and MLPs in 1D x = torch.linspace(start=-10, end=10, steps=100) 100D array # 1D S curves - positive weight See figures 7.15a and 7.15b. w = torch.tensor([1.0], dtype=torch.float32) b = torch.tensor([0.0]) y = Perceptron(X=x.unsqueeze(dim=1), W=w.unsqueeze(dim=1), b=b) # 1D S curves - negative weight + shift See figures 7.15e and 7.15f. w = torch.tensor([-1.0], dtype=torch.float32) b = torch.tensor([5.0]) y = Perceptron(X=x.unsqueeze(dim=1), W=w.unsqueeze(dim=1), b=b) # 1D towers (Cybenko) - various W0 See figures 7.15g and 7.15h. W0 = torch.tensor([[1.0], [-1.0]], dtype=torch.float32) b0 = torch.tensor([5.0, 5.0]) W1 = torch.tensor([[1.0, 1.0]], dtype=torch.float32) b1 = torch.tensor([0.0]) y = MLP(X=x.unsqueeze(dim=1), W0=W0, W1=W1, b0=b0, b1=b1) Listing 7.5 Perceptrons and MLPs in 2D X = torch.linspace(start=-1, end=1, steps=100) 100D array Y = torch.linspace(start=-1, end=1, steps=100) 100D array gridX, gridY = torch.meshgrid(X, Y) 100 ×100 matrix X = torch.tensor([(y, x) for y, x in zip(gridY.reshape(-1), gridX.reshape(-1))) 10,000 ×1 matrix # 2D Step function in X-direction See equation 7.4 and figures 7.16a and 7.16b W = torch.tensor([[1.0, 0.0]], dtype=torch.float32) 268 CHAPTER 7 Function approximation: How neural networks model the world b = torch.tensor([0.0], dtype=torch.float32) Z = Perceptron(X=X, W=W, b=b) # 2D Flipped Step function along X-direction See equation 7.5 and figures 7.16d and 7.16e W = torch.tensor([[-1.0, 0.0]], dtype=torch.float32) b = torch.tensor([0.0], dtype=torch.float32) Z = Perceptron(X=X, W=W, b=b) # 2D wave along X-direction See equation 7.6 and figures 7.16g and 7.16h W0 = torch.tensor([[1.0, 0.0], [-1.0, 0.0]], dtype=torch.float32) b0 = torch.tensor([0.5, 0.5], dtype=torch.float32) W1 = torch.tensor([[1.0, 1.0]], dtype=torch.float32) b1 = torch.tensor([-1.0]) Z = MLP(X=X, W0=W0, W1=W1, b0=b0, b1=b1) # 2D wave along Y-direction See equation 7.7 and figures 7.16j and 7.16k W0 = torch.tensor([[0.0, 1.0], [0.0, -1.0]], dtype=torch.float32) b0 = torch.tensor([0.5, 0.5], dtype=torch.float32) W1 = torch.tensor([[1.0, 1.0]], dtype=torch.float32) b1 = torch.tensor([-1.0]) Z = MLP(X=X, W0=W0, W1=W1, b0=b0, b1=b1) # 2D Tower See equation 7.8 and figures 7.17a and 7.17b W0 = torch.tensor([[1.0, 0.0], [-1.0, 0.0], [0.0, 1.0], [0.0, -1.0]], dtype=torch.float32) b0 = torch.tensor([0.5, 0.5, 0.5, 0.5], dtype=torch.float32) W1 = torch.tensor([[1.0, 1.0, 1.0, 1.0]], dtype=torch.float32) b1 = torch.tensor([-3.5]) Z = MLP(X=X, W0=W0, W1=W1, b0=b0, b1=b1) 7.5.4 MLPs for polygonal decision boundaries We have seen that classifiers form an important use case for neural networks. In sec- tion 7.2.2, we also saw that classifiers essentially model decision boundaries in high- dimensional feature spaces. In this section, we model a simple class bounded with a fixed polygon to understand the process. Figure 7.18a shows a feature space with the class to be identified corresponding to a rectangular region (shaded) bounded by the four straight lines: x0 = −5 x0 = 5 x1 = −2 x1 = 2 7.5 Layered networks of perceptrons: MLPs or neural networks 269 (a) Example feature space with decision boundaries enclosing the class of inter- est (shaded) Rectangle (b) MLP that fires only on points in the shaded region Figure 7.18 Modeling a rectangular decision region with MLPs Each of these lines partitions the feature space into two half-planes, indicated by minus and plus signs in figure 7.18a. The region containing the feature points for the class of interest is indicated by all + signs. The shaded region corresponding to the class of interest is the region where x0 ≥−5 AND x0 ≤5 AND x1 ≥−2 AND x1 ≤2. Now consider the perceptron 휙(x0 + 5). It fires (outputs 1) on the region x0 ≥−5. Similarly, the perceptrons 휙(−x0 + 5), 휙(x1 + 2), and 휙(−x1 + 2) fire on the regions x0 ≤5, x1 ≥−2, and x1 ≤2, respectively. Hence, by logically ANDing the outputs of these perceptrons, we get an MLP that fires only on the shaded region of interest. Figure 7.18b shows this MLP. It implements the following function: y = 휙 ©­­­­­ « h 1 1 1 1 i 휙 ©­­­­­ «  1 0 −1 0 0 1 0 −1   x0 x1  +  5 5 2 2  ª®®®®® ¬ −3.5 ª®®®®® ¬ All shapes on a plane can be approximated by polygons. Hence, given sufficient percep- trons, any shape on a plane can be depicted to an arbitrary level of accuracy. 270 CHAPTER 7 Function approximation: How neural networks model the world Summary In this chapter, we outlined how a large variety of real-world problems can be modeled as function evaluation: Any intelligent task can be modeled by a function. Of particular interest are classification tasks where, given an input, we estimate from a predetermined list of possible classes the class to which the input belongs. For instance, a binary classifier can group input images into two classes: those that contain a human face and those that do not. Classification tasks can be approximated by functions with categorical outputs. Neural networks provide a structured way to approximate arbitrary functions (including classifier functions). This is how they mimic intelligence. Neural networks are created by combining a basic building block called a percep- tron. A perceptron is a simple function that returns a step function applied to the weighted sum of its inputs (plus a bias). A perceptron is effectively a linear classifier that divides space into two half-spaces with a hyperplane. The weights and bias of the perceptron correspond to the orientation and position of the hyperplane— they can be adjusted to separate the regions corresponding to individual classes as much as possible. A single perceptron can approximate only relatively simple functions, such as a classifier whose feature points are separable by a hyperplane. Perceptrons cannot approximate more complex functions, like classifiers whose input regions are to be separated with curved surfaces. Multilayer perceptrons (MLPs) are combinations of perceptrons where the outputs of one set (layer) of perceptrons are fed as input to the next set (layer). A neural network is essentially an MLP and can approximate such arbitrarily complex functions. Simple logical functions like AND, OR, and NOT can be emulated with a single perceptron. A logical XOR cannot be. For XORs and other complicated logical functions, we need MLPs. There is a mechanical way to construct an MLP for any logical function. A logical function can always be represented by a truth table. Each row of the truth table can be viewed as a logical AND function of the inputs, and the final output is a logical OR of the inputs. Since ANDs and ORs can be emulated with perceptrons, any truth table can be emulated as a combination of perceptrons (an MLP). The ability of an MLP to represent arbitrary functions is known as its expressive power. The larger the number of perceptrons and/or connections, the greater the expressive power of a neural network. Cybenko’s theorem proves that a neural network is a universal approximator (meaning it can approximate any function). The basic idea is that any function can be approximated to an arbitrary degree of accuracy as a sum of rectangles (towers). The theorem demonstrates that a tower can be constructed in any dimensional space using MLPs. Summary 271 Neural networks can approximate any shape on a plane to arbitrary accuracy. This is because all shapes can be approximated by rectangles, and we can demonstrably approximate a rectangle on a plane with MLPs. In real-life problems, the regions corresponding to classes are unknown, but we manually label sample input points with desired outputs (ground truth) to create supervised training data. We tune the weights and biases to approximate the training data as closely as possible. This process of tuning is known as training. If the training data set is not a good representative of the real dataset, the neural network will be inaccurate even after training. 8 Training neural networks: Forward propagation and backpropagation This chapter covers Sigmoid functions as differential surrogates for Heaviside step functions Layering in neural networks: expressing linear layers as matrix-vector multiplication Regression loss, forward and backward propagation, and their math So far, we have seen that neural networks make complicated real-life decisions by modeling the decision-making process with mathematical functions. These functions can become arbitrarily involved, but fortunately, we have a simple building block called a perceptron that can be repeated systematically to model any arbitrary function. We need not even explicitly know the function being modeled in closed form. All we need is a reasonably sized set of sample inputs and corresponding correct outputs. This collection of input and output pairs is known as training data. Armed with this training data, we can train a multilayer perceptron (MLP, aka neural network) to emit reasonably correct outputs on inputs it has never seen before. 272 8.1 Differentiable step-like functions 273 Such neural networks, where we need to know the output for each input in the training data set, are known as supervised neural networks. The correct output for the training inputs is typically generated via a manual process called labeling. Labeling is expensive and time-consuming. Much research is going on toward unsupervised, semi- supervised, and self-supervised networks, eliminating or minimizing the labeling process. But as of now, the accuracy of unsupervised and self-supervised networks in general does not match that of supervised networks. In this chapter, we focus on supervised neural networks. In chapter 14, we will study unsupervised networks. What is this process of “training” a neural network? It essentially estimates the parameter values that would make the network emit output values as close as possible to the known correct outputs on the training inputs. In this chapter, we discuss how this is done. But before that, we have to learn a few other things. NOTE The complete PyTorch code for this chapter is available at http://mng.bz/YAXa in the form of fully functional and executable Jupyter notebooks. 8.1 Differentiable step-like functions In equation 7.3, we expressed the perceptron as a combination of a Heaviside step func- tion 휙and an affine transformation ®wT ®x + b. This is the perceptron we used throughout chapter 7 and with which we were able to express (model) pretty much all functions of interest. Despite its expressive power, the Heaviside step function has a drawback: it has a discontinuity at x = 0 and is not differentiable. Why is differentiability important? As we shall see in chapter 8 (and got a glimpse of in section 3.3), optimal training of a neural network requires evaluation of the gradient vector of a loss function with respect to weights. Since the gradient is nothing but a vector of partial derivatives, differentiability is needed for training. In this section, we discuss a few functions that are differentiable and yet can mimic the Heaviside step function. The most significant among them is the sigmoid function. 8.1.1 Sigmoid function The sigmoid function is named after its characteristic S-shaped curve (figure 8.1). The corresponding equation is 휎(x) = 1 1 + e−x (8.1) PARAMETERIZED SIGMOID FUNCTION We can parametrize equation 8.1 as 휎(x) = 1 1 + e−(wx+b) (8.2) This allows us to Adjust the steepness of the linear portion of the S curve by changing w Adjust the position of the curve by changing b 274 CHAPTER 8 Training neural networks: Forward propagation and backpropagation Figure 8.1 Graph of a 1D sigmoid function Figure 8.2 shows how the parametrized sigmoid curve changes with different values for the parameters w and b. In particular, note that for large values of w, the parameterized sigmoid is virtually indistinguishable from the Heaviside step function (compare the dotted curve in figure 8.2a with figure 7.7), even though it remains differentiable. This is exactly what we desire in neural networks. 1D perceptron outputs - various weights 1.0 0.8 0.6 0.4 0.2 0.0 -10.0 -7.5 -5.0 -2.5 0.0 2.5 5.0 7.5 10.0 w = [1.] b = [0.] w = [1.5] b = [0.] w = [3.] b = [0.] w = [1.] b = [0.] w = [1.] b = [1.] w = [1.] b = [2.] 1.0 0.8 0.6 0.4 0.2 0.0 -10.0 -7.5 -5.0 -2.5 0.0 2.5 5.0 7.5 10.0 1D perceptron outputs - various biases (a) (b) Figure 8.2 Sigmoid curves corresponding to various parameter values in equation 8.2 SOME PROPERTIES OF THE SIGMOID FUNCTION The sigmoid function has several interesting properties, some of which are listed here with proof outlines. Expression with positive x: 휎(x) = ex 1 + ex (8.3) This expression can be easily proved by multiplying both the numerator and denominator of equation 8.1 by ex. 8.1 Differentiable step-like functions 275 Sigmoid of negative x: 휎(−x) = 1 1 + ex = e−x 1 + e−x = 1 − 1 1 + e−x = 1 −휎(x) (8.4) Derivative of sigmoid: d휎(x) dx = d dx  (1 + e−x)−1 =  −1 (1 + e−x)2  d dx ((1 + e−x)) =  −1 (1 + e−x)2  (−e−x) =  1 1 + e−x   e−x 1 + e−x  = 휎(x) (1 −휎(x)) (8.5) Figure 8.3 shows the graph of the derivative of the sigmoid superimposed on the sigmoid graph itself. As expected, the derivative has its maximum value at the middle of the sigmoid curve (where the sigmoid is climbing more or less linearly) and is near zero at both ends (where the sigmoid is saturated and flat, hardly changing). 1D sigmoid curve and its derivative Sigmoid Derivative of sigmoid Figure 8.3 Graph of a 1D sigmoid function (solid curve) and its derivative (dashed curve) 8.1.2 Tanh function An alternative to the sigmoid function is the hyperbolic tangent tanh function, shown in figure 8.4. It is very similar to the sigmoid function, but the range of output values is from [−1, 1] as opposed to [0, 1]. In essence, it is the sigmoid function stretched and 276 CHAPTER 8 Training neural networks: Forward propagation and backpropagation Figure 8.4 Graph of a 1D tanh function. shifted so it is centered around 0. The equation of the tanh function is given by tanh (x) = ex −e−x ex + e−x (8.6) Why is tanh preferred over sigmoid? To understand this, consider figure 8.5. It compares the derivatives of the sigmoid and tanh functions. As the plot shows, the derivative (gradient) of the function near x = 0 is much higher for tanh than for sigmoid. Stronger gradients mean faster convergence, as the weight updates happen in larger steps. Note that this holds mainly when the data is centered around 0: in most preprocessing steps, we standardize the data (make it 0 mean) before feeding it into the neural network. Derivative of sigmoid Derivative of TanH Figure 8.5 Graph of the derivatives of 1D sigmoid and tanh functions 8.2 Why layering? In section 7.5, we encountered the idea of layering as the preferred way to organize multiple perceptrons. The main property of a layered network is that neurons in any layer 8.3 Linear layers 277 take their input only from the outputs of the preceding layer. This means connections exist only between successive layers. No other connection exists in the MLP, which greatly simplifies the evaluation and training of the network, which will become apparent as we discuss forward propagation and backpropagation. Why have layers at all? We have seen that multiple perceptrons allow us to model problems that cannot be solved by a single perceptron (such as the XOR problem discussed in section 7.4.1). In theory, it is possible to model all mathematical functions (and hence solve all quantifiable problems) with neurons organized in a single hidden layer (see Cybenko’s theorem and proof in section 7.5.3). However, that does not mean a single hidden layer is the most efficient way of doing all modelings. We can often model complicated problems with fewer perceptrons if we organize them in more than one layer. Why do extra layers help? The primary reason is the extra nonlinearity. Each layer brings in its own nonlinear (such as sigmoid) function. Nonlinear functions, with proper parametrization, can model more complicated functions. Hence, a larger count of nonlinear functions in the model typically implies greater expressive power. 8.3 Linear layers Various types of layers are used in popular neural network architectures. In subsequent chapters, we shall look at different kinds of layers, such as convolution layers. But in this section, we examine the simplest and most basic type of layer: the linear layer. Here every perceptron from the previous layer is connected to every perceptron in the next layer. Such a layer is also known as fully connected layer. Thus if the previous layer has m neurons and the next layer has n neurons, there are mn connections, each with its own weight. NOTE We use the words neuron and perceptron interchangeably. Figure 8.6 shows a linear layer that is a slice of a bigger MLP. Figure 8.7 shows a bigger MLP with a linear layer. Consistent with previous chapters, we have used superscripts for layer IDs and subscripts for source and destination IDs. The weight of the connection from the kth neuron in layer (l −1) to the jth neuron in layer l is denoted w(l) jk . Here the subscript ordering is the destination (j) followed by the source (k). This is slightly counterintuitive but universally followed because it simplifies the matrix notation (described shortly). Note the following: We have split a single perceptron (weighted sum followed by sigmoid) into two separate layers, weighted sum and sigmoid. We have used sigmoid instead of Heaviside as the nonlinear function. 8.3.1 Linear layers expressed as matrix-vector multiplication Let’s revisit the perceptron in the context of the MLP. As we saw in equation 7.3, a single perceptron takes a weighted sum of its inputs and then performs a step function on the result. In an MLP, the inputs to any perceptron in the lth layer come from the previous layer: the (l −1)th layer. 278 CHAPTER 8 Training neural networks: Forward propagation and backpropagation Figure 8.6 Linear layer outputting layer l from layer (l −1). The weights belonging to row 1 of the weight matrix (coming from all the input neurons, layer (l −1), which sum together to form output neuron 1) are shown in bold. ... ... ... Input layer Layer 0 Layer 1 Final layer h0 (L) h1 (L) h1L (L) σ h1 (L-1) h2 (L-1) h0 (L-1) hlL-1 (L-1) h3 (L-1) ... L – 1 layer ... h1 (0) h2 (0) h0 (0) h10 (0) h3 (0) σ σ σ σ ... h1 (1) h2 (1) h0 (1) h11 (1) h3 (1) ... σ σ σ σ σ ... σ σ σ σ σ ... σ σ σ ... w 00 (0) w10 (0) w10n (0) w00 (1) w11l0 (10) w10 (1) w00 (L) w1LlL-1 (L) w01(L) ... x0 x1 xn Figure 8.7 Multilayered neural networks: This is a complete deep neural network, a slice of which is shown in figure 8.6. 8.3 Linear layers 279 Let a(l−1) 0 , a(l−1) 1 , · · · , a(l−1) m denote the outputs of the m neurons in layer (l −1) (the left-most input column of nodes in figure 8.6). And let a(l) 0 , a(l) 1 , · · · , a(l) n denote the outputs of the n neurons in layer l. Note that we typically use the symbol a, standing for activation, to denote the output of individual neurons. Now consider the jth neuron in layer l. For instance, check z(l) 1 in figure 8.6: note the weights going into it and the activations at their source. Its output is a(l) j , where z(l) j = m Õ k=0 w(l) jk a(l−1) k + b(l) j a(l) j = 휎  z(l) j    for j = 0 · · · n We can rewrite the summation in these equations as a dot product between the weight and activation vectors: z(l) j = h w(l) j0 w(l) j1 · · · w(l) jm i  a(l−1) 0 a(l−1) 1 · · · a(l−1) m  + b(l) j a(l) j = 휎  z(l) j    for j = 0 · · · n The complete set of equations for all js together can be written in a super-compact way using matrix-vector multiplication, ®z(l) =W (l) ®a(l−1) + ®b(l) ®a(l) = 휎  ®z(l) (8.7) where W (l) is an n × m matrix representing the weights of all connections from layer l −1 to layer l: W (l) =  w(l) 00 w(l) 01 · · · w(l) 0m w(l) 10 w(l) 11 · · · w(l) 1m ... w(l) j0 w(l) j1 · · · w(l) jm ... w(l) n0 w(l) n1 · · · w(l) nm  (8.8) 280 CHAPTER 8 Training neural networks: Forward propagation and backpropagation ®a(l) represents the activations for the entire layer l. Applying the sigmoid function to a vector is equivalent to applying it individually to each element of the vector: ®a(l) =  a(l) 0 a(l) 1 · · · a(l) n  ®a(l−1) =  a(l−1) 0 a(l−1) 1 · · · · · · a(l−1) m  ®z(l) =  z(l) 0 z(l) 1 · · · z(l) n  휎  ®z(l) =  휎  z(l) 0  휎  z(l) 1  · · · 휎  z(l) n   The matrix-vector notation saves us from dealing with subscripts by working with all the weights, biases, activations, and so on in a global fashion. 8.3.2 Forward propagation and grand output functions for an MLP of linear layers Equation 8.7 describes the forward propagation of a single linear layer. The final output of an MLP with fully connected (aka linear) layers 0 · · · L on input ®x can be obtained by repeated application of this equation: MLP  ®x = ®a(L) = ®y = 휎  W (L) . . . 휎  W (1)휎  W (0) ®x + ®b(0) + ®b(1) · · · + ®b(L) (8.9) In a computer implementation, this expression is evaluated step by step by repeated application of the linear layer: ®a0 = 휎  W (0) ®x + ®b(0) ®a1 = 휎  W (1) ®a0 + ®b(1) · · · ®aL = 휎  W (L) ®aL−1 + ®b(L) (8.10) It’s easy to see that equation 8.10 is a restatement of equation 8.7. Close examination of these equations reveals a beautiful property. The complicated equation 8.9 is never explicitly evaluated. Instead, we evaluate the outputs of successive layers, one layer at a time, as per equation 8.10. Every layer can be evaluated by taking the previous layer’s output as input. No other input is necessary. That is to say, we can evaluate ®a(0) directly from the input ®x, then ®a(1) from ®a(0), ®a(2) from ®a(1), and so forth, all the way to ®a(L) (which is the grand output of the MLP). During the evaluation, we need to keep only the previous and current layers in memory at any given time. This 8.4 Training and backpropagation 281 process greatly simplifies the implementation as well as the conceptualization and is known as forward propagation. Listing 8.1 PyTorch code for forward propagation def Z(x, W_l, b_l): x: activation of layer l-1 (1-d vector) Wl: Weight matrix of layer l bl: Bias vector of layer l return torch.matmul(W_l, x) + b_l def A(z_l): Sigmoid activation function (nonlinear layer) return torch.sigmoid(z_l) def forward(x, W, b): x: 1-d input vector W: list of matrices for layers 0 to L. b: list of vectors for layers 0 to L L = len(W) - 1 a_l = x for l in range(0, L + 1): Loops through layers 0 to L z_l = Z(a_l, W[l], b[l]) Computes Z a_l = A(z_l) Computes activation return a_l 8.4 Training and backpropagation Throughout the book, we have been discussing bits and pieces of this process. In sections 1.1 and 3.3 (specifically, algorithm 3.1), we saw an overview of the process for training a supervised model (you are encouraged to reread those if necessary). Training is an iterative process by which the parameters of the neural network are estimated. The goal is to estimate the parameters (weights and biases) such that on the training inputs, the neural network outputs are close as possible to the known ground-truth outputs. In general, iterative processes improve (get closer to the goal) gradually. In each iteration, we make small adjustments to the parameters. Here, parameter refers to the weights and biases of the MLP, the w(l) jk s and b(l) j s from section 8.2. We keep adjusting the parameters so that in every iteration, the outputs on training data inputs come a little closer to the ground truth (GT). Eventually, after many iterations, we hopefully converge to optimal values. Note that there is no guarantee that the iterative process will converge to the best possible parameter values. The training might go completely astray or get stuck in a local minimum. (Local minima are explained in section 3.6; you are encouraged to reread it if necessary.) There is no good way to know whether we have reached optimal values (global minima) for the weights and biases. We typically run the neural network on test data, and if the results are satisfactory, we stop training. Test data should be held back during training, meaning we should never use test data to train. In the unfortunate event that the network has not reached the desired level of accuracy, we typically throw in more training data and/or try a modified loss func- tion and/or a different architecture. Simply retraining the network from a different 282 CHAPTER 8 Training neural networks: Forward propagation and backpropagation random start may also work. This is an experimental science with a lot of trial and error. How do we know how to adjust the parameter values in each iteration? We define a loss (aka error) function. There are many popular formulations of loss functions, and we review many of them later, but their common property is that when the neural network output agrees more with the known output (GT), the loss becomes lower, and vice versa. Thus if y denotes the output of the neural network and ¯y is the GT, a reasonable expression for the loss is the mean squared error (MSE) function (y −¯y)2. For now, we use the MSE loss as our representative loss function. Later we discuss others. Once the loss function is defined, we have a crisp, quantitative definition of the goal of neural network training. The goal is to minimize the total loss over the entire training data set. Note the clause entire training data set: we do not want to do well on one or two input instances at the cost of doing badly over the rest. If we have to choose between a solution that gives 10% error on all of, say, 100 training input instances versus one that yields 0% error on 50 training input instances but 40% on the remaining 50, we prefer the former. Each weight in the MLP, w(l) jk , is adjusted by an amount proportional to 훿w(l) jk . Simi- larly, each bias b(l) j is adjusted by an amount proportional to 훿b(l) j . We can denote all this compactly by saying we have a weight vector ®w and bias vector ®b. In each iteration, we change ®w by amount 훿®w and ®b by 훿®b so that their new values are ®w −r훿®w and ®b −r훿®b (r is a constant known as the learning rate that needs to be set at the beginning of training). In this context, it is worthwhile to note that in section 8.3.1, we expressed the collection of weights in an MLP with a matrix, while here we are referring to the same thing as a vector. These are not incompatible because we can always rasterize the elements of a matrix (that is, walk over the elements of the matrix from top to bottom and from left to right) into a vector. How do we estimate the adjustment amounts 훿®w and 훿®b? This is where the notion of gradients comes in. These were discussed in detail in sections 3.3.1, 3.3.2, and 3.5 (again, you are encouraged to reread if necessary). In general, if a loss, denoted 핃, is expressed as a function of the parameters, such as 핃  ®w, ®b  , then the change in the parameters that optimally takes us toward lower loss is yielded by the gradient of the loss with respect to the parameters ∇®w,b핃  ®w, b. The high-level process is described later in the chapter in algorithm 8.1. Here we look at the guts of it. 8.4.1 Loss and its minimization: Goal of training Given a training data set 핋that is a set of <input, GT output> pairs 핋= { ®x, ¯y }, the loss can be expressed as 핃= 1 2 Õ x∈핋  ®y −¯y2 (8.11) where ®y = MLP  ®x as per equation 8.9. 8.4 Training and backpropagation 283 Now consider equation 8.7 again. We can rasterize each layer’s weight matrix W (l) into a vector and then concatenate all these vectors from successive layers to form a giant weight vector ®w, the vector of all weights in the MLP: ®w = h w(0) 00 w(0) 01 · · · w(1) 00 w(1) 01 · · · w(L) 00 w(L) 01 · · · i Similarly, we can form a giant vector of all biases in the MLP: ®b = h b(0) 0 b(0) 1 · · · b(1) 0 b(1) 1 · · · b(L) 0 b(L) 1 · · · i The ultimate goal of training is to find ®w and ®b that will minimize the loss 핃. In chapter 3, we saw that we can solve for the minimum by setting the gradients ∇®w핃= 0 and ∇®b핃= 0. Computing the loss gradient from a combination of equations 8.9 and 8.11 is intractable. Instead, we go for an iterative solution: gradient descent on the loss surface, as described in the next section. Listing 8.2 PyTorch code for MSE loss def mse_loss(a_L, y): a: Activation of layer L (1D vector) y: Ground truth (1D vector) return 1./ 2 * torch.pow((a_L - y), 2) See equation 8.11. 8.4.2 Loss surface and gradient descent Geometrically, the loss function 핃  ®w, ®b  can be viewed as a surface in a high-dimensional space. The domain of this space corresponds to all the dimensions in ®w plus all the dimensions in ®b. This is shown in figure 8.8 with a 2D domain. In chapter 3, we also saw that given a function 핃  ®w, ®b  , the best way to progress toward the minimum is to walk on the parameter space along the negative gradient. We adopt this approach to minimize the loss. We compute the gradients of the loss function with respect to weights and biases and update the weights and bias vectors by an amount proportional to the (negative) of these gradients. Doing this repeatedly takes us close to the minimum. In figure 8.8, the gradient descent path is shown with solid arrows, while an arbitrary non-optimal path to the minimum is shown with dashed arrows. Thus the equations for updating weights and biases in gradient descent are ®w = ®w −r∇®w핃 ®b = ®b −r∇®b핃 (8.12) where r is a constant. Here ∇®w핃=  휕핃 휕w(l) jk for all l, j, k  284 CHAPTER 8 Training neural networks: Forward propagation and backpropagation Gradient descent Non-optimal progress Figure 8.8 A representative loss surface 핃(w, b). Note that ®w and ®b have each been reduced to 1D for this figure. ∇®b핃=  휕핃 휕b(l) j for all l, j  (8.13) The vector update equation 8.12 can be expressed in terms of the scalar components as w(l) jk =w(l) jk −r 휕핃 휕w(l) jk for all l, j, k b(l) j = b(l) j −r 휕핃 휕b(l) j for all l, j (8.14) Note that we have to reevaluate these partial derivatives in each iteration since their values will change in every iteration. 8.4.3 Why a gradient provides the best direction for descent Why does updating along the gradient reduce the function optimally? This is discussed in detail in chapter 3. Here we briefly recap the idea. Using multidimensional Taylor expansion, we can evaluate a function in the neighborhood of a known point. For instance, we can evaluate 핃  ®w + 훿®w for small offset 훿®w from ®w as follows 핃  ®w −® 훿w  = 핃  ®w −1 1!  ® 훿w T ∇®w핃+ 1 2!  ® 훿w T H  ® 훿w  + · · · (8.15) where H, called the Hessian matrix, is defined as in equation 3.9. Since we are not going too far from ®w, ∥훿®w∥is small. This means the quadratic and higher-order terms are 8.4 Training and backpropagation 285 negligibly small, and we can drop them (the approximation is perfect in the limit when ∥훿®w∥→0): 핃  ®w −® 훿w  ≈핃  ®w −1 1!  ® 훿w T ∇®w핃 But we know the dot product  ® 훿w T ∇®w핃will attain its maximum value when both the vectors point in the same direction: that is, ® 훿w = r∇®w핃 for some constant of proportionality r. In implementation, r is called the learning rate. A higher learning rate causes the optimization to progress more rapidly but also runs the risk of overshooting the mini- mum. We learn about these in more detail later. For now, simply note that r is a tunable hyperparameter of the system. Thus, the largest decrease in value from 핃  ®w to 핃  ®w −® 훿w  happens when 훿®w is along the negative gradient. This is why we move toward the negative gradient in gradient descent: it is the fastest way to reach the minimum. The straight arrows in figure 8.8 illustrate the direction of the gradient. The dashed arrows show an arbitrary nongradient path for comparison. We can deal with the bias vector ®b similarly. 8.4.4 Gradient descent and local minima We should note that gradient descent can get stuck in a local minimum. Figure 8.9 shows this. Local minimum Global minimum Figure 8.9 A nonconvex func- tion with local and global min- ima.Dependingonthestarting point, gradient descent will take us to one or the other. 286 CHAPTER 8 Training neural networks: Forward propagation and backpropagation In earlier eras, optimization techniques tried hard to avoid local minima and converge to the global minimum. Techniques like simulated annealing and tunneling were carefully designed to avoid local minima. Modern-day neural networks have adopted a different attitude: they do not try very hard to avoid local minima. Sometimes a local minimum is an acceptable (accurate enough) solution. Otherwise, we can retrain the neural network: it will start from a random position, so this time it may go to a better minimum. 8.4.5 The backpropagation algorithm We have seen that gradient descent progresses by repeatedly updating the weights and biases via equation 8.12. This is equivalent to repeatedly updating individual weights and biases using individual partial derivatives via equation 8.14. Obtaining a closed-form solution for the gradients ∇®w핃  ®w, ®b  , ∇®b핃  ®w, ®b  from equa- tions 8.9 and 8.11—or, equivalently, obtaining a closed-form solution for the partial derivatives 휕핃 휕w(l) jk , 휕핃 휕b(l) j —is very difficult. Backpropagation is an algorithm that allows us to evaluate the gradients and update the weights and biases one layer at a time, like forward propagation (equation 8.10). BACKPROPAGATION ALGORITHM ON A SIMPLE NETWORK We first discuss backpropagation on a simple MLP with only a single neuron per layer. The main simplification resulting from this is that individual weights and biases no longer need subscripts, with only one weight and one bias between two successive layers. They still need superscripts to indicate layer IDs, however. Figure 8.10 shows this MLP. We use MSE loss (equation 8.11), but we work on a single input-output pair xi, yi. The Figure 8.10 MLP with layers 0, . . ., L, one neuron per layer. Again, we have split every layer into a weighted sum and a sigmoid. 8.4 Training and backpropagation 287 total loss (summation over all the training data instances) can easily be derived by repeating the same steps. We first define an auxiliary variable: 훿(l) = 휕핃 휕z(l) for l ∈{0, L} The physical significance of 훿(l) is that it is the rate of change of the loss with the (pre- activation) output of layer l (remember, in this network, layer l has a single neuron). Let’s establish a few important equations for the MLP in figure 8.10: Forward propagation for an arbitrary layer l ∈{0, L} z(l) =w(l)a(l−1) + b(l) (8.16) a(l) = 휎  z(l) (8.17) Loss—Here we are working with a single training data instance, xi, whose GT output is ¯yi: 핃= 1 2  a(L) −¯yi 2 Partial derivative of loss with respect to the weight and bias in terms of an auxiliary variable for the last layer, L—Using the chain rule for partial derivatives, 휕핃 휕w(L) = 휕핃 휕z(L) 휕z(L) 휕w(L) Examining the terms on the right, we see 휕핃 휕z(L) = 훿(L) (auxiliary variable for layer L). And using the forward propagation equations, 휕z(L) 휕w(L) = a(L−1) Together, they lead to 휕핃 휕w(L) = 훿(L) · a(L−1) Similarly, 휕핃 휕b(L) = 휕핃 휕z(L) 휕z(L) 휕b(L) = 훿(L) · 1 Consequently, we have the following pair of equations expressing the partial derivative of loss with respect to weight and bias, respectively, in terms of the auxiliary variable for the last layer: 휕핃 휕w(L) = 훿(L) · a(L−1) (8.18) 288 CHAPTER 8 Training neural networks: Forward propagation and backpropagation 휕핃 휕b(L) = 훿(L) (8.19) Auxiliary variable for the last layer, L—Using the chain rule for partial derivatives, 훿(L) = 휕핃 휕z(L) = 휕핃 휕a(L) 휕a(L) 휕z(L) =  a(L) −¯yi  d휎  z(L) dz(L) Using equation 8.5 for the derivative of a sigmoid, we get 훿(L) =  a(L) −¯yi  휎  z(L)  1 −휎  z(L) which, using equation 8.17, leads to 훿(L) =  a(L) −¯yi  a(L)  1 −a(L) (8.20) Partial derivative of the loss with respect to the weight and bias in terms of an auxiliary variable for an arbitrary layer l—Using the chain rule for partial derivatives, 휕핃 휕w(l) = 휕핃 휕z(l) 휕z(l) 휕w(l) Using the definition of the auxiliary variable and the forward propagation equa- tion 8.16, this leads to 휕핃 휕w(l) = 훿(l) a(l−1) (8.21) Similarly, 휕핃 휕b(l) = 휕핃 휕z(l) 휕z(l) 휕b(l) Using the definition of the auxiliary variable and the forward propagation equa- tion 8.16, this leads to 휕핃 휕b(l) = 훿(l) (8.22) Auxiliary variable for an arbitrary layer, l—Using the chain rule for partial derivatives, 훿(l) = 휕핃 휕z(l) = 휕핃 휕z(l+1) 휕z(l+1) 휕a(l) 휕a(l) 휕z(l) Using the definition of the auxiliary variable and the forward propagation equa- tion 8.16, this leads to 훿(l) = 훿(l+1) w(l+1) d휎  z(l) dz(l) = 훿(l+1) w(l+1) 휎  z(l)  1 −휎  z(l) which yields 훿(l) = 훿(l+1) w(l+1) a(l)  1 −a(l) (8.23) We first encountered the one-layer-at-a-time property in section 8.3.2 in connection with the forward propagation equations. Let’s recap that in the context of training our simple network. Consider equations 8.16 and 8.17. We initialize the system with some 8.4 Training and backpropagation 289 values of weights w(l) and biases b(l). Using those, we can evaluate the layer 0 outputs. For starters, we can evaluate z(0) and a(0) easily (since all the inputs are known): z(0) =w(0)x + b(0) a(0) = 휎  z(0) Once we have z(0) and a(0), we can use them to evaluate z(1) and a(1) via equations 8.16 and 8.17. But if we have z(1) and a(1), we can use them to evaluate z(2) and a(2) via equations 8.16 and 8.17 again. And we can proceed in this fashion up to layer L to obtain a(L), which is the grand output of the MLP. In other words, we can itera- tively evaluate the outputs of successive layers using only the outputs from the previ- ous layer. No other layers need to be known. At any given iteration, we only have to keep the previous layer in memory: we can build the current layer from that. A single sequence of applications of equations 8.16 and 8.17 for layers 0 to L is known as a forward pass. A similar trick can be applied to evaluate the auxiliary variables, except we go backward. We can evaluate the auxiliary variable for the last layer, 훿(L), via equation 8.20. But once we have 훿(L), we can evaluate 훿(L−1) via equation 8.23. From that, we can evaluate 훿(L−2). We can proceed in this fashion all the way to layer 0, evaluating successively 훿(L), 훿(L−1), · · · , 훿(0). Every time we evaluate a 훿(l), we can also evaluate the 휕핃 휕w(l) and 휕핃 휕b(l) for the same layer via equations 8.21 and 8.22, respectively. We can also update the weight and bias of that layer right there using the just estimated partial derivatives, since the current values will never be needed again during training. Thus, starting from the last layer, we can update the weights and biases of all layers until layer 0 in this fashion. This is backpropagation. Of course, we have to proceed in tandem: one forward propagation (which sets the values of zs and as) for layers 0 to L, followed by a backpropagation layer for L to 0. Repeat these steps until convergence. NOTE Fully functional code for forward propagation, MSE loss, and backpropa- gation, executable via Jupyter Notebook, can be found at http://mng.bz/pJrw. Listing 8.3 PyTorch code for forward and backward propagation def forward_backward(x, y, W, b): L = len(W) - 1 a = [] for l in range(0, L+1): a_prev = x if l == 0 else a[l-1] Forward propagation z_l = Z(a_prev, W[l], b[l]) a_l = A(z_l) a.append(a_l) loss = mse_loss(a[L], y) Computes MSE loss 290 CHAPTER 8 Training neural networks: Forward propagation and backpropagation deltas = [None for _ in range(L + 1)] W_grads = [None for _ in range(L + 1)] Arrays to store 휹(l), 흏핃 흏w(l) , 흏핃 흏b(l) for layers 0 to L b_grads = [None for _ in range(L + 1)] a_L = a[L] Activation of the last layer - a(L) deltas[L] = (a_L - y) * a_L * (1 - a_L) W_grads[L] = torch.matmul(deltas[L], a[L - 1].T) Computes the 휹and gradients for layer L b_grads[L] = deltas[L] for l in range(L-1, -1, -1): Computes the 휹and gradients for layers 0 to L −1 a_l = a[l] deltas[l] = torch.matmul(W[l+1].T, deltas[l + 1]) * a_l * (1 - a_l) W_grads[l] = torch.matmul(deltas[l], a[l - 1].T) b_grads[l] = deltas[l] return loss, W_grads, b_grads BACKPROPAGATION ALGORITHM ON AN ARBITRARY NETWORK OF LINEAR LAYERS In section 8.4.5, we saw a simple network with only one neuron per layer. There was only one connection and hence one weight, one activation, and one auxiliary variable per layer. Consequently, we could drop the subscripts (although we had to keep the superscript indicating the layer) of all these variables. Now we examine a more generic network consisting of linear layers 0, · · · , L. An arbitrary slice of this network is shown in figure 8.6. The ultimate goal is to evaluate the partial derivatives of the loss with respect to the weights and biases. Using them, we can update the current weights and biases to optimally reduce the loss. Our overall strategy is as follows. We use the auxiliary variables again. We first derive expressions that allow us to compute the auxiliary variable for the last layer. Then we derive an expression that allows us to compute auxiliary variables for an arbitrary layer, l, given the auxiliary variables for layer l + 1. Since we can directly compute auxiliary variables for the last layer, L, we can use this expression to compute auxiliary variables for the second-to-last layer L −1. But once we have them, we can compute auxiliary variables for layer L −2. We proceed like this until we reach layer 0. Thus we can compute all the auxiliary variables. We also derive expressions that allow us to compute, from the auxiliary variables, the partial derivatives of loss with respect to weights and biases. This gives us everything we need. Since we start by computing things pertaining to the last layer and proceed iteratively toward the initial layer, the process is called backpropagation. You will notice the similarity between the expressions derived next and those derived for the one-neuron-per-layer network. The differences are explained: Forward propagation (arbitrary layer l)—Forward propagation through this network has already been described in section 8.3.1 and can be succinctly represented by equation 8.7 (repeated here for handy reference). On the left are the scalar 8.4 Training and backpropagation 291 equations, for one neuron at a time; and on the right are the vector equations, for the entire layer. They are equivalent: z(l) j = m Õ k=0 w(l) jk a(l−1) k + b(l) j ®z(l) =W (l) ®a(l−1) + ®b(l) a(l) j = 휎  z(l) j  ®a(l) = 휎  ®z(l) (8.24) Indices j and k iterate over all the neurons in the relevant layer. By convention, we always use these variables for arbitrary neurons in a layer. The variable l is used to index the layers. When indexing weights, we typically use j to indicate the destination and k to indicate the source—remember that weights are indexed (destination, source) somewhat unexpectedly to simplify the math. Typically, vec- tors correspond to entire layers. Individual vector elements correspond to specific neurons and are indexed by j or k. Loss—Unlike the simple network, here, the final Lth layer can have multiple neurons. Hence the loss function becomes 핃= 1 2 ∥®a(L) −¯y∥2 = 1 2 Õ j  a(L) j −¯yj 2 (8.25) where the summation happens over all neurons in the last layer. Note that ®a(L) is the output of the MLP: that is, ®a(L) = ®y = MLP  ®x for the training input ®x (see equation 8.10). The GT output corresponding to ®x is the constant vector ¯y. The closer ®y is to ¯y, the smaller the loss. Note that we need to average the loss over the entire training data set. Here we are showing the loss computation for a single training data instance. The computation simply needs to be replicated for each training data instance, and the results averaged. Auxiliary variables—Now that a layer has multiple neurons, we have one auxiliary variable per neuron. Thus the auxiliary variable has a subscript identifying the specific neuron in that layer. It continues to have a superscript indicating its layer. We define 훿(l) j = 휕핃 휕z(l) j ∀j ∈{0 · · · number of neurons in layer l}, ∀l ∈{0 · · · L} – Auxiliary variable for the last layer 훿(L) j = 휕핃 휕z(L) j = 휕핃 휕a(L) j 휕a(L) j 휕z(L) j Using equation 8.25 and observing that only one of the terms in the summation— the jth term—will survive the differentiation with respect to a(L) j (since the ajs 292 CHAPTER 8 Training neural networks: Forward propagation and backpropagation are independent of each other), we get 휕핃 휕a(L) j =  a(L) j −¯yj  Also, using the lower-left equation from 8.24 and equation 8.5, we get 휕a(L) j 휕z(L) j = d휎  z(L) j  dz(L) j = a(L) j  1 −a(L) j  Combining these, we get 훿(L) j =  a(L) j −¯yj  a(L) j  1 −a(L) j  (8.26) ®훿(L) =  ®a(L) −¯y  ◦®a(L) ◦  ®1 −®a(L) (8.27) Here, ◦denotes the Hadamard product between two vectors. It is basically a vector of elementwise products of corresponding vector elements. Thus, ®a =  a0 a1 ... an  ®b =  b0 b1 ... bn  (8.28) ®a ◦®b =  a0b0 a1b1 ... anbn  (8.29) Equations 8.26 and 8.27 are identical. The former is a scalar equation expressing individual auxiliary variables of the last layer. The latter is a vector equation expressing all the auxiliary variables of the last layer together. We can compute these directly if we have performed a forward pass and have its results, the a(L) j s available along with the training data GT. – Auxiliary variable for an arbitrary layer, l—This is significantly different and harder to understand than the one-neuron-per-layer case. We are trying to evaluate 훿(l) j = 휕핃 휕z(l) j in the general case: that is, for an arbitrary layer l. The loss does not directly depend on the inner layer variable z(l) j . The loss directly depends only on the last layer activations, which depend on the previous layer, and so forth. The zs in any one layer form a complete dependency set for the loss 핃, meaning the loss can be expressed in terms of only these and no other variables. In particular, we can express the loss as 핃  z(l+1) 0 , z(l+1) 1 , z(l+1) 2 , · · ·  . You can form a mental 8.4 Training and backpropagation 293 picture that z(l) j fans out to 핃through all the zs in the next layer, z(l+1) 0 , z(l+1) 1 , z(l+1) 2 , and so on. Then, using the chain rule of partial differentiation, 훿(l) j = 휕핃  z(l+1) 0 , z(l+1) 1 , z(l+1) 2 , · · ·  휕z(l) j = Õ k 휕핃 휕z(l+1) k 휕z(l+1) k 휕z(l) j = Õ k 휕핃 휕z(l+1) k 휕z(l+1) k 휕a(l) j 휕a(l) j 휕z(l) j Now, by definition, 휕핃 휕z(l+1) k = 훿(l+1) k And using equation 8.24, 휕z(l+1) k 휕a(l) j =w(l+1) kj while 휕a(l) j 휕z(l) j = d휎  z(l) j  dz(l) j = a(l) j  1 −a(l) j  Combining all these, we get the scalar expression for a single auxiliary variable. It is presented here along with its equivalent vector equation for the entire layer: 훿(l) j = Õ k 훿(l+1) k w(l+1) kj a(l) j  1 −a(l) j  (8.30) ®훿(l) =  W (l+1)T ®훿(l+1)  ◦®a(l) ◦  ®1 −®a(l) (8.31) Here, ◦denotes the Hadamard multiplication explained earlier and W (+1l) is the matrix representing the weights of all connections from layer l to layer (l + 1) (see equation 8.8). Equations 8.30 and 8.31 allow us to evaluate 훿(l)s from the 훿(l+1)s if the results of forward propagation (as) are available. We have already shown that the auxiliary variables for the last layer are directly computable from the activations of that layer. Hence, we can evaluate all the layers’ auxiliary variables. Derivatives of loss with respect to weights and biases in terms of auxiliary variables—We have already seen how to compute auxiliary variables. Now we will express the partial derivatives of loss with respect to weights and biases in terms of those. This will provide us with the gradients we need to update the weights and biases along the negative gradient, which is the optimal move to minimize loss: 휕핃 휕w(l) jk = 휕핃 휕z(l) j 휕z(l) j 휕w(l) jk = 훿(l) j a(l−1) k (8.32) 294 CHAPTER 8 Training neural networks: Forward propagation and backpropagation ∇w(l)핃= ®훿(l)  ®a(l−1)T (8.33) Equations 8.32 and 8.33 are equivalent. The first is scalar and pertains to individual weights in layer l, and the second describes the entire layer. Similarly, equations 8.34 and 8.35 are equivalent: 휕핃 휕b(l) j = 휕핃 휕z(l) j 휕z(l) j 휕b(l) j = 훿(l) j (8.34) ∇b(l)핃= ®훿(l) (8.35) The first is scalar and pertains to individual biases in layer l, and the second describes the entire layer. 8.4.6 Putting it all together: Overall training algorithm Previously, we discussed forward propagation: passing an input vector ®x through a sequence of linear layers and generating an output prediction. We learned about MSE loss, 핃, which calculates the deviation of the output prediction from the GT, y. We also learned to compute the gradients of 핃with respect to parameters W and b using backpropagation. In the following algorithm, we describe how these components come together in the training process: Algorithm 8.1 Training a neural network Initialize ®w, b with random values while 핃> threshold do ⊲Forward pass for l ←0 to L do ®z(l) =W (l) ®a(l−1) + ®b(l) ®a(l) = 휎  ®z(l) end for ⊲Loss 핃= 1 2 ∥®a(L) −¯y∥2 ⊲Gradients for the last layer ®훿(L) =  ®a(L) −¯y  ◦®a(L) ◦  ®1 −®a(L) ∇W (L)핃= ®훿(L)  ®a(L−1)T ∇b(L)핃= ®훿(L) ⊲Gradients for the remaining layers for l ←L −1 to 0 do ®훿(l) =  W (l+1)T ®훿(l+1)  ◦®a(l) ◦  ®1 −®a(l) ∇W (l)핃= ®훿(l)  ®a(l−1)T ∇b(l)핃= ®훿(l) end for 8.5 Training a neural network in PyTorch 295 ⊲Parameter update W =W −r∇W 핃 b = b −r∇b핃 end while 8.5 Training a neural network in PyTorch Now that we’ve seen how the training process works, let’s look at how this can be implemented in PyTorch. For this purpose, let’s take the following example. Consider an e-commerce company that’s trying to solve the problem of demand prediction: the company would like to estimate the number of mobile phones that will be sold in the upcoming week so that it can manage its inventory accordingly. Our goal is to develop a model that can make such a prediction. Let’s assume that the demand for a given week is a function of three variables: (a) the number of mobile phones sold in the previous week, (b) discounts offered, and (c) the number of weeks to the next holiday. Let’s call these variables prev_week_sales, discount_fraction, and weeks_to_next_holidays, respectively. This example can be modeled as a regression problem wherein we predict the number of mobile phones sold in the upcoming week from an input vector of the form [prev_week_sales, discount_fraction, weeks_to_next_holidays]. NOTE Fully functional code for this section, executable via Jupyter Notebook, can be found at http://mng.bz/O1Ra. From historical data, we generate a large data set, X, that contains the values of the three variables for the last N weeks. X is represented as an N x 3 matrix, with each row representing an individual training data instance and N being the total number of data points available. We also have a GT vector ¯y of length N, containing the actual sales of mobile phones for each of the weeks in the training data set. Table 8.1 shows sample data points from our training set. Table 8.1 Sample training data for demand prediction Previous week sales Discount fraction (%) Weeks to next holidays Number of units sold 76,440 63 2 94,182 41,512 50 3 51,531 77,395 77 9 95,938 . . . . . . . . . . . . 21,532 70 4 28,559 NOTE In this section, X and ¯y refer to the entire batch of training data instances. This may be infeasible in practical settings because of large data sets. To address this, we typically use mini-batches of X and ¯y. We introduce the concept of mini- batches formally in the next chapter. 296 CHAPTER 8 Training neural networks: Forward propagation and backpropagation One important point about the data set is that the range of values for each feature is completely different. For example, the previous week’s sales are expressed as a number on the order of tens of thousands of units, whereas the discount fraction is a percentage number between 0 and 100. In machine learning, it is a good practice to bring all the values to a common scale, because doing so can help improve the speed of training and reduce the chance of getting stuck at a local minimum. For our example, let’s use min-max normalization to scale all the features to 0–1. The following code snippet shows how to perform min-max normalization in PyTorch. For the rest of the discussion, we assume that we are operating on the normalized data: def min_max_norm(X, y): X, y = X.clone(), y.clone() Clones the data so as not to mutilate the original data X_min, X_max = torch.min(X, dim=0)[0], torch.max(X, dim=0)[0] Calculates the min and max values of each column of X X = (X - X_min) / (X_max - X_min) Normalizes X y_min, y_max = torch.min(y, dim=0)[0], torch.max(y, dim=0)[0] Calculates the min and max values of y y = (y - y_min) / (y_max - y_min) Normalizes y return X, y To solve the regression problem, let’s first define a two-layer neural network model that can take in 3D input vectors of the form [prev_week_sales, discount_fraction, is_holidays_ongoing] and generate output predictions. The following code snippet gives the PyTorch implementation: class TwoLayeredNN(torch.nn.Module): def __init__(self, input_size, hidden1_size, hidden2_size, output_size): super(TwoLayeredNN, self).__init__() self.model = torch.nn.Sequential( Defines the network as a sequence of linear and sigmoid layers torch.nn.Linear(input_size, hidden1_size), First hidden layer with a weight matrix of size (input_size × hidden1_size) torch.nn.Sigmoid(), torch.nn.Linear(hidden1_size, hidden2_size), Second hidden layer with a weight matrix of size (hidden1_size × hidden2_size) torch.nn.Sigmoid(), torch.nn.Linear(hidden2_size, output_size) Output layer with a weight matrix of size (hidden2_size × output_size) ) def forward(self, X): X is an N × 3 matrix. Each row is a (3D vector) representing a single data point. return self.model(X) nn = TwoLayeredNN(input_size=X.shape[-1], hidden1_size=10, hidden2_size=5, output_size=1) 8.5 Training a neural network in PyTorch 297 Neural network models in PyTorch should subclass torch.nn.Module and implement the forward method. Our two-layer neural network contains two linear layers, each followed by a sigmoid (nonlinear) activation layer. Finally, we have a linear layer that converts the final activation into the output prediction. These layers are chained together using the torch.nn.Sequential class to form the two-layer neural network. Whenever our model is called using nn(X), the forward method is invoked, and the input X is passed through the individual layers to obtain the final output. Now that we have defined the neural network and its forward pass, we need to define the loss function. We can use the MSE loss defined in equation 8.11. The loss function essentially compares the demand predicted by the neural network model with the actual demand from the GT and returns larger values when the difference is higher and smaller values when the difference is lower. MSE loss is readily available in PyTorch through the torch.nn.MSELoss class. The following code snippet shows a sample invocation: loss = torch.nn.MSELoss() Instantiates the loss function loss(y_pred, y_gt) compute loss y_pred: Output of the neural network y_gt: ground truth Finally, we need a way to compute the gradients of the loss with respect to the parameters of our model so we can start the training process. Luckily, we don’t have to explicitly compute the gradients ourselves because PyTorch automatically does this for us using automatic differentiation, aka autograd. (Refer to section 3.1 for more details about autograd.) For our current example, we can instruct PyTorch to run backpropagation and compute gradients by calling loss.backward(). With this, we’re ready to start training. PyTorch code for training the neural network is shown next. Listing 8.4 Training a neural network nn = TwoLayeredNN(input_size=X.shape[-1], Instantiates the neural network hidden1_size=10, hidden2_size=5, output_size=1) loss = torch.nn.MSELoss() Instantiates the loss function optimizer = torch.optim.SGD(nn.parameters(), lr=0.2, Instantiates the optimizer momentum=0.9) num_iters = 1000 for i in range(num_iters): Training loop y_out = nn(X) Forward pass mse_loss = loss(y_out, y) Computes the loss optimizer.zero_grad() Clears the gradients and prevents accumulation of gradients from the previous step mse_loss.backward() Runs backpropagation (computes gradients) optimizer.step() Updates the weights 298 CHAPTER 8 Training neural networks: Forward propagation and backpropagation In the training loop, we iteratively run the forward pass, compute the loss, calculate the gradients, and update the weights. The neural network is initialized with random weights and hence makes arbitrary predictions for the demand in the early iterations of the training loop. This translates to a high initial loss value. However, as training proceeds, the weights are updated to minimize the loss value, and the predicted demand comes closer to the actual GT. To update the weights, we use what is known as an optimizer. During training, the gradients are computed by calling the backward() function on the loss object. Following that, the optimizer.step call updates the weights and biases. In this example, we used the stochastic gradient descent–based optimizer, which can be invoked using torch.optim.SGD. PyTorch offers various optimizers, such as Adam, AdaGrad, and so on, which will be discussed in detail in the next chapter. We typically run the training loop until the loss reaches a value low enough to be acceptable. Once the training loop completes, we have a model that can readily take in new data points and generate output predictions. Summary The sigmoid function 휎(x) = 1 1+e−x has an S-shaped graph, is a differential version of the Heaviside step function, and, as such, is used in perceptrons. Thus the overall perceptron function becomes P  ®x ≡휎  ®wT ®x + b. It is parametrized by ®w and b, which control the slope and position, respectively, of the S-shaped curve. Neural networks solve real-life problems that require intelligence by approximat- ing the function that solves the problem in question. They are built of multiple perceptrons interconnected by weighted edges. Instead of connecting perceptrons haphazardly, we connect them as layers. In a layered network, a perceptron is only connected to perceptrons from the immediately preceding layer. Intra-layer and other connections are not allowed. Supervised neural networks have manually generated outputs for a sample set of input values (ground truth). This entire data set consisting of inputs and known outputs is known as the training data set. Loss is defined as the mismatch between the ground truth and actual output gen- erated by the neural network on training data inputs. The simplest way to compute loss is to take the Euclidean distance between the neural network-generated out- put and ground-truth vectors. This is called the MSE (mean squared error) loss. Mathematically, given a training data set 핋that is a set of <input, GT output> pairs 핋= { ®x, ¯y }, the loss can be expressed as 핃= 1 2 Õ xi ∈핋  ®y −¯y2 where the output is ®yi = MLP  ®xi . Training is the process of optimizing the connection weights and biases of a specific neural network so that the loss is minimal. Note that during inferencing, the neural network typically sees data it has never seen during training. Inferencing outputs Summary 299 are good only if the distribution of training inputs roughly matches the overall input distribution. We minimize the loss by iteratively adjusting the weights and biases. The quickest way to reach the closest minimum of a multivariate function is to follow the gradient. Hence, we adjust the weights and biases following the gradient of the loss function. Mathematically, W =W −r∇W 핃 b = b −r∇b핃 A forward pass is the process of generating outputs from inputs with a neural network: more specifically, a multilayer perceptron (MLP). Thus an MLP does inferencing via a forward pass. A beautiful property of a layered network is that we can do a forward pass dealing with one layer at a time, proceeding iteratively from layer 0 (closest to the input) to the output layer. Mathematically, ®z(l) =W (l) ®a(l−1) + ®b(l) ®a(l) = 휎  ®z(l) where W (l), ®b(l) represent the weights and biases for layer l, and ®a(l) represent the output for layer l (activation), which is also the input for layer l + 1. A backward pass is the process by which the gradients of the loss with respect to all the weights and biases are generated. It relies on the result of the preceding forward pass and proceeds from the output layer toward the input layer. It uses auxiliary variables ®훿(l), which can be computed by iterating backward from the last (closest to output) layer to the first (closest to input) layer—hence the name backward propagation—and all the required gradients can be computed from those auxiliary variables. Mathematically, ®훿(l) =  W (l+1)T ®훿(l+1)  ◦®a(l) ◦  ®1 −®a(l) ∇W (l)핃= ®훿(l)  ®a(l−1)T ∇b(l)핃= ®훿(l) Training progresses by alternating forward and backward passes on the training data set. 9 Loss, optimization, and regularization This chapter covers Geometrical and algebraic introductions to loss functions Geometrical intuitions for softmax Optimization techniques including momentum, Nesterov, AdaGrad, Adam, and SGD Regularization and its relationship to Bayesian approaches Overfitting while training, and dropout By now, it should be etched in your mind that neural networks are essentially function approximators. In particular, neural network classifiers model the decision bound- aries between the classes in the feature space (a space where every input feature combination is a specific point). Supervised classifiers mark sample training data inputs in this space with a—perhaps manually generated—class label (ground truth). The training process iteratively learns a function that essentially creates decision boundaries separating the sampled training data points into individual classes. If the training data set is a reasonable representative of the true distribution of possible inputs, the network (the learned function that models the class boundaries) will classify never-before-seen inputs with good accuracy. 300 9.1 Loss functions 301 When we select a specific neural network architecture (with a fixed set of layers, each with a fixed set of perceptrons with specific connections), we have essentially frozen the family of functions we use as a function approximator. We still have to “learn” the exact weights of the connectors between various perceptrons (aka neurons). The training process iteratively sets these weights so as to best classify the training data points. To do this, we design a loss function that measures the departure of the network output from the desired result. The network continually tries to minimize this loss. There are a variety of loss functions to choose from. The iterative process through which loss is minimized is called optimization. We also have a multitude of optimization algorithms to choose from. In this chapter, we study loss functions, optimization algorithms, and associated topics like L1 and L2 regularization and dropout. We also learn about overfitting, a potential pitfall to avoid while training a neural network. NOTE The complete PyTorch code for this chapter is available at http://mng.bz/aZv9 in the form of fully functional and executable Jupyter notebooks. 9.1 Loss functions A loss function essentially measures the badness of the neural network output. In the case of a supervised network, the loss for an individual training data instance is the distance of the actual output of the neural network (aka prediction) from the desired ideal outputs (known or manually labeled ground truth [GT]) on that particular training input instance. Total training loss is obtained by summing the losses from all training data instances. Training is essentially an iterative optimization process that minimizes the total training loss. 9.1.1 Quantification and geometrical view of loss Loss surfaces and their minimization are described in detail in section 8.4.2. Here we only do a quick review. A full neural network can be described by the equation ®y = f  ®x ®w, ®b  (9.1) Equation 9.1 says that given an input ®x, the neural network with weights ®w and biases ®b emits the output vector or prediction vector ®y. The weights and biases may be organized into layers; this equation does not care. The vectors ®w, ®b, respectively, denote the sets of all weights and biases from all layers aggregated. Evaluating the function f (·) is equivalent to performing one forward pass on the network. In particular, given a training input instance ®x(i), the neural network emits ®y(i) = f  ®x(i) ®w, ®b  . We refer to ®y(i) as the output of the ith training data instance. During supervised training, for each training input instance ®x(i), we have the GT (the known output), ¯y(i). We refer to ¯y(i) as the GT vector (as usual, we use superscript indices for training data instances). 302 CHAPTER 9 Loss, optimization, and regularization Ideally, the output vector ®y(i) should match the GT vector ¯y(i). The mismatch between them is the loss for that training data instance 핃(i)  ®y(i), ¯y(i) ®w, ®b  , which we sometimes denote as 핃(i)  ®y(i), ¯y(i) . The overall training loss (to be minimized by the optimization process) is the sum of losses over all training data instances: 핃  ®w, ®b  = n−1 Õ i=0 핃(i)  f  ®x(i) ®w, ®b  , ¯y(i) = n−1 Õ i=0 핃(i)  ®y(i), ¯y(i) ®w, ®b  (9.2) where the summation is over all training data instances, and n is the size of the training data set. Note that this summation over all training data points is needed to compute the loss for each training data instance. Thus an epoch, a single training loop over all training data instances, costs O  n2, where n is the number of training data points. Training usually requires many epochs. This makes the training process very expensive. In section 9.2.2, we study ways of mitigating this. NOTE In this chapter, we use n to indicate the number of training data points and N to indicate the dimensionality of the output vector. For classifiers, the dimensionality of the output vector, N, matches the number of classes. We also use superscript (i) to index training data points and subscript j to index output vector dimensions. For classifiers, j indicates the class. We can visualize 핃  ®w, ®b  as a hypersurface in high-dimensional space. Figures 8.8 and 8.9 show some low-dimensional examples of loss surfaces. These are illustrative examples. In reality, the loss surface is typically high dimensional and very complex. One good mental picture is that of a canyon (see figure 9.1). Traveling “downhill” at any point effectively follows the negative direction of the local gradient (the gradient of a loss surface was introduced in section 8.4.2). Traveling downhill along the gradient does not always lead to the global minimum. For instance, going downhill following the dashed to local minima to global minima Figure 9.1 The loss surface can be viewed as a canyon. 9.1 Loss functions 303 arrow will take us to a local minimum, whereas the global minimum is where the water is going, indicated by the solid arrow. (See also section 8.4.4 and figure 8.9.) Many loss formulations are possible, quantifying the mismatch 핃(i)  ®y(i), ¯y(i) ; some of them are described in the following subsections. 9.1.2 Regression loss Regression loss is the simplest loss formulation. It is the L2 norm of the difference between the output and GT vectors. This loss was introduced in equation 8.11. We restate it here: the loss on the ith training data instance is 핃(i)  ®y(i), ¯y(i) = ∥®y(i) −¯y(i) ∥2 = N−1 Õ j=0  y(i) j −¯y(i) j 2 where the summation is over the components of the output vector. N is the number of classes. The GT vector and output vector are both N-dimensional. NOTE Fully functional code for regression loss, executable via Jupyter Notebook, can be found at http://mng.bz/g1a8. Listing 9.1 PyTorch code for regression loss from torch.nn.functional import mse_loss Imports the regression loss (mean squared error loss) y_pred = torch.tensor([-0.10, -0.24, 1.43, -0.14, -0.59]) N-d prediction vector y_gt = torch.tensor([ 0.59, -1.92, -1.27, -0.40, 0.50]) N-d ground truth vector loss = mse_loss(y_pred, y_gt, reduction='sum') Computes the regression loss 9.1.3 Cross-entropy loss Cross-entropy loss was discussed in the context of entropy in section 6.3. If necessary, this would be a great time to reread that. Here we review the idea quickly. Cross-entropy loss is typically used to measure the mismatch between a classifier neural network output and the corresponding GT in a classification problem. Here, the GT is a one-hot vector whose length equals the number of classes. All but one of its elements are 0. The single nonzero element is 1, and it occurs at the index corresponding to the correct class for that training data instance. Thus the GT vector looks like ¯y(i) = [0, . . . , 0, 1, 0, . . . , 0]. The prediction vector should have elements with values between 0 and 1. Each element of the prediction vector ®y(i) indicates the probability of a specific class. In other words, ®y(i) = [p0, p1, . . . , pN−1], where pj is the probability of the input i belonging to the jth class. In section 6.3, we illustrated with an example image classifier that predicts whether an image contains a cat (class 0), a dog (class 1), an airplane (class 2), or an automobile (class 3). One of the four is always assumed to be present in the image. If, for the ith training data instance, the GT vector is an image of cat, we have ¯y(i) = [1, 0, 0, 0]. A prediction vector ®y(i) = [0.8, 0.15, 0.04, 0.01] is good, while ®y(i) = [0.25, 0.25, 0.25, 0.25] is bad. Note that 304 CHAPTER 9 Loss, optimization, and regularization sum of the elements of the GT as well as the prediction vector is always 1 since they are probabilities. Mathematically, given a training dataset X, N−1 Õ j=0 ¯y(i) j = 1 N−1 Õ j=0 y(i) j = 1∀i ∈X Given such GT and prediction vectors, the cross-entropy loss (CE loss) is 핃(i)  ®y(i), ¯y(i) = − N−1 Õ j=0 ¯y(i) j log  y(i) j  (9.3) where the summation is over the elements of the prediction vector and N is the number of classes. INTUITIONS BEHIND CROSS-ENTROPY LOSS Note that only one element—the one corresponding to the GT class—survives in the summation of equation 9.3. The other elements vanish because they are multiplied by the 0 GT value. The (logarithm of) the predicted probability of the correct GT class is multiplied by 1. Hence, the CE loss always boils down to −log  y(i) j∗  , where j∗is the GT class. If this probability is 1, the CE loss becomes 0, rightly so, as the correct class is being predicted with a probability of 1. If the predicted probability of the correct class is 0, the CE loss is −log (0) = ∞, again rightly so, since this is the worst possible prediction. The closer the prediction for the correct class is to 1, the smaller the loss. NOTE Fully functional code for cross-entropy loss, executable via Jupyter Note- book, can be found at http://mng.bz/g1a8. Listing 9.2 PyTorch code for cross-entropy loss import torch y_pred = torch.tensor([0.8, 0.15, 0.04, 0.01]) N-d prediction vector y_gt = torch.tensor([1., 0., 0., 0.]) N-d one-hot ground truth vector loss = -1 * torch.dot(y_gt, torch.log(y_pred)) Computes the cross-entropy loss SPECIAL CASE OF TWO CLASSES What happens if N = 2 (that is, we have only two classes)? Let’s denote the predicted probability of class 0, for the ith training input, as y(i): that is, ®y(i) 0 = y(i). Then, since these are probabilities, the prediction on the other class ®y(i) 1 = 1 −y(i). Also, let ¯y(i) denote the GT probability for class 0 on this ith training input. Then 1 −¯y(i) is the GT probability for class 1. (We have slightly abused notations—up to this point, ¯y has denoted a vector, but here it denotes a scalar.) Then, following equation 9.3, the CE loss on the ith training data instance becomes 핃(i)  y(i), ¯y(i) = −¯y(i) log  y(i) −  1 −¯y(i) log  1 −y(i) (9.4) 9.1 Loss functions 305 NOTE Fully functional code for binary cross-entropy loss, executable via Jupyter Notebook, can be found at http://mng.bz/g1a8. Listing 9.3 PyTorch code for binary cross-entropy loss from torch.nn.functional import binary_cross_entropy Imports the binary cross-entropy loss y_pred = torch.tensor([0.8]) Outputs the probability of class 0 - y0. A single value is sufficient because y1 = 1 −y0. y_gt = torch.tensor([1.]) The ground truth is either 0 or 1. loss = binary_cross_entropy(y_pred, y_gt) Computes the cross-entropy loss 9.1.4 Binary cross-entropy loss for image and vector mismatches Given a pair of normalized tensors (such as images or vectors) whose elements all have values between 0 and 1, a variant of the two-class CE loss can be used to estimate the mismatch between the tensors. Note that an image with pixel-intensity values between 0 and 255 can always be normalized by dividing each pixel-intensity value by 255, thereby converting it to the 0 to 1 range. Such a comparison of two images is used in image autoencoders, for example. We study autoencoders later; here, we provide a brief overview in the following sidebar. Autoencoders Autoencoders take an image as input, create a low-dimensional descriptor from the image—this descriptor is often referred to as an embedding of the image—and try to reconstruct the input image from the embedding. The image embedding is a com- pressed representation of the image. Reconstruction is a lossy process: the small, subtle variations in the signal are lost, and only the essential part is retained. The loss is the mismatch between the input image and the reconstructed image. By mini- mizing this loss, we incentivize the system to retain the essence of the input as much as possible within the embedding-size budget. Let ¯y denote the input image. Let ®y(i) denote the reconstructed image outputted by the autoencoder. The binary cross-entropy loss is defined as 핃(i)  ¯y(i), ®y(i) = − N−1 Õ j=0  ¯y(i) j log  y(i) j  +  1 −¯y(i) j  log  1 −y(i) j  (9.5) Note that here N is the number of pixels in the image, not the number of classes, as before. The summation is over the pixels in the image. Also, the GT vector ¯y(i) is not a one-hot vector; rather, it is the input image. These differences aside, equation 9.5 is based on the same idea as equation 9.4. 306 CHAPTER 9 Loss, optimization, and regularization WHY DOES IT WORK? Binary cross-entropy loss attains its minimum when the input matches the GT. We outline the proof next. (Note that we drop the superscripts and subscripts for simplicity.) We have −핃= ¯ylog (y) + (1 −¯y) log (1 −y) At the minimum, 휕핃 휕y = 0. This implies −휕핃 휕y = ¯y 1 y −(1 −¯y) 1 1 −y = ¯y y −1 −¯y 1 −y = 0 =⇒y = ¯y Thus, the minimum of the binary cross-entropy loss occurs when the network output matches the GT. However, this does not mean this loss becomes zero when the output matches the GT. NOTE Binary cross-entropy loss is not necessarily zero even in the ideal case of the output matching the input (although the loss is indeed minimal in the ideal case, meaning the loss is higher for non-ideal cases with mismatched input and output). Examining equation 9.5, when the two inputs match, we have −핃 ¯y, ®y ®y=¯y. We intuitively expect this loss to be zero since the output is ideal. But it isn’t. For example, if, for ¯y(i) j = y(i) j = 0.25 핃(i)  ¯y(i) j , ®y(i) j  ¯yj=®y(i) j =0.25 = −¯y(i) j log  y(i) j  −  1 −¯y(i) j  log  1 −y(i) j  = −0.25log (0.25) −0.75log (0.75) = 0.56 ≠0 In fact, the binary cross-entropy loss is zero only in special cases, like y(i) j = ¯y(i) j = 1. 9.1.5 Softmax Suppose we are building a classifier: for instance, the image classifier we illustrated in section 6.3, which predicts whether an image contains a cat (class 0), a dog (class 1), and airplane (class 2), or an automobile (class 3). Our classifier can emit a score vector ®s corresponding to an input image. Element j of the score vector corresponds to the jth class. We take the max of the score vector and call that the neural network-predicted label for the image. For instance, in the example image classifier, a score vector may be h 9.99 10 0.01 −10 i . Since the highest score occurs at index 1, we conclude that the image contains a dog (class 1). The scores are unbounded; they can be any real number in the range [−∞, ∞]. In general, however, neural networks behave better when the loss function involves a bounded set of numbers in the same range. The training converges faster and to better minima, and the inferencing is more accurate. Consequently, it is desirable to convert the example scores to probabilities. These will be numbers in the range [0, 1] (and the elements of the vector will sum to 1). 9.1 Loss functions 307 The softmax function converts unbounded scores to probabilities. Given a score vector ®s = h s0 s1 s2 . . . sN−1 i , the corresponding softmax vector is softmax  ®s =  es0 S es1 S es2 S · · · esN−1 S  where S = N−1 Õ k=0 esk (9.6) A few noteworthy points: The vector has as many elements as possible classes. The sum of the elements in the previous vector is 1. The jth element of the vector represents the predicted probability of class j. The formulation can handle arbitrary scores, including negative ones. So in our example classification problem with the four classes (cat, dog, airplane, automobile), the score vector ®s = h 9.99 10 0.01 −10 i will yield the softmax vector softmax  ®s = h 0.497 0.502 2.30e −5 1.04e −9 i . The probability of cat is 0.497, and the probability of dog is slightly higher, 0.502. The probabilities of airplane and automobile are much lower: the neural network predicts that the image is that of a dog, but it is not very confident; it could also be a cat. WHY THE NAME SOFTMAX? The softmax function is a smooth (differentiable) approximation to the argmaxonehot function, which emits a one-hot vector corresponding to the index of the max score. The argmaxonehot function is discontinuous. To see this, consider a pair of two class score vectors: ®p = h 9.99 10 i ®q = h 10 9.99 i Performing an argmaxonehot operation on them will yield the following one-hot vectors, respectively: argmaxonehot  ®p = h 0 1 i argmaxonehot  ®q = h 1 0 i 308 CHAPTER 9 Loss, optimization, and regularization Thus we see that the vectors argmaxonehot  ®p and argmaxonehot  ®q are significantly far from each other, even though the points ®p and ®q are very close to each other. On the other hand, the corresponding softmax vectors are softmax  ®p = h 0.4975 0.5025 i softmax  ®q = h 0.5025 0.4975 i Although the predicted classes still match those from the argmaxonehot vector, the softmax vectors are very close to each other. The closer the scores, the closer the softmax probabilities. In other words, the softmax is continuous. Figure 9.2 depicts this geometrically. The argmaxonehot functions as a function of the score vector [s0, s1] (for selecting classes 0 and 1, respectively) are shown in figures 9.2a and 9.2c. These are step functions on the (s0, s1) plane, with s0 = s1 be- ing the decision boundary. Their softmax approximations are shown in figures 9.2b and 9.2d. In section 8.1, we introduced the 1D sigmoid function (see figure 8.1), which approximates the 1D step function. Here we see the higher-dimensional analog of that. Listing 9.4 PyTorch code for softmax from torch.nn.functional import softmax Imports the softmax function scores = torch.tensor([9.99, 10, 0.01, -10]) Scores are typically raw, un-normalized outputs of a neural network. output = softmax(scores, dim=0) Computes the softmax 9.1.6 Softmax cross-entropy loss From the preceding discussion, it should be clear that it is desirable to make the last layer of a classifier neural network a softmax layer. Then, given an input, the network will emit probabilities for each class. During training, we can evaluate the loss on these probabilities with regard to the known GT probabilities. This can be done via the CE loss (see section 9.1.3). Thus the softmax is often followed by the CE loss during classifier training. Consequently, the combination (softmax CE loss) is available as a single operation in many deep learning packages, such as PyTorch. This is convenient because we do not need to call softmax and then CE loss. But the deeper reason for combining them is that the combination tends to be numerically better. Let’s look at an example to see how the softmax CE loss changes as the output predic- tion changes. Consider the image classification problem again, where we’d like to classify whether an image contains one of four categories: cat (class 0), dog (class 1), airplane (class 2), or automobile (class 3). Figure 9.3 represents this visually. Suppose the image we’re classifying actually contains a dog (class 1). The GT is represented as a one-hot vector h 0 1 0 0 i . If our classifier predicts the vector h 0.498 0.502 0 0 i , it’s pre- dicting both cat and dog with almost equal probability. This is a bad prediction because 9.1 Loss functions 309 (a) Step function: z = 1 if s0 >= s1, else z = 0 (b) Softmax: differential approximation of the step function (c) Step function: z = 1 if s1 >= s0, else z = 0 (d) Softmax: differential approximation of the step function Figure 9.2 Two-class argmaxonehot and softmax (function of score vector [s0, s1]) on the (s0, s1) plane. The decision boundary is the 45o line s0 = s1. 310 CHAPTER 9 Loss, optimization, and regularization Cat Dog Airplane Automobile 0 1 0 0 9.99 10 0.01 -10 Bad prediction Scores Softmax output 0.498 0.502 0 0 4.27 10 0.01 -10 Good prediction Scores Softmax output 0.003 0.997 0 0 GT One-hot Figure 9.3 Softmax output and cross-entropy loss for good and bad output predictions we would ideally expect it to confidently predict a dog (class 1). Consequently, the CE loss is high (0.688). On the other hand, if our classifier predicts h 0.003 0.997 0 0 i , it is highly certain (with a probability of 0.997) that the image contains a dog. This is a good prediction, and hence the CE loss is low (0.0032). Softmax CE loss is probably the most popular loss method used for training in classifiers at the moment. NOTE Fully functional code for softmax CE loss, executable via Jupyter Notebook, can be found at http://mng.bz/g1a8. Listing 9.5 PyTorch code for softmax cross-entropy loss from torch.nn.functional import cross_entropy scores = torch.tensor([[9.99, 10, 0.01, -10]]) y_gt = torch.tensor([1]) Ground truth class index Ranges from 0 to num_classes 1 loss = cross_entropy(scores, y_gt) Computes the softmax cross-entropy loss 9.1.7 Focal loss As training progresses, where should we focus our attention? This question becomes especially significant when there is data imbalance, meaning the number of training data 9.1 Loss functions 311 instances available for some classes is significantly smaller than others. In such cases, not all training data is equally important. We have to use our training data instances wisely. Intuitively, the greater bang for the buck is trying to improve the training data instances that are not doing well. In other words, instead of trying to squeeze out every bit of juice from the examples in which the network is doing well (the so-called “easy” examples), it is better to focus on the examples where the network is not doing as well (“hard” examples). To stop focusing on easy examples and focus instead on hard examples, we can put more weight on the loss from training data instances that are far from the GT, and vice versa: that is, put less weight on the loss from training data instances that are close to GT. Consider the binary CE loss of equation 9.4 one more time. The loss for the ith training instance can be rewritten as follows: 핃(i)  y(i), ¯y(i) =   −log  y(i) if GT is class 1: that is, ¯y(i) = 1 −log  1 −y(i) if GT is class 0: that is, ¯y(i) = 0 NOTE Going forward in this subsection, we drop the superscript (i) for the sake of notational simplification, although it remains implied. Now, when the GT is class 1 (that is, ¯y = 1), the entity (1 −y) measures the departure of the prediction from GT. We can multiply the loss by this to weigh down the losses from good predictions and weigh up the losses from bad predictions. In practice, we multiply by (1 −y)훾for some value of 훾(such as 훾= 2). Similarly, when the GT is class 0 (that is, ¯y = 0), the entity y measures the departure of the prediction from the GT. In this case, we multiply the loss by y훾. The overall loss then becomes 핃 ®y, ¯y = ( −(1 −y)훾log (y) if the GT is class 1: ¯y = 1 −y훾log (1 −y) if the GT is class 0: ¯y = 0 We can have a somewhat simplified expression 핃 ®y, ¯y = −(1 −yt)훾log (yt) (9.7) where yt = ( y if the GT is class 1: ¯y = 1 1 −y if the GT is class 0: ¯y = 0 Equation 9.7 is the popular expression of focal loss. Its graph at various values of 훾 is shown in figure 9.4. Note how the loss becomes more and more subdued as the probability of the GT increases toward the right until it flattens out at the bottom. NOTE Fully functional code for focal loss, executable via Jupyter Notebook, can be found at http://mng.bz/g1a8. 312 CHAPTER 9 Loss, optimization, and regularization Figure 9.4 Focal loss graph (various 휸values) Listing 9.6 PyTorch code for focal loss def focal_loss(y, y_gt, gamma): y_t = (y_gt * y) + ((1 - y_gt) * (1 - y)) yt = y if ygt is 1 yt = 1 −y if ygt is 0 loss = -1 * torch.pow((1 - y_t), gamma) * torch.log(y_t) return loss 9.1.8 Hinge loss The softmax CE loss becomes zero only under the ideal condition: the correct class has a finite score, and other classes have a score of negative infinity. Hence, that loss will continue to push the network toward improvement until the ideal is achieved (which never happens in practice). Sometimes we prefer to stop changing the network when the correct class has the maximum score, and we do not care about increasing the distance between correct and incorrect scores any further. This is where hinge loss comes in. A hinged door opens in one direction but not in the other direction. Similarly, a hinge loss function increases if a certain goodness criterion is not satisfied but becomes zero (and does not reduce any further) if the criterion is satisfied. This is akin to saying, “If you are not my friend, the distance between us can vary from small to large 9.1 Loss functions 313 (unboundedly), but I don’t distinguish between friends. All my friends are at a distance of zero from me.” MULTICLASS SUPPORT VECTOR MACHINE LOSS: HINGE LOSS FOR CLASSIFICATION Consider again our old friend, the classifier that predicts whether an image contains a cat (class 0), dog (class 1), airplane (class 2), or automobile (class 3). Our classifier emits an output vector ®y corresponding to an input image. Here, the outputs are scores: yj is the score corresponding to the jth class. (In this subsection, we have dropped the superscripts indicating the training data index to simplify notations.) Given a (training data instance, GT label) pair  ®x, c (that is, the GT class correspond- ing to the input ®x is c), the multiclass support vector machine (SVM) loss is N−1 Õ j=0, j≠c max  0, yj −yc + m (9.8) where m is a margin (usually m = 1). To understand this, consider the equation without the margin first: N−1 Õ j=0, j≠c max  0, yj −yc  In equation 9.8, we are summing over all the classes except the one that matches the GT. In other words, we are summing over only the incorrect classes. For these, we want the score yj to be smaller than the score yc for the correct class. There are two possibilities: Good output—Incorrect class score less than correct class score: yj −yc < 0 =⇒max  0, yj −yc  = 0 The contribution to the loss is zero (we do not distinguish between correct scores: all friends are at zero distance). Bad output—Incorrect class score more than correct class score: yj > yc =⇒max  0, yj −yc  = yj −yc The contribution to the loss is positive (non-friends are at a positive distance that varies with the degree of non-friendness). In practical settings, the margin is set to a positive number (usually 1) to penalize predictions where the score of the correct class is marginally greater than that of the incorrect classes. This forces the classifier to learn to predict the correct class with high confidence. Figure 9.5 shows how hinge loss differs for good and bad output predictions. One mental model to have about the multiclass SVM loss is that it is lazy. It stops changing as soon as the correct class score exceeds the incorrect scores by the margin m. The loss does not change if the correct class score goes still higher, which means it does not push the machine to improve beyond that point. This behavior is 314 CHAPTER 9 Loss, optimization, and regularization Cat Dog Airplane Automobile 0 1 0 0 GT One-hot 9.99 10 0.01 -10 Bad prediction Scores 12.7 10 0.01 -10 Good prediction Scores Figure 9.5 Hinge loss for good and bad output predictions different from the softmax CE loss, which tries to push the machine to achieve an infinite score for the correct class. 9.2 Optimization Neural network models define a loss function that estimates the badness of the network’s output. During supervised training, the output on a particular training instance input is compared to a known output (GT) for that particular training instance. The difference between the GT and the network-generated output is called loss. We can sum up the losses from individual training data instances and compute the total loss over all the training data. These losses are, of course, functions of the network parameters, ®w, ®b. We can imagine a space whose dimensionality is dim   ®w + dim  ®b  . At each point in this space, we have a value for the total training loss. Thus, we can imagine a loss surface—a surface whose height represents the loss value—defined over the high-dimensional domain of network parameters (weights and biases). Optimization is nothing but finding the lowest point on this surface. During training, we start with random values of network parameters: this is akin to starting at a random point on the surface. Then we constantly move locally downhill on the loss surface in the direction of the negative gradient. We hope this eventually takes us to the minimum or a sufficiently low point. Continuing our analogy of the loss surface as a ravine, the minimum is at sea level. This minimum provides us with the network parameter values (weights and biases) that will yield the least loss on the training data. If the training data set adequately represents the problem, the trained model will perform well on unseen data. 9.2 Optimization 315 This process of traveling toward the minimum, the iterative updating of weights and biases to have minimal loss over the training dataset, is called optimization. The basic math was introduced in section 8.4.2 (equation 8.12). Here we study many practical nuances and variants. At every iteration, we update the weights and biases, so if t denotes the iteration number, ®wt denotes the weight values at iteration t, 훿®wt denotes the weight updates at iteration t, and so on: ®wt+1 = ®wt −훿®wt ®bt+1 = ®bt −훿®bt (9.9) The basic update is along the direction of the negative gradient (see equation 8.12): 훿®wt = 휂∇®w핃  ®wt, ®bt  훿®bt = 휂∇®b핃  ®wt, ®bt  (9.10) Here, 핃  ®wt, ®bt  denotes the loss at iteration t. Ideally, we should evaluate the loss on every training data instance and average them. But that would imply that we must process every training data instance for every iteration, which is prohibitively expensive. Instead, we use sampling (see section 9.2.2): The constant 휂is called the learning rate (LR). A larger LR results in bigger steps (bigger adjustments to weights and biases per update) and vice versa. We use larger values of LR in the beginning: when the network is completely untrained, we want to take large steps toward the minimum. When we are close to the minimum, on the other hand, we want to take smaller steps, lest we overshoot it. The LR is typically a small number, like 휂= 0.01. In stochastic gradient descent (SGD; a popular approach), the LR 휂is typically held constant during an epoch (an epoch is a single pass over all the training data). Then the LR is decreased after one or more epochs. This process is called learning rate decay. So, the LR is not exactly a constant. We could have written it 휂t to indicate the temporal nature, but we chose to keep it simple because (in SGD, at least) it changes infrequently. We have to reevaluate the loss and its gradients in each iteration since their values will change in every iteration, because the weights and biases of the underlying neural network are changing. How many iterations are necessary? Typically, this is a large number. We iterate multiple times over the entire training dataset. A typical training session has multiple epochs. In this context, note that for proper convergence, it is extremely important to randomly shuffle the order of occurrence of the training data after every epoch. In the following sections, we look at some practical nuances of the process. 316 CHAPTER 9 Loss, optimization, and regularization 9.2.1 Geometrical view of optimization This topic is described in detail in section 8.4.2. You are encouraged to revisit that discussion if necessary. Overall, neural network optimization is an iterative process. Ideally, in each iteration, we compute the gradient of the loss with respect to the current parameters (weights and biases) and obtain improved values for them by moving in the direction of the negative gradient. 9.2.2 Stochastic gradient descent and minibatches How do we compute the gradient of the loss function? The loss is different for every training data instance. The sensible thing to do is to average them out. But as we mentioned earlier, that leads to a practical problem: we would have to process the entire training dataset on every iteration. If the size of the training dataset is n, an epoch is an O  n2 operation (every iteration, we have to process all of the n training data instances to compute the gradient, and an epoch has n iterations). Since n is a large number, often in the millions, O  n2 is prohibitively expensive. In SGD, we do not average over the entire training data set to produce the gradient. Instead, we average over a random sampled subset of the training data. This randomly sampled subset of training data is called a minibatch. The gradient is computed by averaging the loss over the minibatch (as opposed to the entire training dataset). This gradient is used to update the weight and bias parameters. 9.2.3 PyTorch code for SGD Now, let’s implement SGD in PyTorch. NOTE Fully functional code for SGD, executable via Jupyter Notebook, can be found at http://mng.bz/ePyG. Let’s consider the example discussed in section 6.9. Our goal is to build a model that can predict whether a Statsville resident is a man, woman, or child using height and weight as input data. For this purpose, let’s assume we have a large dataset X containing the heights and weights of various Statsville residents. X is of shape (num_samples, 2), where each row represents the (height, weight) pair of a single resident. The corresponding labels are stored in ygt, which contains num_samples elements. Each row of ygt can be 0, 1, or 2, depending on whether the resident is a man, woman, or child. Figure 9.6 shows an example distribution of X. Before training a model, we must first convert the data into a training-friendly for- mat. We subclass torch.utils.data.Dataset to do so and implement the __len__ and __getitem__ methods. The ith training data instance can be accessed by calling data_set[i]. Remember that in SGD, we feed in minibatches that contain batch_size elements in every iteration. This can be achieved by calling the __getitem__ method batch_size times. However, instead of doing this ourselves, we use PyTorch’s DataLoader, which gives us a convenient wrapper. Using DataLoader is recommended in production settings because it provides a simple API through which we can (1) create minibatches, 9.2 Optimization 317 class 0 (man) class 1 (woman) class 2 (child) -0.4 -0.6 -0.8 -1.0 -1.2 -1.4 0.4 0.6 0.8 1.0 1.2 1.4 Height in cm Weight in kg Data distribution Figure 9.6 Height and weight of various Statsville residents. Class 0 (man) is represented by the right-most cluster, class 1 (woman) by the middle cluster, and class 2 (child) by the left-most cluster. (2) speed up data-loading times via multiprocessing, and (3) randomly shuffle data in every epoch to prevent overfitting. The following code creates a custom PyTorch data set. Listing 9.7 PyTorch code to create a custom dataset from torch.utils.data import Dataset, DataLoader class StatsvilleDataset(Dataset): Subclasses torch.utils.data.Dataset def __init__(self, X, y_gt): self.X = X self.y_gt = y_gt def __len__(self): Returns the size of the data set return len(self.X) def __getitem__(self, i): Returns the ith training data element return self.X[i], self.y_gt[i] dataset = StatsvilleDataset(X, y_gt) Instantiates the data set data_loader = DataLoader(dataset, batch_size=10, Instantiates the data loader with a batch size of 10 and shuffle on shuffle=True) Our next step is to create a classifier model that can take the height and weight data (X) as input and predict the output class. Here, we create a simple neural network model that consists of two linear layers followed by a softmax layer. The output of the softmax layer has three values representing the probability of each of the three classes (man, woman, and child), respectively. Note that in the forward pass, we don’t call the softmax 318 CHAPTER 9 Loss, optimization, and regularization layer because our loss function, PyTorch’s CE loss, expects raw, un-normalized scores as input. Hence we pass the output of the second linear layer to the loss function. However, during prediction, we pass the scores to the softmax layer to get a probability vector and then take an argmax to get the predicted class. Notice that we have a function to initialize the weights of the linear layers: this is important because the starting value of the weights can often affect convergence. If the model starts too far from the minimum, it may never converge. Listing 9.8 PyTorch code to create a custom neural network model class Model(torch.nn.Module): Subclasses torch.nn.Module def __init__(self, input_size, hidden_size, output_size): super(Model, self).__init__() self.linear1 = torch.nn.Linear(input_size, hidden_size) Instantiates the linear layers and the softmax layer self.linear2 = torch.nn.Linear(hidden_size, output_size) self.softmax = torch.nn.Softmax(dim=1) def forward(self, X): Feeds forward the input through the two linear layers scores = self.linear2(self.linear1(X)) return scores def predict(self, X): Predicts the output class index scores = self.forward(X) y_pred = torch.argmax(self.softmax(scores), dim=1) return y_pred def initialize_weights(m): if isinstance(m, torch.nn.Linear): torch.nn.init.xavier_uniform_(m.weight.data) Initializes the weights to help the model converge better torch.nn.init.constant_(m.bias.data, 0) model = Model(input_size=2, hidden_size=100, output_size=3) model.apply(initialize_weights) Now that we have our data set and model, let’s define our loss function and instantiate our SGD optimizer. Listing 9.9 PyTorch code for a loss function and SGD optimizer loss_fn = torch.nn.CrossEntropyLoss() optimizer = optim.SGD(model.parameters(), lr=0.02) Instantiates the SGD optimizer with learning rate = 0.02 Now we define the training loop, which is essentially one pass over the entire dataset. We iterate over the dataset in batches of size batch_size, run the forward pass, compute the gradients, and update the weights in the direction of the negative gradient. Note that we call optimizer.zero_grad() in every iteration to prevent the accumulation of gradients from the previous steps. 9.2 Optimization 319 Listing 9.10 PyTorch code for one training loop def train_loop(epoch, data_loader, model, loss_fn, optimizer): for X_batch, y_gt_batch in data_loader: Iterates through the data set in batches scores = model(X_batch) Feeds forward the model to compute scores loss = loss_fn(scores, y_gt_batch) Computes the cross-entropy loss optimizer.zero_grad() Clears the gradients accumulated from the previous step loss.backward() Runs backpropagation and computes the gradients optimizer.step() Updates the weights With this, we are ready to train our model. The following code shows how to do so. Figure 9.7 shows the output predictions and loss at the end of every epoch. class 1 class 1 class 1 class 0 class 1 class 1 class 0 class 1 class 2 class 0 class 1 class 2 class 0 class 1 class 2 class 0 class 1 class 2 End of epoch 2, loss: 0.31 End of epoch 3, loss: 0.25 End of epoch 4, loss: 0.18 Initial loss: 0.83 End of epoch 0, loss: 0.60 End of epoch 1, loss: 0.46 Height in cm Height in cm Height in cm Height in cm Height in cm Height in cm Weight in kg Weight in kg Weight in kg Weight in kg Weight in kg Weight in kg Figure 9.7 Model predictions at the end of every epoch. Notice how the loss is reduced with every epoch. In the beginning, all training data points are wrongly classified as class 1. After the end of epoch 1, most of the training data is classified correctly, and the distribution of the classifier’s output has become close to the ground truth. The loss continues to decrease until epoch 4 (although the improvements are harder to see visually). Listing 9.11 PyTorch code to run the training loop num_epochs times num_epochs = 2 for epoch in range(num_epochs): train_loop(epoch, data_loader, model, loss_fn, optimizer) 320 CHAPTER 9 Loss, optimization, and regularization 9.2.4 Momentum For real-life loss surfaces in high dimensions, the ravine analogy is quite appropriate. A loss surface is hardly like a porcelain cup with smooth walls; it is much more like the walls of the Grand Canyon (see figure 9.8). Furthermore, the gradient estimate (usually done over a minibatch) is noisy. Consequently, gradient estimates are never aligned in one direction—they tend to go hither and thither. Nonetheless, most of them tend to have a good downhill component. The other (non-downhill component) is somewhat random (see figure 9.8). So if we average them, the downhill components reinforce each other and are strengthened, while the non-downhill components cancel each other and are weakened. Figure 9.8 Momentum. Noisy stochastic gradient estimates at different points on the loss surface (thick solid arrows) are not aligned in direction, but they all have a significant downhill component (thin solid arrows). The non-downhill components (thin dashed arrows) point in random directions. Hence, averaging tends to strengthen the downhill component and cancel out the non-downhill components. Furthermore, if there is a small flat region with downhill areas preceding and following it, the vanilla gradient-based approach will get stuck in the small flat region (since the gradient there is zero). But averaging with the past allows us to have nonzero updates that take the optimization out of the local small flat region so that it can continue downhill. A good mental picture in this context is that of a ball rolling downhill. The surface of the hill is rough, and the ball is not going directly downward. Rather, it takes a zigzag path. But as it travels, it gathers downward momentum, and its downward velocity becomes greater. Following this theory, we take the weighted average of the gradient computed at this iteration with the update used in the previous iteration: 9.2 Optimization 321 current update z}|{ 훿®wt = 훾 last update z}|{ 훿®wt−1 +휂 current gradient z }| { ∇®w핃  ®wt, ®bt  current update z}|{ 훿®bt = 훾 last update z}|{ 훿®bt−1 +휂 current gradient z }| { ∇®b핃  ®wt, ®bt  (9.11) where 훾, 휂are positive constants with values less than 1. The weights and bias parameters are updated in the usual fashion using equation 9.9. UNROLLING THE RECURSION OF THE MOMENTUM EQUATION Unraveling the recursive equation 9.11, we see 훿®wt = 휂∇®w핃  ®wt, ®bt  + 훾훿®wt−1 = 휂∇®w핃  ®wt, ®bt  + 휂훾∇®w핃  ®wt−1, ®bt−1  + 훾2훿®wt−2 = 휂∇®w핃  ®wt, ®bt  + 휂훾∇®w핃  ®wt−1, ®bt−1  + 휂훾2∇®w핃  ®wt−2, ®bt−2  + 훾3훿®wt−3 ... = 휂∇®w핃  ®wt, ®bt  + 휂훾∇®w핃  ®wt−1, ®bt−1  + · · · + 휂훾t∇®w핃  ®w0, ®b0  + 훾t+1훿®w−1 Assuming 훿®w−1 = 0, we get 훿®wt = 휂∇®w핃  ®wt, ®bt  + 휂훾∇®w핃  ®wt−1, ®bt−1  + 휂훾2∇®w핃  ®wt−2, ®bt−2  + · · · + 휂훾t∇®w핃  ®w0, ®b0  (9.12) Thus, We are taking a weighted sum of the gradients from past iterations. This is not quite a weighted average, though, as explained later. Older gradients are weighted by higher powers of 훾. Since 훾< 1, weights decrease with age (long-past iterations have less influence). The sum of the weights of the gradients, going backward from now to the beginning of time (the 0th iteration), is St = 휂  1 + 훾+ 훾2 + 훾3 · · · 훾t Now, using Taylor expansion, lim t→∞  1 + 훾+ 훾2 + 훾3 · · · 훾t−1 = 1 1 −훾 Thus the sum of the weights of the past gradients in momentum-based gradient descent is 휂 1−훾≠1. In other words, this is not quite a weighted average (where the weights should sum up to 1). This is a somewhat undesirable property and is rectified in the Adam algorithm discussed later. A similar analysis can be done for the biases. 322 CHAPTER 9 Loss, optimization, and regularization 9.2.5 Geometric view: Constant loss contours, gradient descent, and momentum Consider a network with a tiny two-element weight vector ®w =  w0 w1  and no bias. Further, suppose that the loss function is 핃= ∥®w∥2 =w2 0 +w2 1. The constant loss contours are concentric circles with the origin as the center. The radius of the circle indicates the loss magnitude. If we move along the circle’s circumference, the loss does not change. The loss changes maximally along the orthogonal direction to that: the radius of the circle. This intuitive observation is confirmed by evaluating the gradient ∇®w핃= 2  w0 w1  Thus the gradient is along the radius, and the negative gradient direction is radially inward. So the loss decreases most rapidly if we move radially inward. If we move orthogonal to the radius (that is, along the circumference), the loss remains unchanged; of course, we are moving along the constant loss contour. Optimization then takes us from outer, larger-radius circles to inner, smaller-radius circles. The minimum is at the origin; ideally, optimization should stop once we reach the origin. Figure 9.9 shows optimization for a simple loss function 핃= ∥®w∥2 =w2 0 +w2 1. We start at an arbitrary pair of weight values and repeatedly update them via equation 9.9. For figure 9.9a, we use update without momentum (equation 9.10). For figure 9.9b, we use update with momentum (equation 9.11). When the constant loss contours are concentric circles with the origin as the center, the loss surface is a cone with its apex on the origin. The cross sections are circles on planes parallel to the w0, w1 plane. Optimization takes us down the inner walls of the cone through smaller and smaller circular cross-sections as we approach the minimum. The global minimum is at the origin and corresponds to zero loss. The zero-loss contour is a circle with a zero radius, which is effectively a single point: the origin. In both cases, progress slows down (step sizes become smaller) as we approach the minimum. This is because the magnitude of the gradient becomes smaller and smaller as we get closer to the minimum (imagine a bowl: it becomes flatter as we get closer to the bottom). However, this effect is countered to a certain extent if we have momentum. So, we can see that we need fewer steps to reach a circle with a smaller radius when we have momentum. 9.2.6 Nesterov accelerated gradients One problem with momentum-based gradient descent is that it may overshoot the minimum. This can be seen in figure 9.10a where the loss decreases through a series of updates and then, when we are close to the minimum, an update (shown with a dotted arrow) overshoots the minimum and increases the loss (shown with a dotted circle). 9.2 Optimization 323 Start loss: 50000.0 End loss: 39.2 Num iters: 7 (a) A trajectory through constant loss contours for gradient descent without momentum Start loss: 50000.0 End loss: 30.4 Num iters: 5 (b) A trajectory through constant loss contours for gradient descent with momentum Figure 9.9 Constant loss contours and optimization trajectory for the loss function 핃= ∥®w∥2 =w2 0 +w2 1. The loss surface is a cone with its apex on the origin and its base a circle in a plane parallel to the w0, w1 plane. Optimization takes us down the cone toward smaller and smaller cross-sections as we approach the minimum at the origin. Updates are shown with arrows. Note how the momentum version arrives at a smaller circle in fewer steps. 324 CHAPTER 9 Loss, optimization, and regularization Start loss: 50000.0 End loss: 2784.8 Num iters: 3 Gradient descent overshoots minimum because of momentum. (a) Momentum gradient descent overshooting the minimum. Loss decreases for a while and then increases (dotted circle and arrow). Start loss: 50000.0 End loss: 16.7 Num iters: 6 Nesterov gradient descent does not overshoot minimum. (b) Nesterov reduces the step size when we are about to overshoot the minimum. Figure 9.10 Figure (a) shows momentum-based gradient descent overshooting the minimum. Over- shooting the update is shown with a dotted arrow. A circle with a radius larger than the last step is shown with a dotted outline. Nesterov does better by taking smaller steps when overshooting is imminent. 9.2 Optimization 325 This is the phenomenon that Nesterov’s accelerated gradient-based optimization tries to tackle. In Nesterov’s technique, we do the following: 1 Estimate where this update will take us (that is, the destination point) by adding the previous step’s update to the current point. 2 Compute the gradient at the estimated destination point. This is where the ap- proach differs from the vanilla momentum-based approach, which takes the gradi- ent at the current point. 3 Take a weighted average of the gradient (at the estimated destination point) with the previous step’s update. That is the update for the current step. Mathematically speaking, current update z}|{ 훿®wt = 훾 last update z}|{ 훿®wt−1 +휂 gradient at estimated destination z }| { ∇®w핃 ©­­­ « estimated destination z }| { ®wt −훾훿®wt−1 , estimated destination z }| { ®bt −훾훿®bt−1 ª®®® ¬ current update z}|{ 훿®bt = 훾 last update z}|{ 훿®bt−1 +휂 gradient at estimated destination z }| { ∇®b핃 ©­­­ « estimated destination z }| { ®wt −훾훿®wt−1 , estimated destination z }| { ®bt −훾훿®bt−1 ª®®® ¬ (9.13) where 훾, 휂are constants with values less than 1. The weights and biases are updated in the usual fashion using equation 9.9. Why does this help? Well, consider the following: When we are somewhat far away from the minimum (no possibility of overshooting), the gradient at the estimated destination is more or less the same as the gradient at the current point, so we progress toward the minimum in a fashion similar to momentum-based gradient descent. But when we are close to the minimum and the current update may potentially take us past it (see the dotted arrow in figure 9.10a), the gradient at the estimated destination will lie on the other side of the minimum. As before, imagine this loss surface as a cone with its apex at the origin and base above the apex. We have traveled down one of the cone’s side walls and just started climbing back up. Thus the gradient at the estimated destination is in the opposite direction from the previous step. When we take their weighted average, they will cancel each other in some dimensions, resulting in a smaller magnitude. The resulting smaller step will mitigate the overshooting phenomenon. NOTE Fully functional code for momentum and Nesterov accelerated gradients, executable via Jupyter notebook, can be found at http://mng.bz/p9KR. 326 CHAPTER 9 Loss, optimization, and regularization 9.2.7 AdaGrad The momentum-based optimization approach (equation 9.11) and the Nesterov ap- proach (equation 9.13) both suffer from a serious drawback: they treat all dimensions of the parameter vectors ®w, ®b equally. But the loss surface is not symmetrical in all dimensions. The slope can be high along some dimensions and low along others. We cannot control everything with a single LR for all the dimensions. If we set the LR high, the high-slope dimensions will exhibit too much variance. If we set it low, the low-slope dimensions will barely progress toward the minimum. What we need is per-parameter LR. Then each LR will adapt to the slope of its particular dimension. This is what AdaGrad tries to achieve. Dimensions with historically larger gradients have smaller LRs, while dimensions with historically smaller gradients have larger LRs. How do we keep track of the historic magnitudes of gradients? To do this, AdaGrad maintains a state vector in which it accumulates the sum of the squared partial derivatives for each dimension of the gradient vector seen so far during training: ®st = per-dimension gradient squared z }| { ∇®w핃  ®wt, ®bt  ◦∇®w핃  ®wt, ®bt  +®st−1 =  | 휕핃 휕w0 |2 | 휕핃 휕w1 |2 ... | 휕핃 휕b0 |2 ...  +®st−1 where ◦denotes the Hadamard operator (elementwise multiplication of the two vectors). We can express the previous equation a bit more succinctly as ®st = |∇®w핃  ®wt, ®bt  |2 +®st−1 Unrolling the recursion, we get ®st = |∇®w핃  ®wt, ®bt  |2 +®st−1 = |∇®w핃  ®wt, ®bt  |2 + |∇®w핃  ®wt−1, ®bt−1  |2 +®st−2 ... = |∇®w핃  ®wt, ®bt  |2 + |∇®w핃  ®wt−1, ®bt−1  |2 · · · + |∇®w핃  ®w0, ®b0  |2 +®s−1 assuming ®s−1 = 0, ®st is a vector that holds the cumulative sum over all training iterations of the squared slope for each dimension. For the dimensions with historically high slopes, the corresponding element of ®st is large, and vice versa. The overall update 9.2 Optimization 327 vector looks like this: ®st = |∇®w핃  ®wt, ®bt  |2 +®st−1 훿®wt = 휂 p ®st + 휖 ◦∇®w핃  ®wt, ®bt  (9.14) Here 휖is a very small constant added to prevent division by zero. Then we use equa- tion 9.9 to update the weights as usual. A parallel set of equations exist for the bias. Using AdaGrad, in the loss gradient, the dimensions that have seen big slopes in earlier iterations are given less importance during the update. We emphasize the dimensions that have not seen much progress in the past. This is a bit like saying, “I will pay less attention to a person who speaks all the time, and I will pay more attention to someone who speaks infrequently.” 9.2.8 Root-mean-squared propagation A significant drawback of the AdaGrad algorithm is that the magnitude of the vector ®st keeps increasing as iteration progresses. This causes the LR for all dimensions to become smaller and smaller. Eventually, when the number of iterations is high, the LR becomes close to zero, and the updates do hardly anything; progress toward the minimum slows to a virtual halt. Thus AdaGrad is an impractical algorithm to use in real life, although the idea of per-component LR is good. Root mean squared propagation (RMSProp) addresses this drawback without sacri- ficing the dimension-adaptive nature of AdaGrad. Here again there is a state vector, but its equation is ®st = (1 −훾) |∇®w핃  ®wt, ®bt  |2 + 훾®st−1 Compare this with the state vector equation in AdaGrad: ®st = |∇®w핃  ®wt, ®bt  |2 +®st−1 They are almost the same, but the terms are weighted by (1 −훾) and 훾, where 0 < 훾< 1 is a constant. This particular pair of weights in a recursive equation has a very interesting effect. To see it, we have to unroll the recursion: ®st = (1 −훾) |∇®w핃  ®wt, ®bt  |2 + 훾®st−1 = (1 −훾) |∇®w핃  ®wt, ®bt  |2 + (1 −훾) 훾|∇®w핃  ®wt−1, ®bt−1  |2 + 훾2 ®st−2 = (1 −훾) |∇®w핃  ®wt, ®bt  |2 + (1 −훾) 훾|∇®w핃  ®wt−1, ®bt−1  |2 + (1 −훾) 훾2 |∇®w핃  ®wt−2, ®bt−2  |2 + 훾3 ®st−3 ... = (1 −훾) |∇®w핃  ®wt, ®bt  |2 + (1 −훾) 훾|∇®w핃  ®wt−1, ®bt−1  |2 · · · + (1 −훾) 훾t |∇®w핃  ®w0, ®b0  |2 328 CHAPTER 9 Loss, optimization, and regularization Thus ®st is a weighted sum of the past term-wise squared gradient magnitude vectors. Going back from now to the beginning of time (the 0th iteration), the weights are (1 −훾), (1 −훾) 훾, (1 −훾) 훾2, · · · , (1 −훾) 훾t. If we add these weights, we get (1 −훾)  1 + 훾+ 훾2 + 훾3 · · · 훾t As the number of iterations becomes high (t →∞), this becomes (1 −훾) lim t→∞  1 + 훾+ 훾2 + 훾3 · · · 훾t−1 = 1 1 −훾(1 −훾) = 1 where Taylor expansion has been used to evaluate the term in parentheses. For a large number of iterations, the sum of the weights approaches 1. This implies that after many iterations, the RMSProp state vector effectively takes the weighted average of the past term-wise squared gradient magnitude vectors. With more iterations, the weights are redistributed and older terms become de-emphasized, but the overall magnitude does not increase. This eliminates the vanishing LR problem from AdaGrad. RMSProp continues de-emphasizing the dimensions with high cumulative partial derivatives with lower LRs, but it does so without making the LR vanishingly small. The overall RMSProp update equations are ®st = (1 −훾) |∇®w핃  ®wt, ®bt  |2 + 훾®st−1 훿®wt = 휂 p ®st + 휖 ◦∇®w핃  ®wt, ®bt  (9.15) There is a parallel set of equations for the bias. The weights and bias parameters are updated in the usual fashion using equation 9.9. 9.2.9 Adam optimizer Momentum-based gradient descent amplifies the downhill component more and more as iterations progress. On the other hand, RMSProp reduces the LR for dimensions that are seeing large gradients and vice versa to balance the progress rate along all dimensions. Both of these are desirable properties. We want an optimization algorithm that combines them, and that algorithm is Adam. It is increasingly becoming the optimizer of choice for most deep learning researchers. The Adam optimization algorithm maintains two state vectors: ®vt = (1 −훽1) ∇®w핃  ®wt, ®bt  + 훽1 훿®wt−1 (9.16) ®st = (1 −훽2) |∇®w핃  ®wt, ®bt  |2 + 훽2 ®st−1 (9.17) where 0 < 훽1 < 1 and 0 < 훽2 < 1 are constants. Note the following: Equation 9.16 is essentially the momentum equation of equation 9.11 with one significant difference. We have changed the term weights to 훽1 and (1 −훽1). 9.2 Optimization 329 As we’ve seen, with t →∞, this results in the state vector being a weighted average of all the past gradients. This is an improvement over the original momentum scheme. The second state vector is basically the one from the RMSProp equation 9.15. Using these two state vectors, Adam creates the update vector as follows: 훿®wt = 휂®vt p ®st + 휖 ◦∇®w핃  ®wt, ®bt  (9.18) The ®vt in the numerator pulls in the benefits of the momentum approach (with the enhancement of averaging). Otherwise, the equation is almost identical to the RMSProp; those benefits are also pulled in. Finally, the weights and bias parameters are updated in the usual fashion using equation 9.9. BIAS CORRECTION The sum of weights of past values in the state vectors ®vt, ®st will approach ∞only at large values of t. To improve the approximation at smaller values of t, Adam introduces a bias correction: ˆvt = ®vt  1 −훽t 1  (9.19) ˆst = ®st  1 −훽t 2  (9.20) Instead of ®vt and ®st, we use the bias-corrected entities ˆvt and ˆst in equation 9.18. Listing 9.12 PyTorch code for various optimizers from torch import optim sgd_optimizer = optim.SGD([params], lr=0.01) Sets the learning rate to 0.01 sgd_momentum_optimizer = optim.SGD([params], lr=0.01, momentum=0.9) Sets the momentum to 0.9 sgd_nesterov_momentum_optimizer = optim.SGD([params], lr=0.01, Sets the Nesterov flag to True momentum=0.9, nesterov=True) adagrad_optimizer = optim.Adagrad([params], lr=0.001) rms_prop_optimizer = RMSprop([params], lr=1e-2, Sets the smoothing constant to 0.99 (휸in 9.15) alpha=0.99) adam_optimizer = optim.Adam([params], lr=0.001, betas=(0.9, 0.999)) 330 CHAPTER 9 Loss, optimization, and regularization 9.3 Regularization Suppose we are teaching a baby to recognize cars. We show them red cars, blue cars, black cars, large cars, small cars, medium cars, cars with round tops, cars with rectangular tops, and so on. Soon the baby’s brain realizes that there are too many varieties to remember them all by rote. So the brain starts forming abstractions: mysterious common features that occur together are stored in the baby’s brain with the label car. The brain has learned to classify an abstract entity called a car. Even though it fails to remember every car it has seen, it can recognize cars. We can say it has developed experience. And so it is with neural networks. We do not want our network to remember every training instance. Rather, we want the network to form abstractions that will enable it to recognize an object during inferencing even though the exact likeness of the object instance encountered during inferencing was never seen during training. Overfitting and underfitting If the network has too much expressive power (too many perceptrons or, equivalently, too many weights) relative to the number of training instances, the network can and often will rote-remember the training instances. This phenomenon is called overfitting. Overfitting causes the network to perform very well over the training data but badly during testing or real-life inferencing. Stated another way, the network has adjusted itself to every nook, bend, and kink of the training data and thereby performs great on the training data—to the detriment of test data performance. This is illustrated in figure 9.11. Regularization refers to a bag of tricks that, in general, try to prevent overfitting. This is the topic of this section. There is another phenomenon called underfitting, where the network simply does not have enough expressive power to model the training data. The symptom of underfitting is that the network performs badly on both training and testing data. If we see this, we should try a more complex network with more perceptrons. 9.3.1 Minimum descriptor length: An Occam’s razor view of optimization Is the set of weights and biases that minimizes a particular loss function unique? Let’s examine a single perceptron (equation 7.3) with the output 휙  ®wT ®x + b. Let’s say 휙 is the Heaviside step function (see section 7.3.1). Let ®w∗, b∗be the weights and biases minimizing the loss function. It is easy to see that the perceptron output remains the same if we scale the weights, like 훼®w∗for all positive real values of 훼. Thus the weight vector 7 ®w∗will also minimize the loss function. This is true in general for arbitrary neural networks (composed of many perceptrons): the set of weights and biases minimizing a loss function is non-unique. How does the neural network choose one? Which of them is correct? We can use the principle of Occam’s razor to answer that. Occam’s razor is a philosophical principle. Its literal translation from Latin states, “Entities should not be multiplied beyond necessity.” This is roughly taken 9.3 Regularization 331 Figure 9.11 Overfitting: data points for a binary classifier. Points belonging to different classes are visually demarcated as squares and circles. Filled squares/circles indicate training data, and unfilled squares/circles indicate test data. There are some anomalous training data instances (filled circles in the square zone). The estimated decision boundary (solid line) has become wriggly to accommodate them, which is causing many test points (unfilled squares/circles) to be misclassified. The wiggly solid curve is an example of an overfitted decision boundary. If we had chosen a “simpler” decision boundary (dashed straight line), the two anomalous training points would have been misclassified, but the machine would have performed much better in tests. to mean among adequate explanations, the simplest one is the best. In machine learning, this principle is typically interpreted as follows: Among the set of candidate neural network parameter values (weights and biases) that minimize the loss, the “simplest” one should be chosen. The general idea is as follows. Suppose we are trying to minimize 핃(휃) (here we have used 휃to denote ®w and ®b together). We also want the solution to be as simple 332 CHAPTER 9 Loss, optimization, and regularization as possible. To achieve that, we add a penalty for departing from “simplicity” to the original loss term. Thus we minimize 핃(휃) + 휆R (휃) Here, The expression R (휃) indicates a measure for un-simplicity. It is sometimes called the regularization penalty. Adding a regularization penalty to the loss incentivizes the network to try to minimize un-simplicity R (휃) (or, equivalently, maximize simplicity) while trying to minimize the original loss term 핃(휃). 휆is a hyperparameter. Its value should be carefully chosen via trial and error. If 휆is too low, this is akin to no regularization, and the network becomes prone to overfitting. If 휆is too high, the regularization penalty will dominate, and the network will not adequately minimize the actual loss term. There are two popular ways of estimating R (휃), outlined in sections 9.3.2 and 9.3.3, respectively. Both try to minimize some norm (length) of the parameter vector (which is basically a network descriptor). This is why regularization can be viewed as minimizing the descriptor length. 9.3.2 L2 regularization In L2 regularization, we posit that shorter-length vectors are simpler. In other words, simplicity is inversely proportional to the square of the L2 norm (aka Euclidean norm). Thus, R (휃) =  ∥®w∥2 + ∥®b∥2 Overall, we minimize 핃  ®w, ®b  = n−1 Õ i=0 핃(i)  ®y(i), ¯y(i) + 휆  ∥®w∥2 + ∥®b∥2 (9.21) Compare this with equation 9.2. L2 regularization is by far the most popular form of regularization. From this point on, we often use 핃  ®w, ®b  to mean the L2-regularized version (that is, equation 9.21). The hyperparameter 휆is often called weight decay in PyTorch. Weight decay is usually set to a small number so that the second term of equation 9.21 (the norm of the weight vector) does not drown the actual loss term. The following code shows how to instantiate an optimizer with regularization enabled. Listing 9.13 PyTorch code to enable L2 regularization from torch import optim optimizer = optim.SGD([params], lr=0.2, weight_decay=0.01) Sets the weight decay to 0.01 9.3 Regularization 333 9.3.3 L1 regularization L1 regularization is similar in principle to L2 regularization, but it defines simplicity as the sum of the absolute values of the weights and biases: R (휃) =  | ®w| + |®b|  Thus, here we minimize 핃  ®w, ®b  = n−1 Õ i=0 핃(i)  ®y(i), ¯y(i) + 휆  | ®w| + |®b|  (9.22) 9.3.4 Sparsity: L1 vs. L2 regularization L1 regularization tends to create sparse models where many of the weights are 0. In comparison, L2 regularization tends to create models with low (but nonzero) weights. To understand this, consider figure 9.12, which plots the loss function and its deriva- tive for both L1 and L2 regularization. Let w be a single element of the weight vector ®w. (a) L1 regularization (b) L2 regularization Figure 9.12 L1 and L2 regularization 334 CHAPTER 9 Loss, optimization, and regularization In L1 regularization, 핃(w) = |w| 휕핃(w) 휕w =   −1 if w < 0 0 if w = 0 1 if w > 0 Since the gradient is constant for all values of w, L1 regularization pushes the weight toward 0 with the same step size at all values of w. In particular, the step toward 0 does not reduce in magnitude when close to 0. In L2 regularization, 핃(w) =w2 휕핃(w) 휕w = 2w Here, the gradient keeps decreasing in magnitude as w approaches 0. Hence, w comes closer to 0 but may never reach 0 since the updates take smaller and smaller steps as w approaches 0. Therefore, L2 regularization produces more dense weight vectors than L1 regularization, which produces sparse weight vectors with many 0s. 9.3.5 Bayes’ theorem and the stochastic view of optimization In sections 6.6.2 and 6.6.3, we discussed maximum likelihood estimation (MLE) and maximum a posteriori (MAP) in the context of unsupervised learning (you are en- couraged to revisit those sections if necessary). Here, we study them in the context of supervised learning. In the supervised optimization process, we have samples of the input and known output pairs (in the form of training data) ®x(i), ¯y(i) . Of course, we can do the forward pass and generate the network output at each training data point ®y(i) = f  ®x(i), 휃  where 휃is the network’s parameter set (representing weights and biases together). Suppose we view the sample training data generation as a stochastic process. We can model the probability of a training instance T (i) = ¯y(i), ®x(i) given the current network parameters 휃as p  T (i) 휃  ∝e−∥¯y(i) −f  ®x(i) ,휃  ∥2 = e−∥¯y(i) −®y(i) ∥2 This makes intuitive sense. At an optimal value of 휃, the network output f  ®x(i), 휃  would match the GT ¯y(i). We want our model distribution to have the highest probability density at the location where the network output matches the GT. The probability density should fall off with distance from that location. A little thought reveals that this Gaussian-like formulation, which leads to a regression- loss-like numerator, is not the only one possible. We can use any loss function in the numerator since all of them have the property of being minimum when the network 9.3 Regularization 335 output matches the GT and gradually increase as the mismatch grows. In general, p  T (i) 휃  ∝e−핃(i)  ¯y(i) , f  ®x(i) ,휃  = e−핃(i)  ¯y(i) ,®y(i)  Assuming the training instances are mutually independent, the probability of the entire training dataset occurring jointly is the product of individual instance probabilities. If we denote the entire training dataset as T, T = nD ¯y(i), ®x(i)Eo Then p (T |휃) = N Ö i=0 p  T (i) 휃  ∝ N Ö i=0 e−핃(i)  ¯y(i) ,®y(i)  = e−ÍN i=0 핃(i)  ¯y(i) ,®y(i)  = e−핃(휃) At this point, we can take one of two possible approaches described in the next two subsections. MLE-BASED OPTIMIZATION In this approach, we choose the optimal value for the parameter set 휃by maximizing the likelihood p (T |휃) ∝e−핃(휃) This is equivalent to saying we will choose the optimal parameters 휃such that the probability of occurrence of the training data is maximized. Thus the optimal parameter set 휃∗is yielded by 휃∗= argmax 휃 p (T |휃) = argmax 휃 e−핃(휃) Obviously, the optimal 휃that maximizes the likelihood is the one that minimizes 핃(휃). So, the maximum likelihood formulation is nothing but minimizing the total mismatch between predicted and GT output over the entire training dataset. MAP OPTIMIZATION By Bayes’ theorem (equation 6.1), p (휃|T) = p (T |휃) p (휃) p (T) To estimate the optimal 휃, we can also maximize the posterior probability on the left side of this equation. This is equivalent to saying we will choose the optimal parameters 휃such that 휃has the maximal conditional probability given the training dataset. Thus the optimal value for the parameter set 휃is yielded by 휃∗= argmax 휃 p (휃|T) = argmax 휃 p (T |휃) p (휃) p (T) where the last equality is derived via Bayes’ theorem. 336 CHAPTER 9 Loss, optimization, and regularization Observing the previous equation, we see that the denominator p (T) in the rightmost term does not involve 휃and hence can be dropped from the optimization. So, 휃∗= argmax 휃 p (휃|T) = argmax 휃 p (T |휃) p (휃) How do we model the a priori probability p (휃)? We can use Occam’s razor and say that we assign a higher a priori probability to smaller parameter values. Thus, we can say p (휃) ∝e−휆R(휃) Then the overall posterior probability maximization becomes 휃∗= argmax 휃 p (휃|T) = argmax 휃 p (T |휃) p (휃) = e−(핃(휃)+휆R(휃)) NOTE Maximizing this posterior probability is equivalent to minimizing the regularized loss. Maximizing the likelihood is equivalent to minimizing the un- regularized loss. 9.3.6 Dropout In the introduction for section 9.3, we saw that too much expressive power (too many perceptron nodes) sometimes prevents the machine from developing general abstractions (aka experience). Instead, the machine may remember the training data instances (see figure 9.11). This phenomenon is called overfitting. We have already seen that one way to mitigate this problem is to add a regularization penalty—such as adding the L2 norm of the weights—to the loss to discourage the network from learning large weight values. Dropout is another method of regularization. Here, somewhat crazily, we turn off random perceptrons in the network (set their value to 0) during training. To be precise, we attach a probability pl i with the ith node (perceptron) in layer l. In any training iteration, the node (perceptron) is off with a probability of (1 −p). Typically, dropout is only enabled during training and is turned off during inferencing. What good does it do? Well, Dropout prevents the network from relying too much on a small number of nodes. Instead, the network is forced to use all the nodes. Equivalently, dropout encourages the training process to spread the weights to multiple nodes instead of putting much weight on a few nodes. This makes the effect somewhat similar to L2 regularization. Dropout mitigates co-adaptation: a behavior whereby a group of nodes in the network behave in a highly correlated fashion, emitting similar outputs most of the time. This means the network could retain only one of them with no significant loss of accuracy. DROPOUT SIMULATES AN ENSEMBLE OF SUBNETWORKS Consider a small three-node intermediate layer of some neural network with dropout. The kth input to this layer can turn on with probability pk. This means the probability of that input node turning off is (1 −pk). We can express this with a binary stochastic variable 훿k for k = 0 or k = 1 or k = 2. This variable 훿k takes one of two possible values: 9.3 Regularization 337 0 or 1. The probability of it taking the value 1 is pk. In other words, p (훿k = 1) = pk, and p (훿k = 0) = 1 −pk. The output of this small three-node layer with dropout can be expressed as al = 2 Õ k=0 훿kwkal−1 k We have three variables 훿0, 훿1, and 훿2, each of which can take two values. Altogether we have 23 = 8 possible combinations. Each combination leads to a subnode shown in figure 9.13. Each of these combinations has a probability of occurrence Pi, also shown in the figure. These observations lead to a very important insight: The expected value of the output—that is, 피(al)—is the same as the expected value of the output if we deployed the subnetworks in figure 9.13 randomly, with probabilities Pi. Why does this matter? Well, given a problem, none of us know the right number of perceptrons for the network to deploy. One thing to do under these circumstances is to deploy networks of various strengths randomly and take an average of their outputs. We have just established that dropout (turning inputs off randomly) achieves the same thing. But deploying a network where inputs get turned on or off randomly is much simpler than deploying a large number of subnetworks. All we have to do is deploy a dropout layer. PYTORCH CODE FOR DROPOUT In section 9.2.3, we created a simple two-layer neural network classifier to predict whether a Statsville resident is a man, woman, or child based on height and weight data. In this section, we show the same model with dropout layers added. Note that dropout should be enabled only during training, not during inferencing. To do this in PyTorch, you can call model.eval() before running inferencing. This way, your training and inferencing code remains the same, but PyTorch knows under the hood when to include the dropout layers and when not to. Listing 9.14 Dropout class ModelWithDropout(torch.nn.Module): def __init__(self, input_size, hidden_size, output_size): super(Model, self).__init__() self.net = torch.nn.Sequential( torch.nn.Linear(input_size, hidden_size), torch.nn.Dropout(p=0.2), Instantiates a dropout layer with a probability of dropout = 0.2 torch.nn.Linear(hidden_size, output_size), torch.nn.Dropout(p=0.2) ) def forward(self, X): return self.net(X) 338 CHAPTER 9 Loss, optimization, and regularization (a) Subnetwork 0: probability P0 = (1 −p0) (1 −p1) (1 −p2) (b) Subnetwork 1: probability P1 = (1 −p0) (1 −p1) p2 (c) Subnetwork 2: probability P2 = (1 −p0 ) p1 (1 −p2) (d) Subnetwork 3: probability P3 = (1 −p0 ) p1 p2 Figure 9.13 Dropout simulates subnetworks: illustrated with a three-node intermediate layer of a neural network. The probability of input node a(l−1) k being on is p (휹k = 1) = pk. 9.3 Regularization 339 (e) Subnetwork 4: probability P4 = p0 (1 −p1) (1 −p2) (f) Subnetwork 5: probability P5 = p0 (1 −p1) p2 (g) Subnetwork 6: probability P6 = p0 p1 (1 −p2) (h) Subnetwork 7: probability P7 = p0 p1 p2 340 CHAPTER 9 Loss, optimization, and regularization Summary Training is the process by which a neural network identifies the optimal values for its parameters (weights and biases of the individual perceptrons). Training progresses iteratively: in each iteration, we estimate the loss and run an optimization step that updates the parameter values so as to decrease the loss. After doing this many times, we hope to arrive at optimal parameter values. In a supervised neural network, loss quantifies the mismatch between the desired output and the network output over sampled training data instances. The desired output (ground truth) is often estimated via manual effort. Training the neural network essentially identifies the weights and biases of the neural network that minimize the loss. The discrepancy between the ground truth and network output can be expressed in many different ways. Each corresponds to a different loss function. Denoting the ground truth probabilities of all the possible classes given the ith training input ®x(i) as a vector ¯y(i) and the network output on the same as y(i), – Regression loss takes the L2 norm of the vector difference between the network output and ground-truth vectors and is mathematically expressed as ∥®y(i) − ¯y(i) ∥2 = ÍN−1 j=0  y(i) j −¯y(i) j 2 . – If the neural network is a classifier, it typically outputs a vector of class probabil- ities. This means ®y(i) = h p0  ®x(i) p1  ®x(i) pj  ®x(i) · · · pN−1  ®x(i)i where pj  ®x(i) is the network estimated probability of the input belonging to class j, and N is the number of possible classes. In general, given an input, a neural network computes per-class scores: the class with the highest score is the pre- dicted class. The scores are unbounded numbers and can be arbitrarily large or small (even negative). The softmax operation converts them into probabilities. If ®s = h s0 s1 s2 · · · sN−1 i denotes the score vector, the corresponding soft- max vector is ®y(i) = h es0 S es1 S es2 S · · · esN−1 S i where S = ÍN−1 k=0 esk. The softmax output vector consists of probabilities, meaning they are numbers between 0 and 1 and they sum to 1. – Given the probability of each class, classifiers can employ the cross-entropy loss, which can be expressed as −ÍN−1 j=0 ¯y(i) j log  y(i) j  . Note that since the ground- truth vector is one-hot, only a single term in this expression survives: the one corresponding to the desired class, denoted j∗. The loss becomes the logarithm of the corresponding network output, −log  y(i) j∗  , and is zero if y(i) j∗= 1. This agrees with our expectation: if the network is predicting the GT class with probability 1, there is no loss. – Since during training, softmax is almost always followed by cross-entropy loss, PyTorch has a combined operator called softmax cross-entropy loss. It is preferred over doing these operations individually because it has better numerical properties. Summary 341 – Focal loss tries to tackle the data-imbalance problem by putting more weight on the “hard” examples with higher loss. – Hinge loss is another popular loss that becomes zero when the correct class is predicted with the maximum score. Once that criterion is achieved, it no longer tries to improve the relative values of scores. Total loss can be obtained by adding the losses from all the individual training data instances. However, this requires us to process all the training data points in every iteration, which is prohibitively expensive. Hence, we sample a subset of training data points to create a minibatch, estimate losses for each input data instance in the minibatch, and add them to obtain an estimate for the total loss. This process is known as stochastic gradient descent (SGD). Optimization is the process of updating the neural network parameters (weights and biases of perceptrons) so as to reduce the loss. We can plot the loss against weights and bias values and obtain a hypersurface defined over the domain of weights and bias values. Our ultimate goal is to reach the bottom (minimum point) of this loss hypersurface. Optimization is geometrically equivalent to moving down the hypersurface to reduce the loss. The steepest descent (toward the nearest minimum) happens along a negative gradient. We can combine several other criteria with the gradient to improve the conver- gence to the minimum loss value. Each of them results in a different optimization technique: – Due to noisy estimations, the local gradient may not always point toward the minimum, but it will have a strong component toward that minimum along with other noisy components. Instead of blindly following the current gradient, we can follow the direction corresponding to a weighted average of the current gradient estimate and the gradient estimate from the previous iteration. This is a recursive process, which effectively means the direction followed at any iteration is a weighted average of the current and all gradient estimates from the beginning of training. Recent gradients are weighted higher, and older gradients are weighed lower. All these gradients have strong components toward the minimum that reinforce each other, while the noisy components point in random directions and tend to cancel each other. Thus the overall weighted sum is a more reliable move toward the minimum. This is called momentum-based gradient descent. – A drawback of momentum is that it can result in overshooting the minimum. Nesterov accelerated gradients correct this drawback by calculating gradients via one-step lookahead. If the current update takes us to the other side of the minimum, the gradient will have an opposite direction there. We take a weighted average of the update suggested by momentum-based gradient descent and the gradient at the estimated destination point. If we are far from the minimum, it will be roughly equivalent to momentum-based gradient descent. But if we are about to overshoot the minimum, there will be cancelation, and the update will 342 CHAPTER 9 Loss, optimization, and regularization be smaller than that of the pure momentum case. Thus we reduce the chances of overshooting the minimum. – AdaGrad is an optimization technique that imparts additional weight to infre- quently changing axes of the loss hypersurface. The Adam optimizer combines many advantages of other optimizers and is often the modern optimizer of choice. The principle of Occam’s razor essentially says that among adequate explanations, the simplest one should be preferred. In machine learning, this leads to regular- ization. There are typically many solutions to the loss-minimization problem. We want to choose the simplest one. Accordingly, we add a penalty for departing from simplicity (regularization loss) to whatever other losses we have. This incentivizes the system to go for the simplest solution. Regularization loss is often the total length of the parameter vector; thus, regularization pushes us toward solutions with smaller absolute values for parameters. Minimizing the loss function without the regularization term is equivalent to the Bayesian maximum likelihood estimation (MLE) technique. On the other hand, minimizing loss with the regularized term is equivalent to maximum a posteriori (MAP) estimation. Overfitting is a phenomenon where the neural network has learned all the nuances of the training data. Since anomalous nuances in training data points are often caused by noise, this leads to worse performance during inferencing. Overfitting is symptomized by great accuracy (low loss) on training data but bad accuracy on test data. It often happens because the network has more expressive power than neces- sary for the problem and is trying to memorize all nooks and bends of the training data. Regularization mitigates the problem. Another trick is dropout, where we deliberately turn off a random subset of neurons during training iterations. This forces all the neurons to learn to do the job with a reduced set of neurons. 10 Convolutions in neural networks This chapter covers The graphical and algebraic view of neural networks Two-dimensional and three-dimensional convolution with custom weights Adding convolution layers to a neural network Image analysis typically involves identifying local patterns. For instance, to do face recognition, we need to analyze local patterns of neighboring pixels corresponding to eyes, noses, and ears. The subject of the photograph may be standing on a beach in front of the ocean, but the big picture involving sand and water is irrelevant. Convolution is a specialized operation that examines local patterns in an input signal. These operators are typically used to analyze images: that is, the input is a 2D array of pixels. To illustrate this, we examine a few examples of special-purpose convolution operations that detect the edges, corners, and average illumination in a small neighborhood of pixels from an image. Once we have detected such local properties, we can combine them and recognize higher-level patterns like ears, noses, and eyes. We can combine those in turn to detect still higher-level struc- tures like faces. The system naturally lends itself to multilayer convolutional neural 343 344 CHAPTER 10 Convolutions in neural networks networks—the lowest layers(closest to the input) detect edges and corners, and the next layers detect ears, eyes, noses, and so forth. In section 8.3, we discussed the linear neural network layer (aka fully connected layer). There, every output is connected to all inputs. This means an output is derived by taking a weighted linear combination of all input values. In other words, the output is derived from a global view of the input. Convolution layers are different. These are characterized by: Local connections—Only a small subset of neighboring input values are connected to one output value. Thus, each output is a weighted linear combination of only a small set of adjacent input values. As a consequence, only local patterns in the input are captured. Shared weights—The same weights are slid over the entire input. Consequently, – The number of weights is drastically reduced. Since convolution is typically used on images where the input size is large (number of pixels), fully con- nected layers are prohibitively expensive. Convolution repeats a (usually small) number of weights across the input, thereby keeping the number of weights manageable. – The nature of the local pattern extracted is fixed all over the input. If the convolution is an edge detector, it extracts edges all over the input. We cannot have an edge detector at one region of the input and a corner detector at another region, for instance. Of course, in a multilayered network, we can use different convolution layers to capture different local patterns. In particular, successive layers can capture local patterns in local patterns of the input, and so on, thereby capturing increasingly complex and increasingly global patterns at higher layers of the network. The exact local pattern captured depends on the weights of the convolution operator. We don’t know exactly what local patterns of the input to capture to recognize a specific higher-level structure of interest (such as a face). This means we do not want to specify the weights of the convolutions. The whole point of neural networks is to avoid such tailored feature engineering. Rather, we want to learn—through the process of training described in chapter 8—the weights of the convolution layers. Losses can be backpropagated through convolution just as they are through fully connected (FC) layers. Just like FC layers, convolution layers can be expressed as matrix-vector multiplica- tions. The structure of the weight matrix is a special case of equation 8.8, but it is a matrix all the same. Consequently, the forward propagation equation 8.7 and backpropagation equations 8.31 and 8.33 are still applicable. Forward propagation and backpropagation (training) through convolution proceed exactly as they do with FC layers. Since the convolution is learned—as opposed to specified—in a neural network, there is no telling what local patterns such layers will learn to extract (although, in practice, the initial layers often learn to recognize edges and corners). All we know is that each output in a given layer is derived from only a small subset of spatially adjoint 10.1 One-dimensional convolution: Graphical and algebraical view 345 input values from previous layers. The final output is derived from a hierarchical local examination of the input. NOTE Fully functional code for chapter 10, runnable via Jupyter Notebook, is available at our public GitHub repository at http://mng.bz/M2lW. 10.1 One-dimensional convolution: Graphical and algebraical view As always, we examine the process of convolution with a set of examples. We examine convolutions in one, two, and three dimensions, but we start with one dimension for ease of understanding. The best way to visualize 1D convolution is to imagine a stretched, straightened rope (the input array) over which a measuring ruler (the kernel) is sliding. In figures 10.1, 10.2, and 10.3, the ruler (kernel) is shown as shaded boxes, while the rope (input array) is shown as a sequence of white boxes. Successive steps in the figure represent successive positions (aka slide stops) of the sliding ruler. Notice that the shaded portion occupies a different position in each step. Rulers in successive positions during sliding can overlap. They overlap by varying amounts in figures 10.1, 10.2, and 10.3. The rope and the ruler are discrete 1D arrays in reality. At each slide stop, the ruler array elements rest on a subset of rope array elements. We multiply each input array element by the kernel element resting on it and sum the products. This is equivalent to taking a weighted sum of the input (rope) elements that fall under the current position of the kernel (ruler), with the kernel elements serving as weights. This weighted sum is emitted as a single out- put element. One output element results from each slide stop of the tile. As the ruler slides over the entire rope, left to right, a 1D output array is generated. The following entities are defined for 1D convolution: Input—A one-dimensional array. We typically use the symbol n to represent input array length in 1D convolution. In figure 10.1, n = 7. Output—A one-dimensional array. We typically use the symbol o to represent the output array length in 1D convolution. In figure 10.1, o = 5. Section 10.2 shows how to calculate the output size from the independent parameters. Kernel—A small array of weights whose size is a parameter of the convolution. We typically use the symbol k to represent the kernel size in 1D convolution. In figure 10.1, k = 3; in figure 10.3, k = 2. Stride—The number of input elements over which the kernel slides after completing a single step. We typically use the symbol s to represent the stride in 1D convolution. This is a parameter of the convolution. In figure 10.1, s = 1; in figure 10.2, stride is 2. A stride of 1 means there is a slide stop at each successive element of the input. So, the output has roughly the same number of elements as the input (they may not be exactly equal because of padding, explained next). A stride of 2 means 346 CHAPTER 10 Convolutions in neural networks 14 11 30 4 25 -1 21 14 -1 4 11 21 15 4.62 0.33 0.33 0.33 Input Output Kernel 14 4 11 25 4.62 9.57 0.33 0.33 0.33 21 -1 14 11 4.62 9.57 15.18 0.33 0.33 0.33 21 -1 14 4 25 4.62 9.57 15.18 19.8 0.33 0.33 0.33 Step 0 Step 1 Step 2 Step 3 30 30 30 11 21 -1 4 25 4.62 9.57 15.18 19.8 0.33 0.33 0.33 30 25.08 Step 4 Figure 10.1 1D convolution with a local averaging kernel of size 3, stride 1, and valid padding on the input array of size 7 there is a slide stop at every other input element. So, the output size is roughly half of the input size. A stride of 3 means the output size is roughly one-third the input size. Padding—As the kernel slides toward the extremity of the input array, parts of it may fall outside the input array. In other words, part of the kernel falls over ghost 10.1 One-dimensional convolution: Graphical and algebraical view 347 14 4 25 14 -1 4 11 21 15 4.62 0.33 0.33 0.33 Input Output Kernel 21 -1 14 11 4.62 15.18 0.33 0.33 0.33 Step 0 Step 1 30 30 11 21 -1 4 25 4.62 15.18 0.33 0.33 0.33 30 25.08 Step 2 Figure 10.2 1D convolution with a local averaging kernel of size 3, stride 2, and valid padding input elements. Figure 10.4 shows such a situation: the ghost input array elements are shown with dashed boxes. There are multiple strategies to deal with this: – Valid padding—We stop sliding whenever any element of the kernel falls outside the input array. No ghost input elements are involved; the entire kernel always falls on valid input elements (hence the name valid). Note that this implies we will have fewer outputs than inputs. If we try to generate an output corresponding to, say, the last input element, all but the first kernel element will fall outside the input on ghost elements. So, we have to stop when the right-most kernel element falls on the right-most input element (see figures 10.1, 10.2, and 10.3). At this point, the left-most kernel element falls on the (N −k)th input element. We do not generate output for the last k inputs. Hence, even with a stride of 1 for valid padding, the output is slightly smaller than the input. – Same (zero) padding—Here, we do not want to stop early. If the stride is 1, the output size matches the input size (hence the name same). We continue to slide the kernel until its left end falls on the right-most input. At this point, all but the left-most kernel element is falling on ghost input elements. We pretend these ghost input elements have a value of 0 (zero padding). 348 CHAPTER 10 Convolutions in neural networks 10 50 51 51 49 9 9 49 50 51 10 11 -0.5 1 -0.5 -45.5 0.5 -0.5 Input Output Kernel Step 0 Step 3 Step 5 10 1 10 51 11 9 10 -0.5 -0.5 46 -0.5 -0.5 0.5 9 50 51 10 11 -0.5 1 -0.5 -45.5 0.5 -0.5 10 1 10 51 11 10 -0.5 -0.5 46 -0.5 -0.5 0.5 49 9 9 50 10 11 -0.5 1 -0.5 -45.5 0.5 -0.5 10 1 10 11 10 -0.5 -0.5 46 -0.5 -0.5 0.5 51 51 49 9 9 11 -0.5 1 -0.5 -45.5 0.5 -0.5 10 1 10 11 10 -0.5 -0.5 46 -0.5 -0.5 0.5 10 50 51 51 49 9 9 11 -0.5 1 -0.5 -45.5 0.5 -0.5 10 1 10 11 10 -0.5 -0.5 46 -0.5 -0.5 0.5 Step 7 Step 10 Figure 10.3 1D convolution with an edge-detection kernel of size 2, stride 1, valid padding. Not all slide stops (that is, steps) are shown. Let’s denote the input array’s domain by S. It’s a 1D grid: S = [0, W −1] Every point in S is associated with a value Xx. Together, these values form the input X. On this grid of input points, we define a subgrid So of output points. So is obtained from S by applying stride-based stepping on the input. Assuming s = [sW ] denotes the stride, the first slide stop has the top-left corner of the rope at (x = 0). The next slide stop is at (x = sW ), and the next is (x = 2sW ), and so on. When we reach the right end, we stop. Overall, the output grid consists of the slide-stop points at which the top-left corner of the kernel (ruler) rests as it sweeps over the input volume: that is, So = {(x = 0) , (x = sW ) · · · , }. There is an output for each point in So. 10.1 One-dimensional convolution: Graphical and algebraical view 349 21 30 -1 21 11 14 4 25 14 4 11 15 9.57 0.33 0.33 0.33 Input Output 21 -1 14 11 20 0.33 0.33 0.33 Step 1 Step 2 30 11 -1 4 25 20 0.33 0.33 0.33 30 18.15 Step 3 14 -1 4 21 15 0.33 0.33 0.33 Kernel Step 0 30 0 0 0 0 0 0 0 0 1 1 1 1 9.57 9.57 Figure 10.4 1D convolution with a local averaging kernel of size 3, stride 2, and zero padding Equation 10.1 shows how a single output value is generated in 1D convolution. X denotes input, Y denotes output, and W denote kernel weights: Yx = kW Õ j=0 Xx+jWj ∀(x) ∈So (10.1) Note that when the kernel of dimension kW (ruler) has its origin on x, it covers all input pixels in the domain [x.. (x + kW )]. These are the pixels participating in equa- tion 10.1. Each of these input pixels is multiplied by the kernel element covering it. Match equation 10.1 with figures 10.1, 10.2, and 10.3. 350 CHAPTER 10 Convolutions in neural networks 10.1.1 Curve smoothing via 1D convolution In this section, we look at how to perform local averaging via convolution, from a physical and algebraic viewpoint, to get a comprehensive understanding. The 1D kernel with weight vector ®w = h 1 3 1 3 1 3 i (shown in figure 10.1) essentially takes the moving average of successive sets of three input values. As such, it is a local averaging (aka smoothing) operator. This becomes apparent if we examine the plots of the raw input vector with regard to the input vector convolved with the kernel (figure 10.5). The input (solid line) weaves up and down, while the output is a smooth curve (dashed line) through the mean position of the input. In general, the output produced by convolving by a kernel with all equal weights (the weights must be normalized, meaning the sum of the weights is 1) is a smoothed (locally averaged) version of the input. Why do we want to smooth an input vector? Because it captures the broad trend in the input data while eliminating short-term fluctuations (often caused by noise). If you are familiar with Fourier transforms and frequency domains, you can see that this is essentially a low- pass filter, eliminating short-term, high-frequency noise and capturing the longer-term, low-frequency variation in the input data array. Input Smoothed output Figure 10.5 Graph of the input array (solid) and output array (dashed) from figure 10.1. Note that the output produced by convolving by the kernel with all equal normalized weights is a smoothed (locally averaged) version of the input. Such local soothing captures the low-frequency (long-term) broad trend of the function by eliminating high-frequency (short-term) noise. 10.1.2 Curve edge detection via 1D convolution As mentioned earlier, a convolution’s physical effect on an input array radically changes with the weights of the convolution kernel. Now let’s examine a very different kernel that detects edges in the input data. 10.1 One-dimensional convolution: Graphical and algebraical view 351 An edge is defined as a sharp change in the values in an input array. For instance, if two successive elements in the input array have a large absolute difference in values, that is an edge. If we graph the input array (that is, plot the input array values in the y axis against the array indices), an edge will appear in the graph. For instance, consider the input array in figure 10.3 (graphed in figure 10.6). At indices 0 to 3, we have values in the neighborhood of 10. At index 4, the value jumps to 51. We say there is an edge between indices 3 and 4. The values then remain in the neighborhood of 50 at indices 4 to 7. Then they jump back to the neighborhood of 10 in the remaining indices. We say there is another edge between indices 7 and 8. The convolution we examine here will emit a high response (output value) exactly at the indices of the jump—3 and 7—while emitting a low response at other indices. This is an edge-detection convolution (filter). Why do we want to detect edges? Because edges are important for understanding images. Locations at which the signal changes rapidly provide more semantic clues than flat uniform regions. Experiments on the human visual cortex have established that humans pay more attention to locations where color or shade changes rapidly than to flat regions. Input Edge-detection output Figure 10.6 Graph of the input array (solid) and output array (dashed) from figure 10.3. The output produced by convolving the kernel with anti-symmetric normalized weights spikes at the edges of the input. Edges provide vital clues for understanding the signal. 10.1.3 One-dimensional convolution as matrix multiplication Algebraically, the convolution with a kernel of size 3, stride 1, and valid padding can be shown as follows. Let the input array be 352 CHAPTER 10 Convolutions in neural networks ®x = h x0 x1 x2 x3 x4 · · · xn−3 xn−2 xn−1 i The convolving kernel is a matrix of weights of size 3; let’s denote it as ®w = h w0 w1 w2 i As shown in figure 10.1, in step 0 of the convolution, we place this kernel on the 0th element of the input x0. Thus, w0 falls on x0,w1 falls on x1, and w2 falls on x2: h x0 x1 x2 x3 x4 · · · xn−3 xn−2 xn−1 i , where the bold typeface identifies the input elements aligned with kernel weights. We multiply elements on corresponding positions and sum them, yielding the 0th element of the output y0 =w0x0 +w1x1 +w2x2. Then we shift the kernel by one position (assuming the stride is 1; if the stride were 2, we would move the kernel two positions, and so on). So w0 falls on x1, w1 falls on x2, and w2 falls on x3: h x0 x1 x2 x3 x4 · · · xn−3 xn−2 xn−1 i Again, we multiply elements at corresponding positions and sum them, yielding the first element of the output y1 =w0x1 +w1x2 +w2x2. Similarly, in the next step, we right-shift the kernel one more time: h x0 x1 x2 x3 x4 · · · xn i The corresponding output is y2 =w0x2 +w1x3 +w2x4. Overall, a stride 1, valid padding convolution of a vector ®x with a weight kernel ®w yields the output ®y = ®w ⊛®x =  w0x0 +w1x1 +w2x2 w0x1 +w1x2 +w2x2 ...  Can you see what is happening? We are effectively taking linear combinations (see section 2.9) of successive sets of kernel_size (here, 3) input elements. In other words, the output is a moving weighted local sum of the input array elements. Depending on the actual weights, we are extracting local properties of the input. For valid padding, the last output is yielded by h x0 x1 x2 x3 x4 · · · xn−3 xn−2 xn−1 i which generates the output yn−3 =w0xn−3 +w1xn−2 +w2xn−1 For the same zero padding, the last output is yielded by h x0 x1 x2 x3 x4 · · · xn−1 0 0 i 10.1 One-dimensional convolution: Graphical and algebraical view 353 which generates the output yn−1 =w0 · xn−1 +w1 · 0 +w2 · 0 In section 8.3.1, we saw that the FC (aka linear) layer can be expressed as a multiplication of the input vector by a weight matrix. Now, we will express convolution as matrix- vector multiplication. The weight matrix has a block-diagonal structure, as shown in equation 10.2. It is a special case of equation 8.8. As such, the forward propagation equation 8.7 and backpropagation equations 8.31 and 8.33 are still applicable. Thus, forward propagation and backpropagation (training) through convolution proceeds exactly as with FC layers. Equation 10.2 expresses kernel_size 3, stride 1, valid padding convolution as a multi- plication of a weight matrix W with input vector ®x: ®w ⊛®x =W ®x =  w0x0 +w1x1 +w2x2 w0x1 +w1x2 +w2x2 ...  = W   conv weight matrix: kernel size 2, stride 1, valid pad. dim: (n−2)×n z }| {  w0 w1 w2 0 0 0 · · · 0 0 0 0 w0 w1 w2 0 0 · · · 0 0 0 0 0 w0 w1 w2 0 · · · 0 0 0 0 0 0 w0 w1 w2 · · · 0 0 0 0 0 0 0 w0 w1 · · · 0 0 0 ... 0 0 0 0 0 0 · · · w0 w1 w2  ®x  input vector, size n × 1  z }| {  x0 x1 x2 x3 x4 ... xn−1  (10.2) Notice the sparse, block-diagonal nature of the weight matrix in equation 10.2. This is characteristic of convolution weight matrices. Each row contains all the kernel weights at contiguous positions. The size of the kernel is typically much less than the input vector size. Of course, for matrix multiplication to be possible, the number of columns in the weight matrix must match the size of the input vector. Thus, there are many vacant positions in the row besides those occupied by kernel weights. We fill these vacant elements with zeros. Each row of the weight matrix thus has all the kernel weights appearing somewhere contiguously, and the rest of the row is filled with zeros. The position of kernel weights shifts rightward with each successive row. This is what gives the block-diagonal appearance to the weight matrix. It also simulates the sliding of the kernel required for convolution. Each row represents a specific slide stop and generates one element of the output vector. Since the kernel is at a fixed position of the row and all other row elements are zero, only the input elements corresponding to the kernel 354 CHAPTER 10 Convolutions in neural networks positions are picked up. Other input elements are multiplied by zero: that is, they are ignored. Equation 10.2 depicts a stride of 1. For instance, if the stride is 2, the kernel weights will shift by two positions in successive rows. This is shown in equation 10.3: ®w ⊛®x =W ®x = W  conv weight matrix: kernel size 2, stride 2, valid pad. dim: ⌊(n−2) 2 +1⌋×n  z }| {  w0 w1 w2 0 0 0 · · · 0 0 0 0 0 w0 w1 w2 0 · · · 0 0 0 0 0 0 0 w0 w1 · · · 0 0 0 ... 0 0 0 0 0 0 · · · w0 w1 w2  ®x  input vector, size n × 1  z }| {  x0 x1 x2 x3 x4 ... xn−1  (10.3) Note that while equation 10.3 provides a conceptual matrix-multiplication view of convolution, it is not the most efficient way of implementing convolution. PyTorch and other deep learning software have extremely efficient ways of implementing convolution. 10.1.4 PyTorch: One-dimensional convolution with custom weights We have discussed the convolution of a 1D input vector with two specific 1D kernels. We have seen that a kernel with uniform weights, such as h 1 3 1 3 1 3 i , results in local smoothing of the input vector, whereas a kernel with antisymmetric weights, such as h 1 2 −1 2 i , results in an output vector that spikes at the edge locations in the input vector. Now we will see how to set the weights of a 1D kernel and perform 1D convolution with that kernel in PyTorch. NOTE This is not a typical PyTorch operation. The more typical operation is to create a neural network with a convolution layer (where we specify the size, stride, and padding but not the weights) and then train the network so that the weights are learned. We usually don’t care about the exact values of the learned weight. Then why are we discussing how to set the weights of a kernel in PyTorch? Mainly to show how convolution works in PyTorch, the various parameters of the convolution object, and so forth. Listing 10.1 PyTorch code for 1D local averaging convolution import torch x = torch.tensor( Instantiates a noisy input vector. Follows equation y = 5x [-1., 4., 11., 14., 21., 25., 30.]) 10.1 One-dimensional convolution: Graphical and algebraical view 355 w = torch.tensor([0.33, 0.33, 0.33]) Instantiates the weights of the convolutional kernel x = x.unsqueeze(0).unsqueeze(0) w = w.unsqueeze(0).unsqueeze(0) PyTorch expects inputs and weights to be of the form N × C × L, where N is the batch size, C is the number of channels, and L is the sequence length. Here, N and C are 1. torch.unsqueeze converts our L-length vector into a 1 × 1 × L tensor. conv1d = torch.nn.Conv1d(1, 1, kernel_size=3, Instantiates the smoothing kernel stride=1, padding=[0], bias=False) conv1d.weight = torch.nn.Parameter(w, requires_grad=False) Sets the kernel weights with torch.no_grad(): Instructs PyTorch to not compute gradients since we currently don’t require them y = conv1d(x) Runs the convolution Listing 10.2 PyTorch code for 1D edge detection import torch x = torch.tensor( Instantiates the input vector with edges [10., 11., 9., 10., 101., 99., 100., 101., 9., 10., 11., 10.]) w = torch.tensor([0.5, -0.5]) Instantiates the weights of the edge-detection kernel x = x.unsqueeze(0).unsqueeze(0) Converts the inputs and weights to 1 × 1 × L w = w.unsqueeze(0).unsqueeze(0) conv1d = torch.nn.Conv1d(1, 1, kernel_size=3, Instantiates the edge-detection kernel stride=1, padding=[0], bias=False) conv1d.weight = torch.nn.Parameter(w, requires_grad=False) Sets the kernel weights with torch.no_grad(): Instructs PyTorch to not compute gradients since we currently don’t require them y = conv1d(x) Runs the convolution These listings show how to perform 1D convolution in PyTorch using the torch.nn. Conv1d class. This is typically used in larger neural networks like those in subsequent chapters. We can alternatively use torch.nn.functional.conv1d to directly invoke the mathematical convolution operation. This takes input and weight tensors and returns the convolved output tensor, as shown in listing 10.3. 356 CHAPTER 10 Convolutions in neural networks Listing 10.3 PyTorch code directly invoking the convolution function import torch x = torch.tensor( Instantiates the input tensor [10., 11., 9., 10., 101., 99., 100., 101., 9., 10., 11., 10.]) w = torch.tensor([0.5, -0.5]) Instantiates the weight tensor x = x.unsqueeze(0).unsqueeze(0) Converts the inputs and weights to 1 × 1 × L w = w.unsqueeze(0).unsqueeze(0) y = torch.nn.functional.conv1d(x, w, stride=1) Runs the convolution 10.2 Convolution output size Consider a kernel of size k sliding over an input of size n with stride s. Given a kernel of size k, if the left end is at index l, the right end is at index l + (k −1). Each shift advances the left (as well as the right) end of the kernel by s. If the initial position of the kernel was at index 0, then after m shifts, the left end is at ms. Correspondingly, the right end is at ms + (k −1). Assuming valid padding, this right-end position must be less than or equal to (n −1) (the last valid position of the input array). How many times can we shift before the kernel spills out of the input? In other words, what is the maximum possible value of m, such that ms + (k −1) ≤(n −1) The answer is m = ⌊(n −1) −(k −1) s ⌋= ⌊(n −k) s ⌋ But each shift produces one output value. The output size of the convolution, o, with valid padding, is m + 1 (the plus one is to account for the initial position). Hence, o = ⌊(n −k) s ⌋+ 1 If we are zero-padding with p zeroes on each side of the input, the input size becomes n + 2p. The corresponding output size is o = ⌊(n + 2p −k) s ⌋+ 1 (10.4) This can be extended to an arbitrary number of dimensions by repeating it for each dimension. 10.3 Two-dimensional convolution: Graphical and algebraic view It is often said that an image is worth a thousand words. What is an image? As far as deep learning is concerned, it is a discrete two-dimensional entity—a 2D array of pixel values describing a scene at a fixed time. Each pixel represents a color intensity 10.3 Two-dimensional convolution: Graphical and algebraic view 357 value. The color value can be a single element representing a gray level, or it can be three-dimensional, corresponding to R(ed), G(reen), B(lue) intensity values. (You may want to revisit section 2.3 before proceeding.) At the time of this writing, image analysis is the most popular application of con- volution. These applications use convolution to extract local patterns. How do we do this? In particular, can we rasterize the image (thus converting it into a vector) and use one-dimensional convolution? The answer is no. To see why, examine figure 10.7. What is the spatial neighborhood of the pixel at location (x = 0, y = 0)? If we define the neighborhood of a pixel as the set Step 0 Output 18.59 24.53 30.36 31.24 37.18 43.23 43.89 49.83 55.88 Step 1 Output 18.59 24.53 30.36 31.24 37.18 43.23 43.89 49.83 55.88 Step 2 Output 18.59 24.53 30.36 31.24 37.18 43.23 43.89 49.83 55.88 Step 3 Output 18.59 24.53 30.36 31.24 37.18 43.23 43.89 49.83 55.88 Step 4 Output 18.59 24.53 30.36 31.24 37.18 43.23 43.89 49.83 55.88 Step 5 Output 18.59 24.53 30.36 31.24 37.18 43.23 43.89 49.83 55.88 Step 6 Output 18.59 24.53 30.36 31.24 37.18 43.23 43.89 49.83 55.88 Step 7 Output 18.59 24.53 30.36 31.24 37.18 43.23 43.89 49.83 55.88 Step 8 Output 18.59 24.53 30.36 31.24 37.18 43.23 43.89 49.83 55.88 Input Input Input 31 43 57 25 38 50 19 31 44 12 26 39 70 63 57 75 63 49 37 31 43 57 25 38 50 19 31 44 12 26 39 70 63 57 51 75 63 49 37 31 43 57 25 38 50 19 31 44 12 26 39 70 63 57 51 75 63 49 37 18 12 6 0 23 18 12 6 0 23 18 12 6 0 23 51 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 70 63 57 51 75 70 63 57 51 75 Input Input Input 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 31 43 57 25 38 50 19 31 44 12 26 39 63 49 37 31 43 57 25 38 50 19 31 44 12 26 39 63 49 37 31 43 57 25 38 50 19 31 44 12 26 39 70 63 57 51 75 63 49 37 18 12 6 0 23 18 12 6 0 23 18 12 6 0 23 Input Input Input 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 31 43 57 25 38 50 19 31 44 12 26 39 63 49 37 18 12 6 0 23 31 43 25 38 19 31 12 26 49 37 18 12 6 0 23 31 43 25 38 19 31 12 26 57 50 44 39 63 49 37 18 12 6 0 23 70 63 57 51 75 70 63 57 51 75 70 63 57 51 75 57 50 44 39 63 Figure 10.7 2D convolution with a local averaging kernel of size [3, 3], stride [1, 1], and valid padding. Each pixel is shown as a small rectangle, with the pixel’s gray level written in the rectangle. The shaded rectangle identifies the current location of the kernel. The kernel is sliding over the input in raster order. Successive steps indicate slide stops. For each pixel that is overlapped by the kernel, the weight of the kernel element falling on it is written in small font. 358 CHAPTER 10 Convolutions in neural networks of pixels within a Manhattan distance of [2, 2] with that pixel at the top-left corner, the neighborhood of (x = 0, y = 0) consists of the set of pixels covered by the shaded rectangle in figure 10.7, step 0. But these pixels will not be neighboring elements in a rasterized array representation of the image. For instance, the pixel (x = 0, y = 1), with value 6, is the fifth element in the rasterized array and, as such, will not be considered a neighbor of (x = 0, y = 0), which is the 0th element in the rasterized array. Two-dimensional neighborhoods are not preserved by rasterization. So, two-dimensional convolution has to be a specialized operation beyond merely rasterizing 2D arrays into 1D and applying 1D convolution. Euclidean distance and Manhattan distance Euclidean distance measures the straight line distance between two points, whereas Manhattan distance measures the distance between two points with a constraint that you can only walk parallel to the axes (just like on the streets of Manhattan). Let’s look at an example. Consider two points A (3, 3) and B (6, 7). The Euclidean distance between A and B is the length of the line segment AB, which can be computed as p (6 −3)2 + (7 −3)2 = 5. The Manhattan distance between A and B is (6 −3) + (7 −3) = 3 + 4 = 7. In this chapter, we represent the Manhattan distance as [3, 4] to capture the distance along each axis separately. The best way to visualize 2D convolution is to imagine a wall (the input image) over which a tile (the kernel) is sliding: In figures 10.7, 10.8 and 10.9, the shaded rectangle depicts the tile (kernel), while the larger white rectangle containing it depicts the wall (input image). Successive steps in the figure represent successive positions (aka slide stops) of the sliding tile. Notice that the shaded rectangle occupies a different position in each step. Tiles in successive positions during sliding can overlap. They overlap by varying amounts in figures 10.7, 10.8, and 10.9. The wall and the tile are discrete 2D arrays in reality. At each slide stop, the tile array elements rest on a subset of wall array elements. We multiply each input array element by the kernel element resting on it and sum the products. This is equivalent to taking a weighted sum of the input (wall) elements that fall under the current position of the kernel (tile), with the kernel elements serving as weights. This weighted sum is emitted as a single output element. One output element results from each slide stop of the tile. As the tile slides over the entire wall, left to right and top to bottom, a 2D output array is generated. In 2D convolution, the input array, kernel size, and stride are all 2D vectors. Just as in 1D convolution, the following entities are defined for 2D convolution: 10.3 Two-dimensional convolution: Graphical and algebraic view 359 Input Output Step 0 31 43 57 25 38 50 19 31 44 12 26 39 70 63 57 51 18.59 30.36 43.89 55.88 75 63 49 37 18 12 6 0 23 Input Output Step 1 31 43 57 25 38 50 19 31 44 12 26 39 70 63 57 51 18.59 30.36 43.89 55.88 75 63 49 37 18 12 6 0 23 Input Output Step 2 31 43 57 25 38 50 19 31 44 12 26 39 70 63 57 51 18.59 30.36 43.89 55.88 75 63 49 37 18 12 6 0 23 Input Output Step 3 31 43 57 25 38 50 19 31 44 12 26 39 70 63 57 51 18.59 30.36 43.89 55.88 75 63 49 37 18 12 6 0 23 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 Figure 10.8 2D convolution with a local averaging kernel of size [3, 3], stride [2, 2], and valid padding Input—A two-dimensional array. We typically use the symbol [H, W ] (indicating the height and width of the array, respectively) to represent the input array size in 2D convolution. In figure 10.7, H = 5, W = 5. Output—A two-dimensional array. We typically use the symbol ®o = [oH , oW ] to rep- resent output array dimensions in 2D convolution. For instance, in figure 10.7, ®o = [3, 3]. In section 10.2, we saw how to compute the output size for a single dimension. We have to repeat that computation once per dimension to obtain the output size in higher dimensions. Kernel—A small two-dimensional array of weights whose size is a parameter of the convolution. We typically use the symbol ®k = [kH , kW ] to represent the kernel size (height, width) in 2D convolution. If (x, y) denotes the current position of the top- left corner of the 2D kernel, the bottom-right corner is at (x + kW −1, y + kH −1). In figure 10.7, ®k = [3, 3]; in figure 10.9, ®k = [2, 2]. 360 CHAPTER 10 Convolutions in neural networks Input Output Step 0 100 100 100 100 100 100 10 10 100 10 10 100 100 100 100 100 -0.25 0.25 -0.25 0.25 0 Input 100 100 100 100 100 100 10 10 100 10 10 100 100 100 100 100 Output 0 45 Input 100 100 100 100 100 100 10 10 100 10 10 100 100 100 100 100 Output 0 45 0 -0.25 0.25 -0.25 0.25 -0.25 0.25 -0.25 0.25 Input 100 100 100 100 100 100 10 10 100 10 10 100 100 100 100 100 Output 0 0 45 22.5 0 0 -0.25 0.25 -0.25 0.25 Input 100 100 100 100 100 100 10 10 100 10 10 100 100 100 100 100 Output 0 0 45 22.5 0 -0.25 0.25 -0.25 0.25 Input 100 100 100 100 100 100 100 10 10 100 100 100 100 100 Output 0 0 45 0 -0.25 0.25 -0.25 0.25 Input 100 100 100 100 100 100 10 10 100 10 10 100 100 100 100 100 Output 0 0 0 45 22.5 0 0 0 0 -0.25 0.25 -0.25 0.25 Input 100 100 100 100 100 100 10 10 100 10 10 100 100 100 100 100 Output 0 0 0 45 22.5 0 0 0 -0.25 0.25 -0.25 0.25 Input 100 100 100 100 100 100 10 10 100 10 10 100 100 100 100 100 Output 0 0 0 45 22.5 0 0 -0.25 0.25 -0.25 0.25 10 10 Step 1 Step 2 Step 3 Step 4 Step 5 Step 6 Step 7 Step 8 Figure 10.9 2D convolution with an edge-detection kernel of size 2, stride 1, and valid padding. Not all slide stops (that is, steps) are shown. Notice how the output is zero at a uniform location but spikes when one-half of the kernel falls on low values while the other half falls on high values. Stride—The number of input elements over which the kernel slides on completing a single step. We typically use the symbol ®s = [sH , sW ] to represent the stride size (height, width) in 2D convolution. If (x, y) denotes the current position of the top-left corner of the 2D kernel, the next shift will put the top-left corner of the kernel at (x + sW , y) (see, for instance, the transition from step 0 to step 1 or step 1 to step 2 in figure 10.7). If this transition causes portions of the tile to fall outside the wall—that is, x + sW ≥W —we set the next slide position such that the top-left corner of the kernel falls on (0, y + 1) (see, for instance, the transition from step 2 to step 3 or step 5 to step 6 in figure 10.7). If y + sH ≥H, we stop sliding. Stride size is a parameter of the convolution. In figure 10.7, ®s = [1, 1]; in figure 10.8, stride is ®s = [2, 2]. As in the 1D case, a stride of ®s = [1, 1] means there is a slide stop at each successive element of the input. So, the output has roughly the same number of elements as the input (they may not be exactly equal because of padding). A stride of ®s = [2, 2] means each row of the input will yield half the row size worth of output elements, and each column will generate half the column size worth of output elements. Hence, the output size is roughly a quarter of the input size. Overall, 10.3 Two-dimensional convolution: Graphical and algebraic view 361 the reduction factor of the input-to-output size roughly matches the product of the elements in the stride vector. Padding—As the kernel slides toward the extremity of the input array along the width and/or height, parts of it may fall outside the input array. In other words, part of the kernel falls over ghost input elements. As in the 1D case, we deal with this via padding. Padding strategies in 2D convolution are straightforward extensions from 1D: – Valid padding—We stop sliding whenever any element of the kernel falls outside the input array, either in width and/or in height. No ghost input elements are involved; the entire kernel always falls on valid input elements (hence the name valid). – Same (zero) padding—Here, we do not want to stop early. We keep sliding as long as the top-left corner of the kernel falls on a valid input position. So, if the stride is 1, 1, the output size will match the input size (hence the name same). When we slide near the end of an input row (right extremity of the input), the right-most columns of the kernel will fall outside the input. Similarly, when we slide toward the bottom of the input, the bottom rows of the kernel will fall outside. If we slide near the bottom-right corner of the input, both the right-most columns and bottommost rows will fall outside the input. The rule is that all ghost input values outside the true boundaries of the input array are replaced by zeros. Let’s denote the input image domain by S. It is a 2D grid whose domain is S = [0, H −1] × [0, W −1] Every point in S is a pixel with a color value (which can be a scalar—a gray-level value— or a vector of three values, R, G, B. On this grid of input points, we define a subgrid So of output points. So is obtained from S by applying stride-based stepping on the input. Assuming ®s = [sH , sW ] denotes the 2D stride vector, the first slide stop has the top-left corner of the brick at ®p0 ≡(y = 0, x = 0). The next slide stop is at ®p1 ≡(y = 0, x = sW ), and the next is at ®p2 ≡(y = 0, x = 2sW ). When we reach the right end, we increment y. Overall, the output grid consists of the slide-stop points where the top-left corner of the kernel (brick) rests as it sweeps over the input volume: So =  ®p0, ®p1, · · · , . There is an output for each point in So. The kernel also has two dimensions (in practice, it has two more dimensions corre- sponding to the input channels and batch—we are ignoring them now for simplicity—as discussed in section 10.3.3). Equation 10.5 shows how a single output value is generated in 2D convolution. X denotes input, Y denotes output, and W denote kernel weights: Yy,x = kH Õ i=0 kW Õ j=0 Xy+i,x+jWi, j ∀(y, x) ∈So (10.5) Note that the kernel (tile) has its origin on Xy,x. Its dimensions are (kH , kW ). So, it covers all input pixels in the domain [y.. (y + kH)] × [x.. (x + kW )]. These are the pixels 362 CHAPTER 10 Convolutions in neural networks participating in equation 10.5. Each of these input pixels is multiplied by the kernel element covering it. Match equation 10.5 with figures 10.7, 10.8, and 10.9. 10.3.1 Image smoothing via 2D convolution In section 10.1.1, we discussed one-dimensional local smoothing. We observed how it gets rid of local fluctuations so that longer-term patterns are discernible more cleanly. The same thing happens in two dimensions. Figure 10.10a shows an image with some text written on a background with salt-and-pepper noise. The noise has no semantic significance; it is the text that needs to be analyzed (perhaps via optical character recognition). We can eliminate the noise via 2D convolution using a kernel with uniform weights, such as W =  W0,0 = 1 9 W0,1 = 1 9 W0,2 = 1 9 W1,0 = 1 9 W1,1 = 1 9 W1,2 = 1 9 W2,0 = 1 9 W2,1 = 1 9 W2,2 = 1 9  The resulting denoised/smooth image is shown in figure 10.10b. What does the uniform kernel do? To see that, look at figure 10.8. It should be obvious that the kernel causes each output pixel to be a weighted local average of the neighboring 3 × 3 input pixels. (a) Input image (b) Smoothed/denoised output image Figure 10.10 Denoising/smoothing a noisy image by applying 2D convolution  1 9 1 9 1 9 1 9 1 9 1 9 1 9 1 9 1 9  to figure 10.11a NOTE Fully functional code for image smoothing, executable via Jupyter Note- book, can be found at http://mng.bz/aDM7. 10.3.2 Image edge detection via 2D convolution Not all pixels in an image have equal semantic importance. Imagine a photograph of a person standing in front of a white wall. The pixels belonging to the wall are uniform in color and uninteresting. The pixels that yield the most semantic clues are those belonging to the silhouette: the edge pixels. This agrees with the science of human vision, where, as we mentioned earlier, experiments indicate that the human brain pays 10.3 Two-dimensional convolution: Graphical and algebraic view 363 more attention to regions with sharp changes in color. Humans treat sound in a very similar fashion, ignoring uniform buzz (such sounds often induce sleep) but becoming alert when the volume or frequency of the sound changes. Thus, identifying edges in an image is vital for image understanding. Edges are local phenomena. As such, they can be identified by 2D convolution with specially chosen kernels. For instance, the vertical edges in figure 10.11b were produced by performing 2D convolution on the image in figure 10.11a using the kernel W =  W0,0 = −0.25 W0,1 = 0.25 W1,0 = −0.25 W1,1 = 0.25  | {z } 2D kernel for vertical edge detection Likewise, the vertical edges in figure 10.11c were produced by performing 2D convolu- tion on the image in figure 10.11a using the kernel W =  W0,0 = −0.25 W0,1 = −0.25 W1,0 = 0.25 W1,1 = 0.25  | {z } 2D kernel for horizontal edge detection How do these kernels identify edges? To see this, look at figure 10.9. In a neighborhood with equal pixel values (for example, a flat wall), the kernel in figure 10.11b will yield zero (the positive and negative kernel elements fall on equal values, and their weighted sum is zero). Thus this kernel suppresses uniform regions. On the other hand, it has a high response if there is a sharp jump in color (the negative and positive halves of the kernel fall on very different values, and the weighted sum is a large negative or large positive). NOTE Fully functional code for edge detection, executable via Jupyter Notebook, can be found at http://mng.bz/g4JV. 10.3.3 PyTorch: 2D convolution with custom weights We have discussed the convolution of 2D input arrays with two specific 2D kernels. We have seen that a kernel with uniform weights, such as  1 9 1 9 1 9 1 9 1 9 1 9 1 9 1 9 1 9  , results in local smoothing of the input array, whereas a kernel with antisymmetric weights, such as  1 4 −1 4 1 4 −1 4  , results in an output array that spikes at the edge locations in the input array. Now we will see how to set the weights of a 2D kernel and perform 2D convolution with that kernel in PyTorch. 364 CHAPTER 10 Convolutions in neural networks (a) Input image (b) Vertical edges detected by applying 2D convolution " −0.25 0.25 −0.25 0.25 # to figure 10.11a (c) Horizontal edges detected by applying 2D convolution " −0.25 −0.25 0.25 0.25 # to figure 10.11a Figure 10.11 Edge detection via 2D convolution. Identifying the vertical and horizontal edges in an image often helps us analyze the image. 10.3 Two-dimensional convolution: Graphical and algebraic view 365 NOTE This is not a typical PyTorch operation. The more typical operation is to create a neural network with a convolution layer (where we specify the size, stride, and padding but not the weights) and then train the network so that the weights are learned. We usually don’t care about the exact values of the learned weight. A sample neural network with a 2D convolution layer can be seen in section 10.6. Listing 10.4 shows local averaging convolution in two dimensions. While we saw in section that input arrays are 2D tensors of shape H × W, the PyTorch interface to convolution expects 4D tensors of shape N × C × H × W as input: The first dimension, N, stands for the batch size. In a real neural network, inputs are fed in minibatches instead of one input instance at a time (this is for efficiency reasons, as discussed in section 9.2.2). N stands for the number of input images contained in the minibatch. The second dimension, C, stands for the number of channels. For the input to the entire neural network, in the case of RGB images, we have three channels R (red), G (green), and B (blue); in the case of grayscale images, we only have a single channel. For other layers, the number of channels can be anything, depending on the neural network’s architecture. Typically, layers further from the input and closer to the output have more channels. Only channels at the grand input have fixed, clearly discernible physical significance (like R, G, B). Channels at the input to successive layers do not. The third dimension, H, stands for the height. The fourth dimension, W, stands for the width. The weight tensor of a PyTorch Conv2D object has to be a 4D tensor. The listing shows a single grayscale image of size 5 × 5 as input. Hence N = 1, C = 1, H = 5, and W = 5. x is instantiated as a 2D tensor of size 5 × 5. To convert it to a 4D tensor, we use the torch.unsqueeze() function, which adds an extra dimension to the input. Listing 10.4 PyTorch code for 2D local averaging convolution import torch x = load_img() Loads a noisy grayscale input image w = torch.tensor( Instantiates the weights of the convolutional kernel [ [0.11, 0.11, 0.11], [0.11, 0.11, 0.11], [0.11, 0.11, 0.11] ] ) x = x.unsqueeze(0).unsqueeze(0) PyTorch expects inputs and weights to be of the form N × C × H × W , where N is the batch size, C is the number of channels, H is the height, and W is the width. Here, N = 1 because we have a single image. C = 1 because we are considering a grayscale image. H and W are both 5 because the input is a 5 × 5 array. unsqueeze converts our 5 × 5 tensor into a 1 × 1 × 5 × 5 tensor. w = w.unsqueeze(0).unsqueeze(0) 366 CHAPTER 10 Convolutions in neural networks conv2d = torch.nn.Conv2d(1, 1, kernel_size=2, stride=1, bias=False) Instantiates the 2D smoothing kernel conv2d.weight = torch.nn.Parameter(w, requires_grad=False) Sets the kernel weights with torch.no_grad(): Instructs PyTorch to not compute gradients since we currently don’t require them y = conv2d(x) Runs the convolution Listing 10.5 PyTorch code for 2D edge detection import torch x = load_img() Loads a grayscale input image with edges w = torch.tensor( Instantiates the weights of the convolutional kernel [[-0.25, 0.25], [-0.25, 0.25]] ) x = x.unsqueeze(0).unsqueeze(0) Converts the inputs to 1 × 1 × 4 × 4 w = w.unsqueeze(0).unsqueeze(0) conv2d = torch.nn.Conv2d(1, 1, kernel_size=2, Instantiates a 2D edge-detection kernel stride=1, bias=False) conv2d.weight = torch.nn.Parameter(w, requires_grad=False) Sets the kernel weights with torch.no_grad(): Instructs PyTorch to not compute gradients since we currently don’t require them y = conv2d(x) Runs the convolution 10.3.4 Two-dimensional convolution as matrix multiplication In section 10.1.3, we saw how 1D convolution can be viewed as multiplying the input vector by a block-diagonal matrix (shown in equation 10.3). The idea can be extended to higher dimensions, although the matrix of weights becomes significantly more complex. Nonetheless, it is important to have a mental picture of this matrix. Among other things, it will help us better understand transposed convolution. In this matrix multiplication-oriented view of 2D convolution, the input image is represented as a rasterized 1D vector. Thus, an input matrix of size m × n becomes an mn-sized vector. The corresponding weight matrix has rows of length mn. Each row corresponds to a specific slide stop. For ease of understanding, let’s consider an input image with [H, W ] = [4, 4] (never mind that this image is unrealistically small). On this image, we are performing 2D convolution with a [kH , kW ] = [2, 2] kernel with stride [sH , sW ] = [1, 1]. The situation is 10.3 Two-dimensional convolution: Graphical and algebraic view 367 exactly as shown in figure 10.9. The input image X with size H = 4, W = 4 X =  X0,0 X0,1 X0,2 X0,3 X1,0 X1,1 X1,2 X1,3 X2,0 X2,1 X2,2 X2,3 X3,0 X3,1 X3,2 X3,3  Δ= ®x =  X0,0 X0,1 X0,2 X0,3 X2,0 X2,1 X2,2 X2,3 X3,0 X3,1 X3,2 X3,3  rasterizes to the input vector ®x of length 4 ∗4 = 16. Let the kernel weights be denoted as  w0,0 w0,1 w1,0 w1,1  Consider the successive slide stops (steps in figure 10.9). The exact elements of the rasterized input vector that are multiplied by kernel weights for a specific step are shown below—these correspond to the shaded items for the same steps in figure 10.9: 2D convolution between an image X and a kernel W, denoted Y =W ⊛X, in the special case of an input image with [H, W ] = [4, 4]. For this image, 2D convolution with a [kH , kW ] = [2, 2] kernel with stride [sH , sW ] = [1, 1] and valid padding can be expressed as the following matrix multiplication: Y =W ⊛X =  Y0,0 Y0,1 Y0,2 Y1,0 Y1,1 Y1,2 y2,0 Y2,1 y2,2  =  w0,0X0,0 +w0,1X0,1 +w1,0X1,0 +w1,1X1,1 w0,0X0,1 +w0,1X0,2 +w1,0X1,1 +w1,1X1,2 w0,0X0,2 +w0,1X0,3 +w1,0X1,2 +w1,1X1,3 w0,0X1,0 +w0,1X1,1 +w1,0X2,0 +w1,1X2,1 w0,0X1,1 +w0,1X1,2 +w1,0X2,1 +w1,1X2,2 w0,0X1,2 +w0,1X1,3 +w1,0X2,2 +w1,1X2,3 w0,0X2,0 +w0,1X2,1 +w1,0X3,0 +w1,1X3,1 w0,0X2,1 +w0,1X2,2 +w1,0X3,1 +w1,1X3,2 w0,0X2,2 +w0,1X2,3 +w1,0X3,2 +w1,1X3,3  368 CHAPTER 10 Convolutions in neural networks This can be expressed as W ⊛X =  w0,0 w0,1 0 0 w1,0 w1,1 0 0 0 0 0 0 0 0 0 0 0 w0,0 w0,1 0 0 w1,0 w1,1 0 0 0 0 0 0 0 0 0 0 0 w0,0 w0,1 0 0 w1,0 w1,1 0 0 0 0 0 0 0 0 0 0 0 0 w0,0 w0,1 0 0 w1,0 w1,1 0 0 0 0 0 0 0 0 0 0 0 w0,0 w0,1 0 0 w1,0 w1,1 0 0 0 0 0 0 0 0 0 0 0 w0,0 w0,1 0 0 w1,0 w1,1 0 0 0 0 0 0 0 0 0 0 0 0 w0,0 w0,1 0 0 w1,0 w1,1 0 0 0 0 0 0 0 0 0 0 0 w0,0 w0,1 0 0 w1,0 w1,1 0 0 0 0 0 0 0 0 0 0 0 w0,0 w0,1 0 0 w1,0 w1,1   X0,0 X0,1 X0,2 X0,3 X2,0 X2,1 X2,2 X2,3 X3,0 X3,1 X3,2 X3,3  (10.6) Note the following: The 2D convolution weight matrix shown in equation 10.6 is for the special case, but it illustrates the general principle. The 2D convolution weight matrix is block diagonal, just like the 1D version. The kernel weights are placed precisely to emulate figure 10.9. The convolution weight matrix has 9 rows and 16 columns. Thus it takes a 16-element input vector (rasterized from a 4 × 4 input image) and generates a 9-element output matrix (which can be folded into a 3 × 3 convolution output image. 10.4 Three-dimensional convolution If a picture is worth a thousand words, a video is worth 10,000 words. Videos are a rich source of information about dynamic real-life scenes. As deep learning-based image analysis (2D convolution) is becoming more and more successful, at the time of this writing, video analysis is becoming the next research frontier to conquer. Videos are essentially three-dimensional entities. The representation is discrete in all three dimensions. The three dimensions correspond to space, which is two-dimensional, having height and width, and time. A video consists of a sequence of frames. Each frame is an image: a discrete 2D array of pixels. A frame represents the entire video scene at a specific (sampled) point. A pixel in a frame represents the color of a sampled location in space belonging to the scene at the time corresponding to the frame. Thus a video is a sequence of frames representing the dynamic scene at a sampled set of discrete points (pixels) in space and time. The video extends over a spatio-temporal volume (aka ST volume), which can be imagined as a cuboid. Each cross-section is a rectangle representing a frame. This is shown in figure 10.12. To analyze the video, we need to extract local patterns from this 3D volume. Can we do it via repeated 2D convolutions? The answer is no. There is extra information when we view the successive frames together, which is absent when we view the frames one at a time. For instance, imagine you are presented with an image of a half-open door. From that single image, can 10.4 Three-dimensional convolution 369 Figure 10.12 A spatio-temporal volume (light-shaded cuboid) representing a video. Individual frames of the video are cross-sectional rectangles in this ST volume. A single frame is also shown in darker shading. you determine whether the door is opening or closing? No, you cannot. To make that determination, we need to see several successive frames. In other words, analyzing a video one frame at a time robs us of a vital modality of information: motion, which can be understood only if we analyze multiple successive frames together. This is why we need 3D convolution. The best way to visualize a 3D convolution is to imagine a brick sliding over the entire volume of a room. The room corresponds to the ST volume of the video input to the convolution. The brick corresponds to the kernel. While sliding, the brick stops at successive positions; we call these slide stops. Figure 10.13 shows four slide stops at different positions. Each slide stop emits one output point. As the brick sweeps over the entire input ST volume, an output ST volume is generated. At each slide stop, we multiply each input pixel value by the kernel element covering it and take a sum of the products. This is effectively a weighted sum of all the input (room) ele- ments covered by the kernel (brick), with the covering kernel elements serving as the weights. Let’s denote the input ST volume by S. It is a 3D grid whose domain is S = [0, T −1] × [0, H −1] × [0, W −1] Every point in S is a pixel with a color value (which can be a scalar—a gray-level value—or a vector of three values, R, G, B. On this grid of input points, we define a subgrid So of output points. So is obtained from S by applying stride-based stepping on the input. Assuming ®s = [sT , sK, sW ] denotes the 3D stride vector, the first slide stop has the top-left corner of the brick at ®p0 ≡(t = 0, y = 0, x = 0). The next slide stop is at ®p1 ≡(t = 0, y = 0, x = sW ), and the next is at ®p2 ≡(t = 0, y = 0, x = 2sW ). When we reach the right end, we increment y. When we reach the bottom, we increment t. When we reach the end of the room, we stop. So =  ®p0, ®p1, · · · , are the points at which the top-left corner of the kernel (brick) rests as it sweeps over the input volume. There is an output for each point in So. The kernel also has three dimensions (in practice, it has two more dimensions corresponding to the input channels and batch—we are ignoring them now for simplicity—as discussed in section 10.6). 370 CHAPTER 10 Convolutions in neural networks x = 0, y = 0, t = 0 Input ST volume (H × W × T) Output ST volume (oH × oW × oT) Kernel (kH × kW × kT) (a) slide stop x = 0, y = 0, t = 0. x = 0, y = 0, t = 3 Input ST volume (H × W × T) Output ST volume (oH × oW × oT) Kernel (kH × kW × kT) (b) slide stop x = 0, y = 0, t = 0. x = 0, y = 0, t = T – 1 Input ST volume (H × W × T) Output ST volume (oH × oW × oT) Kernel (kH × kW × kT) (c) slide stop x = 0, y = 0, t = 0. x = 2, y = 1, t = 3 Input ST volume (H × W × T) Output ST volume (oH × oW × oT) Kernel (kH × kW × kT) (d) slide stop x = 0, y = 0, t = 0. Figure 10.13 Spatio-temporal view of 3D convolution. The larger, light-shaded cuboid on the left of each figure represents the input ST volume (room). The small, dark-shaded cuboid inside the room represents the kernel (brick). The brick slides all over the room’s internal volume. Neighboring positions of the brick may overlap in volume. Each position of the brick represents a slide stop; a weighted sum is taken of all points in the room (input points) covered by the brick. The brick point (kernel value) covering each input point serves as the weight. Four different slide stops are shown. Each slide stop generates a single output point. As the brick sweeps the input volume, an output ST volume (the smaller light-shaded cuboid) is generated. Equation 10.7 shows how a single output value is generated in 3D convolution. X denotes the input, Y denotes the output, and W denote the kernel weights: Yt,y,x = kT Õ k=0 kH Õ i=0 kW Õ j=0 Xt+k,y+i,x+jWk,i, j ∀(t, y, x) ∈So (10.7) Note that the kernel (brick) has its origin on Xt,y,x. Its dimensions are (kT , kH , kW ). So, it covers all input pixels in the domain [t.. (t + kT)] × [y.. (y + kH)] × [x.. (x + kW )]. These are the pixels participating in equation 10.7. Each of these input pixels is multiplied by the kernel element covering it. Match equation 10.7 with figure 10.13. 10.4.1 Video motion detection via 3D convolution A moving object in a dynamic scene changes position from one video frame to another. Consequently, pixels are covered or uncovered at the boundary of motion. Pixels belonging to the background in one frame may be covered by the object in a subsequent 10.4 Three-dimensional convolution 371 frame and vice versa. If the background is a different color than the object, this will cause a color difference between pixels at identical spatial locations at different times, as illustrated in figure 10.14. The output of applying convolution to an ST volume is another ST volume. Figure 10.15 shows a few frames from the output resulting from applying our video motion detector to the input shown in figure 10.14. Figure 10.14 Successive frames of a synthetic video of a moving ball, shown in a superimposed fashion with gradually increasing opacity for illustration purposes How does a kernel extract motion information from a set of successive frames? As mentioned earlier, motion causes pixels at the same position in successive frames to have different colors. However, a single isolated pair of pixels may have different colors due to noise—we cannot draw any conclusions from that. If we average the pixel values in a small neighborhood in one frame and average the pixel values in the same neighborhood in the subsequent frames, and these two averages are different, that is a more reliable way to estimate motion. Following is a 2 × 3 × 3 3D kernel to do exactly that—average pixel values in a 3 × 3 spatial neighborhood in two successive frames and subtract one from the other: kernel weights, t=0 z }| {  w0,0,0 = −1 w0,0,1 = −1 w0,0,2 = −1 w0,1,0 = −1 w0,1,1 = −1 w0,1,2 = −1 w0,2,0 = −1 w0,2,1 = −1 w0,2,2 = −1  | {z } negative spatial average kernel weights, t=1 z }| {  w1,0,0 = 1 w1,0,1 = 1 w1,0,2 = 1 w1,1,0 = 1 w1,1,1 = 1 w1,1,2 = 1 w1,2,0 = 1 w1,2,1 = 1 w1,2,2 = 1  | {z } positive spatial average | {z } temporal difference of spatial averages; motion detector kernel The result of the subtraction is high in regions of motion and low in regions of no motion. In this context, it is worthwhile to note that since the object is of uniform color, pixels within the object are indistinguishable. Consequently, no motion is observed at the center of the object; motion is observed only at the boundary. A few individual frames of the result of this 3D convolution are shown in figure 10.15. 372 CHAPTER 10 Convolutions in neural networks (a) Output frame 0 (b) Output frame 1 (c) Output frame 2 (d) Output frame 3 Figure 10.15 Result of applying a 3D convolution motion detector to the synthetic video of a moving ball. Gray signifies “no motion”; most of the output frames are gray. White and black signify motion. NOTE Fully functional code for video motion detection, executable via Jupyter Notebook, can be found at http://mng.bz/enJQ. 10.4.2 PyTorch: Three-dimensional convolution with custom weights In section 10.4.1, we saw how to detect motion in a sequence of input images using 3D convolutions. In this section, we see how to implement this in PyTorch. The PyTorch interface to 3D convolutions expects 5-dimensional input tensors of the form N × C × D × H ×W . In addition to the dimensions discussed in section 10.4, there is an additional 10.4 Three-dimensional convolution 373 dimension for the input channels. Thus, there is a separate brick for each input channel. We are combining (taking the weighted sum of) them all: As discussed in the case of 2D convolutions (section 10.3.3), the first dimension N stands for the batch size (minibatches are fed to a real neural network instead of individual input instances for efficiency reasons), and C stands for the number of input channels. D stands for the sequence length. In our motion detector example, D represents the number of successive image frames fed to the 3D convolution layer. The third dimension, H, stands for height, and the fourth dimension, W, stands for width. In our motion detector example, we have a sequence of five grayscale images as input, each with height = 320 and width = 320. Since we are considering only a single image sequence, N = 1. All images are grayscale, which implies that C = 1. The sequence length, D, is equal to 5. H and W are both 320. PyTorch expects the 3D kernels to be of the form Cout × Cin × kT × kH × kW : The first dimension, Cout, represents the number of output channels. You can think of the convolutional kernel as a bank of 3D filters, where each filter produces one output channel. Cout is the number of 3D filters in the bank. The second dimension, Cin, represents the number of input channels. This depends on the number of channels in the input tensor. When we are dealing with grayscale images, Cin is 1 at the grand input. For RGB images, Cin is 3 at the grand input. For layers further from the input, Cin equals the number of channels in the tensor fed to that layer. The third, fourth, and fifth dimensions, kT, kH, and kW , represent the kernel sizes along the T, H, and W dimensions, respectively In our motion detector example, we have a single kernel with kT=2, kH=3, and kW = 3. Since we only have a single kernel, Cout = 1. And since we are dealing with grayscale images, Cin is also 1. Listing 10.6 PyTorch code for 3D convolution import torch images = load_images() Loads a sequence of five grayscale images with shape 320 × 320 x = torch.tensor(images) Converts to a tensor of shape T × H ×W = 5 × 320 × 320 w_2d_smoothing = torch.tensor( Instantiates a 2D smoothing kernel of shape 3 × 3. Pads an extra dimension so that two 2D kernels can be stacked together to form a 3D kernel. [[0.11, 0.11, 0.11], [0.11, 0.11, 0.11], [0.11, 0.11, 0.11]]).unsqueeze(0) 374 CHAPTER 10 Convolutions in neural networks w = torch.cat( [-w_2d_smoothing, w_2d_smoothing]) Concatenates the 2D smoothing kernel and its inverted version along the first dimension to form a 3D kernel of shape 2 × 3 × 3 x = x.unsqueeze(0).unsqueeze(0) Converts the input tensor to N × C ×T × H ×W = 1 × 1 × 5 × 320 × 320 w = w.unsqueeze(0).unsqueeze(0) Converts the 3D kernel to Cout × Cin × kT × kH × kW = 1 × 1 × 2 × 3 × 3 conv3d = nn.Conv3d(1, 1, kernel_size=[2, 3, 3], Instantiates and sets the weights of the Conv3d layer stride=1, padding=0, bias=False) conv3d.weight = torch.nn.Parameter(w, requires_grad=False) with torch.no_grad(): Instructs PyTorch to not compute gradients since we currently don’t require them y = conv3d(x) Runs the convolution 10.5 Transposed convolution or fractionally strided convolution As usual, we examine this topic with an example. Consider a 1D convolution with kernel ®w = h w0 w1 w2 i of size 3, with valid padding. Let’s consider a special case where the input size n is 5. Following equation 10.2, this convolution can be expressed as a multiplication of a block-diagonal matrix W constructed from the weights vector ®w, with input vector ®x as follows: ®y = ®w ⊛®x =W ®x =  w0x0 +w1x1 +w2x2 w0x1 +w1x2 +w2x2 w0x2 +w1x32 +w2x4  =  w0 w1 w2 0 0 0 w0 w1 w2 0 0 0 w0 w1 w2   x0 x1 x2 x3 x4  What happens if we multiply the output vector ®y by the transposed matrix W T? ˜x =W T ®y =  w0 0 0 w1 w0 0 w2 w1 w0 0 w2 w1 0 0 w2   y0 y1 y2  =  w0y0 w1y0+ w0y1 w2y0+ w1y1+ w0y2 w2y1+ w1y2 w2y2  10.5 Transposed convolution or fractionally strided convolution 375 Following are some observations: We haven’t quite recovered ®x from ®y, but we have generated a vector, ˜x, the same size as ®x. Multiplying by the transpose of the weight matrix of the convolution performs a kind of upsampling, undoing the downsampling resulting from the forward convolution. It is impossible to recover ®x from ®y. This is because when constructing ®y from ®x, we multiplied by W and converted a vector with five independent elements to a vector with three independent elements—some information was irretrievably lost. This intuition is consistent with the fact that a 5 × 3 matrix W is non-invertible: there is no W −1, so there is no way to get ®x =W −1®y. During transpose convolution, we are distributing elements of ®y back to the ele- ments of ˜x in the same proportion as when we were doing the forward convolution (see figure 10.16). This should remind you of backpropagation from chapter 8. There, in equation 8.24 (right-hand side), we saw that for linear layers, forward propagation amounts to multiplying by an arbitrary weight matrix W(shown in equation 8.8). Backpropagation involves multiplying by the transpose of the same weight matrix (equation 8.31). The backpropagation does a proportional blame distribution—the loss is distributed back to the inputs in the same proportion as their contribution in creating the output. The same thing is happening here. Thus, multiplying by the transposed weight matrix in general distributes the output back in the same ratio in which it contributes to the output. The idea extends to higher dimensions. Figure 10.17 illustrates a 2D transpose convolu- tion operation. 10.5.1 Application of transposed convolution: Autoencoders and embeddings Transposed convolution is typically required in autoencoders. We provide a very brief outline of autoencoders at this point to explain why they need transposed convolution. Most of the neural networks we have looked at so far are examples of supervised classifiers in that they take an input and directly output the class to which the input belongs. This is not the only paradigm possible. As hinted in section 6.9, we can also map an input to a vector (often called the embedding, aka descriptor vector) that captures the essential aspects of the class of interest and throws away the variable aspects. For instance, if the class of interest is a human, then given an image, the embedding will only capture the features that recognize the humans in the image and ignore the background (sky, sea, forest, building, and so on). The mapping from input to embedding is done by a neural network called an encoder. If the input is an image, the encoder typically contains a sequence of convolution layers. How do we train this neural network? How do we define its loss? Well, one possibility is that the embedding must maintain fidelity to the original input: that is, we should be able to reconstruct (at least approximately) the input from the embedding. Remember, the embedding is smaller in size (with fewer degrees of freedom) than the input, so perfect reconstruction is impossible. Still, we can define loss as the difference (for 376 CHAPTER 10 Convolutions in neural networks Figure 10.16 1D convolution and its transpose example, Euclidean distance or binarized cross-entropy loss) between the original input and the reconstructed input. How do we reconstruct the input from the embedding? This is where transposed convolution comes in. Remember, we did convolution (perhaps many times) in our encoder to generate the embedding. We can do a set of transposed convolutions on the embedding to generate a tensor of the same size as the input. The network to do this reconstruction is called the decoder. The decoder generates our reconstructed input. 10.5 Transposed convolution or fractionally strided convolution 377 1 3 2 4 5 7 6 8 Input Kernel 1 3 2 4 5 7 6 8 Input Kernel 5 15 10 20 6 18 12 24 1 3 2 4 5 7 6 8 Input Kernel 1 3 2 4 5 7 6 8 Input Kernel 1 3 2 4 5 7 6 8 Input Kernel 7 14 21 28 8 16 24 32 Step 0 Step 1 Step 2 Step 3 z 5 22 16 60 12 40 21 52 32 Output Figure 10.17 2D convolution and its transpose We define a loss as the difference between the original and reconstructed input. We can train to minimize the loss and learn the weights of both the encoder and decoder. This is called end-to-end learning, and the encoder-decoder pair is called an autoencoder. We train the autoencoder with many data instances, all belonging to the class of interest. Since it does not have the luxury of remembering the entire image (the embedding being smaller in size than the input), it is forced to learn how to retain the features common to all the training images: that is, the features that describe the class of interest. In our example, the autoencoder will learn to retain features that identify a human and drop the background. Note that this could also lead to a very effective compression technique—the embedding is a compact representation of the image in which only the objects of interest have been retained. 10.5.2 Transposed convolution output size The output size of transposed convolution can be obtained by inverting equation 10.4: o ′ =  n ′ −1  s + k −2p (10.8) For instance, transposed convolution with stride s = 1 on a ®y of size n ′ = 3 with valid padding (p = 0) and a kernel of size k = 3 creates an output ˜x of size o ′ = 5. 378 CHAPTER 10 Convolutions in neural networks 10.5.3 Upsampling via transpose convolution In the previous section, we briefly discussed autoencoders, where an encoder network maps an input image into an embedding and a decoder network tries to reconstruct the input image from the embedding. The encoder network converts a higher-resolution input into a lower-resolution embedding by passing the input through a series of convolution and pooling layers (we discuss pooling layers in detail in the next chap- ter). The decoder network, which tries to reconstruct the original image from the embedding, has to upscale/upsample a lower-resolution input into a higher-resolution output. Many interpolation techniques, such as nearest neighbor, bilinear, and bicubic interpolation, can be used to perform this upsampling operation. These techniques typically use predefined mathematical functions to map lower-resolution inputs to higher-resolution outputs. However, a more optimal way to perform upsampling is through transpose convolutions, where the mapping function is learned during the training process instead of being predefined. The neural network will learn the best way to distribute the input elements across a higher-resolution output map so that the final reconstruction error is minimized (that is, the final output is as close to the original input image as possible). We do not get into the details of training an autoencoder in this chapter; however, we show how input images can be upsampled using transpose convolutions: The input array is converted to a 4D tensor of shape N × Cin × H × W, where N is the batch size, Cin is the number of input channels, H is the height, and W is the width. The kernel is a 4D tensor of shape Cin × Cout × kH × kW , where Cin is the number of input channels, Cout is the number of output channels, kH is the kernel height, and kW is the kernel width. Note how this differs from the regular 2D convolutional kernel, which is expected to be of shape Cout × Cin × kH × kW . Essentially, the input and output channel dimensions are interchanged. Figure 10.18 shows an example with input of shape 1 × 1 × 2 × 2. The kernel is of shape 1 × 1 × 2 × 2. Transpose convolution with stride 2 results in an output of shape 1 × 1 × 4 × 4. NOTE Fully functional code for transpose convolution, executable via Jupyter Notebook, can be found at http://mng.bz/radD. Listing 10.7 PyTorch code for upsampling using transpose convolutions import torch x = torch.tensor([ Instantiates the input tensor [5., 6.], [7., 8.] ]) 10.5 Transposed convolution or fractionally strided convolution 379 w = torch.tensor([ Instantiates the weights of the kernel [1., 2.], [3., 4.] ]) x = x.unsqueeze(0).unsqueeze(0) Converts the input tensor to N × Cin × H × W = 1 × 1 × 2 × 2 w = w.unsqueeze(0).unsqueeze(0) Converts the kernel to Cin × Cout × kH × kW = 1 × 1 × 2 × 2 transpose_conv2d = torch.nn.ConvTranspose2d( Instantiates the transpose convolution layer 1, 1, kernel_size=2, stride=2, bias=False) transpose_conv2d.weight = torch.nn.Parameter(w, Sets the kernel weights requires_grad=False) with torch.no_grad(): Instructs PyTorch to not compute gradients since we currently don’t require them y = transpose_conv2d(x) Runs the transpose convolution. y is of shape 4 × 4. 1 3 2 4 5 7 6 8 Input Kernel 1 3 2 4 5 7 6 8 Input Kernel 5 15 10 20 1 3 2 4 5 7 6 8 Input Kernel 1 3 2 4 5 7 6 8 Input Kernel 1 3 2 4 5 7 6 8 Input Kernel Step 0 Step 1 Step 2 Step 3 Output 6 18 12 24 7 14 21 28 8 16 24 32 5 15 10 20 6 18 7 14 8 12 24 16 21 28 24 32 Figure 10.18 Upscaling using 2D transpose convolution with stride 2 380 CHAPTER 10 Convolutions in neural networks 10.6 Adding convolution layers to a neural network Until now, we have been discussing convolution layers with custom weights that we set. While this gives us a conceptual understanding of how convolution works, in real neural networks, we do not set the convolution weights ourselves. Rather, we expect the weights to be learned from loss minimization via backpropagation, as described in chapters 8 and 9. We look at popular neural network architectures in the next chapter. But from a programming point of view, the most important thing to learn is how to add a convolution layer to a neural network. This is what we learn in the following section. As part of setting up the neural network, we specify its dimensions but not the weights. We also initialize the weight values. The weight values are updated during the backpropagation (the loss.backward() call) somewhat behind the scene (although PyTorch allows us to view their values if we choose to). 10.6.1 PyTorch: Adding convolution layers to a neural network Let’s see how a convolutional layer is implemented as part of a larger neural network in PyTorch (the full neural network architecture is discussed in detail in the next chapter): A neural network typically subclasses the torch.nn.Module base class and imple- ments the forward() method. The layers of the neural network are instantiated in the __init__() function. torch.nn.Sequential is used to chain multiple layers one after another. The output of the first layer is fed into the second layer, and so on. Each torch.nn.Conv2d() represents a single convolutional layer. Our code snippet instantiates three such convolutional layers with other layers in between (details are covered in the next chapter). Listing 10.8 PyTorch code for a sample convolutional neural network import torch class SampleCNN(torch.nn.Module): def __init__(self, num_classes): super(LeNet, self).__init__() self.nn = torch.nn.Sequential( torch.nn.Sequential is used to chain a sequence of layers together. torch.nn.Conv2d( in_channels=1, out_channels=6, kernel_size=5, stride=1), Instantiates the convolutional layer ... torch.nn.Conv2d( in_channels=6, out_channels=16, kernel_size=5, stride=1), ... 10.7 Pooling 381 torch.nn.Conv2d( in_channels=16, out_channels=120, kernel_size=5, stride=1), Implements the forward pass ... ) def forward(self, x): Runs the convolution out = self.nn(x) return out 10.7 Pooling Until now, we have seen how a convolution layer slides over an input image and generates an output feature map that contains important features that describe the image. We looked at this in 1D, 2D, and 3D settings. In a typical deep neural network, multiple such convolution layers are stacked one after another to recognize more and more complex structures in the image. (We talk more about this in the next chapter.) A major drawback of the convolution layer is that it is very sensitive to the location of the features in the input. Minor variations in the position of input features can result in a different output feature map. Such variations can occur in the real world due to camera angle changes, rotations, crops, objects being present at varying distances from the camera, and so on. How do we handle such variations and make the neural network more robust? One way to do so is via downsampling. A lower-resolution version of the feature map still contains the important features but at a lower precision/granularity. So even if important features are present at slightly varying locations in higher-resolution feature maps, they will be more or less at the same location in the lower-resolution feature maps. This is also known as local translation invariance. In convolution neural networks, the downsampling operation is performed by pooling layers. Pooling layers essentially slide a small filter across the entire image. At each filter location, they capture a summary of the local patch using a pooling operation. The two most popular types of pooling operations are as follows: Max pooling—Calculates the maximum value for each patch Average pooling—Calculates the average value for each patch Figure 10.19 illustrates this in detail. The size of the output feature map depends on the kernel size and the stride of the pooling layer. For example, if we use a 2 × 2 kernel with a stride of 2, as in figures 10.19 and 10.20, the output feature map becomes half the size of the input feature map. Similarly, using a 3 × 3 kernel with stride = 3 makes the output feature map one-third the size. 382 CHAPTER 10 Convolutions in neural networks Input Output Step 0 31 43 57 25 38 50 19 31 44 12 26 39 19 18 12 6 0 Input Output Step 1 19 44 Input Output Step 2 19 31 44 Input Output Step 3 19 31 44 57 31 43 57 25 38 50 19 31 44 12 26 39 18 12 6 0 31 43 57 25 38 50 19 31 44 12 26 39 18 12 6 0 31 43 57 25 38 50 19 31 44 12 26 39 18 12 6 0 Figure 10.19 Max pooling using a 2 × 2 kernel with stride 2. The resulting output feature map is half the size of the input feature map. Each value of the output feature map is a max of the corresponding local patch in the input feature map. Listing 10.9 PyTorch code for max and average pooling import torch X = torch.tensor([ Instantiates a 4 × 4 input tensor [0, 12, 26, 39], [6, 19, 31, 44], [12, 25, 38, 50], [18, 31, 43, 57] ], dtype=torch.float32).unsqueeze(0).unsqueeze(0) max_pool_2d = torch.nn.MaxPool2d( Instantiates a 2 × 2 max pooling layer with stride 2 kernel_size=2, stride=2) out_max_pool = max_pool_2d(X) Output feature map is of size 2 × 2 avg_pool_2d = torch.nn.AvgPool2d( Instantiates a 2 × 2 average pooling layer with stride 2 kernel_size=2, stride=2) out_avg_pool = avg_pool_2d(X) Output feature map is of size 2 × 2 Summary 383 Input Output Step 0 31 43 57 25 38 50 19 31 44 12 26 39 9.25 18 12 6 0 Input Output Step 1 9.25 35 Input Output Step 2 9.25 21.5 25 Input Output Step 3 9.25 21.5 35 47 31 43 57 25 38 50 19 31 44 12 26 39 18 12 6 0 31 43 57 25 38 50 19 31 44 12 26 39 18 12 6 0 31 43 57 25 38 50 19 31 44 12 26 39 18 12 6 0 Figure 10.20 Average pooling using a 2 × 2 kernel with stride 2. The resulting output feature map is half the size of the input feature map. Each value of the output feature map is an average of the corresponding local patch in the input feature map. Summary In this chapter, we took an in-depth look at 1D, 2D, and 3D convolutions and their application to image and video analysis: Convolutional layers help capture local patterns in input data because they connect only a small set of adjacent input values to an output value. This is different from the fully connected layers (aka linear layers) discussed in the previous chapters, where all inputs are connected to every output value. A convolution operation involves sliding a kernel over an input array. It can con- ceptually be viewed as a matrix multiplication (though it is not implemented this way for efficiency reasons). The kernel size, stride, and padding affect the size of the output. The number of input elements over which the kernel slides upon completing a single step is known as stride. As the kernel reaches the extremities of the input array, parts of it may fall outside the array. To deal with such cases, multiple padding strategies can be applied. In 384 CHAPTER 10 Convolutions in neural networks valid padding, the convolution operation stops when even a single kernel element falls outside the input array. In same (zero) padding, an input value of zero is assumed for all kernel elements that are outside the input array. 1D convolutions can conceptually be viewed as sliding a measuring ruler (1D ker- nel) across a stretched, straightened rope (1D input array). Real-world applications of 1D convolutions include smoothing and edge detection in curves. 2D convolutions can conceptually be viewed as sliding a tile (2D kernel) over the entire surface area of a wall (2D input array). Real-world applications of 2D convolutions include smoothing and edge detection in images. 3D convolutions can conceptually be viewed as sliding a brick (3D kernel) over the entire volume of a room (3D input array). Real-world applications of 3D convolutions include motion detection in an image sequence. In transpose convolutions, the input array elements are multiplied by the kernel weights and then distributed across the output array. Real-world applications of transpose convolutions include upsampling, where lower-resolution inputs are con- verted into higher-resolution outputs. Autoencoders use transpose convolutions to reconstruct images from embeddings. Pooling layers essentially slide a kernel across the input, capturing a summary of the local patch at each kernel location. They help improve the robustness of convolutional neural networks to minor variations in input features. The two most popular pooling operations are max pooling (calculates the maximum value of the local patch) and average pooling (calculates the average value of the local patch). Pooling layers result in downsampling of the input array. The output size depends on the size and stride of the pooling kernel. 11 Neural networks for image classification and object detection This chapter covers Using deeper neural networks for image classification and object detection Understanding convolutional neural networks and other deep neural network architectures Correcting imbalances in neural networks If a human is shown the image in figure 11.1, they can instantly recognize the objects in it, categorizing them as a bird, a plane, and Superman. In image classification, we want to impart this capability to computers—the ability to recognize objects in an image and classify them into one or more known and predetermined categories. Apart from identifying the object categories, we can also identify the location of the objects in the image. An object’s location can be described by a bounding box: a rectangle whose sides are parallel to coordinate axes. A bounding box is typically specified by four parame- ters: [(xtl, ytl), (xbr, ybr)], where (xtl, ytl) are the xy coordinates of the top-left cor- ner and (xbr, ybr) are the xy coordinates of the bottom-right corner of the bounding 385 386 CHAPTER 11 Neural networks for image classification and object detection Figure 11.1 Is it a bird? Is it a plane? Is it Superman? box. The problem of identifying and categorizing the objects present in the image is called image classification. If we also want to identify their location in the image, it is referred to as object detection. Image classification and object detection are some of the most fundamental problems in computer vision. While the human brain can both classify and localize objects in images almost intuitively, how do we train a machine to do this? Before deep learning, computer vision techniques involved hand-crafting image features (to encode color, edges, and shapes) and designing rules on top of these features to classify/localize objects. However, this is not a scalable approach because images are extremely complex and varied. Think of a simple object like an automobile. It can come in various sizes, shapes, and colors. It can be seen from afar or close (scales), from various viewpoints (perspectives), and on a cloudy day or a sunny day (lighting conditions). The car can be on a busy street or a mountain road (backgrounds). It is nearly impossible to engineer features and rules that can handle all such variations. Over the last 10 years, a new class of algorithms has emerged: convolutional neural networks (CNNs). They do not rely on hand-engineered features but instead learn the relevant features from data. These models have shown tremendous success in several computer vision tasks, achieving (and sometimes even surpassing) human-level accuracy. They are increasingly used in the industry for applications ranging from medical diagnostics to e-commerce to manufacturing. In this chapter, we detail some of the most popular deep neural network architectures used for image classification and object detection. We look at some of their salient features, take a deep dive into the architectural details to understand how and why they work, and apply them to real-world problems. NOTE Fully functional code for this chapter, executable via Jupyter Notebook, can be found at http://mng.bz/vojq 11.1 CNNs for image classification: LeNet In chapter 10, we discussed the convolution operation in 1D, 2D, and 3D scenarios. We also saw how to implement a single convolutional layer as part of a larger neural network. This section shows how a neural network with multiple convolutional layers can be used 11.1 CNNs for image classification: LeNet 387 for image classification. (If needed, you are encouraged to revisit chapter 10.) For this purpose, let’s consider the MNIST data set, a large collection of handwritten digits (0 through 9). It contains a training set of 60,000 images and a test set of 10,000 images. Each image is 28 × 28 in size and contains a center crop of a single digit. Figure 11.2 shows sample images from the MNIST data set. Figure 11.2 Sample images from the MNIST data set. (Source: “Gradient-based learning applied to document recognition”; http://mng.bz/Wz0a.) We’d like to build a classifier that takes in a 28 × 28 image as input and emits a label from 0 to 9 based on the digit contained in the image. One of the most popular neural network architectures for this task is the LeNet, which was proposed by LeCun et al. in their 1998 paper, “Gradient-based learning applied to document recognition” (http://mng.bz/Wz0a). The LeNet architecture is illustrated in figure 11.3 (LeNet expects input images of size 32 × 32, so the 28 × 28 MNIST images are resized to 32 × 32 before being fed into the network): It consists of three convolutional layers with 5 × 5 kernels convolved with a stride of 1. The first convolution layer produces 6 feature maps of size 28 × 28, the Input 32×32 C1: f. maps 16@10×10 C1: feature maps 6@28×28 S2: f. maps 6@14×14 S4: f. maps 16@5×5 C5: layer 120 Output 10 F6: layer 84 Figure 11.3 LeNet. (Source: “Gradient-based learning applied to document recognition”; http://mng.bz/Wz0a.) 388 CHAPTER 11 Neural networks for image classification and object detection second convolution layer produces 16 feature maps of size 10 × 10, and the third convolution layer produces 120 feature maps of size 1 × 1 (which are flattened into a 120-dimensional vector) The first two convolutional layers are followed by subsampling (aka pooling) layers, which perform a local averaging and subsampling of the feature map, thus reducing the resolution of the feature map and the sensitivity of the output to shifts and distortions in the input. A pooling kernel of size 2 × 2 is applied, reducing the feature map size to half its original size. Refer to section 10.7 for more about pooling. Every feature map is followed by a tanh activation layer. This introduces nonlinearity into the network, increasing its expressive power because it can now model the output as a nonlinear combination of the inputs. If we did not have a nonlinear activation function, no matter how many layers we had, the neural network would still behave as a single-linear-layer network because the combination of multiple linear layers is just another linear layer. While the original LeNet paper used tanh as the activation function, several activation functions such as ReLU and sigmoid can also be used. ReLU is discussed in detail in section 11.2.1. Detailed discussions of sigmoid and tanh can be found in sections 8.1 and 8.1.2. The output feature map is passed through two fully connected (FC, aka linear) layers, which finally produce a 10-dimensional logits vector that represents the score for every class. The logits scores are converted into probabilities using the softmax layer. CrossEntropyLoss, discussed in section 6.3, is used to compute the difference between the predicted probabilities and the ground truth. NOTE A feature map is a 2D array of points (that is, a grid) with a fixed-size vector associated with every point. An image is an example of a feature map, with each point being a pixel and the associated vector representing the pixel’s color. A convolution layer transforms an input feature map into an output feature map. The output feature map usually has smaller width and height but a longer per-point vector. The LeNet performs very well on the MNIST data set, achieving test accuracies greater than 99%. A PyTorch implementation of LeNet is presented next. 11.1.1 PyTorch: Implementing LeNet for image classification on MNIST NOTE Fully functional code for training the LeNet, executable via Jupyter Note- book, can be found at http://mng.bz/q2gz. 11.2 Toward deeper neural networks 389 Listing 11.1 PyTorch code for the LeNet import torch class LeNet(torch.nn.Module): def __init__(self, num_classes): super(LeNet, self).__init__() self.conv1 = torch.nn.Sequential( torch.nn.Conv2d( in_channels=1, out_channels=6, 5 × 5 conv kernel_size=5, stride=1), torch.nn.Tanh(), Tanh activation torch.nn.AvgPool2d(kernel_size=2)) 2 × 2 average pooling self.conv2 = torch.nn.Sequential( torch.nn.Conv2d( in_channels=6, out_channels=16, kernel_size=5, stride=1), torch.nn.Tanh(), torch.nn.AvgPool2d(kernel_size=2)) self.conv3 = torch.nn.Sequential( torch.nn.Conv2d( in_channels=16, out_channels=120, kernel_size=5, stride=1), torch.nn.Tanh()) self.fc1 = torch.nn.Sequential( torch.nn.Linear( in_features=120, out_features=84), First FC layer torch.nn.Tanh()) self.fc2 = torch.nn.Linear( in_features=84, out_features=num_classes) Second FC layer def forward(self, X): X.shape: N × 3 × 32 × 32. N is the batch size. conv_out = self.conv3(self.conv2(self.conv1(X))) batch_size = conv_out.shape[0] conv_out = conv_out.reshape(batch_size, -1) conv_out.shape: N × 120 × 1 × 1 logits = self.fc2(self.fc1(conv_out)) logits.shape: N × 10 return logits def predict(self, X): logits = self.forward(X) probs = torch.softmax(logits, dim=1) Computes the probabilities using softmax return torch.argmax(probs, 1) 11.2 Toward deeper neural networks The LeNet model is not a very deep network since it has only three convolutional layers. While this is sufficient to achieve accurate results on a simple data set like MNIST, it doesn’t work well on real-world image classification problems since it does not have 390 CHAPTER 11 Neural networks for image classification and object detection enough expressive power to model complex images. So, we typically go for much deeper neural networks with multiple convolutional layers. Adding more layers does the following: Brings extra expressive power due to extra nonlinearity—Since every layer brings with it a new set of learnable parameters and extra nonlinearity, a deeper network can model more complex relationships between input data elements. Lower layers typically learn simpler features of the object, like lines and edges, whereas higher layers learn more abstract features of the object, like shapes or sets of lines. Achieves the same reach with fewer parameters—Let’s examine this via an example. Consider two output feature maps, one produced by a single 5 × 5 convolution on the input and another produced by two 3 × 3 convolutions applied one after another in sequence on the input. Assume a stride of 1 and the same (zero) padding. Figure 11.4 illustrates this scenario. Consider a single grid point in the output feature map. In both cases, the output value of the grid point is derived from a 5 × 5 patch in the input. We say the indicated 5 × 5 input patch is the receptive field of the output grid points. Thus, in both cases, the output grid point is a digest of the same input: that is, it expresses the same information. However, in the deeper network, there are fewer parameters. The number of parameters in a single 5 × 5 filter is 25, whereas that in two 3 × 3 filters is 2 × 9 = 18 (assuming a single channel input image). This is a 38% difference. Similarly, if we compare one 7 × 7 filter with three 3 × 3 filters, they have the same receptive field, but the 7 × 7 filter has 81% more parameters than the 3 × 3 filter. 1 conv layer with 5×5 filter 2 conv layers with 3×3 filters 5×5 conv 3×3 conv 3×3 conv # parameters: 25 # parameters: 18 Output feature map Input feature map Input feature map Output feature map Intermediate feature map Figure 11.4 A single 5 × 5 convolution layer vs. two 3 × 3 convolution layers 11.2 Toward deeper neural networks 391 Now, let’s look at some of the most popular deep convolutional networks used for image classification. The first deep network that reignited the deep learning revolution was AlexNet, which was published by Krizhevsky et al. in 2012. It significantly outperformed all previous state-of-the-art algorithms on the ImageNet Large Scale Visual Recognition Challenge (ILSVRC), a complex data set with 1.3 million images across 1,000 classes. Since AlexNet, several deep networks have improved on the previous state of the art, such as GoogleNet, VGG, and ResNet. In this chapter, we discuss the key concepts that make each of these networks work. For a detailed review of their architectures, training methodologies, and final results, you are encouraged to read the original papers linked in each section. 11.2.1 VGG (Visual Geometry Group) Net The VGG family of networks was created by the Visual Geometry Group from the University of Oxford (https://arxiv.org/pdf/1409.1556.pdf). Their main contribution was a thorough evaluation of networks of increasing depth using an architecture with very small (3 × 3) convolution filters. They demonstrated that by using 3 × 3 convolutions and networks with 16–19 weight layers, they could outperform previous state-of-the-art results on the ILSVRC-2014 challenge. The VGG network had two main differences compared to prior works: Use of smaller (3 × 3) convolution filters—Prior networks often relied on larger kernels of size 7 × 7 or 11 × 11 in the first convolution layers. VGG instead only used 3 × 3 kernels throughout the network. As discussed in section 11.2, three 3 × 3 filters have the same receptive field as a single 7 × 7 filter. So what does replacing the 7 × 7 filter with three smaller filters buy? – More nonlinearity and hence more expressive power because we have a ReLU activation function applied at the end of every convolution layer – Fewer parameters (49C2 vs. 27C2), which means faster learning and more robustness to overfitting Removal of the local response normalization (LRN) layers—LRN was first introduced in the AlexNet architecture. Its purpose was twofold: to bound the output of the ReLU layer, which is an unbounded function and can produce outputs as large as the training permits; and to encourage lateral inhibition wherein a neuron can suppress the activity of its neighbors (this in effect acts as a regularization). The VGG paper demonstrated that adding LRN layers did not improve accuracy, so VGG chose to remove them from its architecture. The VGG family of networks comes in five different configurations, which mainly differ in the number of layers (VGG-11, VGG-13, VGG-16, and VGG-19). Regardless of the exact configuration, the VGG family of networks follows a common structure. Here, we discuss these commonalities (a detailed description of the differences can be found in the original paper): All architectures work on 224 × 224 input images. 392 CHAPTER 11 Neural networks for image classification and object detection All architectures have five convolutional blocks (conv blocks): – Each block can have multiple convolution layers followed by a max pool layer at the end. – All individual convolution layers use 3 × 3 kernels with a stride of 1 and same padding. Therefore, they don’t change the spatial resolution of the output feature map. – All convolution layers within a single conv block have the same-sized output feature maps. – Each convolution layer is followed by a ReLU layer that adds nonlinearity. – The max pool layer at the end of every conv block reduces the spatial resolution to half. Since each conv block downsamples by 2, the input feature map is reduced 25 (32) times, resulting in an output feature map of size 7 × 7. Additionally, at each conv block, the number of feature maps is doubled. All architectures end with three FC layers: – The first takes a 51,277-sized input and converts it into a 4,096-dimensional output. – The second takes the resulting 4,096-dimensional output and converts it into another 4,096-dimensional output. – The final takes the resulting 4,096-dimensional output and converts it into a C-dimensional output, where C stands for the number of classes. In the case of ImageNet classification, C is 1,000. The architecture diagram for VGG-11 is shown in figure 11.5. The column on the left represents the shape of the input tensor to each layer. The column on the right represents the shape of the output tensor from each layer. RELU NONLINEARITY As we’ve discussed previously, nonlinear layers give the deep neural network more expressive power to model complex mathematical functions. In chapter 8, we looked at two nonlinear functions: sigmoid and tanh. However, the VGG network (like AlexNet) consists of a different nonlinear layer called rectified linear unit (ReLU). To understand the rationale for this choice, let’s revisit the sigmoid function and look at some of its drawbacks. Figure 11.6 plots the sigmoid function along with its derivative. As the plot shows, the gradient (derivative) is maximum when the input is 0, and it quickly tapers down to 0 as the input increases/decreases. This is true for the tanh activation function as well. It means when the output of a neuron (before the sigmoid layer) is either high or low, the gradient becomes small. While this may not be an issue in shallow networks, it becomes a problem in larger networks because the gradients can become too small for training to work effectively. Gradients of neural networks are calculated using backpropagation. By the chain rule, the derivatives of each layer are multiplied down the network, starting from the final layer and moving toward the initial layers. If the gradients at each layer are 11.2 Toward deeper neural networks 393 Conv: 3×3 kernel, depth = 64, ReLU Pooling: 2×2 kernel Conv: 3×3 kernel, depth = 128, ReLU Pooling: 2×2 kernel Conv: 3×3 kernel, depth = 256, ReLU Pooling: 2×2 kernel Conv: 3×3 kernel, depth = 256, ReLU Conv: 3×3 kernel, depth = 512, ReLU Pooling: 2×2 kernel Conv: 3×3 kernel, depth = 512, ReLU Conv: 3×3 kernel, depth = 512, ReLU Pooling: 2×2 kernel Conv: 3×3 kernel, depth = 512, ReLU Linear: 4096D outputs Linear: 1000D outputs Linear: 4096D outputs Softmax Input Input shape (batch size = 32) Output shape 32 × 3 × 224 × 224 32 × 64 × 224 × 224 32 × 64 × 112 × 112 32 × 128 × 112 × 112 32 × 128 × 56 × 56 32 × 256 × 56 × 56 32 × 256 × 56 × 56 32 × 256 × 28 × 28 32 × 512 × 28 × 28 32 × 512 × 28 × 28 32 × 512 × 14 × 14 32 × 512 × 14 × 14 32 × 512 × 14 × 14 32 × 512 × 7 × 7 32 × 4096 32 × 4096 32 × 1000 32 × 1000 32 × 64 × 224 × 224 32 × 64 × 112 × 112 32 × 128 × 112 × 112 32 × 128 × 56 × 56 32 × 256 × 56 × 56 32 × 256 × 56 × 56 32 × 256 × 28 × 28 32 × 512 × 28 × 28 32 × 512 × 28 × 28 32 × 512 × 14 × 14 32 × 512 × 14 × 14 32 × 512 × 14 × 14 32 × 512 × 7 × 7 32 × 4096 32 × 4096 32 × 1000 Figure 11.5 VGG-11 architecture diagram. All shapes are of the form N × C × H × W, where N is the batch size, C is the number of channels, H is the height, and W is the width. 394 CHAPTER 11 Neural networks for image classification and object detection Sigmoid Derivative of sigmoid Figure 11.6 Graph of a 1D sigmoid function (dotted curve) and its derivative (solid curve) small, a small number multiplied by another small number is an even smaller number. Thus the gradients at the initial layers are very close to 0, making the training ineffective. This is known as the vanishing gradient problem. The ReLU function addresses this problem. Figure 11.7 shows a graph of the ReLU function. Its equation is given by ReLU (x) = max(0, x) (11.1) The derivative of ReLU is 1 (constant) when x is greater than 0, and 0 everywhere else. Therefore, it doesn’t suffer from the vanishing gradient problem. Most deep networks today use ReLU as their activation function. The AlexNet paper demonstrated that using ReLU nonlinearity significantly speeds up training because it helps with faster convergence. PYTORCH: VGG Now let’s see how to implement the VGG network in PyTorch. First, let’s implement a single conv block, which is the core component of the VGG net. This conv block will later be repeated multiple times to form the entire VGG network. NOTE Fully functional code for the VGG network, executable via Jupyter Note- book, can be found at http://mng.bz/7WE4. 11.2 Toward deeper neural networks 395 Figure 11.7 Graph of the ReLU function Listing 11.2 PyTorch code for a convolutional block class ConvBlock(nn.Module): def __init__(self, in_channels, num_conv_layers, num_features): super(ConvBlock, self).__init__() modules = [] for i in range(num_conv_layers): modules.extend([ nn.Conv2d( in_channels, num_features, 3 × 3 conv kernel_size=3, padding=1), ReLU nonlinearity nn.ReLU(inplace=True) ]) in_channels = num_features modules.append(nn.MaxPool2d(kernel_size=2)) 2 × 2 max pooling self.conv_block = nn.Sequential(*modules) def forward(self, x): return self.conv_block(x) Next, let’s implement the convolutional backbone (conv backbone) builder, which allows us to create different VGG architectures via simple configuration changes. Listing 11.3 PyTorch code for the conv backbone class ConvBackbone(nn.Module): def __init__(self, cfg): Cfg: [(in_channels, num_conv_layers, num_features),] The different VGG networks can be created without duplicating code by passing in the right cfg. super(ConvBackbone, self).__init__() self.cfg = cfg 396 CHAPTER 11 Neural networks for image classification and object detection self.validate_config(cfg) modules = [] for block_cfg in cfg: Iterates over conv block configurations in_channels, num_conv_layers, num_features = block_cfg modules.append(ConvBlock( Instantiates the conv block defined in listing 11.2 in_channels, num_conv_layers, num_features)) self.features = nn.Sequential(*modules) def validate_config(self, cfg): assert len(cfg) == 5 # 5 conv blocks for i, block_cfg in enumerate(cfg): assert type(block_cfg) == tuple and len(block_cfg) == 3 if i == 0: assert block_cfg[0] == 3 There must be three input channels. else: assert block_cfg[0] == cfg[i-1][-1] out_Features of the previous block should be equal to in_features of the current block. def forward(self, x): return self.features(x) The conv backbone is instantiated with a config that contains the list of configurations for each of the conv blocks. The config for VGG-11 contains fewer layers, whereas that for VGG-19 contains more layers. The output of the conv backbone is fed into the classifier, which consists of three FC layers. Together, the conv backbone and the classifier form the VGG module. Listing 11.4 PyTorch code for the VGG network class VGG(nn.Module): def __init__(self, conv_backbone, num_classes): super(VGG, self).__init__() self.conv_backbone = conv_backbone Backbone network defined in listing 11.3 self.classifier = nn.Sequential( nn.Linear(512 * 7 * 7, 4096), nn.ReLU(True), nn.Dropout(), nn.Linear(4096, 4096), The classifier is made up of three linear Layers. The first two are followed by ReLU nonlinearity. nn.ReLU(True), nn.Dropout(), nn.Linear(4096, num_classes) ) def forward(self, x): conv_features = self.conv_backbone(x) logits = self.classifier( conv_features.view( conv_features.shape[0], -1)) Flattens the conv features before passing it to the classifier return logits 11.2 Toward deeper neural networks 397 A VGG-11 network can be instantiated as follows. Listing 11.5 PyTorch code instantiating a VGG network from a specific config vgg11_cfg = [ Creates the cfg for VGG-11 (3, 1, 64), (64, 1, 128), (128, 2, 256), (256, 2, 512), (512, 2, 512) ] vgg11_backbone = ConvBackbone(vgg11_cfg) Instantiates the conv backbone num_classes = 1000 vgg11 = VGG(vgg11_backbone, num_classes) Instantiates the VGG network While we have discussed how to implement VGG in PyTorch, we don’t do this in practice because the torchvision package already implements the VGG network, along with several other popular deep networks. It is recommended that you use the torchvision implementation, as shown here: import torchvision vgg11 = torchvision.models.vgg11() 11.2.2 Inception: Network-in-network paradigm Previously, we saw how increasing the depth of a neural network—that is, the number of layers—can improve accuracy because it increases the expressive power of the network. Alternatively, we could increase the width of the network—the number of units at each level—to improve accuracy. However, both these methods suffer from two main draw- backs. First, blindly increasing the size of the network can lead to overfitting, wherein the network memorizes certain patterns in the training data that don’t extend well to test data. And second, increased computation resources are required during both training and inference times. The Inception architecture, introduced by Szegedy et al. in their paper ”Going deeper with convolutions” (https://arxiv.org/pdf/1409.4842v1.pdf), aims to address both these drawbacks. The Inception architecture increases the network’s depth and width while keeping the computational budget constant. In this section, we examine the main idea behind the Inception architecture. While there have been several improvements to it (Inception_v2, Inception_v3, Inception_ResNet, and so on), we discuss the original: Inception_v1. Prior deep learning architectures typically stacked convolutional filters sequentially: each layer applied a set of convolutional filters of the same size and passed it to the subsequent layer. The kernel size of the filter at each layer depended on the architecture. But with such an architecture, how do we know we have chosen the right kernel size for each layer? If we are detecting a car, say, the fraction of the image area (that is, the number of pixels) occupied by the car is different in an image taken close up than in one taken from far away. We say the scale of the car object is different in the two images. 398 CHAPTER 11 Neural networks for image classification and object detection Consequently, the number of pixels that must be digested to recognize the car will differ at different scales. A larger kernel is preferred for information at a larger scale, and vice versa. An architecture that is forced to choose one kernel size may not be optimal. The Inception module tackles this problem by having multiple kernels of different sizes at each level and taking weighted combinations of the outputs. The network can learn to weigh the appropriate kernel more than others. The naive implementation of the Inception module performs convolutions on the input using three kernel sizes: 1 × 1, 3 × 3, and 5 × 5. Max pooling is also performed, using a 3 × 3 kernel with stride 1 and padding 1 (for output and input to be the same size). The outputs are concatenated and sent into the next Inception module. See figure 11.8 for details. Previous layer Concat Naive inception block 1×1 conv 3×3 conv 5×5 conv 3×3 max pool Previous layer Concat Dimensionality reduced inception block 1×1 conv 1×1 conv 3×3 max pool 1×1 conv 3×3 conv 1×1 conv 5×5 conv Figure 11.8 Inception_v1 architecture This naive Inception block has a major flaw. Using even a small number of 5 × 5 filters can prohibitively increase the number of parameters. This becomes even more expensive when we add the pooling layer, where the number of output filters equals the number of filters in the previous stage. Thus, concatenating the output of the pooling layer with the outputs of convolutional layers would lead to an inevitable increase in the number of output features. To fix this, the Inception module uses 1 × 1 convolution layers before the 3 × 3 and 5 × 5 filters to reduce the number of input channels. This drastically reduces the number of parameters of the 3 × 3 and 5 × 5 convs. While it may seem counterintuitive, 1 × 1 convs are much cheaper than 3 × 3 and 5 × 5 convs. Additionally, 1 × 1 convolution is applied after pooling (see figure 11.8). A neural network architecture was built using the dimension-reduced Inception module and was popularly known as GoogLeNet. GoogLeNet has nine such Inception modules stacked linearly. It is 22 layers deep (27, including the pooling layers). It uses global average pooling at the end of the last Inception module. With such a deep network, there is always the vanishing gradient problem; to prevent the middle part of the network from “dying out,” the paper introduced two auxiliary classifiers. This is done by applying softmax to the output of two of the intermediate Inception modules and computing an auxiliary loss over the ground truth. The total loss function is a 11.2 Toward deeper neural networks 399 weighted sum of the auxiliary loss and the real loss. You are encouraged to read the original paper to understand the details. PYTORCH: INCEPTION BLOCK Let’s see how to implement an Inception block in PyTorch. We typically don’t do this in practice because end-to-end deep network architectures containing Inception blocks are already implemented in the torchvision package. However, we implement the Inception block from scratch to understand the details. NOTE Fully functional code for the Inception block, executable via Jupyter Note- book, can be found at http://mng.bz/mxn0. Listing 11.6 PyTorch code for a naive Inception block class NaiveInceptionModule(nn.Module): def __init__(self, in_channels, num_features=64): super(NaiveInceptionModule, self).__init__() self.branch1x1 = torch.nn.Sequential( 1 × 1 branch nn.Conv2d( in_channels, num_features, kernel_size=1, bias=False), nn.BatchNorm2d(num_features, eps=0.001), nn.ReLU(inplace=True)) self.branch3x3 = torch.nn.Sequential( nn.Conv2d( 3 × 3 branch in_channels, num_features, kernel_size=3, padding=1, bias=False), nn.BatchNorm2d(num_features, eps=0.001), nn.ReLU(inplace=True)) self.branch5x5 = torch.nn.Sequential( 5 × 5 branch nn.Conv2d( in_channels, num_features, kernel_size=5, padding=2, bias=False), nn.BatchNorm2d(num_features, eps=0.001), nn.ReLU(inplace=True)) self.pool = torch.nn.MaxPool2d( 3 × 3 pooling kernel_size=3, stride=1, padding=1) def forward(self, x): conv1x1 = self.branch1x1(x) conv3x3 = self.branch3x3(x) conv5x5 = self.branch5x5(x) pool_out = self.pool(x) out = torch.cat( Concatenates the outputs of the parallel branches [conv1x1, conv3x3, conv5x5, pool_out], 1) return out 400 CHAPTER 11 Neural networks for image classification and object detection Listing 11.7 PyTorch code for a dimensionality reduced Inception block class Inceptionv1Module(nn.Module): def __init__(self, in_channels, num_1x1=64, reduce_3x3=96, num_3x3=128, reduce_5x5=16, num_5x5=32, pool_proj=32): super(Inceptionv1Module, self).__init__() self.branch1x1 = torch.nn.Sequential( nn.Conv2d( 1 × 1 branch in_channels, num_1x1, kernel_size=1, bias=False), nn.BatchNorm2d(num_1x1, eps=0.001), nn.ReLU(inplace=True)) self.branch3x3_1 = torch.nn.Sequential( 1 × 1 conv in the 3 × 3 branch nn.Conv2d( in_channels, reduce_3x3, kernel_size=1, bias=False), nn.BatchNorm2d(reduce_3x3, eps=0.001), nn.ReLU(inplace=True)) self.branch3x3_2 = torch.nn.Sequential( 3 × 3 conv in the 3 × 3 branch nn.Conv2d( reduce_3x3, num_3x3, kernel_size=3, padding=1, bias=False), nn.BatchNorm2d(num_3x3, eps=0.001), nn.ReLU(inplace=True)) self.branch5x5_1 = torch.nn.Sequential( 1 × 1 conv in the 5 × 5 branch nn.Conv2d( in_channels, reduce_5x5, kernel_size=5, padding=2, bias=False), nn.BatchNorm2d(reduce_5x5, eps=0.001), nn.ReLU(inplace=True)) self.branch5x5_2 = torch.nn.Sequential( 5 × 5 conv in the 5 × 5 branch nn.Conv2d( reduce_5x5, num_5x5, kernel_size=5, padding=2, bias=False), nn.BatchNorm2d(num_5x5, eps=0.001), nn.ReLU(inplace=True)) self.pool = torch.nn.Sequential( Max pooling followed by a 1 × 1 conv torch.nn.MaxPool2d( kernel_size=3, stride=1, padding=1), nn.Conv2d( in_channels, pool_proj, kernel_size=1, bias=False), nn.BatchNorm2d(pool_proj, eps=0.001), nn.ReLU(inplace=True)) def forward(self, x): 11.2 Toward deeper neural networks 401 conv1x1 = self.branch1x1(x) conv3x3 = self.branch3x3_2(self.branch3x3_1((x))) conv5x5 = self.branch5x5_2(self.branch5x5_1((x))) pool_out = self.pool(x) out = torch.cat( Concatenates the outputs of the parallel branches [conv1x1, conv3x3, conv5x5, pool_out], 1) return out 11.2.3 ResNet: Why stacking layers to add depth does not scale We start with a fundamental question: is learning better networks as easy as stacking multiple layers? Consider the graphs in figure 11.9. Figure 11.9 Training error (left) and test error (right) on the CIFAR-10 data set with 20-layer and 56-layer networks. (Source: “Deep residual learning for image recognition”; https://arxiv.org/pdf /1512.03385.pdf.) This image from the ResNet paper “Deep residual learning for image recognition” (https://arxiv.org/pdf/1512.03385.pdf) shows the training and test error rates for two networks: a shallower network with 20 layers and a deeper network with 56 layers, on the CIFAR-10 data set. Surprisingly, the training and test errors are higher for the deeper (56-layer) network. This result is extremely counterintuitive because we expect deeper networks to have more expressive power and hence higher accuracies/lower error rates than their shallower counterparts. This phenomenon is referred to as the degradation problem: with the network depth increasing, the accuracy becomes saturated and degrades rapidly. We might attribute this to overfitting, but that is not the case because even the training errors are higher for the deeper network. Another cause could be vanishing/exploding gradients. However, the authors of the ResNet paper investigated the gradients at each layer and established that they are healthy (not vanishing/exploding). So, what causes the degradation problem, and how do we solve it? Let’s consider a shallower architecture with n layers and a deeper counterpart that adds more layers to it (n + m layers). The deeper architecture should be able to achieve no higher loss than the shallow architecture. Intuitively, a trivial solution is to learn the exact 402 CHAPTER 11 Neural networks for image classification and object detection n layers of the shallow architecture and the identity function for the additional m layers. The fact that this doesn’t happen in practice indicates that the neural network layers have a hard time learning the identity function. Thus the paper proposes ”shortcut/skip connections” that enable the layers to potentially learn the identity function easily. This “identity shortcut connection” is the core idea of ResNet. Let’s look at a mathematical analogy. Let h (x) be the function we are trying to model (learn) via a stack of layers (not necessarily the entire network). It is reasonable to expect that the function g (x) = h (x) −x is simpler than h (x) and hence easier to learn. But we already have x at the input. So if we learn g (x) and add x to it to obtain h (x), we have effectively modeled h (x) by learning the simpler g (x) function. The name residual comes from g(x) = h(x) −x. Figure 11.10 shows this in detail. Plain net: no skip connection Weight layer Weight layer Residual block: skip connection Weight layer Weight layer Skip connection ReLU ReLU ReLU ReLU Figure 11.10 The left column shows a plain network block without skip connections, and the right column shows a residual block with skip connections. Now let’s revisit the earlier problem of degradation. We posited that normal neural network layers generally have difficulty learning the identity function. In the case of residual learning, to learn the identity function, h(x) = x, the layers need to learn g(x)=0. This can easily be done by driving all the layers’ weights to 0. Here is another way to think about it: if we initialize a regular neural network’s weights and biases to be 0 at the start, then every layer starts with the “zero” function: g(x) = 0. Thus, the output of every stack of layers with a shortcut connection, h(x) = g(x) + x, is already the identity function: h(x) = x when g(x) = 0. In real cases, it is important to note that identity mappings are unlikely to be optimal: the network layers will want to learn actual features. In such cases, this reformulation isn’t preventing the network lawyers from doing so; the layers can still learn other functions like a regular stack of layers. We can think of this reformulation as preconditioning, 11.2 Toward deeper neural networks 403 which makes learning the identity function easier if needed. Additionally, by adding skip connections, we allow a direct path for the gradient to flow from layer to layer: the deeper layer has a direct path to x. This allows for better learning as information from the lower layers passes directly into the higher layers. RESNET ARCHITECTURE Now that we have seen the basic building block—a stack of convolutional (conv) layers with a skip connection—let’s delve deeper into the architecture of ResNet. ResNet architectures are constructed by stacking multiple building blocks on top of each other. They follow the same idea as VGG: The convolutional layers mostly have 3 × 3 filters. The layers have the same number of filters for a given output feature-map size. If the feature-map size is halved, the number of filters is doubled to preserve the time complexity per layer. ResNet uses conv layers with a stride of 2 to downsample, unlike VGG, which had multiple max pooling layers. The core architecture consists of the following components: Five convolutional layer blocks—The first convolutional block consists of a 7 × 7 kernel with stride=2, padding=3, and num_features=64, followed by a max pooling layer with a 3 × 3 kernel, stride=2, and padding=1. The feature map size is reduced from (224, 224) to (56, 56). The remaining convolutional blocks (ResidualConvBlock) are built by stacking multiple basic shortcut blocks together. Each basic block uses 3 × 3 filters, as described. Classifier—An average pooling block that runs on top of the conv block output, followed by a FC layer, which is used for classification. You are encouraged to examine the diagrams in the original paper to understand the details. Now, let’s see how to implement a ResNet in PyTorch. PYTORCH: RESNET In this section, we discuss how to implement a ResNet-34 from scratch. Note that this is seldom done in practice. The torchvision package provides ready-made implemen- tations for all ResNet architectures. However, by building the network from scratch, we gain a deeper understanding of the architecture. First, let’s implement a basic skip connection block (BasicBlock) to see how the shortcut connection works. NOTE Fully functional code for ResNet, executable via Jupyter Notebook, can be found at http://mng.bz/5K9q. Listing 11.8 PyTorch code for BasicBlock class BasicBlock(nn.Module): def __init__(self, in_channels, num_features, stride=1, downsample=None): super(BasicBlock, self).__init__() self.conv1 = nn.Sequential( Instantiates two conv layers of filter size 3 × 3 404 CHAPTER 11 Neural networks for image classification and object detection nn.Conv2d( in_channels, num_features, kernel_size=3, stride=stride, padding=1, bias=False), nn.BatchNorm2d(num_features, eps=0.001), nn.ReLU(inplace=True)) self.conv2 = nn.Sequential( nn.Conv2d( num_features, num_features, kernel_size=3, stride=1, padding=1, bias=False), nn.BatchNorm2d(num_features, eps=0.001)) self.downsample = downsample When input and output feature maps are not the same size, the input feature map is downsampled using a 1 × 1 convolution layer. self.relu = nn.ReLU(inplace=True) def forward(self, x): conv_out = self.conv2(self.conv1(x)) identity = x if self.downsample is not None: identity = self.downsample(x) assert identity.shape == conv_out.shape, f''Identity identity.shape and conv out conv_out.shape have different shapes'' out = self.relu(conv_out + identity) Creates a skip connection return out Notice how the output of the residual block is a function of both the input and the output of the convolutional layer: ReLU(conv_out+x). This assumes that x and conv_out have the same shape. (Shortly, we discuss what to do when this isn’t the case.) Also note that adding the skip connections does not increase the number of parameters. The shortcut connections are parameter-free. This makes the solution cheap from a computational point of view and is one of the charms of shortcut connections. Next, let’s implement a residual conv block consisting of a number of basic blocks stacked on top of each other. We have to handle two cases when it comes to basic blocks: Case 1—Output feature map spatial resolution = Input feature map spatial resolu- tion AND Number of output features = Number of input features. This is the most common case. Since there is no change in the number of features or the spatial resolution of the feature map, we can easily add the input and output via shortcut connections. Case 2—Output feature map spatial resolution = 1/2 * Input feature map spatial resolution AND Number of output features = 2 * Number of input features. Re- member that ResNet uses conv layers with a stride of 2 to downsample. The number of features is also doubled. This is done by the first basic block of every conv block (except the second conv block). In this case, the input and output are not the same size. So how do we add them together as part of the skip connection? 1 × 1 11.2 Toward deeper neural networks 405 convs are the answer. The spatial resolution of the input feature map is halved, and the number of input features is doubled by using a 1 × 1 conv with stride=2 and num_features=2 * num_input_features. Listing 11.9 PyTorch code for ResidualConvBlock class ResidualConvBlock(nn.Module): def __init__(self, in_channels, num_blocks, reduce_fm_size=True): super(ResidualConvBlock, self).__init__() num_features = in_channels * 2 if reduce_fm_size else in_channels modules = [] for i in range(num_blocks): The residual block is a stack of basic blocks. if i == 0 and reduce_fm_size: stride = 2 downsample = nn.Sequential( nn.Conv2d( 1 × 1 convs to downsample the input feature map in_channels, num_features, kernel_size=1, stride=stride, bias=False), nn.BatchNorm2d(num_features, eps=0.001), ) basic_block = BasicBlock( in_channels=in_channels, num_features=num_features, stride=stride, downsample=downsample) else: basic_block = BasicBlock( in_channels=num_features, num_features=num_features, stride=1) modules.append(basic_block) self.conv_block = nn.Sequential(*modules) def forward(self, x): return self.conv_block(x) With this, we are ready to implement ResNet-34. Listing 11.10 PyTorch code for ResNet-34 class ResNet34(nn.Module): def __init__(self, num_basic_blocks, num_classes): super(ResNet, self).__init__() conv1 = nn.Sequential( Instantiates the first conv layer nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False), nn.BatchNorm2d(64, eps=0.001), nn.ReLU(inplace=True), nn.MaxPool2d( kernel_size=3, stride=2, padding=1) ) 406 CHAPTER 11 Neural networks for image classification and object detection assert len(num_basic_blocks) == 4 List of size 4, specifying the number of basic blocks per ResidualConvBlock conv2 = ResidualConvBlock( Instantiates four residual blocks in_channels=64, num_blocks=num_basic_blocks[0], reduce_fm_size=False) conv3 = ResidualConvBlock( in_channels=64, num_blocks=num_basic_blocks[1], reduce_fm_size=True) conv4 = ResidualConvBlock( in_channels=128, num_blocks=num_basic_blocks[2], reduce_fm_size=True) conv5 = ResidualConvBlock( in_channels=256, num_blocks=num_basic_blocks[3], reduce_fm_size=True) self.conv_backbone = nn.Sequential(*[conv1, conv2, conv3, conv4, conv5]) self.avg_pool = nn.AdaptiveAvgPool2d((1, 1)) self.classifier = nn.Linear(512, num_classes) def forward(self, x): conv_out = self.conv_backbone(x) conv_out = self.avg_pool(conv_out) logits = self.classifier( Flattens the conv feature before passing it to the classifier conv_out.view(conv_out.shape[0], -1)) return logits As discussed earlier, we typically don’t implement our own ResNet. Instead, we use the ready-made implementation from the torchvision package like this: import torchvision resnet34 = torchvision.models.resnet34() Instantiates resnet34 from the torchvision package While we looked at the ResNet-34, there are deeper ResNet architectures like ResNet- 50, ResNet-101, and ResNet-151 that use a different version of BasicBlock called BottleneckLayer. Similarly, there are several other variants inspired by ResNet, like ResNext, Wide ResNet, and so on. We don’t discuss these individual variants in this book because the core idea behind them remains the same. You are encouraged to read the original papers for a deeper understanding of the subject. 11.2.4 PyTorch Lightning Let’s revisit the problem of digit classification that we looked at earlier. We primarily discussed the LeNet architecture and implemented it in PyTorch. Now, let’s implement the end-to-end code for training the LeNet model. Instead of doing it in vanilla Py- Torch, we use the Lightning framework because it significantly simplifies the model development and training process. Although PyTorch has all we need to train models, there’s much more to deep learning than attaching layers. When it comes to the actual training, we need to write a lot of boilerplate code, as we have seen in previous examples. This includes transferring data from CPU to GPU, implementing the training driver, and so on. Additionally, if 11.2 Toward deeper neural networks 407 we need to scale training/inferencing on multiple devices/machines, another set of integrations often needs to be done. PyTorch Lightning is a solution that provides the APIs required to build models, data sets, and so on. It provides clean interfaces with hooks to be implemented. The underlying Lightning framework calls these hooks at appropriate points in the training process. The idea is that Lightning leaves the research logic to us while automating the rest of the boilerplate code. Additionally, Lightning brings in features like multi-GPU training, floating-point 16, and training on TPU inherently without requiring any code changes. More details about PyTorch Lightning can be found at https://www.pytorch- lightning.ai/tutorials. Training a model using PyTorch Lightning involves three main components: DataModule, LightningModule, and Trainer. Let’s see what each of these does. DATAMODULE DataModule is a shareable, reusable class that encapsulates all the steps needed to process data. All data modules must inherit from LightningDataModule, which provides methods to be overridden. In this specific case, we will implement MNIST as a data module. This data module can now be used across multiple experiments spanning various models and architectures. Listing 11.11 PyTorch code for an MNIST data module class MNISTDataModule(LightningDataModule): DATASET_DIR = ''datasets'' def __init__(self, transform=None, batch_size=100): super(MNISTDataModule, self).__init__() if transform is None: transform = transforms.Compose( [transforms.Resize((32, 32)), transforms.ToTensor()]) self.transform = transform self.batch_size = batch_size def prepare_data(self): Download, tokenizes, and prepares the raw data datasets.MNIST(root = MNISTDataModule.DATASET_DIR, train=True, download=True) datasets.MNIST(root=MNISTDataModule.DATASET_DIR, train=False, download=True) def setup(self, stage=None): train_dataset = datasets.MNIST( root = MNISTDataModule.DATASET_DIR, train=True, download=False, transform=self.transform) self.train_dataset, self.val_dataset = random_split( Splits the training data set into training and validation sets train_dataset, [55000, 5000]) self.test_dataset = datasets.MNIST( root = MNISTDataModule.DATASET_DIR, train = False, download = False, transform=self.transform) 408 CHAPTER 11 Neural networks for image classification and object detection def train_dataloader(self): Creates the train data loader, which provides a clean interface for iterating over the data set. It handles batching, shuffling, and fetching data via multiprocessing, all under the hood. return DataLoader( self.train_dataset, batch_size=self.batch_size, shuffle=True, num_workers=0) def val_dataloader(self): Creates the val data loader return DataLoader( self.val_dataset, batch_size=self.batch_size, shuffle=False, num_workers=0) def test_dataloader(self): Creates the test data loader return DataLoader( self.test_dataset, batch_size=self.batch_size, shuffle=False, num_workers=0) @property def num_classes(self): Number of object categories in the data set return 10 LIGHTNINGMODULE LightningModule essentially groups all the research code into a single module, making it self-contained. Notice the clean separation between DataModule and LightningModule— this makes it easy to train/evaluate the same model on different data sets. Similarly, different models can be easily trained/evaluated on the same data set. A Lightning module consists of the following: A model or system of models defined in the init method A training loop defined in training_step A validation loop defined in validation_step A testing loop defined in testing_step Optimizers and schedulers defined in configure_optimizers Let’s see how we can define the LeNet classifier as a Lightning module. Listing 11.12 PyTorch code for LeNet as a Lightning module class LeNetClassifier(LightningModule): def __init__(self, num_classes): In the init method, we typically define the model, the criterion, and any other setup steps required for training the model. super(LeNetClassifier, self).__init__() self.save_hyperparameters() self.conv1 = torch.nn.Sequential( torch.nn.Conv2d( in_channels=1, out_channels=6, kernel_size=5, stride=1), torch.nn.Tanh(), 11.2 Toward deeper neural networks 409 torch.nn.AvgPool2d(kernel_size=2)) self.conv2 = torch.nn.Sequential( torch.nn.Conv2d( in_channels=6, out_channels=16, kernel_size=5, stride=1), torch.nn.Tanh(), torch.nn.AvgPool2d(kernel_size=2)) self.conv3 = torch.nn.Sequential( torch.nn.Conv2d( in_channels=16, out_channels=120, kernel_size=5, stride=1), torch.nn.Tanh()) self.fc1 = torch.nn.Sequential( torch.nn.Linear(in_features=120, out_features=84), torch.nn.Tanh()) self.fc2 = torch.nn.Linear(in_features=84, out_features=num_classes) self.criterion = torch.nn.CrossEntropyLoss() Instantiates cross-entropy loss self.accuracy = torchmetrics.Accuracy() def forward(self, X): Implements the model’s forward pass. In this case, the input is a batch of images, and the output is the logits. X.shape: [batch_size, C, H, W]. conv_out = self.conv3( self.conv2(self.conv1(X))) batch_size = conv_out.shape[0] conv_out = conv_out.reshape( batch_size, -1) logits = self.fc2(self.fc1(conv_out)) return logits Logits.shape: [batch_size, num_classes] def predict(self, X): Runs the forward pass, performs softmax to convert the resulting logits into probabilities, and returns the class with the highest probability logits = self.forward(X) probs = torch.softmax(logits, dim=1) return torch.argmax(probs, 1) def core_step(self, batch): Abstracts out common functionality between the training and test loops, including the running forward pass, computing loss, and accuracy X, y_true = batch y_pred_logits = self.forward(X) loss = self.criterion(y_pred_logits, y_true) accuracy = self.accuracy(y_pred_logits, y_true) return loss, accuracy def training_step(self, batch, batch_idx): Implements the basic training step: run forward pass, compute loss, accuracy. Logs any necessary values and returns the total loss. loss, accuracy = self.core_step(batch) if self.global_step % 100 == 0: self.log(''train_loss'', loss, on_step=True, on_epoch=True) self.log(''train_accuracy'', accuracy, on_step=True, on_epoch=True) return loss 410 CHAPTER 11 Neural networks for image classification and object detection def validation_step(self, batch, batch_idx, dataset_idx=None): Implements the basic validation step: run forward pass, compute loss and accuracy, return them. return self.core_step(batch) def validation_epoch_end(self, outputs): Called at the end of all test steps for each epoch. The output of every test step is available via outputs. Here we compute the average test loss and accuracy by averaging across all test batches. avg_loss = torch.tensor([x[0] for x in outputs]).mean() avg_accuracy = torch.tensor([x[1] for x in outputs]).mean() self.log(''val_loss'', avg_loss) self.log(''val_accuracy'', avg_accuracy) print(f''Epoch self.current_epoch, Val loss: avg_loss:0.2f, Accuracy: avg_accuracy:0.2f'') return avg_loss def configure_optimizers(self): Configures the SGD optimizer return torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9) def checkpoint_callback(self): Implements logic to save the model. We save the model with the best val accuracy. return ModelCheckpoint(monitor=''val_accuracy'', mode=''max'', save_top_k=1) The model is independent of the data. This allows us to potentially run the LeNet Classifier model on other data modules without any code changes. Note that we are not doing the following steps: 1 Moving the data to a device 2 Calling loss.backward 3 Calling optimizer.backward 4 Setting model.train() or eval() 5 Resetting the gradients 6 Implementing the trainer loop All of these are taken care of by PyTorch Lightning, thus eliminating a lot of boilerplate code. TRAINER We are ready to train our model, which can be done using the Trainer class. This abstraction achieves the following: We maintain control over all aspects via PyTorch code without an added abstraction. The trainer uses best practices embedded by contributors and users from top AI labs. The trainer allows us to override any key part that we don’t want automated. 11.3 Object detection: A brief history 411 Listing 11.13 PyTorch code for Trainer dm = MNISTDataModule() Instantiates the data set model = LeNetClassifier(num_classes=dm.num_classes) Instantiates the model exp_dir = ''/tmp/mnist'' trainer = Trainer( Instantiates the trainer default_root_dir=exp_dir, callbacks=[model.checkpoint_callback()], gpus=torch.cuda.device_count(), # Number of GPUs to run on max_epochs=10, num_sanity_val_steps=0 ) trainer.fit(model, dm) Trains the model Note that we do not write the trainer loop: we just call trainer.fit to train the model. Additionally, the logging automatically enables us to look at the loss and accuracy curves via TensorBoard. Listing 11.14 PyTorch code for inferencing a model X, y_true = (iter(dm.test_dataloader())).next() with torch.no_grad(): y_pred = model.predict(X) Runs model.predict() To run inferencing using the trained model, we run model.predict on the input. 11.3 Object detection: A brief history Until now, we have discussed the classification problem wherein we categorize an image as 1 of N object categories. But in many cases, this is not sufficient to truly describe an image. Consider figure 11.11—a very realistic image with four animals standing one on top of another, posing for the camera. It would be useful to know the object categories of each of the animals and their location (bounding-box coordinates) in the image. This is referred to as the object detection/localization problem. So, how do we localize objects in images? Let’s say we could extract regions in the image so that each region contained only one object. We could then run an image classifier deep neural network (which we looked at earlier) to classify each region and select the regions with the highest confidence. This was the approach adopted by one of the first deep learning-based object detectors, a region-based CNN (R-CNN; https://arxiv.org/pdf/1311.2524.pdf). Let’s look at this in more detail. 11.3.1 R-CNN The R-CNN approach to object detection consists of three main stages: Selective search to identify regions of interest—This step uses a computer vision-based algorithm capable of extracting candidate regions. We do not go into the details 412 CHAPTER 11 Neural networks for image classification and object detection Example training image Figure 11.11 An image with multiple objects of different shapes and sizes of the selective search; you are encouraged to go through the original paper to understand the details. Selective search generates around 2,000 region proposals per image. Feature extraction—A deep convolution neural network extracts features from each region of interest. Since deep neural networks typically take in fixed-sized inputs, the regions (which could be arbitrarily sized) are warped into a fixed size before being fed into the deep neural network. Classification/Localization—A class-specific support vector machine (SVM) is trained on the extracted features to classify the region. Additionally, bounding-box regres- sors are added to fine-tune the object’s location within the region. During training, each region is assigned a ground-truth (GT) class based on its overlap with GT boxes. It is assigned a positive label if there is a high overlap and a negative label otherwise. 11.3.2 Fast R-CNN One of the biggest disadvantages of the R-CNN-based approach is that we have to extract features for every region proposal independently. So, if we generate 2,000 proposals for a single image, we have to run 2,000 forward passes to extract the region features. This is prohibitively expensive and extremely slow (during both training and inference). Additionally, training is a multistage pipeline—selective search, the deep network, the SVMs on top of the features, and the bounding-box regressors—that is cumbersome to train and inference. To solve these problems, the authors of the R-CNN introduced a new technique called a Fast R-CNN (https://arxiv.org/pdf/1504.08083.pdf). It significantly improved speeds: it is 9× faster than the R-CNN during training and 213× faster at test time. Additionally, it improves the quality of object detection. 11.3 Object detection: A brief history 413 Fast R-CNN makes two major contributions: Region of interest (RoI) pooling—As mentioned, one of the fundamental issues with R-CNN is the need for multiple forward passes to extract the features for the region proposals of a single image. Instead, can we extract the features in one go? This problem is solved using RoI pooling. The Fast R-CNN uses the entire image as the input to the CNN instead of a single region proposal. Then, the RoIs (region proposal bounding boxes) are used on top of the CNN output to extract the region features in one pass. We will go into the details of RoI pooling as part of our Faster R-CNN discussion. Multitask loss—The Fast R-CNN eliminates the need to use SVMs. Instead, the deep neural network does both classification and bounding-box regression. Unlike R-CNN, which only uses deep networks for feature extraction, the Fast R-CNN is more end-to-end. It is a single architecture for region proposal feature extraction, classification, and regression. The high-level algorithm is as follows: 1 Use selective search to generate 2,000 region proposals/RoIs per image. 2 In a single pass of the Fast R-CNN, extract all the RoI features in a single pass using RoI pooling and then classify and localize objects using the classification and regression heads. Since the feature extraction for all the region proposals happens in one pass, this approach is significantly faster than the R-CNN, where every proposal needs a separate forward pass. Additionally, since the neural network is trained end to end—that is, asked to do classification and regression—the accuracy of object detection is also improved. 11.3.3 Faster R-CNN Why settle for fast when we can be faster? The Fast R-CNN was significantly faster than the R-CNN. However, it still needed selective search to be run to obtain region proposals. The selective-search algorithm can only be run on CPUs. Additionally, the algorithm is slow and time-consuming. Thus it became a bottleneck. Is there a way to get rid of selective search? The obvious idea to consider is using deep networks to generate region proposals. This is the core idea of Faster R-CNN (FRCNN; https://arxiv.org/pdf/1506.01497.pdf): it eliminates the need for selective search and lets a deep network learn the region proposals. It was one of the first near-real-time object detectors. Since we are using a deep network to learn the region proposals, the region proposals are also better. Thus the resulting accuracy of the overall architecture is also much better. We can view the FRCNN as consisting of two core modules: Region proposal network (RPN)—This is the module responsible for generating the region proposals. RPNs are designed to efficiently predict region proposals with a wide range of scales and aspect ratios. R-CNN module—This is the same as the Fast R-CNN. It receives a bunch of region proposals and performs RoI pooling followed by classification and regression. 414 CHAPTER 11 Neural networks for image classification and object detection Another important thing to note is that the RPN and the R-CNN module share the same convolutional layers: the weights are shared rather than learning two separate networks. In the next section, we discuss the Faster R-CNN in detail. 11.4 Faster R-CNN: A deep dive Figure 11.12 shows the high-level architecture of the FRCNN. The convolutional layers (which we also call the convolutional backbone) extract feature maps from the input image. The RPN operates on these feature maps and emits candidate RoIs. The RoI pooling layer generates a fixed-sized feature vector for each region of interest and passes it on to a set of FC layers that emit softmax probability estimates over K object classes (plus a catch-all “background” class) and four numbers representing the bounding-box coordinates for each of the K classes. Let’s look at each of the components in more detail. Classifier RoI pooling Feature maps Proposals Region proposal network Conv layers Image Figure 11.12 Faster R-CNN architecture. (Source: “Faster R-CNN: Toward real-time object detection with region proposal networks”; https:// arxiv.org/abs/1506.01497.) 11.4.1 Convolutional backbone In the original implementation, the FRCNN used the convolution layers of VGG-16 as the convolutional backbone for both the RPN and the R-CNN modules. There has been one minor modification: the last pooling layer after the fifth convolution layer (conv5) 11.4 Faster R-CNN: A deep dive 415 is removed. As we’ve discussed regarding VGG architectures earlier, VGG reduces the spatial size of the feature map by 2 in every conv block via max pooling. Since the last pooling layer is removed, the spatial size is reduced by a factor of 24 = 16. So a 224 × 224 image is reduced to a 14 × 14 feature map at the output. Similarly, an 800 × 800 image would be reduced to a 50 × 50 feature map. 11.4.2 Region proposal network The RPN takes in an image (of any arbitrary size) as input and emits a set of rectangular proposals that could potentially contain objects as output. The RPN operates on top of the convolutional feature map output by the last shared convolution layer. With the VGG backbone, an input image of size (h, w) is scaled down to (h/16, w/16). So each 16 × 16 spatial region in the input image is reduced to a single point on the convolutional feature map. Thus each point in the output convolutional feature map represents a 16 × 16 patch in the input image. The RPN operates on top of this feature map. Another subtle point to remember is that while each point in the convolutional feature map is chosen to correspond to a 16 × 16 patch, it has a significantly larger receptive field (the region in the input feature map that a particular output feature is affected by). The embedding at each point in the feature map is thus, in effect, the digest of a large receptive field. ANCHORS A key aspect of the object-detection problem is the variety of object sizes and shapes. Objects can range from very small (cats) to very large (elephants). Additionally, objects can have different aspect ratios. Some objects may be wide, some may be tall, and so on. A naive solution is to have a single neural network detector head capable of identifying and recognizing all these objects of varying sizes and shapes. As you can imagine, this would make the job of the neural network detector extremely complex. A simpler solution is to have a wide variety of neural network detector heads, each responsible for solving a much simpler problem. For example, one head will only focus on large, tall objects and will only fire when such objects are present in the image. The other heads will focus on other sizes and aspect ratios. We can think of each head as being responsible for doing a single simple job. This type of setup greatly aids and benefits learning. This was the intuition behind the introduction of anchors. Anchors are like reference boxes of varying shapes and sizes. All proposals are made relative to anchors. Each anchor is uniquely characterized by its size and aspect ratio and is tasked with detecting similarly shaped objects in the image. At each sliding-window location, we have multiple anchors spanning different sizes and aspect ratios. The original FRCNN architecture supported nine anchor configurations spanning three sizes and three aspect ratios, thus supporting a wide variety of shapes. These correspond to anchor boxes of scales (8, 16, 32) and aspect ratios (0.5, 1.0, and 2.0), respectively (see figure 11.13). Anchors are now ubiquitous across object detectors. 416 CHAPTER 11 Neural networks for image classification and object detection Anchors at grid point (25, 25) Anchor locations Figure 11.13 The left column shows the various grid-point locations on the output convolution feature map over which the small network is convolved. At each grid point location, we sample k (=9 in the original implementation) anchors across multiple sizes and aspect ratios. The right column shows the various anchors at a particular grid point. NOTE Fully functional code for generating anchors, executable via Jupyter Note- book, can be found at http://mng.bz/nY48. Listing 11.15 PyTorch code to generate anchors at a particular grid point def generate_anchors_at_grid_point( ctr_x, ctr_y, subsample, scales, aspect_ratios): anchors = torch.zeros( (len(aspect_ratios) * len(scales), 4), dtype=torch.float) for i, scale in enumerate(scales): for j, aspect_ratio in enumerate(aspect_ratios): Generates the height and width for different scales and aspect ratios w = subsample * scale * torch.sqrt(aspect_ratio) h = subsample * scale * torch.sqrt(1 / aspect_ratio) xtl = ctr_x - w / 2 Generates a bounding box centered around (ctr_x, ctr_y) with width w, and height h ytl = ctr_y - h / 2 xbr = ctr_x + w / 2 ybr = ctr_y + h / 2 index = i * len(aspect_ratios) + j anchors[index] = torch.tensor([xtl, ytl, xbr, ybr]) return anchors 11.4 Faster R-CNN: A deep dive 417 Listing 11.16 PyTorch code to generate all anchors for a given image def generate_all_anchors( This isn’t the most efficient way to generate anchors. We’ve written simple code to ease understanding. input_img_size, subsample, scales, aspect_ratios): _, h, w = input_img_size conv_feature_map_size = (h//subsample, w//subsample) all_anchors = [] Generates anchor boxes centered at every point in the conv feature map, which corresponds to a 16 × 16 (subsample, subsample) region in the input ctr_x = torch.arange( subsample/2, conv_feature_map_size[1]*subsample+1, subsample) ctr_y = torch.arange( subsample/2, conv_feature_map_size[0]*subsample+1, subsample) for y in ctr_y: for x in ctr_x: all_anchors.append( generate_anchors_at_grid_point( Uses a function defined in listing 11.15 x, y, subsample, scales, aspect_ratios)) all_anchors = torch.cat(all_anchors) return all_anchors input_img_size = (3, 800, 800) Defines config parameters and generates anchors c, height, width = input_img_size scales = torch.tensor([8, 16, 32], dtype=torch.float) aspect_ratios = torch.tensor([0.5, 1, 2]) subsample = 16 anchors = generate_all_anchors(input_img_size, subsample, scales, aspect_ratios) The RPN slides a small network over the output convolution feature map. The small network operates on an n × n spatial window of the convolution feature map. At each sliding-window location, it generates a lower-dimensional feature vector (512 dimensions for VGG) that is fed into a box-regression layer (reg) and a box-classification layer (cls). For each of the anchor boxes centered at that sliding window location, the classifier predicts objectness: a value from 0 to 1, where 1 indicates the presence of the object and the regressor predicts the region proposal relative to the anchor box. This architecture is naturally implemented with an n × n convolutional layer followed by two sibling 1 × 1 convolutional layers (for reg and cls), respectively. The original implementation in the FRCNN paper uses n = 3, which results in an effective receptive field of 228 pixels when using the VGG backbone. Figure 11.14 illustrates this in detail. Note that this network consists of only convolutional layers. Such an architecture is called a fully convolutional network (FCN). FCNs do not have an input size restriction. Because they 418 CHAPTER 11 Neural networks for image classification and object detection Convolution feature map 3×3 conv Sliding window 512D feature map 2,000 scores 4,000 coordinates 1×1 conv 1×1 conv Figure 11.14 RPN architecture. From each sliding window, a 512-dimensional feature vector is generated using 3 × 3 convs. A 1 × 1 conv layer (classifier) takes the 512-dimensional feature vector as input and generates 2k scores (denoting the presence/ absence of an object), where k is the number of anchors. Similarly, another 1 × 1 conv layer (regressor) generates 4k bounding-box coordinates from the 512-dimensional feature vector. consist of only convolution layers, they can work with arbitrary-sized inputs. In the FCN, the combination of the n × n and 1 × 1 layers is equivalent to applying an FC layer over every embedding at each point in the convolutional feature map. Also, because we are convolving a convolutional network on top of the feature map to generate the regression and classification scores, the convolutional weights are common/shared across different positions on the feature map. This makes the approach translation invariant. A cat at the top of the image and a cat at the bottom of the image are picked up by the same anchor configuration (scale, aspect ratio) if they are similarly sized. NOTE Fully functional code for the fully convolutional network of the RPN, executable via Jupyter Notebook, can be found at http://mng.bz/nY48. Listing 11.17 PyTorch code for the FCN of the RPN class RPN_FCN(nn.Module): def __init__(self, k, in_channels=512): Instantiates the small network that is convolved over the output conv feature map. It consists of a 3 × 3 conv layer followed by a 1 × 1 conv layer for classification and another 1 × 1 conv layer for regression. super(RPN_FCN, self).__init__() self.conv = nn.Sequential( nn.Conv2d( in_channels, 512, kernel_size=3, stride=1, padding=1), nn.ReLU(True)) self.cls = nn.Conv2d(512, 2*k, kernel_size=1) self.reg = nn.Conv2d(512, 4*k, kernel_size=1) def forward(self, x): out = self.conv(x) Output of the backbone: a convolutional feature map of size (batch_size, in_channels, h, w) 11.4 Faster R-CNN: A deep dive 419 rpn_cls_scores = self.cls(out).view( Converts (batch_size, h, w, 2k) to (batch_size, h*w*k, 2) x.shape[0], -1, 2) rpn_loc = self.reg(out).view( Converts (batch_size, h, w, 4k) to (batch_size, h*w*k, 4) x.shape[0], -1, 4) (batch_size, num_anchors, 2) tensor representing the classification score for each anchor box return rpn_cls_scores ,rpn_loc (batch_size, num_anchors, 4) tensor representing the box coordinates relative to the anchor box GENERATING GT FOR AN RPN So far, we have generated many anchor bounding boxes and a neural network capable of generating the classification and regression offsets for every anchor. While training the RPN, we need to provide a target (GT) that both the classifier and regressor should predict for each anchor box. To do so, we need to look at the objects in the image and assign them to relevant anchors that contain the object. The idea is as follows: out of the thousands of anchors,the anchors that contain most of the object should try predicting and localizing the object. We saw earlier that the intuition behind creating anchors was to ensure that each anchor is responsible for one particular type of object (shape, aspect ratio). Thus it makes sense that only anchors that contain the object are responsible for classifying it. To measure whether the object lies within the anchor, we rely on intersection over union (IoU) scores. The IoU between two bounding boxes is defined as area of overlap area of union . So, if the two bounding boxes are very similar, their overlap is high, and their union is close to the overlap, resulting in a high IoU. If the two bounding boxes are varied, then their area of overlap is minimal, resulting in a low IoU (see figure 11.15). Figure 11.15 Intersection over the union between two bounding boxes. It is equivalent to the inter- section of the two areas divided by the union of the two areas. 420 CHAPTER 11 Neural networks for image classification and object detection FRCNN provides some guidelines for assigning labels to the anchor boxes: We assign a positive label 1 (which represents an object being present in the anchor box) to two kinds of anchors: – The anchor(s) with the greatest IoU overlap with a GT box – An anchor that has an IoU overlap greater than 0.7 with the GT box We assign a negative label 0 (which represents no object being present in the anchor box, implying that it contains only background) to a non-positive anchor if its IoU ratio is less than 0.3 for all GT boxes. Anchors that are neither positive nor negative do not contribute to the training objective. Note that a single GT object may assign positive labels to multiple anchors. These outputs must be suppressed later to prevent duplicate detections (we discuss this in the subsequent sections). Also, any anchor box that lies partially outside the image is ignored. NOTE Fully functional code for assigning GT labels to anchor boxes, executable via Jupyter Notebook, can be found at http://mng.bz/nY48. Listing 11.18 PyTorch code to assign GT labels for each anchor box valid_indices = torch.where( (anchors[:, 0] >=0) & (anchors[:, 1] >=0) & (anchors[:, 2] <=width) & Finds valid anchors that lie completely inside the image (anchors[:, 3] <=height))[0] rpn_valid_labels = -1 * torch.ones_like( Assigns s label of -1 (not any class) for each valid anchor valid_indices, dtype=torch.int) valid_anchor_bboxes = anchors[valid_indices] Obtains the valid anchor boxes ious = torchvision.ops.box_iou( Tensor of shape (num_gt_bboxes, num_valid_anchor_bboxes), representing the IoU between the GT and anchors gt_bboxes, valid_anchor_bboxes) assert ious.shape == torch.Size( [gt_bboxes.shape[0], valid_anchor_bboxes.shape[0]]) gt_ious_max = torch.max(ious, dim=1)[0] Finds the highest IoU for every GT bounding box # Find all the indices where the IOU = highest GT IOU gt_ious_argmax = torch.where( Finds all the indices where the IOU = highest GT IOU gt_ious_max.unsqueeze(1).repeat(1, gt_ious_max.shape[1]) == ious)[1] 11.4 Faster R-CNN: A deep dive 421 anchor_ious_argmax = torch.argmax(ious, dim=0) Finds the highest IoU for every anchor box anchor_ious = ious[anchor_ious_argmax, torch.arange(len(anchor_ious_argmax))] pos_iou_threshold = 0.7 neg_iou_threshold = 0.3 rpn_valid_labels[anchor_ious < neg_iou_threshold] = 0 Assigns 0 (background) for negative anchors where IoU < 0.3 rpn_valid_labels[anchor_ious > pos_iou_threshold] = 1 Assigns 1 (objectness) for positive anchor where IoU > 0.7 rpn_valid_labels[gt_ious_argmax] = 1 For every GT bounding box, assigns the anchor with the highest IoU as a positive anchor DEALING WITH IMBALANCE Given our strategy of assigning labels to anchors, notice that the number of negative anchors is significantly greater than the number of positive anchors. For example, for the example image, we obtained only 24 positive anchors as opposed to 7,439 negative anchors. If we train directly on such an imbalanced data set, neural networks can typically learn a local minimum by classifying every anchor as a negative anchor. In our example, if we predicted every anchor to be a negative anchor, our resulting accuracy would be 7439/(7439 + 22): 99.7%. However, the resulting neural network is practically useless because it has not learned anything. In other words, the imbalance will lead to bias toward the dominant class. To deal with this imbalance, there are typically three strategies: Undersampling—Sample less of the dominant class. Oversampling—Sample more of the less-dominant class. Weighted loss—Set the cost for misclassifying less-dominant classes much higher than the dominant class. FRCNN utilizes the idea of undersampling. For a single image, there are multiple positive and negative anchors. From these thousands of anchors, we randomly sample 256 anchors in an image to compute the loss function, where the sampled positive and negative anchors have a ratio of up to 1:1. If there are fewer than 128 positive samples in an image, we pad the minibatch with negative ones. ASSIGNING TARGETS TO ANCHOR BOXES We have seen how to sample and assign labels to anchors. The next question is how to come up with the regression targets: Case 1: label = −1—Unsampled/invalid anchor. These do not contribute to the training objective, so regression targets do not matter. Case 2: label = 0—Background anchor. These anchors do not contain any objects, so they also should not contribute to regression. Case 3: label = 1—Positive anchor. These anchors contain objects. We need to generate regression targets for these anchors. 422 CHAPTER 11 Neural networks for image classification and object detection Let’s consider only the case of positive anchors. The key intuition here is that the anchors already contain a majority of the object. Otherwise, they wouldn’t have become positive anchors. So there is already significant overlap between the anchor and the object in question. Therefore it makes sense to learn the offset from the anchor bounding box to the object bounding box. The regressor is tasked with learning this offset: that is, what delta we must make to the anchor bounding box for it to become the object bounding box. the FRCNN adopts the following parameterization: tx = (x −xa)/wa ty = (y −ya)/ha tw = log(w/wa) th = log(h/ha) (11.2) where x, y, w, and h denote the GT bounding box’s center coordinates and its width and height, and xa, ya, wa, and ha denote the anchor bounding box’s center coordinates and its width and height. tx, ty, tw, and th are the regression targets. The regressor is, in effect, learning to predict the delta between the anchor bounding box and the GT bounding box. NOTE Fully functional code for assigning regression targets to anchor boxes, executable via Jupyter Notebook, can be found at http://mng.bz/nY48. Listing 11.19 PyTorch code to assign regression targets for each anchor box def transform_bboxes(bboxes): (n, 4) tensor in (xtl, ytl, xbr, ybr) format height = bboxes[:, 3] - bboxes[:, 1] width = bboxes[:, 2] - bboxes[:, 0] x_ctr = bboxes[:, 0] + width / 2 y_ctr = bboxes[:, 1] + height /2 return torch.stack( (n, 4 tensor) in (x, y, w, h) format [x_ctr, y_ctr, width, height], dim=1) def get_regression_targets(roi_bboxes, gt_bboxes): (n, 4) tensors representing the bounding boxes for the region of interest and GT, respectively assert roi_bboxes.shape == gt_bboxes.shape roi_bboxes_t = transform_bboxes(roi_bboxes) gt_bboxes_t = transform_bboxes(gt_bboxes) tx = (gt_bboxes_t[:, 0] - roi_bboxes_t[:, 0]) / roi_bboxes_t[:, 2] ty = (gt_bboxes_t[:, 1] - roi_bboxes_t[:, 1]) / roi_bboxes_t[:, 3] tw = torch.log(gt_bboxes_t[:, 2] / roi_bboxes_t[:, 2]) th = torch.log(gt_bboxes_t[:, 3] / roi_bboxes_t[:, 3]) return torch.stack([tx, ty, tw, th], dim=1) (n, 4) tensor containing the regression targets 11.4 Faster R-CNN: A deep dive 423 RPN LOSS FUNCTION We have defined the RPN fully convolutional network and how we can generate labels and regression targets for the outputs of the RPN FCN. Now we need to discuss the loss function that enables us to train the RPN. As you would expect, there are two loss terms: Classification loss—Applies to both the positive and negative anchors. We use the standard cross-entropy loss used in any standard classifier. Regression loss—Applies only to the positive anchors. Here we use smooth L1 loss, which is defined as L1;smooth = ( 0.5(xn −yn)2/beta, if |xn −yn| < beta |xn −yn| −0.5 ∗beta, otherwise (11.3) We can think of smooth L1 loss as a combination of L1 and L2 loss. If the value is < beta, it behaves like an L2 loss (mean squared error [MSE]). Otherwise, it behaves like an L1 loss. In the case of the FRCNN, beta is set to 1. The intuition behind this is simple. If we use pure L2 loss (MSE), then higher loss terms contribute to exponential loss because of the quadratic nature of the loss. This can lead to a bias where loss can be reduced by focusing on high-value items. Instead, if we use pure L1 loss, the higher loss terms still contribute more loss, but the effect is linear instead of quadratic. This still has a slightly worse bias toward higher loss terms. We get the best of both worlds by using L2 loss when the loss values are small and L1 loss when the loss values are large. When the loss value is small, because we are using L2 loss, its contribution is exponential/quadratic. And when the loss value is high, it still contributes linearly via L1 loss. Thus the network is incentivized to pay attention to low- and high-loss items. Overall loss for an image can be defined as follows: Lcls = Í i CrossEntropy(pi, p∗ i ) Ncls Lreg = Í i p∗ i L1;smooth(ti, t∗ i ) Npos LRPN = Lcls + 휆Lreg (11.4) where, pi is the predicted objectness probability for the anchor i. p∗ i is the true objectness label for anchor i. It is 1 if the anchor is positive and 0 if the anchor is negative. ti = (tx, ty, tw, th) are the regression predictions for anchor i, t∗ i = (t∗ x, t∗ y , t∗ w, t∗ h) are the regression targets for anchor i, Ncls is the number of anchors, and Npos is the number of positive anchors. NOTE Fully functional code for the RPN loss function, executable via Jupyter Notebook, can be found at http://mng.bz/nY48. 424 CHAPTER 11 Neural networks for image classification and object detection Listing 11.20 PyTorch code for the RPN loss function def rpn_loss( rpn_cls_scores, rpn_loc, rpn_labels, rpn_loc_targets, lambda_ = 10): rpn_cls_scores: (num_anchors, 2) tensor representing RPN classifier scores for each anchor. rpn_loc: (num_anchors, 4) tensor representing RPN regressor predictions for each anchor. rpn_labels: (num_anchors) representing the class for each anchor (-1, 0, 1). rpn_loc_targets: (num_anchors, 4) tensor representing RPN regressor targets for each anchor. classification_criterion = nn.CrossEntropyLoss( ignore_index=-1) Ignores -1 as they are not sampled reg_criterion = nn.SmoothL1Loss(reduction=''sum'') cls_loss = classification_criterion(rpn_cls_scores, rpn_labels) positive_indices = torch.where(rpn_labels==1)[0] Finds the positive anchors pred_positive_anchor_offsets = rpn_loc[positive_indices] gt_positive_loc_targets = rpn_loc_targets[positive_indices] reg_loss = reg_criterion( pred_positive_anchor_offsets, gt_positive_loc_targets) / len(positive_indices) return { ''rpn_cls_loss'': cls_loss, ''rpn_reg_loss'': reg_loss, ''rpn_total_loss'': cls_loss + lambda_* reg_loss } GENERATING REGION PROPOSALS We have so far discussed how the RPN works. The RPN predicts objectness and the regres- sion offsets for every anchor. The next task is to generate good region proposals/RoIs and use them for training the R-CNN module. Since we are emitting objectness and regression offsets for every anchor, we have thousands of predictions. We cannot use all of them as RoIs. We need to generate the best RoIs from these scores and offsets to train our R-CNN. An obvious way to do this is to rely on the objectness scores: the higher the objectness score, the greater the likelihood that it contains an object and thus is a good RoI. Before we get there, we must do some basic processing steps: 1 Convert the predicted offsets to bounding boxes. This is done by reversing the sequence of transformations x∗= t∗ x ∗wa + xa y∗= t∗ y ∗ha + ya w∗= et∗ w ∗wa h∗= et∗ h ∗ha (11.5) 11.4 Faster R-CNN: A deep dive 425 where x∗, y∗, w∗, and h∗denote the predicted bounding box’s center coordinates and its width and height, and t∗ x, t∗ y, t∗ w, and t∗ h are the RPN loc predictions. The bounding boxes are then converted back into xtl, ytl, xbr, ybr format. 2 The predicted bounding boxes can lie partially outside the image. We clip all the predicted bounding boxes to within the image. 3 Remove any predicted bounding boxes with height or width less than min_roi_ threshold. Once these processing steps are done, we sort the predicted bounding boxes by ob- jectness score and select N candidates. N = 12000 during training and N = 6000 while testing. NOTE Fully functional generating region proposals from the RPN output, exe- cutable via Jupyter Notebook, can be found at http://mng.bz/nY48. Listing 11.21 PyTorch code to generate region proposals from RPN output rois = generate_bboxes_from_offset(rpn_loc, anchors) rois = rois.clamp(min=0, max=width) Clips the ROIs roi_heights = rois[:, 3] - rois[:, 1] Threshold based on min_roi_threshold roi_widths = rois[:, 2] - rois[:, 0] min_roi_threshold = 16 valid_idxes = torch.where((roi_heights > min_roi_threshold) & (roi_widths > min_roi_threshold))[0] rois = rois[valid_idxes] valid_cls_scores = rpn_loc[valid_idxes] objectness_scores = valid_cls_scores[:, 1] sorted_idx = torch.argsort( Sorts based on objectness objectness_scores, descending=True) n_train_pre_nms = 12000 n_val_pre_nms = 300 rois = rois[sorted_idx][:n_train_pre_nms] Selects the top regions of interest. Shape: (n_train_pre_nms, 4). objectness_scores = objectness_scores[ sorted_idx][:n_train_pre_nms] Selects the top objectness scores. Shape: (n_train_pre_nms,). NON-MAXIMAL SUPPRESSION (NMS) Many of the proposals will overlap. We are effectively selecting anchors at a stride of 16 pixels. Therefore even a reasonably sized object is picked up by multiple anchors, each of which will try to predict the object independently. We can see this overlapping nature when we look at the positive anchors in figure 11.16. We want to choose the most effective set of RoIs. But it is evident that choosing all the similar proposals does 426 CHAPTER 11 Neural networks for image classification and object detection Positive anchors Random negative anchors Figure 11.16 For the image under consideration, we have 24 positive anchors and 7,439 negative anchors. Training directly on such an imbalanced data set can lead to the network learning a local minimum where every anchor is classified as negative. To prevent this, the FRCNN under-samples the negative anchors before training. not make a good set of RoIs because they carry redundant information. To address this problem, we use a technique called non-maximal suppression (NMS). NMS is an algorithm that suppresses highly overlapping bounding boxes. The algorithm takes in bounding boxes and scores and works as follows. Algorithm 11.1 Non-maximal suppression Input: A list of bounding boxes B, corresponding scores S, and overlap threshold N Output: A list of filtered bounding boxes D while likelihood is increasing do Select bounding box with highest confidence score Remove it from B and add it to the final list D Compare selected bounding box with remaining boxes in B using IoU Remove all bounding boxes from B with IoU > threshold end while return D We use NMS with a 0.7 threshold to suppress the highly overlapping RoIs and choose the top N RoIs post-NMS to train the R-CNN. N = 2000 during training, and N = 300 while testing. Figure 11.17 shows bounding boxes on a sample image before and after NMS. NOTE Fully functional code for NMS, executable via Jupyter Notebook, can be found at http://mng.bz/nY48. 11.4 Faster R-CNN: A deep dive 427 Pre NMS Post NMS Figure 11.17 The left column shows the bounding boxes (24) before NMS. The right column shows the bounding boxes (4) that remain after NMS. Listing 11.22 PyTorch code for NMS of RoIs n_train_post_nms = 2000 n_val_post_nms = 300 nms_threshold = 0.7 post_nms_indices = torchvision.ops.nms( Calls NMS implemented by torchvision rois, objectness_scores, nms_threshold) post_nms_rois = rois[post_nms_indices[:n_train_post_nms]] 11.4.3 Fast R-CNN In the previous section, we saw how the RPN network takes an input image and emits a set of regions of interest that are likely to contain objects. Now let’s discuss the second leg of the FRCNN architecture, which takes in the RoIs and generates class probabilities and bounding-box coordinates for each object in the image. We briefly discussed this earlier. Here we revisit it in greater detail. We are given a set of RoIs (some of which contain the object). Our task is to train an object detector capable of localizing the objects. To do this, we need to extract the features corresponding to each RoI and pass them through a neural network (classifier and regressor) that learns to predict the class and the regression targets. The R-CNN solved this in a naive way: it extracted each RoI one at a time, warped it to make it a fixed size, and passed it through a deep CNN to extract the features corresponding to the RoI. Each RoI required a separate forward pass, making the approach very slow. The question, as always, is: can we do better? 428 CHAPTER 11 Neural networks for image classification and object detection ROI POOLING Let’s consider the convolutional backbone. It processes the whole image with several conv and max pooling layers to produce a conv feature map. We have also seen a subsampling factor of 16: that is, 16 × 16 pixels in the input image are reduced to a single point in the feature map. Also remember that the embedding at every grid point on the feature map is the representation/digest of a region in the input image. Key idea 1 is that the features corresponding to each RoI are already present in the conv feature map, and we can extract them via the feature map. For example, say our RoI is (0, 0, 256, 256). We know that the (0, 0, 256, 256) region in the input image is represented by (0, 0, 256/16, 256/16): that is, the (0, 0, 16, 16) region in the conv feature map. Since the embedding for a point in the conv feature map is a digest of the receptive field, we can use these features directly as the features of the RoI. So to obtain the features for an RoI of (0, 0, 256, 256), we take all the embeddings corresponding to the region (0, 0, 16, 16) in the conv feature map. Since we are performing this feature extraction directly on the convolutional feature map, which is obtained for the entire image, we can obtain the RoI features for all the RoIs in a single forward pass. This eliminates the need for multiple forward passes. Key idea 2 is as follows. We discussed a clever way of extracting the features correspond- ing to each RoI, and we want to use these features to train our classifier and regressor. However, there is a problem. As we know, RoIs are different sizes. And different-sized RoIs will lead to different feature embedding sizes. For example, if our RoI is (0, 0, 256, 256), our RoI feature embeddings are (16, 16, 512): that is, all the embeddings (of size 512) in the (0, 0, 16, 16) region of the conv feature map. If our RoI is (0, 0, 128, 128), then our RoI feature embeddings are (8, 8, 512): all the embeddings in the (0, 0, 8, 8) region of the conv feature map. And we know that neural networks typically need same-sized input. So how do we deal with input embeddings of different sizes? The answer is RoI pooling. Let’s fix the size of the input ROI feature map that goes into the neural network. Our task is to reduce variable-sized RoI feature maps to a fixed size. If the fixed feature map size is set to be H, W, and our RoI corresponds to (r, c, h, w) in the conv feature map, we divide h and w into equal-sized blocks of size h/H and w/W, respectively, and apply max pooling on these blocks to obtain a H, W feature map. Going back to our example, let’s fix H =W = 4. Our expected fixed feature map size is (4, 4, 512). So when our RoI is (0, 0, 256, 256), our RoI feature embeddings are (16, 16, 512): h =w = 16. We divide the 16 × 16 region into four (16/4, 16/4) regions and perform max pooling on each region to obtain a fixed-size (4, 4, 512) feature. Similarly, when our RoI is (0, 0, 128, 128), h =w = 8. We divide the 8 × 8 region into four (8/4, 8/4) regions and perform max pooling to obtain the fixed-size (4, 4, 512) feature. Astute readers will notice that we have carefully chosen our RoIs so that they are multiples of H and W, resulting in integer values for h/H and w/W, respectively. But in reality, this rarely happens. h/H and w/W are often floating-point numbers. What do we do in this case? The answer is quantization: that is, we choose the integer closest to h/H and w/W, respectively (floor operation, in the original implementation). This has been 11.4 Faster R-CNN: A deep dive 429 improved on by RoIAlign, which uses bilinear interpolation instead of quantization. We do not get into the details of RoIAlign here. In effect, if we have a large RoI, we divide the feature map into a fixed number of large regions and perform max pooling. And when we have a small RoI, we divide the feature map into a fixed number of small regions and perform max pooling. The size of the region used for pooling can change, but the output size remains fixed. The dimension of the RoI pooling output doesn’t depend on the size of the input feature map or the size of the RoIs: it’s determined solely by the number of sections we divide the RoI into—H and W (see figure 11.18). Conv feature map RoI 1 RoI 2 RoI-pooled feature maps are the same size even though RoIs are different sizes. 2×2 2×2 2×2 2×2 3×3 3×3 3×3 3×3 6 6 4 4 2 2 2 2 Figure 11.18 A conv feature map with two regions of interest of different sizes. RoI pooling ex- tracts a fixed-sized output feature map (2 × 2 in this image) from each of the RoIs in a single pass via max pooling. This enables us to extract fixed-sized representa- tive feature vectors for each RoI, which are then fed into further classifier and regressor layers for classification and localization. Thus, the purpose of RoI pooling is to perform max pooling on inputs of non-uniform sizes to obtain fixed-size feature maps. The Fast R-CNN and Faster R-CNN use 7 × 7 as the fixed feature map size. FAST R-CNN ARCHITECTURE Given the conv feature map and a set of RoIs, we have seen how the RoI pooling layer extracts a fixed-length feature vector from the feature map. Each feature vector is fed into a sequence of FC layers that finally branch into two sibling output layers: a classifier that produces softmax probability estimates over K object classes plus a catch- all “background” class, and a regressor that produces four real-valued numbers for each of the K object classes. 430 CHAPTER 11 Neural networks for image classification and object detection GENERATING GT FOR THE FAST R-CNN For every image, we have a list of RoIs generated by the RPN and a list of GT bounding boxes. How do we generate the GT and regression targets for each RoI? The idea remains the same as for our RPN: we use IoU scores. The algorithm is as follows: 1 Compute the IoU between all RoIs and GT boxes. 2 For each RoI, determine the GT bounding box with the highest IoU. 3 If the highest IoU is greater than a threshold (0.5), assign the corresponding GT label as the label for the RoI. 4 If the IoU is between [0.1, 0.5], assign the background label. Using a lower bound of 0.1 ensures that certain RoIs with small intersections with the GT are selected as background. This is helpful as it chooses hard examples for background; it is a form of hard negative mining. NOTE Fully functional code for the Fast R-CNN RoI head, executable via Jupyter Notebook, can be found at http://mng.bz/nY48. Listing 11.23 PyTorch code for the Fast R-CNN RoI head class Fast_RCNN_ROI_Head(nn.Module): def __init__(self, num_classes, H, W, subsample=16, embedding_size=512): super(Fast_RCNN_ROI_Head, self).__init__() self.num_classes = num_classes self.H = H self.W = W self.embedding_size = embedding_size self.subsample = 16 self.roi_head_classifier = nn.Sequential( nn.Linear(H*W*embedding_size, 4096), nn.ReLU(True), nn.Linear(4096, 4096), nn.ReLU(True), ) self.cls = torch.nn.Linear(4096, num_classes+1) num_classes + background self.reg = torch.nn.Linear(4096, (num_classes+1)*4) def forward(self, x, rois): x : (1, c, h, w) tensor representing the conv feature map. rois: (n, 4) tensor representing bounding boxes of RoIs assert x.shape[0] == 1 # This code only supports batch size of 1 roi_pooled_features = torchvision.ops.roi_pool( x, [rois], output_size=(self.H, self.W), spatial_scale=1/subsample) roi_pooled_features = roi_pooled_features.view( -1, self.H*self.W*self.embedding_size) 11.4 Faster R-CNN: A deep dive 431 fc_out = self.roi_head_classifier(roi_pooled_features) roi_cls_scores = self.cls(fc_out) roi_loc = self.reg(fc_out) return roi_cls_scores, roi_loc roi_cls_scores: (n, num_classes+1) tensor representing classification scores for each RoI. roi_loc: (n, (num_classes + 1) * 4) tensor representing the regression scores for each RoI TRAINING THE FAST R-CNN The RPN generates about 2,000 RoIs per image. Due to computational constraints, we cannot use all N RoIs. Instead, we sample a subset of them. The training minibatches are sampled hierarchically, first by sampling K images and then by sampling R/K RoIs from each image. R is set to 128 in the FRCNN. For this discussion, we assume that K = 1: that is, we have a single image per minibatch. So, given the RoIs for a single image, how do we sample 128 RoIs from it? A simple solution is to randomly sample 128 RoIs. However, this runs into the same data-imbalance issue that we discussed earlier: we end up sampling backgrounds a lot more frequently than the classes. To solve this problem, we adopt a sampling strategy similar to before. In particular, for a single image, we sample 128 RoIs such that the ratio of background to object is 0.75:0.25. If fewer than 32 RoIs contain the objects, we pad the minibatch with more background RoIs. ASSIGNING TARGETS TO ROI BOXES Just as in the case of the RPN, we generate regression targets as offsets of the GT box from the region of interest for all RoIs that contain objects. For all background RoIs, the regression targets are not applicable. FAST R-CNN LOSS FUNCTION We have defined the Fast R-CNN network and how we can generate labels and regression targets for its outputs. We need to discuss the loss function that enables us to train the Fast R-CNN. As you would expect, there are two loss terms: Classification loss— We use the standard cross-entropy loss used in any standard classifier. Regression loss—The regression loss applies only to the object RoIs: background RoIs do not contribute to regression. Here we use the smooth L1 loss as we did in the RPN. Thus the overall loss for a single RoI can be defined as follows: Lcls = CrossEntropy(p, u) Lreg = L1;smooth(tu, v) LRCNN = Lcls + 휆[u > 0]Lreg (11.6) where p is the predicted label for the RoI, u is the true label for the RoI, tu = (tx, ty, tw, th) are the regression predictions for class u, and v = (vx, vy, vw, vh) are the regression targets. 432 CHAPTER 11 Neural networks for image classification and object detection The overall loss can therefore be defined as Lcls = Í i CrossEntropy(pi, p∗ i ) Nroi Lreg = Í {∀i|p∗ i !=0} L1;smooth(ti, t∗ i ) Npos LRCNN = Lcls + 휆Lreg (11.7) where pi are the prediction probabilities for the RoI i, p∗ i is the true label for RoI i; ti = (tx, ty, tw, th) are the regression predictions for RoI i corresponding to class p∗ i , t∗ i = (t∗ x, t∗ y , t∗ w, t∗ h) are the regression targets for RoI i, t∗ i = (t∗ x, t∗ y , t∗ w, t∗ h) are the regression targets for RoI i, and Npos is the number of object RoIs (non-background RoIs). NOTE Fully functional code for the Fast R-CNN loss function, executable via Jupyter Notebook, can be found at http://mng.bz/nY48. Listing 11.24 PyTorch code for the Fast R-CNN loss function def rcnn_loss( roi_cls_scores, (128, num_classes) tensor: RCNN classifier scores for each RoI roi_loc, (128, num_classes*4) tensor: RCNN regressor predictions for each class, RoI roi_labels, (128,) tensor: true class for each RoI rcnn_loc_targets, (128, 4) tensor: RoI regressor targets for each RoI lambda_ = 1): classification_criterion = nn.CrossEntropyLoss() reg_criterion = nn.SmoothL1Loss(reduction=''sum'') cls_loss = classification_criterion(roi_cls_scores, roi_labels) pos_roi_idxes = torch.where(roi_labels>0)[0] Finds the positive RoIs pred_all_offsets = roi_loc[pos_roi_idxes] num_pos_rois = len(pos_roi_idxes) pred_all_offsets = pred_all_offsets.view( (n, num_classes*4) to (n, num_classes, 4) num_pos_rois, -1, 4) pred_cls_offsets = pred_all_offsets[ torch.arange(num_pos_rois) , roi_labels[pos_roi_idxes]] gt_offsets = rcnn_loc_targets[pos_roi_idxes] reg_loss = reg_criterion(pred_cls_offsets, gt_offsets) / num_pos_rois return { ''rcnn_cls_loss'': cls_loss, ''rcnn_reg_loss'': reg_loss, ''rcnn_total_loss'': cls_loss + lambda_* reg_loss } 11.4 Faster R-CNN: A deep dive 433 FAST R-CNN INFERENCE We have looked at how to train the Fast R-CNN module. Once the model is trained, the next question is how to use the model to generate output classes and bounding boxes. The Fast R-CNN model outputs a classification score and regression offsets for every RoI. We can safely ignore the background RoIs. For the rest of the RoIs, the class with the highest probability is chosen as the output label, and the offsets corresponding to that class are chosen. We apply post-processing steps similar to that of the RPN: 1 We translate the offsets back to (xtl, ytl, xbr, ybr) format using the RoI. 2 We clip the output bounding box to within the image boundaries We face a problem similar to before: the output probably has multiple bounding boxes corresponding to the same object. We deal with it in the same way as earlier: using NMS. There is one difference, however. In the case of the RPN, we applied a global NMS across all bounding boxes predicted by the RPN. Here, NMS is applied only across the bounding boxes belonging to the same class. This is done for all classes, which should intuitively make sense: there is no point in suppressing highly overlapping bounding boxes if the bounding boxes represent different classes. NOTE Fully functional code for the Fast R-CNN inference, executable via Jupyter Notebook, can be found at http://mng.bz/nY48. Listing 11.25 PyTorch code for the Fast R-CNN inference def fast_rcnn_inference( frcnn_roi_head, Trained instance of Fast_RCNN_ROI_Head rois, RoIs to inference conv_feature_map, (n, c, h, w) convolutional feature map nms_threshold=0.7): frcnn_roi_head.eval() Sets eval mode roi_cls_scores, roi_loc = frcnn_roi_head(conv_feature_map, rois) output_labels = torch.argmax( The predicted class is the class with the highest score. roi_cls_scores, dim=1) output_probs = nn.functional.softmax( roi_cls_scores, dim=1)[torch.arange( The predicted probabilities are obtained via softmax. The highest probability is chosen as the probability score for this prediction. rois.shape[0]), output_labels] output_offsets = roi_loc.view( Converts locs from (n, num_classes*4) to (n, num_classes, 4) rois.shape[0], -1, 4) output_offsets = output_offsets[ Selects offsets corresponding to the predicted label torch.arange(rois.shape[0]), output_labels] assert output_offsets.shape == torch.Size( Asserts that we have outputs for each RoI [rois.shape[0], 4]) 434 CHAPTER 11 Neural networks for image classification and object detection output_bboxes = generate_bboxes_from_offset( Converts offsets to (xtl, ytl, xbr, ybr) output_offsets, rois) rois = output_bboxes.clamp(min=0, max=width) Clips bounding boxes to within images post_nms_labels, post_nms_probs, post_nms_boxes = [], [], [] for cls in range(1, frcnn_roi_head.num_classes+1): 0 is background, thus ignored cls_idxes = torch.where(output_labels == cls)[0] Performs NMS for each class cls_labels = output_labels[cls_idxes] cls_bboxes = output_bboxes[cls_idxes] cls_probs = output_probs[cls_idxes] keep_indices = torchvision.ops.nms( cls_bboxes, cls_probs, nms_threshold) post_nms_labels.append(cls_labels[keep_indices]) post_nms_probs.append(cls_probs[keep_indices]) post_nms_boxes.append(cls_bboxes[keep_indices]) return { ''labels'': torch.cat(post_nms_labels), ''probs'': torch.cat(post_nms_probs), ''bboxes'': torch.cat(post_nms_boxes) } 11.4.4 Training the Faster R-CNN As we have seen, the FRCNN consists of two subnetworks: An RPN responsible for generating good region proposals that contain objects A Fast R-CNN responsible for object classification and detection from a list of RoIs Thus the FRCNN is a two-stage object detector. We have one stage that generates good region proposals and another that takes the region proposals and detects objects in the image. So how do we train the FRCNN? A simple idea would be to train two independent networks (RPN and Fast R-CNN). However, we do not want to do this because it is expensive. Additionally, if we do so, each network will modify the convolutional layers in its own way. As discussed earlier, we want to share the convolutional layers across the RPN and the Fast R-CNN mod- ules. This ensures efficiency (only one conv backbone as opposed to two independent backbones). Additionally, both the RPN and FRCNN are performing similar tasks, so it intuitively makes sense to share the same set of convolutional features. Therefore, we need to develop a technique that allows for sharing convolutional layers between the two networks rather than learning two separate networks. The original FRCNN paper proposed two techniques to train the model: Alternate optimization (AltOpt)—We first train RPN and use the proposals to train the Fast R-CNN. The network tuned by the Fast R-CNN is then used to initialize 11.4 Faster R-CNN: A deep dive 435 the RPN, and this process is iterated. This involves multiple rounds of training alternating between training the RPN and Fast R-CNN. Approximate joint training—The RPN and Fast R-CNN networks are merged into one network during training. In each SGD iteration, the forward pass generates region proposals that are treated just like fixed, precomputed proposals when training a Fast R-CNN detector. We combine the RPN and Fast R-CNN losses and perform backpropagation as usual. This training is significantly faster as we are training both networks together end to end. However, the optimization is approximate because we treat the RPN-generated proposals as fixed, whereas in reality, they are a function of the RPN. So, we’re ignoring one derivative. Both techniques give similar accuracy. So joint training, which is significantly faster, is preferred. 11.4.5 Other object-detection paradigms So far, we have looked at the FRCNN in detail and discussed several key ideas that contribute to its success. Several other object-detection paradigms have also been de- veloped. Some are inspired by the FRCNN, borrowing and/or improving on the ideas established by the FRCNN. In this section, we briefly look at a few of them. YOU ONLY LOOK ONCE (YOLO) The FRCNN is a two-stage detector: an RPN followed by the Fast R-CNN, which runs on the region proposals generated by the RPN. YOLO (https://arxiv.org/pdf/1506.02640 .pdf), as the name implies, is a single-stage object detector. A single convolutional network simultaneously predicts multiple bounding boxes and class probabilities for those boxes directly from full images in one go. Some of the salient features of YOLO are as follows: YOLO is significantly faster ( 10× faster than the FRCNN) due to its much simpler architecture. YOLO can even be used for real-time object detection. Unlike the FRCNN, where the R-CNN module looks only at the region proposals, YOLO looks directly at the full image during training and testing. The speed of YOLO comes at the cost of accuracy. While YOLO is significantly faster, it is less accurate than the FRCNN. Several other improvements have been made on top of YOLO to improve accuracy while trying to maintain the simple, fast architecture. These include YOLO v2, YOLO v3, and so on. MULTIBOX SINGLE-SHOT DETECTOR (SSD) SSD (https://arxiv.org/pdf/1512.02325.pdf) tries to achieve a good balance between speed and accuracy. It is a single-stage network like YOLO: that is, it eliminates the proposal-generation (RPN) and subsequent feature-resampling stages. It also borrows the ideas of anchors from the FRCNN: applying a conv net on top of feature maps to make predictions relative to a fixed set of bounding boxes. Thus, a single deep network predicts class scores and box offsets for a fixed set of default bounding boxes using small convolutional filters applied to feature maps. To 436 CHAPTER 11 Neural networks for image classification and object detection achieve high detection accuracy, feature maps at different scales are used to make predictions at different scales. SSD is much more accurate than YOLO; however, it is still not as accurate as the FRCNN (especially for small objects). FEATURE PYRAMID NETWORK (FPN) The feature maps generated by conv nets are pyramidal: as we go deeper, the spatial resolution of the feature map keeps decreasing, and we expect the semantic information represented by the feature map to be more meaningful. High-resolution maps have low-level features, whereas low-resolution maps have more semantic features. In the case of the FRCNN, we applied object detection on only the last convolution map. SSD shows that there is useful information by using other feature maps for predic- tion. But SSD builds this pyramid high up the network (past the fourth convolution layer [conv4] of VGG). It specifically avoids the use of lower-layer features. Thus it misses the opportunity to reuse the higher-resolution maps of the feature hierarchy. FPN shows that these features are important, especially for detecting small objects. The FPN (https://arxiv.org/pdf/1612.03144.pdf) relies on an architecture that combines low-resolution, semantically strong features with high-resolution, semantically weak features via a top-down pathway and lateral connections. The bottom-up pathway is the forward pass of the convolutional layers. In the top-down path, there is a back connection from lower resolution to higher resolution via simple upsampling (it is merged with the bottom-up feature map using 1 × 1 convolutions). This merged feature map is used at every level to learn and make predictions. The FPN was originally implemented on top of the FRCNN. It is much more accurate, but it is much slower than YOLO/SSD-style approaches. We have only briefly mentioned the other prominent detection paradigms. The fundamental principles behind them remain the same. You are encouraged to read the papers to get a deeper and better understanding. Summary In this chapter, we took an in-depth look at various deep-learning techniques for object classification and localization: LeNet is a simple neural network that can classify handwritten digits from the MNIST data set. Simple networks like LeNet don’t extend well to more real-world image classifica- tion problems. Hence deeper neural networks that have more expressive power are needed. The VGG network is one of the most popular deep convolutional neural networks. It improves on prior state-of-the-art deep neural networks by using more convo- lution layers with smaller (3 × 3) filters. Such an architecture has two advantages: (1) more expressive power because of the added nonlinearity that comes from stacking more layers, and (2) a reduced number of parameters. Three 3 × 3 filters Summary 437 have 27C2 parameters, whereas a single 7 × 7 filter (which cover the same receptive field) has 49C2 parameters (81% more). VGG (and AlexNet) use ReLU nonlinear layers instead of sigmoid layers because they do not suffer from the vanishing gradient problem. Using ReLUs speeds up training, resulting in faster convergence. Inception blocks provide an efficient way to increase the depth and width of a neural network while keeping the computational budget constant. Multiscale filters are used at each convolution layer to learn patterns of different sizes, and 1 × 1 convolutions are used for dimensionality reduction (which reduces the number of parameters needed, thereby improving computational efficiency). ResNet is another popular convolutional neural network. The ResNet architecture was motivated by the fact that simply stacking layers beyond a certain point does not help and causes degradation even in training accuracies. This is a counterintuitive result because we expect that deeper networks can, at the very least, learn as much as their shallower counterparts. The authors of the ResNet papers showed that this may happen because the identity function is hard for neural networks to learn. To tackle this, they proposed shortcut/skip connections to simplify the neural network’s learning objective. This is the key idea behind ResNet and enables training of much deeper neural networks. Faster R-CNN is one of the most popular object detectors. It is a two-stage net- work consisting of (1) a region proposal network (RPN), which is responsible for predicting regions of interest that could potentially contain objects; and (2) an R-CNN module, which takes the region proposals as input and emits class scores and bounding boxes efficiently. The RPN module uses multiple anchor boxes (at each point on the conv feature map) to handle objects of different sizes and aspect ratios. It convolves a small network over the conv feature map to make predictions about objectness and bounding boxes at each sliding window location. Remember that the small network is a fully convolutional network (FCN) comprising 3 × 3 convs followed by 1 × 1 convs, enabling this approach to work with arbitrary-sized inputs and making it translation invariant. RoI pooling provides an efficient way to extract a fixed-sized feature vector from region proposals of varying sizes, all in one pass. These feature vectors are fed to a classifier and regressor for classification and localization, respectively. Non-maxima suppression (NMS) is a technique to de-duplicate overlapping bound- ing boxes. FRCNN can be trained using two methods: alternative optimization (AltOpt) and approximate joint training. Both approaches lead to similar accuracy numbers, but approximate joint training is significantly faster. You Only Look Once (YOLO), MultiBox Single-Shot Detector (SSD), and feature pyramid networks (FPN) are some other popular object detectors. 12 Manifolds, homeomorphism, and neural networks This chapter covers Introduction to manifolds Introduction to homeomorphism Role of manifolds and homeomorphism in neural networks This is a short chapter that briefly introduces (barely scratching the surface of) a topic that could fill an entire mathematics textbook. A rigorous or even comprehen- sive treatment of manifolds is beyond the scope of this book. Instead, this chapter primarily focuses on geometric intuitions required for deep learning. 12.1 Manifolds A manifold is a generalization of the notions of curve, surface, and volume into a uni- fied concept that works in arbitrary dimensions. In machine learning, the input space can be viewed as a manifold. Usually, the input manifold is not very conducive to classification. We need to transform (map) that manifold to a different manifold that is friendlier to the classification problem at hand. This is what a neural network does. 438 12.1 Manifolds 439 In a multilayered neural network, each layer transforms (maps) from one manifold to another. For the classification problem, the end manifold is expected to be one where the classes can be separated by a linear surface (hyperplane). The last layer provides this linear separator. Figure 12.1 provides an example of a transformation to a space where classification is easier. Feature space Output space X Y Z X Y Model Transform Figure 12.1 The input points lie on a cylindrical manifold (surface). Here, the two input classes (indicated by + and -, respectively) cannot be separated linearly. If we unroll (map) the cylinder surface into a plane, the two classes can be separated linearly. Unlike physical surfaces we see and touch in everyday life, manifolds are not limited to three dimensions. Human imagination may fail in visualizing higher-dimension manifolds, but we can typically use three-dimensional analogies as surrogates—they often work, although not always. A neural network layer or sequence of layers can be viewed as mapping points from one manifold to another. Manifolds are locally Euclidean. To get an intuitive understanding of this, imagine a circle with a very thin string wrapped around it. Now, take any point on the circle and take a small arc of the circle containing that point. If we cut off the portion of the string corresponding to that segment, we can straighten that little piece of string into a straight line without twisting or tearing it. In other words, the small neighborhood on the circle around the chosen point has 1:1 mapping with a line segment. All points on the circle satisfy this property. Such a curve is said to be locally Euclidean. The concept can be extended to higher dimensions. Consider the surface of a sphere (this is an example of a 2-manifold). Imagine a rubber sheet tightly fitted around the sphere. Take an arbitrary 440 CHAPTER 12 Manifolds, homeomorphism, and neural networks point and a small patch of the spherical surface containing the point (see figure 12.2c). If we cut off the rubber sheet corresponding to the patch, it can be flattened into a plane without twisting or tearing. So, the sphere surface is locally Euclidean. The torus surface (donut-shaped object) is another example of a 2-manifold. In general, d-manifold is a space (set of points) on which every point has a small neighborhood of points in the space that contains the point and can be mapped 1:1 to ℝd without twisting or tearing. For instance, a circle is a 1D manifold, and every point on it has a container arc that can be mapped to a line (ℝ1). A sphere surface is 2D manifold, and every point of it has a containing patch that maps 1:1 to a plane (ℝ2). We say manifolds are locally Euclidean. Figure 12.2 illustrates the concept. Circle Smooth curve (a) Any arbitrary continuous curve (for example, a circle) is a 1D manifold. The local neighborhood of this point cannot be mapped to a line. (b) The 8-curve is a non-manifold. The 휒-shaped neighborhood of the marked point cannot be mapped 1 : 1 to a straight-line segment. (c) A sphere is an example of a 2-manifold. (d) An hourglass is an example of 2D non- manifold surface. Figure 12.2 Manifolds and non-manifolds in 1D and 2D A little thought will reveal that the locally Euclidean property makes calculus possi- ble. For instance, how do we compute the area under a curve, f (x) in the interval 12.1 Manifolds 441 x = a and x = b? The formula is ∫x=b x=a f (x) dx. We take infinitesimally small segments of the curve and pretend they are straight lines, which can be projected onto a tiny line segment on the X axis. The resulting narrow quadrilateral can be approximated by a rectangle whose sides are parallel to the X and Y axes. We sum (integrate) the areas of all the tiny rectangles that cover the same area as the one we are computing (see figure 12.3). This scheme depends on the ability to represent tiny segments of the curve as straight lines. A similar case can be made about computing the length of the curve segment. In higher dimensions, the same idea applies: calculus is depen- dent on the ability to pretend that a curve or surface can be locally approximated by something flat. The graph of any continuous vector function ®y = f  ®x, where ®x is any open subset of ℝn—that is, ®x ∈핌⊂ℝn and f  ®y ∈ℝm—yields a manifold in  ®x, ®y in ℝm × ℝn. Figure 12.3 Area under a curve is computed by locally approximating curve segment with tiny straight lines—needs locally Euclidean property In this context, note that the entire sphere cannot be mapped to a plane (try opening the sphere onto a plane). This is why it is impossible to draw a perfect map of the globe on a piece of paper with all regions drawn to scale. Typically, the polar regions occupy areas disproportionately large on the paper map. But small patches on the spherical surface can be mapped onto planes, which is enough to call this surface a manifold. Hence the word local in locally Euclidean. 12.1.1 Hausdorff property Manifolds usually have another property, known as the Hausdorff property. If we take any pair of points on a manifold—no matter how close they are—we can find a pair of disjoint neighborhoods around the respective points consisting entirely of points on the manifold. Loosely speaking, this means if we take any pair of points on the manifold, we can find an infinite number of points between them, all belonging to the manifold. This is illustrated in figure 12.4. It’s easy to see that this is true for the real line (ℝ1): take any pair of points, and there are enough points between them to create a disjoint pair of neighborhoods centered on each. 442 CHAPTER 12 Manifolds, homeomorphism, and neural networks Figure 12.4 Hausdorff property: on a manifold, for any pair of points, we can find a disjoint neighborhood pair containing the respective points and consisting of points from the manifold. 12.1.2 Second countable property Manifolds are second-countable. To explain this, we will first briefly outline a few concepts. (Disclaimer: the following explanations err toward ease of understanding as opposed to mathematical rigor.) OPEN SETS, CLOSED SETS, AND BOUNDARIES Consider the set of points belonging to the interval A ≡0 < x < 1 (you can imagine it to be a segment of the real line). Take any point in this set A, say x = 0.93. You can find a point on its left (say, 0.92) and on its right (say, 0.94), both of which are in the same set A. In some sense, the point is surrounded by points belonging to the same set. Hence it is an internal point. The funny thing is, in this set, all points are internal. In comparison, consider the set Ac ≡0 ≤x ≤1. This includes the previous set A as well as the boundary points x = 0 and x = 1. Note that the boundary points can be approached from both inside and outside the set A. If we take a small neighborhood of any point in the boundary, consisting of all points within a small distance 휖, there are points inside and outside A. A is an open set. If we add its boundary to itself, it becomes a closed set Ac. The concept extends to higher dimensions. For instance, the set of 2D points belong- ing to the unit disc S ≡x2 + y2 < 1 is an open set. If we add the boundary—the circle Sc ≡x2 + y2 = 1—we get a closed set. All this is illustrated in figure 12.5. Figure 12.5 The open set, disk without a bound- ary is shown in gray. The boundary is shown in black. Gray+black is the closed set. 12.2 Homeomorphism 443 BOUNDED, COMPACT, AND PRECOMPACT SETS A set is said to be bounded if all its points lie within a fixed distance of each other. The sets A, Ac, and S, Sc discussed previously are bounded. A compact set is bounded as well as closed. The sets Ac and Sc are compact. And a set is precompact if it can be converted into a compact set by just adding its boundary (for example, A and S). Note that not all open sets are precompact: for example, −∞< x < ∞is open but not precompact. All precompact sets are open, however. Manifolds may or may not include a boundary. A disc is a 2-manifold with a boundary. Its boundary is the circle’s circumference, which is a 1-manifold. A three-dimensional ball is a 3-manifold with a boundary. The boundary is the surface of the sphere, which is a 2-manifold. The open set of points on the disc sans the boundary is also a 2-manifold. A square area is a 2-manifold with a boundary whose boundary is the square, which is a 1-manifold. A three-dimensional cube is a 3-manifold with a boundary whose boundary is a 2-manifold corresponding to the surface of the cube. In general, the boundary of a d-manifold with boundary is a d −1-manifold. Now, let’s go back to the second countable property of manifolds. The second countable property of a manifold implies that every manifold has a basis of open sets. This means for every manifold M, there exists a countable collection U ≡{Ui}, where Ui are precompact subsets of M and any open subsets of M can be expressed as a union of elements of U. This is illustrated in figure 12.6. Figure 12.6 Second countable property for manifolds. The solid curve indicates the (missing) boundary of an arbitrary open subset of the manifold. The dashed balls indicate the (missing) boundaries of elements of the basis set Ui. The area within the solid curve is shown to be covered by a union of dashed ball-bounded areas. 12.2 Homeomorphism We have been speaking about 1:1 mappings between an arc of a circle and a line segment. If we have a piece of string tightly fitted on the arc, all we have to do is to grab the string’s two ends and pull it out to get the corresponding straight line—we do not have to do any twisting or tearing (see the black arc in figure 12.2a). Similarly, the mapping between a patch on the surface of a sphere can be mapped 1:1 to a plane by simply pulling an imaginary rubber sheet fitted on the patch (see the patch in figure 12.2c). 444 CHAPTER 12 Manifolds, homeomorphism, and neural networks These are examples of a general class of mappings called homeomorphism. Formally, a homeomorphic mapping comprises a pair of functions f and f −1 between two sets of points X and Y such that ®y ∈Y = f  ®x ∈X f : X ↦→Y ®x ∈X = f −1  ®y ∈Y f −1 :Y ↦→X where f is 1:1: it maps each ®x to a unique ®y, and distinct ®xs are mapped to distinct ®ys. f −1 is 1:1: it maps each ®y to an unique ®x, and distinct ®ys are mapped to distinct ®xs. f is a continuous function: it maps nearby values of ®x to nearby values of ®y. f −1 is continuous function: it maps nearby values of ®y to nearby values of ®x. An intuitive way to visualize homeomorphism is that it transforms one manifold to another by stretching or squishing but never by cutting, breaking, or folding. Homeo- morphism preserves path-connectedness. A set of points is said to be path-connected if a path between any pair of them exists, comprising points belonging to the set. 12.3 Neural networks and homeomorphism between manifolds Consider two classes A and B defined on the real line: A ≡{−1 ≤x ≤1} B ≡{−3 ≤x ≤−2 or 2 ≤x ≤3} The 1-manifolds corresponding to these classes are not clearly separable in their original space (see figure 12.7a) because class A is “surrounded” by class B. But if we pinch the origin and pull it up—in other words, perform a specific homeomorphism—to transform it, as shown in figure 12.7c, it is possible to separate the transformed manifolds with a straight line. Similarly, figure 12.7b shows two classes A and B: A ≡  ∥®x∥2 ≤1 B ≡  ∥®x∥2 ≥4 and ∥®x∥2 ≤9 These are 2-manifolds that are hard to separate in their original space because, again, class B surrounds class A. But if we pinch and pull the origin to create the manifolds shown in figure 12.7d, they become separable by a plane (see figure 12.7d). A linear layer of a neural network does the following transformation (discussed in detail in equation 8.7): ®z = 휎  W ®x + ®b  Note that all of these operations, multiplication with the weight matrix W, translation by ®b, and the sigmoid nonlinearity are continuous invertible functions. Hence, they are homeomorphisms. The composite operation where these are applied in sequence is another homeomorphism. (Strictly speaking, multiplication by a weight matrix is invertible Summary 445 Class A Class B Pinch and pull up here. -3 -2 -1 0 1 2 3 Class B (a) A linear classifier cannot separate points on the line. Pinch and pull up here. (b) A linear classifier cannot separate points on the disk. Linear classifier Class B Class A Class B (c) Lines transformed to a curved shape allow classification with a linear classifier (line). Linear classifier (d) Disks transformed to a 3D bell-like shape allow classi- fication with a linear classifier (plane). Figure 12.7 Homeomorphism to a friendlier manifold can help the classification. only when the weight matrix W is square with a nonzero determinant. And if the weight matrix has a zero determinant, the layer effectively does a dimensionality reduction.) One way to view a multilayered neural network is that the successive layers homeo- morphically transform the input manifold so that it becomes easier and easier to separate the classes. The final layer may be a simple linear separator (as in figure 12.7). Summary A manifold is a hyperdimensional collection of points (space) that satisfies three properties: locally Euclidean, Hausdorff, and second countable. A d-manifold is a space in which every point has a small neighborhood that contains the point and can be mapped 1:1 to a subregion of ℝd without folding, twisting, or tearing. In other words, the local neighborhood around any point on a manifold can be approximated by something flat. For instance, a continuous curve in 3D space is a 1-manifold: if we imagine the curve as a string, any local neighborhood is a substring that can be pulled and straightened into a straight line. Any continuous surface in 3D is a 2-manifold: if we imagine the surface as a rubber membrane, any local neighborhood can be pulled and stretched into a flat planar patch. This property (the ability to pretend that a curve or a surface can be locally replaced by a linear piece) is what enables us to perform calculus. 446 CHAPTER 12 Manifolds, homeomorphism, and neural networks Homeomorphism is a special class of transformation (mapping) that transforms one manifold to another via stretching/squishing without tearing, breaking, or folding. Homeomorphism preserves path-connectedness. Roughly speaking, a neural network layer can be viewed as a homeomorphic transform that maps points from its input manifold to an output manifold that is (hopefully) more suitable for the end goal. The entire multilayered perceptron (neural network) can be viewed as a sequence of homeomorphisms that, in a series of steps, transform the input manifold to a manifold that makes the end goal easier. In particular, a classifier neural network maps the input manifold to an output manifold where the points belonging to different classes are well-separated. 13 Fully Bayes model parameter estimation This chapter covers Fully Bayes parameter estimation for unsupervised modeling Injecting prior belief into parameter estimation Estimating Gaussian likelihood parameters with known or unknown mean and precision Normal-gamma and Wishart distributions Suppose we have a data set of interest: say, all images containing a cat. If we represent images as points in a high-dimensional feature space, our data set of interest forms a subspace of that feature space. Now we want to create an unsupervised model for our data set of interest. This means we want to identify a probability density function p  ®x whose sample cloud (the set of points obtained by repeatedly sampling the probability distribution many times) largely overlaps our subspace of interest. Of course, we do not know the exact subspace of interest, but we have collected a set of samples X from the data set of interest: that is, the training data. We can use the point cloud for X as a surrogate for the unknown subspace of interest. Thus, we are essentially trying to identify a probability density function p  ®x whose sample cloud, by and large, overlaps X. 447 448 CHAPTER 13 Fully Bayes model parameter estimation Once we have the model p  ®x, we can use it to generate more data samples; these will be computer-generated cat images. This is generative modeling. Also, given a new image ®a, we can estimate the probability of it being an image of a cat by evaluating p  ®a. 13.1 Fully Bayes estimation: An informal introduction Let’s recap Bayes’ theorem: posterior probability z }| { p (휃|X) = p (X, 휃) p (X) = likelihood z }| { p (X|휃) prior probability z}|{ p (휃) p (X) |{z} evidence (13.1) Here, X =  ®x1, ®x2, · · · denotes the training data set. Our ultimate goal is to identify the likelihood function p  ®x 휃. Estimating the likelihood function has two aspects: selecting the function family and estimating the parameters. We usually preselect the family from our knowledge of the problem and then estimate the model parameters. For instance, the family for our model likelihood function might be Gaussian: p  ®x 휃 = N  ®x; ®휇, 횺 (as before, the semicolon separates the model variables from model parameters). Then 휃=  ®휇, 횺 are the model parameters to estimate. We estimate 휃such that the overall likelihood p (X|휃) = Î i p  ®xi 휃 best fits the training data X. We want to re-emphasize the mental picture that best fit implies that the sample cloud of the likelihood function (repeated samples from p  ®x 휃) largely overlaps the training data set X. For the Gaussian case, this implies that the mean ®휇should fall at a place where there is a very high concentration of training data points, and the covariance matrix 횺should be such that the elliptical base of the likelihood function tightly contains as many training data points as possible. 13.1.1 Parameter estimation and belief injection There are various possible approaches to parameter estimation. The simplest approach is maximum likelihood parameter estimation (MLE), introduced in section 6.6.2. In MLE, we choose the parameter values that maximize p (X|휃), the likelihood of observing the training data set. This makes some sense. After all, the only thing we know to be true is that the input data set X has been observed—this being unsupervised data, we do not know anything else. It is reasonable to choose the parameters that maximize the probability of that known truth. If the training data set is large, MLE estimation works well. However, in the absence of a sufficiently large amount of training data, it often helps to inject our prior knowledge about the system into the estimation—prior knowledge can cover for the lack of data. This injection of guess/belief into the system is done via the prior probability density. To do this, we can no longer maximize the likelihood, as likelihood ignores the prior. We have to do maximum a posteriori (MAP) estimation, which maximizes the posterior probability. The posterior probability is the product of 13.2 MLE for Gaussian parameter values (recap) 449 likelihood (which depends on the data) and the prior (which does not depend on data; we will make it reflect our prior belief). There are two possible MAP paradigms. We saw one of them in section 6.6.3, where we injected our belief that the unknown parameters must be small in magnitude and set p (휃) ∝e−∥휃∥2 as a regularizer. The system was incentivized to select parameter values that are relatively smaller in magnitude. In this chapter, we study a different paradigm; let’s illustrate it with an example. Suppose we model the likelihood as a Gaussian: p  ®x 휃 = N  ®x; ®휇, 횺. We have to estimate the parameters 휃=  ®휇, 횺 from the training data X, for which we must max- imize the posterior probability. To compute the posterior probability, we need the prior probability. In addition, we must somehow inject constant values as our belief (lacking observed data) about the parameter values. How about modeling the prior probability as a Gaussian probability density function in the parameter space? Ignoring the covariance matrix parameter for the sake of simplicity, we can model the probability density of the mean parameter as p   ®휇 = N   ®휇; ®휇0, 횺0 . We are essentially saying that we believe the parameter ®휇is likely to have a value near ®휇0 with a confidence 횺0. In other words, we are injecting a constant value as our belief in the parameter ®휇. We can treat the covariance similarly. Later, we prove that in this paradigm, with a low volume of training data, the prior dominates. Once sufficient training data is digested, the effect of the prior fades, and the solution gets closer and closer to the MLE. This is the fully Bayes parameter estimation technique in a nutshell. In this chapter, we discuss Bayes estimation of parameters for a Gaussian likelihood function for a series of increasingly complex scenarios. In section 13.3, we deal with the case where the variance of the parameters to be estimated is known (constant) but the mean is unknown, so the mean is expressed as a (Gaussian) random variable. Then, in section 13.6, we examine the case where the mean is known (constant) but the variance is unknown. Finally, in section 13.7, we examine the case where both are unknown. Both the univariate and multivariate cases are dealt with for each scenario. NOTE Fully functional code for this chapter, executable via Jupyter Notebook, can be found at http://mng.bz/woYW. 13.2 MLE for Gaussian parameter values (recap) We have discussed the details of this in section 6.8. Here we recap the main results. Suppose we have a data set X =  x(1), x(2), · · · , x(n) . We have decided to model the data distribution as a Gaussian N (x; 휇, 휎)—we want to estimate the parameters 휇, 휎that best “explain” or “fit” the observed data set X. MLE is one of the simplest approaches to solving this problem. Here we estimate the parameters such that the likelihood of the data observed during training is maximized. This can be loosely visualized as estimating a probability density function whose peak coincides with the region in the input space with the densest population of training data. We looked at MLE in section 6.8. Here we simply restate the expressions. 450 CHAPTER 13 Fully Bayes model parameter estimation Let’s denote the (as yet unknown) mean and variance of the data distribution as 휇 and 휎. Then from equation 5.22, we get p  x(i) 휇, 휎  = 1 √ 2휋휎 e −  x(i) −휇 2 2휎2 p (X| 휇, 휎) = p  x(1), · · · , x(n) 휇, 휎  = n Ö i=1 p  x(i) 휇, 휎  = 1 √ 2휋휎 n e −Ín i=1  x(i) −휇 2 2휎2 Maximizing the log-likelihood p (X| 휇, 휎) has a closed-form solution: 휇= ¯x = 1 n n Õ i=1 x(i) 휎2 = s = 1 n n Õ i=1  x(i) −¯x 2 (13.2) Thus the MLE mean and variance are essentially the mean and variance of the training data samples (see section 6.6 for the derivation of these expressions). The corresponding expressions for multivariate Gaussian MLE are ®휇= ¯®x = 1 n n Õ i=1 ®x(i) 횺= S = 1 n n Õ i=1  ®x(i) −¯®x   ®x(i) −¯®x T (13.3) These MLE parameter values are to be used to evaluate p (x) = N (x; 휇, 휎)—the prob- ability of an unknown data point x coming from the distribution represented by the training data set X. 13.3 Fully Bayes parameter estimation: Gaussian, unknown mean, known precision MLE may not be that accurate when the available data set is small (that is, n, size of the data set X, is small). In many problems, we have a prior idea of the mean and sigma of the data set. Unfortunately, MLE provides no way to bake such a prior belief into the estimation. Fully Bayes parameter estimation techniques try to fix this drawback: here we are not simply maximizing the likelihood of the observed data. Instead, we maximize the posterior probability of the estimated parameter(s). This posterior probability involves the product of the likelihood and a prior probability (see equation 13.1). The likelihood term captures the effect of the training data—maximizing it alone is MLE—but does not capture the effect of a prior belief. On the other hand, the prior term does not depend on the data. This is where we bake in our belief or guess or prior knowledge about the data distribution. Thus, our estimate for the data distribution parameters will consider the data and the prior guess. We will soon see that the estimation is such that as the size of the data set (n, length of X) increases, the effect of the prior term 13.3 Fully Bayes parameter estimation: Gaussian, unknown mean, known precision 451 decreases, and the effect of the likelihood term increases. In the limit, at infinite data availability, the Bayesian inference yields the MLE. At the other extreme, when no data is available (n = 0), the Fully Bayes estimates for the parameters are the same as the prior estimates. Let’s we examine Bayesian parameter estimation. For starters, we deal with a relatively simple case where we have a Gaussian data distribution with a known (constant) variance but unknown and modeled mean. The data distribution is Gaussian (as usual, the semicolon in N (x; 휇n, 휎) separates the variables from parameters): p (x| 휇, 휎) = p (x| 휇) = N (x; 휇, 휎) ∝e −(x−휇)2 2휎2 The training data set is denoted X =  x(1), x(2), · · · , x(n) , and its overall likelihood is p (X| 휇) = p  x(1), · · · , x(n) 휇  = n Ö i=1 p  x(i) 휇  ∝e −Ín i=1  x(i) −휇 2 2휎2 The variance is known by assumption—hence it is treated as a constant instead of a random variable. The mean 휇is unknown and is treated as a Gaussian random variable, with mean 휇0 and variance 휎0 (not to be confused with 휇and 휎, the mean and variance of the data itself ). So, the prior is p ( 휇) = N ( 휇; 휇0, 휎0) ∝e − 휇−휇0 2 2휎2 0 The posterior probability of the unknown 휇parameter is a product of two Gaussians, which is a Gaussian itself. Let’s denote the (as-yet-unknown) mean and variance of this product Gaussian as 휇n and 휎n. Here the subscript n is to remind us that the posterior has been obtained by digesting n data instances from X =  x(1), x(2), · · · , x(n) . Thus, the Gaussian posterior can be denoted as p ( 휇|X) = N ( 휇; 휇n, 휎n) ∝e −( 휇−휇n )2 2휎2n Using Bayes’ theorem, p ( 휇|X) ∝p (X| 휇) p ( 휇) or posterior z }| { e −( 휇−휇n )2 2휎2n ∝ likelihood z }| { e− Ín i=1  x(i) −휇 2 2휎2 prior z }| { e −  휇−휇0 2 2휎2 0 By comparing the coefficients of 휇2 and 휇on the exponents of the left and right sides, we determine the unknown parameters of the posterior distribution: 452 CHAPTER 13 Fully Bayes model parameter estimation 1 휎2 n = 1 휎2 0 + n 휎2 or 휎2 n = 휎2 0 휎2 n휎2 0 + 휎2 휇n = 휎2 n Ín i=1 x(i) 휎2 + 휇0 휎2 0 ! = 휎2 0 휎2 n휎2 0 + 휎2 n¯x 휎2 + 휇0 휎2 0 ! = ¯x 1 + 휎2 n휎2 0 + 휇0 1 + n휎2 0 휎2 (13.4) The significance of various closely named variables should be clearly understood: 휇, 휎are the mean and variance of the data distribution p (x)—assumed to be Gaussian. The final goal is to estimate 휇, 휎that best fits the data set X. On the other hand, 휇0, 휎0 are the mean and variance of the parameter distribution p ( 휇), which captures our prior belief about the value of the data mean 휇(remember, by assumption, the data mean is also a Gaussian random variable). 휇n, 휎n are the mean and variance of the posterior distribution p ( 휇|X) for the data mean 휇as computed from n data point samples. This is a Gaussian random variable because it is a product of two Gaussians. The posterior distribution of the unknown mean parameter, p ( 휇|X), is a Gaussian with mean 휇n. So, it will attain a maximum when 휇= 휇n. In other words, the MAP estimate for the unknown mean 휇is 휇MAP = 휇n. Even though 휇n is the best estimate of 휇, 휎n is not approximating the 휎of the data, 휎is known in this case by assumption. Here, 휎n is the variance of the posterior distribution, reflecting our uncertainty about the estimate of 휇. That is why, as the number of data instances becomes very large, 휎n approaches 0 (indicating we have zero uncertainty or full confidence in the estimate of the mean.) The estimate for our data distribution is p (x) = N (x; 휇n, 휎), where 휇n is given by equa- tion 13.4. Note that it is a combination of the MLE ¯x and prior guess 휇0. Using this, given any arbitrary data instance x, we can infer the probability of x belonging to the class of the training data set X. NOTE Fully functional code for Bayesian estimation with unknown mean and known variance, executable via Jupyter Notebook, can be found at http://mng.bz /ZA75. Listing 13.1 PyTorch: Bayesian estimation with unknown mean, known variance import torch def inference_unknown_mean(X, prior_dist, sigma_known): mu_mle = X.mean() n = X.shape[0] mu_0 = prior_dist.mean sigma_0 = prior_dist.scale Parameters of the prior 13.4 Small and large volumes of training data, and strong and weak priors 453 mu_n = mu_mle / (1 + sigma_known**2 / (n * sigma_0**2)) + mu_0 / (1 + n * sigma_0**2 / sigma_known**2) Mean of the posterior, following equation 13.4 sigma_n = math.sqrt( (sigma_0**2 * sigma_known**2) / (n*sigma_0**2+sigma_known**2)) Standard deviation of the posterior, following equation 13.4 posterior_dist = torch.Normal(mu_n, sigma_n) return posterior_dist 13.4 Small and large volumes of training data, and strong and weak priors Let’s examine the behavior of equation 13.4 when n = 0 (no data) and when n −→∞ (lots of data): lim n−→0 ( 휇n = 휇0 휎n = 휎0 lim n−→∞ ( 휇n = ¯x = 휇MLE 휎n = 0 This agrees with our notion that with little data, the posterior is dominated by the prior, while with lots of data, the posterior is dominated by the likelihood. With lots of data, the variance of the parameter is zero (we are saying with full certainty that the best value for the mean is the sample mean for the data, aka the MLE estimate for the mean). In general, with more training data (that is, larger values of n), the posterior shifts closer to the likelihood. This can be seen by analyzing equation 13.4. It agrees with our intuition that with little data, we try to compensate with our pre-existing (prior) belief as to the value of the parameters. As the number of training data instances increases, the effect of the prior is reduced, and the likelihood (which is a function of the data) begins to dominate. A low variance for the prior (that is, small 휎0) essentially means we have low un- certainty in our prior belief (remember, the entropy/uncertainty of a Gaussian is proportional to its variance). Such high-confidence priors resist being overwhelmed by the data and are called strong priors. On the other hand, a large 휎0 implies low certainty/confidence in the prior mean value. This is a weak prior that is easily over- whelmed by the data. We can see this in the final expression for mean in equation 13.4: we have n휎2 0 휎2 in the denominator of the second term. In general, the second term vanishes with larger n, thereby removing the effect of the prior 휇0 and making the posterior mean coincide with the MLE mean. But the smaller the 휎0, the larger the n required to achieve this, and vice versa. 454 CHAPTER 13 Fully Bayes model parameter estimation 13.5 Conjugate priors In section 13.3, given a Gaussian likelihood, choosing the Gaussian family for the prior made the posterior also belong to the Gaussian family. This simplified things considerably. If the prior was chosen from another family, the posterior—which is the product of the likelihood and prior—may not belong to a simple or even known distribution family. Thus, a Gaussian likelihood with a Gaussian prior results in a Gaussian posterior for the mean. Such priors are said to be conjugate. Formally, for a specific family of likelihood, the choice of the prior that results in the posterior belonging to the same family as the prior is called a conjugate prior. For instance, Gaussians for the mean (with known variance) are conjugate to a Gaussian likelihood. Soon we will see that for a Gaussian likelihood, a gamma distribution for the precision (inverse of the variance) results in a gamma posterior. In other words, a gamma prior to the precision is conjugate to a Gaussian likelihood. In the multivariate case, instead of gamma, we have the Wishart distribution as a conjugate prior. 13.6 Fully Bayes parameter estimation: Gaussian, unknown precision, known mean In section 13.3, we discussed fully Bayes parameter estimation with the assumption that we somehow know the variance 휎and only want to estimate the mean 휇. Now we examine the case where the mean is known but the variance is unknown and expressed as a random variable. The computations become simpler if we use precision 휆instead of variance 휎. They are related by the expression 휆= 1 휎2 . Thus we have a data set X, which is assumed to be sampled from a Gaussian distribution with a constant mean 휇, while the precision 휆is a random variable with a gamma distribution. The probability density function for the data is thus p (x| 휇, 휆) = N  x; 휇, 1 √ 휆  . We model the prior random variable for precision with a gamma distribution. The likelihood is Gaussian, and since the product of a Gaussian and gamma is another gamma (due to the conjugate prior property of gamma), the resulting posterior is a gamma. The gamma function parameters for the posterior can be derived via coeffi- cient comparison. The maximum of the posterior is our estimate for the parameter. Gamma distribution The gamma distribution is introduced in the appendix; if necessary, please read that first. Here we state the relevant properties. The probability density function for a ran- dom variable 휆having a gamma distribution is a function with two parameters 훼, 훽: p (휆|훼, 훽) = 훾(휆; 훼, 훽) = 훽훼 Γ (훼) 휆(훼−1)e−훽휆 where 훼, 훽> 0, 휆≥0 (13.5) 13.6 Fully Bayes parameter estimation: Gaussian, unknown precision, known mean 455 Maximum of a gamma distribution To maximize the gamma probability density function p (휆|X) = 휆(훼n−1)e−훽n휆for a ran- dom variable 휆, we take the derivative and equate to zero: d d휆  휆훼−1e−훽휆 = 0 =⇒(훼−1) 휆훼−2e−훽휆+ 휆훼−1 (−훽) e−훽휆= 0 휆= 훼−1 훽 13.6.1 Estimating the precision parameter Let’s return to the fully Bayes estimation of the precision parameter when the mean is known. We model the data distribution with a Gaussian: p (x| 휇, 휆) = N  x; 휇, 1 √ 휆  (we have expressed this Gaussian in terms of the precision, 휆, which is related to the variance 휎as 휆= 1 휎2 ). The training data set is X =  x(1), x(2), · · · , x(n) , and its overall likelihood is p (X| 휇, 휆) = p (X|휆) = n Ö i=1  N  x(i); 휇, 1 √ 휆  ∝휆 n 2 e−휆 2 Ín i=1  x(i) −휇 2 We model the prior for the precision with a gamma distribution with parameters 훼0, 훽0: p (휆) = 훾(휆; 훼0, 훽0) ∝휆(훼0−1)e−훽0휆 We know the corresponding posterior—a product of a Gaussian and a gamma—is another gamma distribution (due to the conjugate prior property of gamma distribu- tion). Let’s denote the posterior as p (휆|X) = 훾(휆; 훼n, 훽n) From Bayes’ theorem, p (휆|X) ∝p (X|휆) p (휆) or posterior z }| { 휆(훼n−1)e−훽n휆∝ likelihood z }| { 휆 n 2 √ 2휋 e−휆 2 Ín i=1  x(i) −휇 2 prior z }| { 휆(훼0−1)e−훽0휆 Substituting s = 1 n n Õ i=0  x(i) −휇 2 and comparing the powers of 휆and e, we get 훼n = n 2 + 훼0 훽n = 1 2 n Õ i=1  x(i) −휇 2 + 훽0 = n 2 s + 훽0 (13.6) 456 CHAPTER 13 Fully Bayes model parameter estimation Notice that as before, at low values of n, the posterior is dominated by the prior but gets closer and closer to the likelihood estimate as n increases. In other words, in the absence of sufficient data, we let our belief take over the estimation; but if and when data is available, the estimation is dominated by the data-based entity likelihood. The MAP point estimate for the parameter 휆given data set X is obtained by maximiz- ing this posterior distribution p (휆|X) = 훾(휆; 훼n, 훽n), which yields 휆MAP = 1 휎2 MAP =  훼n−1 훽n  . (Section A.5 in the appendix shows how to obtain the maximum of a gamma distri- bution.) Thus our estimate for the training data distribution is p (x) = N (x; 휇, 휎MAP), where 1 휎2 MAP =  훼n−1 훽n  . Given a large volume of data, the MAP estimate for the unknown precision/variance becomes identical to the MLE estimate (proof outline shown): lim n→∞ 1 휎2 MAP = lim n→∞  훼n −1 훽n  = lim n→∞ n 2 + 훼0 −1 n 2 s + 훽0 = lim n→∞ 1 + 훼0−1 n 2 s + 훽0 n 2 = 1 s or lim n→∞휎2 MAP = s = 1 n n Õ i=0  x(i) −휇 2 = 휎2 MLE On the other hand, given no data, the MAP estimate for the unknown precision/variance is completely determined by the prior (proof outline shown): lim n→0 1 휎2 MAP = lim n→0  훼n −1 훽n  = lim n→0 n 2 + 훼0 −1 n 2 s + 훽0 = 훼0 −1 훽0 or lim n→0 휎2 MAP = 훽0 훼0 −1 NOTE Fully functional code for Bayesian estimation with a known mean and unknown variance, executable via Jupyter Notebook, can be found at http://mng.bz /2nZ9. Listing 13.2 PyTorch: Bayesian estimation with unknown variance, known mean import torch def inference_unknown_variance(X, prior_dist): sigma_mle = torch.std(X) n = X.shape[0] alpha_0 = prior_dist.concentration beta_0 = prior_dist.rate Parameters of the prior alpha_n = n / 2 + alpha_0 beta_n = n / 2 * sigma_mle ** 2 + beta_0 Parameters of the posterior posterior_dist = torch.Gamma(alpha_n, beta_n) return posterior_dist 13.7 Fully Bayes parameter estimation: Gaussian, unknown mean, unknown precision 457 13.7 Fully Bayes parameter estimation: Gaussian, unknown mean, unknown precision In section 13.3, we saw that if the variance is known, the conjugate prior to the mean is a Gaussian (aka normal) distribution. Likewise, when the mean is known, the conjugate prior to the precision is a gamma distribution. If both are unknown, we end up with a normal-gamma distribution. 13.7.1 Normal-gamma distribution Normal-gamma is a probability distribution of two random variables, say, 휇and 휆, whose density is defined in terms of four parameters 휇 ′, 휆 ′, 훼 ′, and 훽 ′, as follows: p  휇, 휆; 휇 ′, 휆 ′, 훼 ′, 훽 ′ = N훾  휇, 휆; 휇 ′, 휆 ′, 훼 ′, 훽 ′ = 훽 ′훼 ′ √ 휆 ′ Γ (훼 ′) √ 2휋 e−휆 2 휆 ′  휇−휇 ′ 2 휆훼 ′ −1 2 e−훽 ′휆 Although it looks complicated, a simple way to remember it is a product of a normal and a gamma distribution. The normal-gamma distribution attains a maximum at 휇= 휇 ′ 휆= 훼 ′ −1 2 훽 ′ 13.7.2 Estimating the mean and precision parameters As before, we model the data distribution with a Gaussian: p (x| 휇, 휆) = N  x; 휇, 1 √ 휆  (we have expressed this Gaussian in terms of the precision, 휆, which is related to the variance 휎as 휆= 1 휎2 ). The training data set is X =  x(1), x(2), · · · , x(n) , and its overall likelihood is p (X| 휇, 휆) = n Ö i=1  N  x(i); 휇, 1 √ 휆  ∝휆 n 2 e−휆 2 Ín i=1  x(i) −휇 2 We model the prior for the mean as a Gaussian with mean 휇0 and precision 휆0휆: p ( 휇|휆) = N ( 휇; 휇0, 휆0휆) ∝휆 1 2 e−휆0휆 2 ( 휇−휇0)2 We model the prior for the precision as a gamma distribution with parameters 훼0, 훽0: p (휆) = 훾(휆; 훼0, 훽0) ∝휆(훼0−1)e−훽0휆 The overall prior probability for the mean and precision parameters is the product of the two, a normal-gamma distribution with parameters 휇0, 휆0, 훼0, 훽0: p ( 휇, 휆) = N훾  휇, 휆 휇0, 휆0, 훼0, 훽0 ∝휆 1 2 e−휆0휆 2 ( 휇−휇0)2 휆(훼0−1)e−훽0휆 The posterior probability for the mean and precision parameters is the joint (that is, product) of the likelihood and the prior. As such, we know it is another normal-gamma distribution (due to the conjugate prior property of normal-gamma): 458 CHAPTER 13 Fully Bayes model parameter estimation p ( 휇, 휆|X) = N훾( 휇, 휆| 휇n, 휆n, 훼n, 훽n) ∝e−휆 2 휆n ( 휇−휇n)2휆훼n−1 2 e−훽n휆 Using Bayes’ theorem, p ( 휇, 휆|X) ∝p (X| 휇, 휆) p ( 휇, 휆) or posterior z }| { e−휆 2 휆n ( 휇−휇n)2휆훼n−1 2 e−훽n휆∝ likelihood z }| { 휆 n 2 e−휆 2 Ín i=1  x(i) −휇 2 prior mean z }| { 휆 1 2 e−휆0휆 2 ( 휇−휇0)2 prior precision z }| { 휆(훼0−1)e−훽0휆 Substituting s = 1 n n Õ i=0  x(i) −휇 2 and comparing coefficients, the unknown parameters of the posterior distribution can be determined: 휇n = (n¯x + 휇0휆0) n + 휆0 휆n = n + 휆0 훼n = n 2 + 훼0 훽n = ns 2 + 훽0 + n휆0 2 (n + 휆0) (¯x −휇0)2 (13.7) To obtain the fully Bayes parameter estimate, we take the maximum of the normal- gamma posterior probability density function: 휇= 휇n 휆= 훼n −1 2 훽n Thus the final probability density function for the data is p (x) = N  x; 휇n, r 훽n 훼n−1 2  . NOTE Fully functional code for Bayesian estimation with an unknown mean and unknown variance, executable via Jupyter Notebook, can be found at http://mng.bz /1oQy. Listing 13.3 PyTorch code for a normal-gamma distribution import torch class NormalGamma(): Since PyTorch doesn’t implement normal-gamma distribution, we implement a bare-bones version. def __init__(self, mu_, lambda_, alpha_, beta_): self.mu_ = mu_ self.lambda_ = lambda_ self.alpha_ = alpha_ self.beta_ = beta_ 13.8 Example: Fully Bayesian inferencing 459 @property def mean(self): return self.mu_, self.alpha_/ self.beta_ @property def mode(self): return self.mu_, (self.alpha_-0.5)/ self.beta_ Listing 13.4 PyTorch: Bayesian estimation with unknown mean, unknown variance import torch def inference_unknown_mean_variance(X, prior_dist): mu_mle = X.mean() sigma_mle = X.std() n = X.shape[0] mu_0 = prior_dist.mu_ lambda_0 = prior_dist.lambda_ alpha_0 = prior_dist.alpha_ beta_0 = prior_dist.beta_ Parameters of the prior mu_n = (n * mu_mle + mu_0 * lambda_0) / (lambda_0 + n) lambda_n = n + lambda_0 alpha_n = n / 2 + alpha_0 beta_n = n / 2 * sigma_mle ** 2 + beta_0 + 0.5* n * lambda_0/ (n + lambda_0) * (mu_mle - mu_0) ** 2 Parameters of the posterior posterior_dist = NormalGamma(mu_n, lambda_n, alpha_n, beta_n) return posterior_dist 13.8 Example: Fully Bayesian inferencing Let’s revisit the problem discussed in section 6.8 of predicting whether a resident of Statsville is female based on height. For this purpose, we have collected height samples from adult female residents of Statsville. Unfortunately, due to unforeseen circumstances, we collected a very small sample. Armed with our knowledge of Bayesian inference, we do not want to let this deter us from trying to build a model. Based on physical considerations, we can assume that the distribution of heights is Gaussian. Our goal is to estimate the parameters (휇, 휎) of this Gaussian. NOTE Fully functional code for this example, executable via Jupyter Notebook, can be found at http://mng.bz/Pn4g. Let’s first create the data set by sampling five points from a Gaussian distribution with 휇= 152 and 휎= 8. In real-life scenarios, we do not know the mean and standard deviation of the true distribution. But for the sake of this example, let’s assume that the mean height is 152 cm and the standard deviation is 8 cm. Our data matrix, X, is as follows: 460 CHAPTER 13 Fully Bayes model parameter estimation X =  164.32 149.65 134.56 156.54 143.32  13.8.1 Maximum likelihood estimation If we relied on MLE, our approach would be to compute the mean and standard devia- tion of the data set and use this normal distribution as our model. We use the following equations to compute the mean and standard deviation of our normal distribution: 휇MLE = 1 N n Õ i=1 xi 휎MLE = 1 N n Õ i=1 (xi −휇)2 The mean, 휇, comes out to be 149.68, and the standard deviation, 휎, is 11.52. This differs significantly from the true mean (152) and standard deviation (8) because the number of data points is low. In such low-data scenarios, the maximum likelihood estimates are not very reliable. 13.8.2 Bayesian inference Can we do better than MLE? One potential method is to use Bayesian inference with a good prior. How do we select a good prior? Well, let’s say that we know from an old survey that the average and standard deviation of the height of adult female residents of Neighborville, the neighboring town, are 150 cm and 9 cm, respectively. Additionally, we have no reason to believe that the distribution of heights at Statsville is significantly different. So we can use this information to “initialize” our prior. The prior distribution encodes our beliefs about the parameter values. Given that we are dealing with an unknown mean and unknown variance, we model the prior as a normal-gamma distribution: p(휃) = N훾( 휇0, 휆0, 훼0, 훽0) We choose p(휃) such that 휇0 = 150, 휆0 = 100, 훼0 = 10.5, and 훽0 = 810. This implies that p(휃) = N훾(150, 100, 10.5, 810) p(휃|X) is a normal-gamma distribution whose parameters can be computed using equations described in section 13.7. The PyTorch code for computing the posterior is shown next. 13.9 Fully Bayes parameter estimation: Multivariate Gaussian, unknown mean, known precision 461 Listing 13.5 PyTorch: Computing posterior probability using Bayesian inference prior_dist = NormalGamma(150, 100, 10.5, 810) Initializes the normal-gamma distribution posterior_dist = inference_unknown_mean_variance(X, prior_dist) Computes the posterior map_mu, map_precision = posterior_dist.mode The mode of the distribution refers to parameter values with the highest probability density. map_std = math.sqrt(1 / map_precision) Computes the standard deviation using precision map_dist = Normal(map_mu, map_std) map_mu and map_std refer to the parameter values that maximize the posterior distribution. The MAP estimates for 휇and 휎obtained using Bayesian inference are 149.98 and 9.56, respectively, which are better than the MLE estimates of 149.68 and 11.52 (the true 휇 and 휎are 152 and 9, respectively). Now that we’ve estimated the parameters, we can find out the probability that a sample lies in the range using the formula p(a < X <= b) = ∫b a p(X)dX The details of this can be found in section 6.8. 13.9 Fully Bayes parameter estimation: Multivariate Gaussian, unknown mean, known precision This is the multivariate case; the univariate version is discussed in section 13.3. The computations follow along the same lines as the univariate ones. We model the data distribution as a Gaussian p  ®x ®휇, 횲 = N  ®x; ®휇, 횲−1 , where we have expressed the Gaussian in terms of the precision matrix 횲instead of the covariance matrix 횺, where 횲= 횺−1. The training data set is X ≡  ®x(1), ®x(2), · · · , ®x(i), · · · , ®x(n) , and its overall likelihood is p  X ®휇 =∝e−1 2 Ín i=1  ®x(i) −®휇 T 횲  ®x(i) −®휇  We model the prior for the mean as a Gaussian: p   ®휇 = N  ®휇; ®휇0, 횲−1 0  ∝e−1 2 ( ®휇−®휇0)T 횲0( ®휇−®휇0) The posterior probability density is a Gaussian (because it is the product of two Gaus- sians). Let’s denote it as p   ®휇 X = N  ®휇; ®휇n, 횲−1 n  ∝e−1 2 ( ®휇−®휇n)T 횲n ( ®휇−®휇n) 462 CHAPTER 13 Fully Bayes model parameter estimation Using Bayes’ theorem, p   ®휇 X ∝p  X ®휇 p   ®휇 or posterior z }| { e−1 2 ( ®휇−®휇n)T 횲n ( ®휇−®휇n) ∝ likelihood z }| { e−1 2 Ín i=1  ®x(i) −®휇 T 횲  ®x(i) −®휇  prior z }| { e−1 2 ( ®휇−®휇0)T 횲0( ®휇−®휇0) ∝e −1 2 Ín i=1  ®x(i) −®휇 T 횲  ®x(i) −®휇  +( ®휇−®휇0)T 횲0( ®휇−®휇0)  Let’s examine the exponent of the rightmost expression. n Õ i=1  ®x(i) −®휇 T 횲  ®x(i) −®휇  +   ®휇−®휇0 T 횲0   ®휇−®휇0  = n Õ i=1 ®x(i)T 횲®x(i) + n Õ i=1 ®휇T횲®휇−2 ®휇T횲 n¯®x z }| { n Õ i=1 ®x(i) + ®휇T횲0 ®휇−2 ®휇T횲0 ®휇0 + ®휇T 0 횲0 ®휇0 = n Õ i=1 ®x(i)T 횲®x(i) + n ®휇T횲®휇−2n ®휇T횲¯®x + ®휇T횲0 ®휇−2 ®휇T횲0 ®휇0 + ®휇T 0 횲0 ®휇0 = ®휇T (n횲+ 횲0) ®휇−2 ®휇T  n횲¯®x + 횲0 ®휇0  + constant terms without ®휇 We ignored the last constant terms (because they will be rolled into the overall constant of proportionality). Thus p   ®휇 X ∝e−1 2 ( ®휇−®휇n)T 횲n ( ®휇−®휇n) ∝e−1 2  ®휇T (n횲+횲0) ®휇−2 ®휇T  n횲¯®x+횲0 ®휇0  + constant terms without ®휇  Comparing coefficients: 횲n = n횲+ 횲0 ®휇n = 횲−1 n  n횲¯®x + 횲0 ®휇0  (remember 횲is known) The posterior probability maximizes at ®휇n. Thus ®휇MAP = ®휇n is the MAP estimate for the mean parameter of the multivariate Gaussian data distribution: p  ®x = N  ®x; ®휇n, 횲−1 . Note the following: limn→∞n횲−1 n = limn→∞n (n횲+ 횲0)−1 = limn→∞  n−1−1 (n횲+ 횲0)−1 = limn→∞  n−1n횲+ n−1횲0 −1 = 횲−1 limn→∞횲−1 n = lim→∞n−1n횲−1 n = limn→∞n−1횲−1 = 0   lim n→∞®휇n = ¯®x lim n→0 ®휇n = ®휇0 With a large volume of data, the estimated mean parameter ®휇MAP = ®휇n approaches the MLE ®휇MLE = ¯®x. With a low volume of data, the estimated posterior mean parameter ®휇MAP = ®휇n approaches the prior ®휇0. 13.10 Fully Bayes parameter estimation: Multivariate, unknown precision, known mean 463 NOTE Fully functional code for multivariate Bayesian inferencing of the mean of a Gaussian likelihood with known precision, executable via Jupyter Notebook, can be found at http://mng.bz/J2AP. Listing 13.6 PyTorch: Multivariate Bayesian inferencing, unknown mean def inference_known_precision(X, prior_dist, precision_known): mu_mle = X.mean(dim=0) n = X.shape[0] mu_0 = prior_dist.mean precision_0 = prior_dist.precision_matrix Parameters of the prior precision_n = n * precision_known + precision_0 Parameters of the posterior mu_n = torch.matmul( n * torch.matmul( mu_mle.unsqueeze(0), precision_known) + torch.matmul( mu_0.unsqueeze(0), precision_0), torch.inverse(precision_n) ) posterior_dist = MultivariateNormal( mu_n, precision_matrix=precision_n) return posterior_dist 13.10 Fully Bayes parameter estimation: Multivariate, unknown precision, known mean In section 13.6, we discussed the univariate case, and now we examine the multivari- ate case. For the univariate case, we had to look at the gamma distribution. For the multivariate case, we have to look at the Wishart distribution. 13.10.1 Wishart distribution Suppose we have a Gaussian random data vector ®x with probability density function N  ®x; ®휇, 횺. Once again, we use precision matrix 횲instead of the covariance matrix 횺, where 횲= 횺−1. Consider the case where we know the mean ®휇but want to estimate the precision 횲. How do we express the prior? Note that p (횲) is the probability density function of a matrix. So far, we have encountered probability distributions of scalars and vectors, not a matrix. Also, this is not an arbitrary matrix. We are talking about a symmetric, non-negative definite matrix (all covariance and precision matrices belong to this category). Consequently, the distribution we are looking for is not a joint distribution of all the d2 matrix elements (d denotes the dimensionality of the data: that is, all ®x and ®휇 vectors are d × 1). Rather, it is a joint distribution of d(d+1) 2 elements in the matrix—the diagonal and those above or below (diagonal elements above and below are identical because the matrix is symmetric). 464 CHAPTER 13 Fully Bayes model parameter estimation The space of such matrices is called a Wishart ensemble. The probability of a random- precision matrix 횲of size d × d can be expressed as a Wishart distribution. This distribu- tion has two parameters: 휈, a scalar, satisfying 휈> d −1 W , a d × d symmetric non-negative definite matrix The probability density function is p (횲; 휈, W ) = W (횲; 휈, W ) = |횲| 휈−d−1 2 e−1 2Tr  W −1횲  2 휈d 2 |W | 휈 2 Γd  휈 2  where W denotes Wishart. |W |, |횲| denote the determinants of the matrices W and 횲, respectively. Tr (A) denotes the trace of a matrix A (sum of the diagonal elements). Γ denotes the multivariate gamma function Γd n 2  = 휋 d(d−1) 4 d Ö j=1 Γ n −j + 1 2  The Wishart is the multivariate version of the gamma distribution. Its expected value is 피(횲) = 휈W Its maxima occur at 횲= (휈−d −1)W for 휈≥d + 1 13.10.2 Estimating precision As before, we model the data distribution as a Gaussian p  ®x ®휇, 횲 = N  ®x; ®휇, 횲−1 , where we have expressed the Gaussian in terms of the precision matrix 횲instead of the covariance matrix 횺, where 횲= 횺−1. The training data set is X ≡  ®x(1), ®x(2), · · · , ®x(i), · · · , ®x(n) , and its overall likelihood is p (X|횲) =∝|횲| n 2 e−1 2 Ín i=1  ®x(i) −®휇 T 횲  ®x(i) −®휇  We model the prior probability of the precision matrix as a Wishart distribution. Hence, p (횲) = W (횲; 휈0, W0) ∝|횲| 휈0−d−1 2 e−1 2Tr  W0−1횲  The posterior is another Wishart (owing to the Wishart conjugate prior property): p (횲|X) = W (횲; 휈n, Wn) ∝|횲| 휈n −d−1 2 e−1 2Tr  Wn−1횲  13.10 Fully Bayes parameter estimation: Multivariate, unknown precision, known mean 465 Using Bayes’ theorem for the training data set X, p (횲|X) ∝p (X|횲) p (횲) posterior z }| { |횲| 휈n −d−1 2 e−1 2Tr  Wn−1횲  ∝ likelihood z }| { |횲| n 2 e−1 2 Ín i=1  ®x(i) −®휇 T 횲  ®x(i) −®휇  prior z }| { |횲| 휈0−d−1 2 e−1 2Tr  W0−1횲  Let’s study a pair of simple lemmas that will come in handy. ®xT A®x =Tr  ®xT A®x  ®xT A®x =Tr  A®x®xT  where Tr refers to Trace of a matrix (sum of diagonal elements). The first lemma is almost trivial—the quadratic form ®xT A®x is a scalar, so of course it is the same as its trace. The second lemma follows directly from the matrix property of a trace: Tr (BC) = Tr (CB). Using the lemmas, the exponent of the likelihood term is n Õ i=1  ®x(i) −®휇 T 횲  ®x(i) −®휇  = n Õ i=1 Tr  ®x(i) −®휇   ®x(i) −®휇 T 횲  =Tr ©­­­­­­ « nS z }| { n Õ i=1  ®x(i) −®휇   ®x(i) −®휇 T 횲 ª®®®®®® ¬ =Tr (nS횲) where S = 1 n n Õ i=1  ®x(i) −®휇   ®x(i) −®휇 T (remember, ®휇is known). Thus, the posterior density is p (횲|X) p (횲) ∝|횲| n 2 e−1 2Tr (nS횲)|횲| 휈0−d−1 2 e−1 2Tr  W0−1횲  ∝|횲| n+휈0−d−1 2 e−1 2  Tr (nS횲)+Tr  W0−1횲  Since Tr (A) +Tr (B) =Tr (A + B), p (X|횲) p (횲) ∝|횲| 휈n z}|{ n + 휈0 −d−1 2 e −1 2Tr ©­­­ « ©­­­ « z }| { nS +W0−1ª®®® ¬ 횲 ª®®® ¬ ∝|횲| 휈n −d−1 2 e−1 2Tr  Wn−1횲  466 CHAPTER 13 Fully Bayes model parameter estimation Comparing coefficients, we determine the unknown parameters of the posterior distribution: Wn−1 =  nS +W0−1 휈n = n + 휈0 where S = 1 n n Õ i=1  ®x(i) −®휇   ®x(i) −®휇 T The maximum of the posterior density function, W (횲; 휈n, Wn), yields an estimate for the precision parameter of the data distribution: 횲= (휈n −d −1)Wn for 휈n ≥d + 1 i.e., p  ®x = N ®x; ®휇,  nS+W0−1 (휈n−d−1) ! . Summary A generative model that models the underlying data distribution can be more powerful than a black box discriminative model. Once we choose a model family, we need to estimate the model parameters, 휃. We can estimate the best values of 휃 from the training data X using Bayes’ theorem. The posterior distribution p (휃|X) is a function of the product of likelihood p (X|휃) and the prior p (휃). The prior expresses our belief in the value of the parameters. The posterior is dominated by the prior for small data sets and the likelihood for large data sets. Injecting belief via a good prior distribution can be helpful in settings with very little training data. Maximum likelihood estimation only relies on the data, in contrast to maxi- mum a posteriori (MAP) estimation, which relies on the data as well as the prior information. We can use Bayesian estimation for the mean of a Gaussian likelihood when the variance is known. When the likelihood is Gaussian p(X) ∼N ( 휇, 휎), we model the prior as a normal distribution p ( 휇) ∼N ( 휇0, 휎0). The posterior distribution is also a normal distribution p ( 휇|X) ∼N ( 휇n, 휎n), where 휎2 n = 휎2 0 휎2 n휎2 0 +휎2 and 휇n = ¯x 1+ 휎2 n휎2 0 + 휇0 1+ n휎2 0 휎2 . We can also use the estimated parameter to make predictions about new instances of data. Weak priors imply a high degree of uncertainty/lower confidence in our prior belief and can easily be overwhelmed by the data. In contrast, strong priors imply a lower degree of uncertainty/higher confidence in our prior belief and will resist data overload. For a specific family of likelihood, the choice of the prior that results in the posterior belonging to the same family as the prior is called a conjugate prior. Summary 467 The gamma function is (Γ (훼) = ∫∞ x=0 x(훼−1)e−xdx), and the gamma distribution is p (휆|훼, 훽) = 훾(휆; 훼, 훽) = 훽훼 Γ(훼) 휆(훼−1)e−훽휆. The gamma distribution varies with different values of 훼and 훽. In the case of Bayesian estimation of the precision of the Gaussian likelihood for a known mean, the precision 휆is the inverse of the variance. We can model the prior as a gamma distribution p (휆) = 훽 훼0 0 Γ(훼0) 휆(훼0−1)e−훽0휆. The posterior distribution is also a gamma distribution, p (휆|X) = 훽훼n n Γ(훼n) 휆(훼n−1)e−훽n휆, where 훼n = n 2 + 훼0 and 훽n = 1 2 Ín i=1  x(i) −휇 2 + 훽0 = n 2 s + 훽0. In Bayesian estimation of both the mean and precision of a Gaussian likelihood, we model the prior as a normal-gamma distribution. The posterior is another normal-gamma distribution. The posterior distribution can be used to predict new data instances. The multivariate setting of Bayesian inferencing of the mean of a Gaussian like- lihood is known as precision. We can model the prior as a multivariate normal distribution; the posterior is also a multivariate normal distribution. The Wishart distribution is the multivariate version of the gamma distribution. With multivariate Bayesian inferencing of the precision of a Gaussian likelihood with a known mean, we can model the prior as a Wishart distribution. The corresponding posterior is also a Wishart distribution. 14 Latent space and generative modeling, autoencoders, and variational autoencoders This chapter covers Representing inputs with latent vectors Geometrical view, smoothness, continuity, and regularization for latent spaces PCA and linear latent spaces Autoencoders and reconstruction loss Variational autoencoders (VAEs) and regularizing latent spaces Mapping input vectors to a transformed space is often beneficial in machine learning. The transformed vector is called a latent vector—latent because it is not directly observable—while the input is the underlying observed vector. The latent vector (aka embedding) is a simpler representation of the input vector where only features that help accomplish the ultimate goal (such as estimating the probability of an input belonging to a specific class) are retained, and other features are forgotten. Typically, 468 14.1 Geometric view of latent spaces 469 the latent representation has fewer dimensions than the input: that is, encoding an input into a latent vector results in dimensionality reduction. The mapping from input to latent space (and vice versa) is usually learned—we train a machine, such as a neural network, to do it. The latent vector needs to be as faithful a representation as possible of the input within the dimensionality allo- cated to it. So, the neural network is incentivized to minimize the loss of information caused by the transformation. Later, we see that in autoencoders, this is achieved by reconstructing the input from the latent vector and trying to minimize the difference between the actual and reconstructed input. However, given the reduced number of dimensions, the network does not have the luxury of retaining everything in the in- put. It has to learn what is essential to the end goal and retain only that. Thus the embedding is a compact representation of the input that is streamlined to achieve the ultimate goal. 14.1 Geometric view of latent spaces Consider the space of all digital images of height H, widthW , with each pixel represent- ing a 24-bit RGB color value. This is a gigantic space with  224HW points. Every possible RGB × H ×W image is a point in this space. But if an image is a natural image, neigh- boring points tend to have similar colors. This means points corresponding to natural images are correlated: they are not distributed uniformly over the space of possible im- ages. Furthermore, if the images have a common property (say, they all contain giraffes), the corresponding points form clusters in the  224HW -sized input space. In stochas- tic parlance, the probability distribution of natural images with a common property over the space of possible images is highly non-uniform (low entropy). Figure 14.1a illustrates points with some common property clustered around a planar manifold. Similarly, figure 14.1b illustrates points with some common property clustered around a curved manifold. These points have a common property. At the moment, we are not interested in what that property is or whether the manifold is planar or curved. All we care about is that these points of interest are distributed around a manifold. The manifold captures the essence of that common property, whatever it is. If the common property is, say, the presence of a giraffe in the image, then the manifold captures giraffeness: the points on or near the manifold all correspond to images with giraffes. If we travel along the manifold, we encounter various flavors of giraffe photos. If we go far from the manifold—that is, travel a long distance in a direction orthogonal to the manifold—the probability of the point representing a photo with a giraffe is low. Given training data consisting of sampled points of interest (such as many giraffe photos), we can train a neural network to learn this manifold—it is the optimal manifold that minimizes the average distance of all the training data points from the manifold. Then, at inference time, given an arbitrary input point, we can estimate its distance from the manifold, giving us the probability of that input satisfying the property represented by the manifold. 470 CHAPTER 14 Latent space and generative modeling, autoencoders, and variational autoencoders Orthogonal component Latent vector for (in-subspace) (a) Planar latent subspaces Orthogonal component Latent vector for (in-subspace) (b) Curved latent subspace Figure 14.1 Two examples of latent subspaces, with planar and curved manifolds, respectively. The solid line shows the latent vector, and the dashed line represents the information lost by projecting onto the latent subspace. Thus, the input vector can be decomposed into an in-manifold component (solid line in figure 14.1) and an orthogonal-to-manifold component (dashed line in figure 14.1). Latent space modeling effectively eliminates the orthogonal component and retains the in-manifold component as the latent vector (aka embedding). Equivalently, we are projecting the input vector onto the manifold. This is the core idea of latent space modeling—we learn a manifold that represents a property of interest and represents all inputs by a latent vector, which is the input point’s projection onto this manifold. The latent vector is a more compact representation of the input where only information related to the property of interest is retained. Latent space modeling in a nutshell In latent space modeling, we train a neural network to represent a manifold around which the input points satisfying a property of interest are distributed. The property of interest could be membership in a specific class, such as images containing a giraffe. Thus, the learned manifold is a collection of points that satisfy the property. The input point is projected onto this manifold to obtain a latent vector representation of the input (aka embedding). This is equivalent to throwing away the input vector component that is orthogonal to the manifold. The eliminated component is orthogonal to the manifold and hence unrelated to the property of interest (may represent background pixels of the image), so the information loss caused by the projection does not hurt. We have created a less noisy, more compact representation of the input that focuses on the things we care about. Training data consists of a set of sampled data inputs, all satisfying the property of interest. The system essentially learns the manifold, which is optimally located to 14.2 Generative classifiers 471 minimize its average distance from all the training data points. During inferencing, given an arbitrary input point, its distance from the manifold is an indicator of the probability of that input satisfying the property of interest. A subtle point is that the latent vector is the in-manifold component of the original point’s position vector. By switching to the latent vector representation, we lose the location of the point in the original higher-dimensional input space. We can go back to the higher-dimensional space by providing the location of the manifold for the lost orthogonal component, but doing so does not recover the original point: it recovers only the projection of the original point onto the subspace. We are replacing the individual orthogonal components with an aggregate entity (the location of a manifold) but do not recover the exact original point. Some information is irretrievably lost during projection. A special case of latent space representation is principal component analysis (PCA), introduced in section 4.4 (section 14.4 provides a contextual recap of PCAs). It projects input points to an optimal planar latent subspace (as in figure 14.1a). But except for some lucky special cases, the best latent subspace is not a hyperplane. It is a complex curved surface (see figure 14.1b). Neural networks, such as autoencoders, can learn such nonlinear projections. 14.2 Generative classifiers During inferencing, the supervised classifiers we have encountered in previous chapters typically emit the class to which an input belongs, perhaps along with a bounding box. This is somewhat black-box-like behavior. We do not know how well the classifier has mastered the space except through the quantized end results. Such classifiers are called discriminative classifiers. On the other hand, latent space models map arbitrary input points to probabilities of belonging to the class of interest. Such models are called generative models, and they have some desirable properties: Smoother, denser manifolds—Discriminative models learn decision boundaries sepa- rating data points of interest from those not of interest in the input space. On the other hand, generative models try to model the distribution of the data points of interest in the input space using smooth probability density functions. As such, the generative model can’t learn a very irregularly shaped function that overfits the training data. This is illustrated in figure 14.2, whereas the discriminative model may converge to a manifold that follows the nooks and bends of the training data too closely (overfits) as in figure 14.2b. This difference between discriminative and generative classifiers becomes especially significant when we have less training data. We can always create a discriminative classifier from a generative classifier by putting a threshold on the probability. NOTE We can always create a discriminative classifier from a generative classifier by putting a threshold on the probability. 472 CHAPTER 14 Latent space and generative modeling, autoencoders, and variational autoencoders Region of interest (a) A good discriminative classifier— smooth decision boundary Wrongly classified point Region of interest (b) A bad discriminative classifier— irregular decision boundary (c) Generative model—no decision boundary (heat map indicates the probability density) Figure 14.2 Solid circles indicate training data points (all belonging to the class of interest). The dashed curve indicates the decision boundary separating the class of interest from the class of non-interest. In a generative model, there is no decision boundary. Every point in the space is associated with a probability of belonging to the class of interest (indicated as a heat map in figure 14.2c). Extra insight—Generative models offer more insight into the inner workings of the model. Consider a model that recognizes horses. Suppose we feed some horse images to the model, and it calls them horses (good). Then we feed the model some zebra images, and it calls them horses, too (bad). Do we have a useless model that calls everything a horse? If it is a discriminative model, we must test it with totally different images (say, bird images) to get the answer. But if we have a generative model, it says the probabilities of the true horse images are, say, 0.9 and above, while the probabilities for the zebra images are around 0.7. We begin to see that the model is behaving reasonably and does realize that zebras are less “horsy” than real horses. New class instances— A generative model learns the distribution of input points belonging to the class. An advantage related to learning the distribution is that we can sample the distribution to generate new members of the class (for example, to generate artificial horse images). This leads to the name generative modes. If we train a generative model with writings of Shakespeare, it will emit Shakespeare-like text pieces. Believe it or not, this has been tried with some success. 14.3 Benefits and applications of latent-space modeling Let’s recap at a high level why we want to do latent-space modeling: Generative models are often based on latent space models—all the benefits of generative modeling as outlined in section 14.2 apply to latent space modeling too. Attention to what matters—Redundant information that does not contribute to the end goal is eliminated, and the system focuses on truly discriminative information. To visualize this, imagine an input data set of police mugshots consisting of people standing in front of the same background. Latent-space modeling trained to recog- nize people typically eliminates the common background from the representation and focuses on the photograph’s subject matter (people). Streamlined representation of data—The latent vector is a more compact representa- tion of the input vector (reduced dimensions and hence smaller) with no meaning- ful information lost. 14.3 Benefits and applications of latent-space modeling 473 Noise elimination—Latent-space modeling eliminates the low-variance orthogonal- to-latent-subspace component of the data. This is mostly data that does not help in the problem of interest and hence is noise. Transformation to a manifold that is friendlier toward the end goal—We have seen this notion previously, but here let’s look at an interesting simple example. Consider a set of 2D points in Cartesian coordinates (x, y). Suppose we want to classify the points into two sets: those that lie inside the circle x2 + y2 = a2 and those that lie outside the circle. In the original Cartesian space, the decision boundary is not linear (it is circular). But if we transform the Cartesian input points to a latent space in polar coordinates—that is, each (x, y) is mapped to (r, 휃) such that x = rcos (휃) , y = rsin (휃)—the circle transforms into a line r = a in the latent space . A simple linear classifier r = a in the latent space can achieve the desired classification. Some applications of latent-space modeling are as follows: Generating artificial images or text (as explained in the context of generative modeling). Similarity estimation between inputs—If we map inputs to latent vectors, we can assess the similarity between inputs by computing the Euclidean distance between the latent vectors. Why is this better than taking the Euclidean distance between the input vectors? Suppose we are building a recommendation engine that suggests other clothing items “similar” to the one a potential buyer is currently browsing. We want to retrieve other clothing items that look similar but not identical to the one viewed. But similarity is a subjective concept, not quite measurable via the similarity of the inputs’ pixel colors. Consider a shirt with black vertical stripes on a white base. If we switch the stripe color with the base color, we get a shirt with white vertical stripes on a black base. If we do pixel-to-pixel color matching, these are very different, yet they are considered similar by humans. For this problem, we have to train the latent space model, creating neural networks so that images perceived to be similar by humans map to points in latent space that are close to each other. For example, both white-on-black and black-on-white shirts should map to latent vectors that are close to each other in the latent space even though they are far apart in the input space. Image or other data compression—The latent vector approximates the data with a smaller-dimensional vector that mimics the original vector as faithfully as possible. Thus the latent vector is a lossy compressed representation of the input. Denoising—The latent vector eliminates the non-meaningful part of the input information, which is noise. NOTE Fully functional code for this chapter, executable via Jupyter Notebook, can be found at http://mng.bz/6XG6. 474 CHAPTER 14 Latent space and generative modeling, autoencoders, and variational autoencoders 14.4 Linear latent space manifolds and PCA PCAs (which we discussed in section 4.4) project input data onto linear hyperplanar manifolds. Revisiting this topic will set up the correct context for the rest of this chapter. Consider a set of 3D input data points clustered closely around the X0 = X2 plane, as shown in figure 14.3. Original points 0 100 80 60 40 20 0 80 60 40 20 0 100 80 60 40 20 (a) Original 3D data Reconstructed points 0 100 80 60 40 20 0 80 60 40 20 0 100 80 60 40 20 (b) Lower-dimensional 2D representation obtained by setting the third principal value to zero First principal vector Second principal vector Third principal vector 4 2 0 -2 -4 -2 -4 4 2 0 -2 -4 4 2 0 First principal vector Second principal vector Third principal vector (c) The principal vectors of the original data. The third principal vector is normal to X0 = X2 plane; the other two are in-plane. Figure 14.3 The original 3D data in figure 14.3a shows high correlation: points are clus- tered around the X0 = X2 plane. The first prin- cipal component corresponds to the direction of maximum variance. The last (third) principal component corresponds to the direction of lowest variance. Eliminating components of the input vector along the third principal vector effectively projects all points into the X0 = X2 plane (latent subspace). This is latent vector modeling with the 3D input reduced to a 2D latent vector. NOTE We denote the successive axes (dimensions) as X0, X1, X2 instead of the more traditional X,Y , Z for easy extension to higher dimensions. 14.4 Linear latent space manifolds and PCA 475 Using PCA, we can recognize that the data has low variation along some dimensions. When we do PCA, we get the principal value and principal vector pairs. The largest principal value corresponds to the direction of maximum variance in the data. The corresponding principal vector yields that direction, and that principal value indicates the magnitude of the variance along that direction. The next principal value, the principal vector pair, is the orthogonal direction with the next-highest variance, and so on. For instance, in figure 14.3, the principal vectors corresponding to the larger two principal values lie in the X0 = X2 plane, while the smallest principal value corresponds to the normal-to-plane vector. The third principal value is significantly smaller than the others. This tells us that variance along that axis is low, and components along that axis can be dropped with relatively little loss of information: that is, low reconstruction loss. The variations along the small principal value axes are likely noise, so eliminating them cleans up the data. In figure 14.3, this effectively projects the data onto the X0 = X2 plane. Dimensionality reduction PCA essentially projects inputs to the best-fit plane for the training data. Assuming all the training data points are sampled with a common property, this plane represents that common property. By projecting, we eliminate that common property and retain only the discriminating aspects of the data. The eliminated information is remembered approximately in the parameters of the plane and supplied during reconstruction (aka decoding) to map us back to the same dimensionality as the input (but not exactly the same point). This is the essence of dimensionality reduction via PCA. Following are the steps involved in PCA-based dimensionality reduction. This was described in detail with proofs in section 4.5; here we recap the main steps without proof. NOTE This treatment is similar but not identical to that in section 4.5. Here we have switched the variables m and n to be consistent with our use of n to denote the data instance count. We have also switched to a slightly different flavor of the SVD. 1 Represent the data as a matrix X, where each row is an individual data instance. The number of rows n is the size of the data set. The number of columns d is the original (input) dimensionality of the data. Thus X is a n × d matrix. 2 Compute the mean data vector ®휇= 1 n n Õ i=1 ®x(i) where ®x(i) for i = 1 to i = n denote the training data vector instances (which form rows of the matrix X). 476 CHAPTER 14 Latent space and generative modeling, autoencoders, and variational autoencoders 3 Shift the origin of the coordinate system to the mean by subtracting the mean vector from each data vector: ®x(i) = ®x(i) −®휇 for all i The data matrix X now has the mean-subtracted data instances as rows. 4 The matrix XT X (where X is the mean-subtracted data matrix) is the covariance matrix (as discussed in detail in section 5.7.2). The eigenvalue, eigenvector pairs of the matrix XT X are known as principal values and principal vectors (together referred to as principal components). Since XT X is a d × d matrix, there are d scalar eigenvalues and d eigenvectors, each of dimension d × 1. Let’s denote the principal components as  휆1, ®v1 ,  휆2, ®v2 , · · · ,  휆dm, ®vd . 5 We can assume 휆1 ≥휆2 ≥· · · ≥휆d (if necessary, we can make this true by renumber- ing the principal components). Then the first principal component corresponds to the direction of maximum variance in the data (proof with geometrical intuition can be found in section 5.7.2). The corresponding principal value yields the actual variance. The next principal value corresponds to the second-highest variance (among directions orthogonal to the first principal direction), and so forth. For every component, the principal value yields the actual variance, and the principal vector yields the direction. 6 Consider the matrix of principal vectors: V = h ®v1 ®v2 · · · ®vd i If we want the data to be a space with m dimensions with minimal loss of informa- tion, we should drop the last m vectors of V . This eliminates the m least-variance dimensions. Dropping the last m vectors from V yields a matrix Vd−m = h ®v1 ®v2 · · · ®vd−m i Note that the best way to obtain the V matrix is to perform SVD on the mean- subtracted X (see section 4.5). 7 Premultiplying Vd−m, the truncated principal vectors matrix, with the original data matrix X projects the data onto a space corresponding to the first d −m principal components. Thus, to create d −m-dimensional linearly encoded latent vectors from d-dimensional data, Xd−m = XVd−m Xd−m is the reduced dimension data set. Its dimensionality is n × (d −m). It can be shown that XVd−m =UΣd−m 14.4 Linear latent space manifolds and PCA 477 where U is from SVD (see section 4.5) and Σd−m is a truncated version of the diagonal matrix Σ from SVD with its smallest m elements chopped off. This offers an alternative way to do PCA-based dimensionality reduction. 8 How do we reconstruct? In other words, what is the decoder? Well, to reconstruct, we need to save the original principal vectors: that is, the V matrix. If we have that, we can introduce m zeros at the right of every row in Xd−m to make it a n × d matrix again. Then we post-multiply by V T, which rotates the coordinate system back from one with principal vectors as axes to one with the original input axes. Finally, we add the mean ®휇to each row to shift the origin back to its original position, which yields the reconstructed data matrix ˜X. The reconstruction loss is ∥X −˜X∥2. Note that, in effect, ˜X is UΣV T with the last m diagonal elements of Σ set to zero. 9 The reconstructed data ˜X is not identical to the original data. The information we lost during dimensionality reduction (the normal-to-plane components) is lost permanently. Nonetheless, this principled way of dropping information ensures that the reconstruction loss is minimal in some sense, at least among all ˜X linearly related to X. 14.4.1 PyTorch code for dimensionality reduction using PCA Now, let’s implement dimensionality reduction in PyTorch. Let X be a data matrix representing points clustered around the X0 = X2 plane. X is of shape [1000, 3], with each row of X representing a three-dimensional data point. The following listing shows how to project X into a lower-dimensional space with minimal loss of information. It also shows how to reconstruct the original data points from the lower-dimensional representations. Note that the reconstructions are approximate because we have lost information (albeit minimal) in the dimensionality-reduction process. NOTE Fully functional code for dimensionality reduction using PCA, executable via Jupyter Notebook, can be found at http://mng.bz/7yJg. Listing 14.1 PyTorch: PCA revisited import torch X = get_data() Data matrix of shape (1000, 3) X_mean = X.mean(axis=0) Stores the mean so we can reconstruct the original data points later X = X - X_mean Subtracts the mean before performing SVD U, S, Vh = torch.linalg.svd(X, full_matrices=False) Runs SVD V = Vh.T Columns of V are the principal vectors. 478 CHAPTER 14 Latent space and generative modeling, autoencoders, and variational autoencoders V_trimmed = V[:, 0: 2] Removes the last principal vector. This is along the direction of least variance (perpendicular to X0 = X2 plane). X_proj = torch.matmul(X, V_trimmed) Projects the input data points into the lower-dimensional space X_proj = torch.cat([X_proj, Pads with zeros to make an n × d matrix torch.zeros((X_proj.shape[0], 1))], axis=1) X_recon = torch.matmul(X_proj, Vh) Post-multiplies with V T to project back to the original space X_recon = X_recon + X_mean Adds the mean 14.5 Autoencoders Autoencoders are neural network systems trained to generate latent-space representa- tions corresponding to specified inputs. They can do nonlinear projections and hence are more powerful than PCA systems (see figure 14.4). The neural network mapping the input vector to a latent vector is called an encoder. We also train a neural network called a decoder that maps the latent vector back to the input space. The decoder output is the reconstructed input from the latent vector. The reconstructed input (that is, the output of the decoder) will never match the original input exactly—information was lost during encoding and cannot be brought back—but we can try to ensure that they match as closely as possible within the constraints of the system. The reconstruction loss is a measure of the difference between the original input and the reconstructed input. The encoder-decoder pair is trained end to end to minimize reconstruction loss (along with, potentially, some other losses). This is an example of representation learning, whereby we learn to represent input vectors with smaller latent vectors representing the input as closely as possible in the stipulated size budget. The budgeted size of the latent space is a hyperparameter. Figure 14.4 A 2D data distribution with a curved underlying pattern. It is impossible to find a straight line or vector such that all points are near it. PCA will not do well. NOTE A hyperparameter is a neural network parameter that is not learned. Its value is set based on our knowledge of the system and held constant during training. 14.5 Autoencoders 479 The desired output is implicitly known in autoencoders: it is the input. Consequently, no human labeling is needed to train autoencoders; they are unsupervised. An autoencoder is shown schematically in figure 14.5. Latent space representation Reconstructed image Input image Decoder Encoder Bottleneck Figure 14.5 Schematic representation of an autoencoder. The encoder transforms input into a latent vector. The decoder transforms the latent vector into reconstructed input. We minimize the reconstruction loss—the distance between the reconstructed input and the original input. The encoder takes an input ®x and maps it to a lower-dimensional latent vector ®z. An example of an encoding neural network for image inputs is shown in listing 14.2. Note how the image height and width keep decreasing with each successive sequence of convolution, ReLU, and max pool layers. The decoder is a neural network that generates reconstructed image ˜®x from the latent vector ®z. Listing 14.3 shows an example of a decoder neural network. Note the transposed convolutions and how the height and width of the image keep increasing with each successive sequence of transposed convolution, batch nor- malization, and ReLU. (Transposed convolutions are discussed in section 10.5.) The decoder essentially remembers—not exactly, but in an average sense—the information discarded during encoding. Equivalently, it remembers the position of the latent space manifold in the overall input space. Adding that back to the latent space representation takes us back to the same dimensionality as the input vector but not to the same input point. The system minimizes the information loss from the encoding (the reconstruc- tion loss). We are ensuring that for each input, the corresponding latent vector produced by the encoder can be mapped back to a reconstructed value by the decoder that is as close as possible to the input. Equivalently, each latent vector is a faithful representation of the input, and there is a 1 : 1 mapping between inputs and latent vectors. The encoder and decoder need not be symmetric. 480 CHAPTER 14 Latent space and generative modeling, autoencoders, and variational autoencoders Mathematically, Encoder: latent vector z}|{ ®z = E  ®x Decoder: reconstructed image z}|{ ˜®x = D  ®z Loss:L = distance between input and reconstructed image z }| { ∥®x −˜®x∥2 The end-to-end system is trained to minimize the loss L. NOTE Fully functional code for autoencoders, executable via Jupyter Notebook, can be found at http://mng.bz/mOzM. Listing 14.2 PyTorch: Autoencoder encoder from torch import nn nz = 10 input_image_size = (1, 32, 32) Input image size in (c, h, w) format conv_encoder = nn.Sequential( nn.Conv2d(in_channels, 32, kernel_size=3, stride=1, padding=1), nn.BatchNorm2d(32), nn.ReLU(), nn.MaxPool2d(kernel_size=2), Reduces to a (32, 16, 16)-sized tensor nn.Conv2d(32, 128, kernel_size=3, stride=1, padding=1), nn.BatchNorm2d(128), nn.ReLU(), nn.MaxPool2d(kernel_size=2), Reduces to a (128, 8, 8)-sized tensor nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1), nn.BatchNorm2d(256), nn.ReLU(), nn.MaxPool2d(kernel_size=2), Reduces to a (256, 4, 4)-sized tensor nn.Flatten() Flattens to a 4096-sized tensor ) fc = nn.Linear(4096, nz) Reduces the 4096-sized tensor to an nz-sized tensor Listing 14.3 PyTorch: Autoencoder decoder from torch import nn decoder = nn.Sequential( nn.ConvTranspose2d(self.nz, out_channels=256, kernel_size=4, stride=1, padding=0, bias=False), Converts (nz, 1, 1) to a (256, 4, 4)-sized tensor 14.6 Smoothness, continuity, and regularization of latent spaces 481 nn.BatchNorm2d(256), nn.ReLU(True), nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2, padding=0, bias=False), Increases to a (128, 8, 8)-sized tensor nn.BatchNorm2d(128), nn.ReLU(True), nn.ConvTranspose2d(128, 32, kernel_size=2, stride=2, padding=0, bias=False), Increases to a (32, 16, 16)-sized tensor nn.BatchNorm2d(32), nn.ReLU(True), nn.ConvTranspose2d(32, in_channels, kernel_size=2, stride=2, padding=0, bias=False), Increases to a (1, 32, 32)-sized tensor nn.Sigmoid() ) Listing 14.4 PyTorch: Autoencoder training from torch import nn from torch.nn import functional as F conv_out = conv_encoder(X) Passes the input image through the convolutional encoder z = fc(conv_out) Reduces to nz dimensions Xr = decoder(z) Reconstructs the image using z via the decoder recon_loss = F.mse_loss(Xr, X) Computes the reconstruction loss 14.5.1 Autoencoders and PCA It is important to realize that autoencoders perform a much more powerful dimen- sionality reduction than PCA. PCA is a linear process; it can only project data points to best-fit hyperplanes. Autoencoders can fit arbitrary complex nonlinear hypersurfaces to the data, limited only by the expressive powers of the encoder-decoder pair. If the encoder and decoder have only a single linear layer (no ReLU or other nonlinearity), then the autoencoder projects the data points to a hyperplane like PCA (not necessarily the same hyperplane). 14.6 Smoothness, continuity, and regularization of latent spaces Minimizing the reconstruction loss does not yield a unique solution. For instance, figure 14.6 shows two examples of transforming 2D inputs into 1D latent-space rep- resentations, linear and curved, respectively. Both the regularized (solid line) and the non-regularized zigzag manifold (dashed line) fit the training data well with low reconstruction error. But the former is smoother and more desirable. Note the pair of points marked p1, p2 and p3, p4(square markers). The distance between them is more or less the same in the input space. But when projected on the dashed curve (unregularized latent space), their distances (measured along the curve) 482 CHAPTER 14 Latent space and generative modeling, autoencoders, and variational autoencoders P3 P4 P1 P2 (a) Linear latent space P1 P2 P3 P4 (b) Curved latent space Figure 14.6 Two examples of mapping from a 2D input space to a 1D latent space. Both show regularized (solid) vs. unregularized (dashed) latent space manifolds. Solid little circles depict training data points. 14.7 Variational autoencoders 483 become quite different. This is undesirable and does not happen in the regularized latent space (here, the distance is measured along a solid line). It becomes much more pronounced in high dimensions. The zigzag curve segment containing the training data set is longer than the smooth one. A good latent manifold typically has fewer twists and turns (is smooth) and hence has a “length” that is minimal in a sense. This is reminiscent of the minimum descriptor length (MDL) principle, which we discussed in section 9.3.1. How do we ensure that this smoothest latent space is chosen over others that also minimize the reconstruction loss? By putting additional constraints (losses) over and above the ubiquitous reconstruction loss. Recall the notion of regularization, which we looked at in sections 6.6.3 and 9.3. There we introduced an explicit loss that penalizes longer solutions (which was equivalent to maximizing the a posteriori probability of parameter values as opposed to the likelihood). A related approach that we explore in this chapter is to model the latent space as a probability distribution belonging to a known family (for example, Gaussian) and minimize the difference (KL divergence) of this estimated distribution from a zero-mean univariance Gaussian. The encoder- decoder neural network pair is trained end to end to minimize a loss that is a weighted sum of the reconstruction loss and this KL divergence. Trying to remain close to the zero- mean unit-variance Gaussian penalizes departure from compactness and smoothness. This is the basic idea of variational autoencoders (VAEs). The overall effect of regularization is to create a latent space that is more compact. If we only minimize the reconstruction loss, the system can achieve that by mapping points very far from each other (space being infinite). Regularization combats that and incentivizes the system to not map the training points too far from one another. It tries to limit the total latent-space volume occupied by the points corresponding to the training inputs. 14.7 Variational autoencoders VAEs are a special case of autoencoders. They have the same architecture: a pair of neural networks that encode and decode the input vector, respectively. They also have the reconstruction loss term. But they have an additional loss term called KL divergence loss that we explain shortly. NOTE Throughout this chapter, we denote latent variables with ®z and input variables with ®x. 14.7.1 Geometric overview of VAEs Figure 14.7 attempts to provide a geometrical view of VAE latent-space modeling. During training, given an input ®x, the encoder does not directly emit the corresponding latent- space representation ®z. Instead, the encoder emits the parameters of a distribution from a prechosen family. For instance, if the prechosen family is Gaussian, the encoder emits a pair of parameter values ®휇 ®x, 횺 ®x. These are the mean and covariance matrix of a specific Gaussian distribution N  ®z; ®휇 ®x , 횺 ®x in the latent space. The latent-space 484 CHAPTER 14 Latent space and generative modeling, autoencoders, and variational autoencoders Latent space Input space Space separator Figure 14.7 Geometric depiction of VAE latent-space modeling distributions representation ®z corresponding to the input ®x is obtained by sampling this distribution emitted by the encoder. Thus in the Gaussian case, we have ®z ∼N  ®z; ®휇 ®x , 횺 ®x. NOTE The symbol ∼indicates sampling from a distribution. This distribution, which we call the latent-space map of the input ®x, is shown by hollow circles with dark borders in figure 14.7. Such mapping is called stochastic mapping. The latent-space map distribution should have a narrow, single-peaked probability density function (for example, a Gaussian with small variance: that is, small ∥횺∥). The narrow-peakedness of the probability density function implies that the cloud of sample points forms a tight, small cluster—any random sample from the distribution will likely be close to the mean. So, sampling ®z from such a distribution is not very different from a deterministic mapping from ®x to ®z = ®휇 ®x. This sampling to obtain the latent vector is done only during training. During inferencing, we use the mean emitted by the encoder directly as the latent-space representation of the input: that is, ®z = ®휇 ®x. The decoder maps the latent vector representation ®z back to a point, say ˜x, in the input space. This is the reconstructed version of the input vector (shown by a little white square with a black border in figure 14.7). The decoder is thus estimating (reconstructing) the input given the latent vector. 14.7 Variational autoencoders 485 14.7.2 VAE training, losses, and inferencing Training comprises the following steps: 1 Choose a simple distribution family for q(®z | ®x). Gaussian is a popular choice. 2 Each input ®x maps to a separate distribution. The encoder neural network emits the parameters of this distribution. For the Gaussian case, the encoder emits 휇(®x), Σ(®x). The latent vector ®z is sampled from this emitted distribution. 3 The decoder neural network takes ®z as input and emits the reconstructed input ˜x. Given the input, reconstructed input, and latent vector we can compute the reconstruc- tion loss and KL Divergence loss described below. The goal of the training process is to iteratively minimize these losses. Thus, the VAE is trained to minimize a weighted sum of the following two loss terms on each input batch: Reconstruction Loss—Just as in an autoencoder, in a properly trained VAE, the reconstruction ˜x should be close to the original input ®x. So, reconstruction loss is Lrecon = ∥®x −˜x∥2 KL divergence loss— In VAE, we also have a loss term proportional to the KL di- vergence between the distribution emitted by the encoder and the zero mean unit variance Gaussian. KL divergence measured the dissimilarity between two probability distributions and was discussed in detail in section 6.4. Here we state (following equation 6.13) that the KL divergence loss for VAE is Lkld = KLD  q  ®z ®x , p  ®z = ∫ ®z∈D q  ®z ®x ; log q  ®z ®x p  ®z ! d®z where q  ®z ®x denotes the latent-space map probability distribution and p  ®z is a fixed target distribution. We want our global distribution of latent vectors to mimic the target distribution. The target is typically chosen to be a compact distribution so that the global latent vector distribution is also compact. The popular choice for the prechosen distribution family is Gaussian and for the fixed distribution is the zero-mean unit covariance matrix Gaussian: q  ®z ®x = N  ®z; ®휇 ®x , 횺 ®x and p  ®z = N  ®z; ®0, I  It should be noted that for the above choice of prior, we can evaluate the KLD loss via a closed form formula as described in section 14.7.7. Minimizing Lkld essentially demands that q  ®z ®x is high—that is, close to one— at the ®z values where p  ®z is high (see figure 14.8), because then their ratio is close to one and the logarithm is close to zero. The values of p  ®z at the places where q  ®z ®x is low (close to zero) do not matter because q  ®z ®x appears as a factor in Lkld—the contributions to the loss by these terms are close to zero anyway. 486 CHAPTER 14 Latent space and generative modeling, autoencoders, and variational autoencoders (a) Low KL divergence loss (high q (®z| ®x) coincides with high p (®z) (b) High KL divergence loss (high q (®z| ®x) coincides with low p (®z) Figure 14.8 KL divergence loss in VAEs is high when there is a large amount of overlap in mass between the encoder-generated distribution (N  ®z; ®흁  ®x , 횺  ®x ) and the target distribution p  ®z (here p  ®z ≡N  ®z; ®0, I  ) or, equivalently, low KL divergence between them. Thus, KLD loss essentially tries to ensure that most of the sample point cloud of q  ®z ®x falls on a densely populated region of the sample point cloud of p  ®z. Geometrically, this means the cloud of little hollow circles with black borders has a lot of overlapping mass with the target distribution. If every training data point is like that, the overall global cloud of latent vectors will also have significant overlap with the target distribution. Since the target distribution is typically chosen to be compact, this in turn ensures that the overall latent vector distribution (dark filled circles in figure 14.7) is compact. For instance, in the case when the target distribution is the zero-mean unit covariance matrix Gaussian N  ®z; ®0, I  , most of the mass of the latent vectors is contained within the unit radius ball. Without the KL divergence term, the latent vectors will spread throughout the latent space. In short, the KLD loss regularizes the latent space. The encoder-decoder pair of neural networks is trained end to end to minimize the weighted sum of reconstruction loss and KLD loss. In particular, the encoder learns to emit the parameters of the q  ®z ®x distribution. During inferencing, only the encoder is used. The encoder takes an input ®x and outputs ®휇 ®x and 횺 ®x. We do not sample here. Instead, we use the mean directly as the latent-space representation of the input. Notice that each input point ®x maps to a separate Gaussian distribution q(®z|®x = N (®z; ®휇(®x), Í(®x)). The overall distribution p(®x) modeled by all of these together can be very complex. Yet that complexity does not affect our computation which involves only q(z|x) and p(z). This is what makes the approach powerful. 14.7 Variational autoencoders 487 14.7.3 VAEs and Bayes’ theorem During training, the encoder neural network stochastically maps a specific input data instance, a point ®x in the input space, to a latent-space point ®z ∼N  ®z; ®휇 ®x , 횺 ®x. Thus the latent-space map effectively models the posterior probability p  ®z ®x. Note that we are using the symbol q  ®z ®x to denote the actual distribution emitted by the encoder, while we are using the symbol p  ®z ®x to denote the true (unknown) posterior probability distribution. Of course, we want these two to be as close as possible to each other: that is, we want the KL divergence between them to be minimal. Later in this section, we see how minimizing the KL divergence between q  ®z ®x and p  ®z ®x leads to the entire VAE algorithm. The decoder maps this point (®z) in latent space back to the input space point ˜x. As such, it models the probability distribution p  ®x ®z. The global distributions of the latent vectors ®z effectively model p  ®z (shown by dark-shaded filled little circles in figure 14.7). These probabilities are connected by our old friend, Bayes’ theorem: encoder / posterior z }| { p  ®z ®x = decoder / likelihood z }| { p  ®x ®z prior z}|{ p  ®z p  ®x |{z} evidence, constant wrt ®z 14.7.4 Stochastic mapping leads to latent-space smoothness Sampling the encoder’s output from a narrow distribution is similar, but not identical, to deterministic mapping. It has a rather unexpected advantage over direct encoding. A specific input point is mapped to a slightly different point in the latent space every time it is encountered during training—all these points have to decode back to the same region in the input space. This enforces an overall smoothness over the latent space: nearby ®z values all correspond to nearby ®x values. 14.7.5 Direct minimization of the posterior requires prohibitively expensive normalization The Bayes’ theorem expression of a VAE in section 14.7.3 gives us an idea. Why not train the neural network to directly maximize the posterior probability p  ®z ®X  , where X denotes the training data set? It certainly makes theoretical sense; we are choos- ing the latent space whose posterior probability is maximum given the training data. Of course, we must optimize one batch at a time, as we always do with neural networks. How do we evaluate the posterior probability? The formula is as follows: p  ®z ®x = p  ®x ®z p  ®z p  ®x = p  ®x ®z p  ®z ∫ ®z p  ®x ®z p  ®z d®z The denominator contains a sum over all values of ®z. Remember, with every iteration, the neural network weights change, and all previously computed latent vectors become 488 CHAPTER 14 Latent space and generative modeling, autoencoders, and variational autoencoders invalid. This means we have to recompute all latent vectors every iteration, which is intractable. Each iteration is O (n), and each epoch then is O  n2, where n is the number of training data instances (could be on the order of millions). We have to look for other methods. That takes us to evidence lower bound (ELBO) types of approaches. 14.7.6 ELBO and VAEs We do not know the true probability distribution p  ®z ®x. Let’s try to learn an approximate probability distribution q  ®z ®x that is as close as possible to p  ®z ®x. In other words, we want to minimize the KL divergence between the two (KL divergence was introduced in section 6.4). This KL divergence is KLD(q, p) = ∫ ®z∈D q  ®z ®x ln q  ®z ®x p  ®z ®x ! d®z We can expand this as KLD(q, p) = ∫ ®z∈D q  ®z ®x ln q  ®z ®x p  ®z ®x ! d®z = ∫ ®z∈D q  ®z ®x ln  q  ®z ®x d®z − ∫ ®z∈D q  ®z ®x ln  p  ®z ®x d®z = ∫ ®z∈D q  ®z ®x ln  q  ®z ®x d®z − ∫ ®z∈D q  ®z ®x ln  p  ®z, ®x d®z + ∫ ®z∈D q  ®z ®x ln  p  ®x d®z = −Hq (negative of entropy) z }| { ∫ ®z∈D q  ®z ®x ln  q  ®z ®x d®z − Eq (ln(p(®x,®z))) z }| { ∫ ®z∈D q  ®z ®x ln  p  ®z, ®x d®z + ln  p  ®x =1 z }| { ∫ ®z∈D q  ®z ®x d®z = −Hq −Eq  ln  p  ®x, ®z + ln  p  ®x where D is the domain of ®z: that is, the latent space, Hq is the entropy of the proba- bility distribution (entropy was introduced in section 6.2), and Eq  ln  p  ®x, ®z is the expected value of ln  p  ®x, ®z under the probability density q  ®z ®x. Rearranging terms, we get KLD(q, p) + Hq + Eq  ln  p  ®x, ®z = constant z }| { ln  p  ®x where the right-hand side is constant because it is a property of the data and cannot be adjusted during optimization. Defining the evidence lower bound (ELBO) as ELBO = Hq + Eq  ln  p  ®x, ®z we get KLD(q, p) + ELBO = constant 14.7 Variational autoencoders 489 So, minimizing the KL divergence between p  ®z ®x and its approximation q  ®z ®x is equiv- alent to maximizing the ELBO. We soon see that this leads to a technique for optimizing variational autoencoders. SIGNIFICANCE OF THE NAME ELBO Why do we call it evidence lower bound? Well, the answer is hidden in the relation KLD(q, p) + ELBO = ln  p  ®x. The right-hand side is the evidence log-likelihood. Re- member, KL divergence is always non-negative. So, the lowest value of ln  p  ®x hap- pens when KL divergence is zero when ln  p  ®x = ELBO. This means the evidence log-likelihood cannot be lower than the ELBO value. Thus the ELBO is the lower bound of the evidence log-likelihood; in short, it is the evidence lower bound. PHYSICAL SIGNIFICANCE OF THE ELBO Let’s look at the physical significance of ELBO maximization: ELBO = Hq + Eq  ln  p  ®x, ®z The first term is entropy. As we saw in section 6.2, this is a measure of the diffuseness of the distribution. If the points are evenly spread out in the distribution—the probability density is flat with no high peak—the entropy is high. When the distribution has few tall peaks and low values elsewhere, entropy is low (remember, for a probability density, having tall peaks implies low values elsewhere since the total volume under the function is constant: one). Thus, maximizing the ELBO means we are looking for a diffuse distribution q  ®z ®x. This, in turn, encourages smoothness in the latent space since we are effectively saying an input point ®x can map to any point around the mean ®휇 ®x (as emitted by the encoder) with almost equal high probability. Note that this fights a bit with the notion that each input should map to a unique point in the latent space. The solution tries to optimize between these conflicting requirements. The other term—expectation of the log of joint density p  ®x, ®z under the probability density q  ®z ®x—effectively measures the overlap between the two. Maximizing it is equivalent to saying q  ®z ®x must be high where p  ®x, ®z is high. This seems intuitively true. The joint density p  ®x, ®z = p  ®z ®x p  ®z. It is high where both the posterior p  ®z ®x and prior p  ®z are high. If q  ®z ®x approximates the posterior, it should be high where the joint is high. Let’s continue to explore the ELBO. More physical significances will emerge along with an algorithm for VAE optimization: ELBO = Hq + Eq  ln  p  ®x, ®z = − ∫ ®z∈D q  ®z ®x ln  q  ®z ®x d®z + ∫ ®z∈D q  ®z ®x ln  p  ®z, ®x d®z = − ∫ ®z∈D q  ®z ®x ln  q  ®z ®x d®z + ∫ ®z∈D q  ®z ®x ln  p  ®x ®z p  ®z d®z = − ∫ ®z∈D q  ®z ®x ln  q  ®z ®x d®z + ∫ ®z∈D q  ®z ®x ln  p  ®x ®z d®z + ∫ ®z∈D q  ®z ®x ln  p  ®z d®z 490 CHAPTER 14 Latent space and generative modeling, autoencoders, and variational autoencoders Rearranging terms and simplifying ELBO = Eq (ln(p(®x|®z))) z }| { ∫ ®z∈D q  ®z ®x ln  p  ®x ®z d®z − KLD(q(®z| ®x),p(®z)) z }| { ∫ ®z∈D q  ®z ®x ln q  ®z ®x p  ®z ! d®z ELBO = Eq  ln  p  ®x ®z −KLD  q  ®z ®x , p  ®z This last expression yields more physical interpretation and leads to the VAE algorithm. Let’s examine the two terms in the final ELBO expression in detail. The first term is Eq  ln  p  ®x ®z. This is high when q  ®z ®x and p  ®x ®z are both high at the same ®z values. For a given ®x, q  ®z ®x is high at those ®z values that are likely encoder outputs (that is, latent representations) of input ®x. High p  ®x ®z at these same ®z locations implies a high probability of decoding back to the same ®x value from those ®z locations. Thus, this term basically says if ®x encodes to ®z with a high probability, then ®z should decode back to ®x with a high probability, too. Stated differently, a round trip from input to latent space back to input space should not take us far from the original input. In figure 14.7, this means the input point marked ®x lies close to the output point marked ˜®x. In other words, minimizing reconstruction loss leads to ELBO maximization. Now consider the second term. It comes with a minus sign. Maximizing this is equiva- lent to minimizing the KL divergence between q  ®z ®x and p  ®z. This is the regularizing term. Viewed in another way, this is the term through which we inject our belief about the basic organization of the latent space into the system. Remember that the KL diver- gence KLD  q  ®z ®x , p  ®z sees very little contribution from the small values of q  ®z ®x. It is dominated by the large values of q  ®z ®x. In terms of figure 14.7, minimizing this KL divergence essentially ensures that most of the hollow circles fall on an area highly populated with filled circles. Thus, overall, maximization of ELBO is equivalent to minimizing reconstruction loss with regularization in the form of minimizing KL divergence from a specific prior distribution. This is what we do in VAEs. In every iteration, we minimize the reconstruc- tion loss (as in ordinary AEs) and also minimize divergence from a known (or guessed) prior. Note that this does not require us to encode all training inputs per iteration. The approach is incremental—one input or input batch at a time—like any other neural network optimization. Also, although we started from finding an approximation to p  ®z ®x, the final expression does not have that anywhere. There is only the prior p  ®z for which we can use some suitable fixed distribution. 14.7.7 Choice of prior: Zero-mean, unit-covariance Gaussian The popular choice for the known prior is a zero-mean, unit-covariance matrix Gaussian, N  ®0, I  , where I is the d × d identity matrix (d is the dimensionality of the latent space), ®0 is d × 1 vector of all zeros. Note that minimizing the KL divergence from N  ®0, I  is equivalent to restricting most of the mass within the unit ball (a hypersphere with its 14.7 Variational autoencoders 491 center at the origin and radius 1). In other words, this KL divergence term restrains the latent vectors from spreading over the ℜd and remains mostly within the unit ball. Remember that a compact set of latent vectors translates in a sense to the simplest (minimum descriptor length) representations for the input vectors: that is, a regularized latent space (section 14.6). KL divergence from a Gaussian has a closed-form expression that we derive in sec- tion 6.4.1. We first repeat equation 6.14 for KL divergence between two Gaussians and then obtain the expression for the special case where one of the Gaussians is a zero-mean, unit-covariance Gaussian: D (q, p) = 1 2  tr  횺−1 p 횺q  +   ®휇p −®휇q T 횺−1 p   ®휇p −®휇q  −d + log det 횺p det 횺q  (14.1) where the operator tr denotes the trace of a matrix (sum of diagonal elements) and operator det denotes the determinant. By assumption, p  ®z = N  ®0, I  : that is, ®휇p = ®0 and 횺p = I. Thus, D (q, p) = 1 2  tr  횺q  + ®휇T q I ®휇q −d −log  det 횺q  At this point, we introduce another simplifying assumption: that the covariance matrix 횺q is a diagonal matrix. This means the matrix can be expressed compactly as 횺q = ®휎q where ®휎q contains the elements of the main diagonal and we are not redundantly expressing the zeros in the off-diagonal elements. Note that this is not an outlandish assumption to make. We are approximating p  ®z ®x with a Gaussian q  ®z ®x whose axes are uncorrelated. Because of this assumption, Tr  횺q  = d Õ i=1 ®휎2 q [i] log  det 횺q  = d Õ i=1 log  ®휎2 q [i]  D (q, p) = 1 2 d Õ i=1 ®휎2 q [i] + ∥®휇q∥2 − d Õ i=1 log  ®휎2 q [i]  a −d ! = 1 2 ∥®휇q∥2 + d Õ i=1  ®휎2 q [i] −2log  ®휎q [i] −d ! = 1 2 d Õ i=1  ®휇2 q [i] + ®휎2 q [i] −2log  ®휎q [i] −1 ! 492 CHAPTER 14 Latent space and generative modeling, autoencoders, and variational autoencoders It is easy to see the expression  ®휎2 q [i] −2log  ®휎q [i] reaches a minimum when ®휎q [i] = 1. Thus, overall, KL divergence with the zero-mean, unit-covariance Gaussian is minimized when the mean is at the origin and the variances are all ones. This is equivalent to minimizing the spread of the latent vectors outside the ball of unit radius centered on the origin. An alternative choice for the prior is a Gaussian mixture with as many components as the known number of classes. We do not discuss that here. 14.7.8 Reparameterization trick We have avoided talking about one nasty problem so far. We said that in VAEs, the encoder emits the mean and variance of the probability density function p  ®z ®x from which we sample the encoder output. There is a problem, however. The encoder-decoder pair are neural networks that learn via backpropagation. That is based on differentiation. Sampling is not differentiable. How do we deal with this? We can use a neat trick: the so-called reparameterization trick. Let’s first explain it in the univariate case. Sampling from a Gaussian N ( 휇, 휎) can be viewed as a combination of the following two steps: 1 Take a random sample from x from N (0, 1). Note that there is no learnable parameter here; it’s a sample from a constant density function. 2 Translate the sample (add 휇), and scale it (multiply by 휎). This essentially takes the sampling part out of the path for backpropagation. The encoder emits 휇and 휎, which are differentiable entities that we learn. Sampling is done separately from a constant density function. The idea can be extended to a multivariate Gaussian. Sampling from N   ®휇, 횺 can be broken down into sampling from N  ®0, I  and scaling the vector by multiplying by the matrix 횺and translating by ®휇. Thus, we have a multivariate encoder that can learn via backpropagation. NOTE Fully functional code for VAEs, executable via Jupyter Notebook, can be found at http://mng.bz/5QYD. Listing 14.5 PyTorch: Reparameterization trick def reparameterize(mu, log_var): std = torch.exp(0.5 * log_var) Converts the log variance to the standard deviation eps = torch.randn_like(std) Samples from N  ®0, I  return mu + eps * std Scales by multiplying by 횺and translates by ®흁 14.7 Variational autoencoders 493 Listing 14.6 PyTorch: VAE from torch import nn nz = 10 input_image_size = (1, 32, 32) Input image size in (c, h, w) format conv_encoder = nn.Sequential( nn.Conv2d(in_channels, 32, kernel_size=3, stride=1, padding=1), nn.BatchNorm2d(32), nn.ReLU(), nn.MaxPool2d(kernel_size=2), Reduces to a (32, 16, 16)-sized tensor nn.Conv2d(32, 128, kernel_size=3, stride=1, padding=1), nn.BatchNorm2d(128), nn.ReLU(), nn.MaxPool2d(kernel_size=2), Reduces to a (128, 8, 8)-sized tensor nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1), nn.BatchNorm2d(256), nn.ReLU(), nn.MaxPool2d(kernel_size=2), Reduces to a (256, 4, 4)-sized tensor nn.Flatten() Flattens to a 4096-sized tensor ) mu_fc = nn.Linear(4096, nz) Reduces a 4096-sized tensor to an nz-sized 휇tensor logvar_fc = nn.Linear(4096, nz) Reduces a 4096-sized tensor to an nz-sized log(흈2) tensor Listing 14.7 PyTorch: VAE decoder from torch import nn decoder = nn.Sequential( nn.ConvTranspose2d(self.nz, out_channels=256, kernel_size=4, stride=1, padding=0, bias=False), Converts (nz, 1, 1) to a (256, 4, 4)-sized tensor nn.BatchNorm2d(256), nn.ReLU(True), nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2, padding=0, bias=False), Increases to a (128, 8, 8)-sized tensor nn.BatchNorm2d(128), nn.ReLU(True), nn.ConvTranspose2d(128, 32, kernel_size=2, stride=2, padding=0, bias=False), Increases to a (32, 16, 16)-sized tensor nn.BatchNorm2d(32), nn.ReLU(True), nn.ConvTranspose2d(32, in_channels, kernel_size=2, stride=2, padding=0, bias=False), Increases to a (1, 32, 32)-sized tensor nn.Sigmoid() ) 494 CHAPTER 14 Latent space and generative modeling, autoencoders, and variational autoencoders Listing 14.8 PyTorch: VAE loss recon_loss = F.binary_cross_entropy(Xr, X, reduction=''sum'') Binary cross-entropy loss kld_loss = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp()) KLD  q  ®z ®x , p  ®z where ®z ∼N  ®0, I  total_loss = recon_loss + beta * kld_loss Computes the total loss Listing 14.9 PyTorch: VAE training conv_out = conv_encoder(X) Passes the input image through the convolutional encoder mu = mu_fc(conv_out) Computes 흁, an nz-dimensional tensor log_var = logvar_fc(conv_out) Computes log(흈2), an nz-dimensional tensor z = reparameterize(mu, log_var) Samples z via the reparameterization trick Xr = self.decoder(z) Reconstructs the image using z via the decoder total_loss = recon_loss + beta * kld_loss Computes the total loss AUTOENCODERS VS. VAES Let’s revisit the familiar MNIST digits data set. It contains a training set of 60,000 images and a test set of 10,000 images. Each image is 28 × 28 in size and contains a center crop of a single digit. Earlier, we used this data set for classification. Here, we use it an unsupervised manner: we ignore the labels during training/testing. We train both the autoencoder and the VAE end to end on this data set and look at the results (see figures 14.9 and 14.10). Reconstructed images (a) Autoencoder- reconstructed images Reconstructed images (b) VAE-reconstructed images Figure 14.9 Comparing the reconstructed images on the test set for the autoen- coder and VAE trained end to end. On a simple data set like MNIST, both the autoencoder and VAE do a pretty good job of reconstructing images from the test set. Summary 495 10.0 7.5 5.0 2.5 0.0 -2.5 -5.0 -7.5 -10.0 -10.0 -7.5 -5.0 -2.5 0.0 2.5 5.0 7.5 10.0 0 1 2 3 4 5 6 7 8 9 (a) Autoencoder latent space (nz=2) 10.0 7.5 5.0 2.5 0.0 -2.5 -5.0 -7.5 -10.0 -10.0 -7.5 -5.0 -2.5 0.0 2.5 5.0 7.5 10.0 0 1 2 3 4 5 6 7 8 9 (b) VAE latent space (nz=2) Figure 14.10 The difference between the learned latent spaces of the autoencoder and VAE. We train an autoencoder and a VAE with nz = 2 on MNIST and plot the latent space for the test set. Autoencoders only minimize the reconstruction loss, so any latent space is equally acceptable as long as the recon- struction loss is low. As expected, the learned latent space is sparse and has a very high spread. VAEs, in contrast, minimize reconstruction loss with regularization. This is done by minimizing the KL divergence between the learned latent space and a known prior distribution N  ®0, I  . Adding this regularization term ensures that the latent space is constrained within a unit ball. This can be seen in figure 14.10b, where the learned latent space is much more compact. The autoencoder is trained to minimize the MSE between the input image and the reconstructed image. There is no other restriction on the latent space. The VAE is trained to maximize the ELBO. As we saw in the previous section, we can maximize the ELBO by minimizing the reconstruction loss with regularization in the form of minimizing KL divergence from a specific prior distribution: N  ®0, I  in the case of VAE. So the network is incentivized to ensure that the latent space learned is constrained within the unit ball. One minor implementation detail to note is that we use binary cross-entropy instead of MSE when training VAEs. In practice, this leads to better convergence. Summary In latent-space modeling, we map input data points onto a lower-dimensional latent space. The latent space is typically a manifold consisting of points that have a property of interest in common. The property of interest can be membership to a specific class, such as all paragraphs written by Shakespeare. The latent vectors are simpler, more compact representations of the input data in which only information related to the property of interest is retained and other information is eliminated. In latent space modeling, all training data input satisfies the property of interest. For instance, we can train a latent space model on paragraphs written by Shakespeare. Then the learned manifold contains points corresponding to various Shakespeare 496 CHAPTER 14 Latent space and generative modeling, autoencoders, and variational autoencoders like paragraphs. Points far away from the manifold are less Shakespeare-like. By inspecting this distance, we can estimate the probability of a paragraph being written by Shakespeare. By sampling the probability distribution, we may even be able to emit pseudo-Shakespeare paragraphs. Geometrically speaking, we project the input point onto the manifold. PCA per- forms a special form of latent space modeling where the manifold is a best-fit hyperplane for the training data. Autoencoders can perform a much more powerful dimensionality reduction than PCA. An autoencoder consists of an encoder (E), which maps the input data point into the lower-dimensional space, and a decoder (D), which maps the lower- dimensional representation back into the input space. It is trained to minimize the reconstruction loss: that is, the distance between the input and reconstructed (encoded, then decoded) vectors. Variational autoencoders (VAEs) model latent spaces as probability distributions to impose additional constraints (over and above reconstruction loss) so that we can generate more regularized latent spaces. – In VAEs, the encoder maps the input to a latent representation via a stochastic process (rather than a deterministic one). It emits p  ®z ®x as opposed to directly emitting ®z. ®z is obtained by sampling p  ®z ®x. The decoder maps a point in latent space ®z back to the input space. It is also modeled as a probability distribution p  ®x ®z. – The latent space learned by a VAE is much more compact and smoother (and hence more desirable) than that learned by an autoencoder. appendix A.1 Dot product and cosine of the angle between two vectors In section 2.5.6, we stated that the component of a vector ®a along another vector ®b is ®a · ®b = ®aT ®b. This is equivalent to ∥®a∥∥®b∥cos (휃), where 휃is the angle between the vectors ®a and ®b. In this section, we offer a proof of this for the two-dimensional case to deepen your intuition about the geometry of dot products. From figure A.1b, we can see that ax =∥®a∥cos (휃+ 휙) ay =∥®a∥sin (휃+ 휙) bx =∥®b∥cos (휙) by =∥®b∥sin (휙) which can be rewritten as cos (휃+ 휙) = ax ∥®a∥ sin (휃+ 휙) = ay ∥®a∥ (A.1) cos (휙) = bx ∥®b∥ sin (휙) = by ∥®b∥ (A.2) Using well-known trigonometric identities in equation A.1, we get cos (휃+ 휙) = cos휙cos휃−sin휙sin휃 = ax ∥®a∥ sin (휃+ 휙) = sin휙cos휃+ cos휙sin휃 = ay ∥®a∥ 497 498 APPENDIX Y X (a) Components of a 2D vector along coordinate axes. Note that ∥®a∥is the length of hypotenuse. Y X (b) Dot product as a component of one vector along another ®a · ®b = ®aT ®b = axbx + ayby = ∥®a∥∥®b∥cos (휃). Figure A.1 Vector components and dot product Substituting for cos휙and sin휙from equation A.2, we have a system of simultaneous linear equations with cos휃and sin휃as unknowns: bx ∥®b∥ cos휃−by ∥®b∥ sin휃= ax ∥®a∥ by ∥®b∥ cos휃+ bx ∥®b∥ sin휃= ay ∥®a∥ This system of simultaneous linear equations can be compactly written in matrix-vector form  bx ∥®b∥ −by ∥®b∥ by ∥®b∥ bx ∥®b∥   cos휃 sin휃  =  ax ∥®a∥ ay ∥®a∥  which can be simplified to 1 ∥®b∥  bx −by by bx   cos휃 sin휃  = 1 ∥®a∥  ax ay  This equation can be solved to yield cos휃= axbx + ayby ∥®a∥∥®b∥ sin휃= aybx −axby ∥®a∥∥®b∥ So, ∥®a∥∥®b∥cos휃= axbx + ayby, which was to be proved. A.3 Computing the variance of a Gaussian distribution 499 A.2 Determinants Determinant computation is tedious and numerically unstable if done naively. You should never compute one by hand—all linear algebra software packages provide routines to do this. Hence we only describe the algorithm to compute the determinant of a 2 × 2 matrix. This determinant can be computed as det(A) = a11a22 −a12a21 The inverse is A−1 = 1 det(A)  a22 −a12 −a21 a11  A.3 Computing the variance of a Gaussian distribution From the integral form of equation 5.13, we have vargaussian (x) = ∞ ∫ x=−∞ (x −휇)2 p (x) dx Substituting equation 5.22, p (x) = 1 √ 2휋휎e −(x−휇)2 2휎2 in that, we get vargaussian (x) = ∞ ∫ x=−∞ (x −휇)2 1 √ 2휋휎 e −(x−휇)2 2휎2 dx Substituting y = (x−휇) √ 2휎, which implies dy = dx √ 2휎and 2휎2y2 = (x −휇)2, we get vargaussian (x) = 2휎2 √휋 ∞ ∫ −∞ y2e−y2dy = 2휎2 √휋 ∞ ∫ −∞ y  ye−y2 dy Using integration by parts, ∞ ∫ −∞ y  ye−y2 dy =  y ∫ ye−y2dy ∞ −∞ − ∞ ∫ −∞ dy dy ∫ ye−y2 dy Now, substituting v = y2, which implies dv 2 = ydy, ∫ ye−y2dy = ∫ e−y2 (ydy) = 1 2 ∫ e−vdv = −e−v 2 = −e−y2 2 Hence,  y ∫ ye−y2dy ∞ −∞ =  y e−y2 2 !∞ −∞ = 1 2  lim y→∞ y ey2 −lim y→−∞ y ey2  500 APPENDIX Such limits can be evaluated using L’Hospital’s rule: lim y→∞ y ey2 = lim y→∞ dy dy d  ey2  dy = lim y→∞ 1 2yey2 = 0 lim y→−∞ y ey2 = lim y→−∞ dy dy d  ey2  dy = lim y→−∞ 1 2yey2 = 0 In both cases, the limit is zero because ey2 goes to positive infinity regardless of whether y goes to positive or negative infinity. Hence the denominator goes to infinity in both cases, causing the fraction to go to zero. Thus the first term in the computation of vargaussian (x) becomes  y ∫ ye−y2dy ∞ −∞ = 0 −0 = 0. The second term ∞ ∫ −∞ dy dy ∫ ye−y2 dy = 1 2 ∞ ∫ −∞ e−y2dy This last integral is a special one. To evaluate it, we need to go from one to two dimensions—this may be one of the very few cases where making a problem more complex helps. It is worth examining, so let’s look at it. Let I = ∞ ∫ −∞ e−x2dx Since the variable of integration does not matter, we can also write I = ∞ ∫ −∞ e−y2dy Let’s multiply them together: I2 = ∞ ∫ −∞ e−x2dx ∞ ∫ −∞ e−y2dy = ∞ ∫ −∞ ∞ ∫ −∞ e−x2e−y2dxdy = ∞ ∫ −∞ ∞ ∫ −∞ e− x2+y2 dxdy This double integral’s domain (aka region of integration) is the infinite XY plane, where x and y both range from −∞to ∞. This same plane can also be viewed as an infinite- radius circle (an infinite-radius circle is the same as a rectangle with infinite-length sides!). Consequently, we can switch to polar coordinates, using the transformation x = rcos (휃) y = rcos (휃) A.4 Two theorems in statistics 501 which implies x2 + y2 = r2 dxdy = rdrd휃 I2 = r=∞ ∫ r=0 휃=2휋 ∫ 휃=0 e−r2r dr d휃= r=∞ ∫ r=0 e−r2r dr 휃=2휋 ∫ 휃=0 d휃= 2휋 r=∞ ∫ r=0 e−r2r dr Substituting v = r2, which implies dv = 2rdr, we get I2 = 휋 ∞ ∫ 0 e−vdv = 휋  −e−v v=∞ v=0 = 휋⇒I = √휋 Thus, the second term of vargaussian (x) evaluates to 휎2 √휋 √휋 2 . We have already shown that the first term evaluates to zero. So, we get vargaussian (x) = 휎2 Thus the 휎in the probability density function p (x) = 1 √ 2휋휎e −(x−휇)2 2휎2 is the standard devia- tion (square root of the variance), and the 휇is the expected value. A.4 Two theorems in statistics In this section, we study two important inequalities in multivariate statistics: Jensen’s inequality and the log-sum inequality. A.4.1 Jensen’s Inequality Consider a random variable X. For now, let’s think of these as discrete variables, although the results we will come up with apply equally well to continuous variables. Thus, let the random variable take the discrete values ®x1, ®x2, · · · , ®xn, with probabilities p  ®x1 , p  ®x2 , · · · , p  ®xn . Now suppose g  ®x is a convex function whose domain includes these random vari- ables. From equation 3.11, section 3.7, we know that given any convex function g  ®x, for an arbitrary set of its input values ®xi, i = 1 · · · n and a set of weights 훼i i = 1 · · · n satisfying Ín i=1 훼i = 1, the weighted sum of the function outputs is greater than or equal to the function’s output on the weighted sum of inputs: that is, Ín i=1 훼i g  ®xi  ≥g  Ín i=1 훼i ®xi . In particular, let’s choose the set of all random variables as input values and their probabilities as weights (훼i = p  ®xi ). We can do this because probabilities sum to 1, exactly as weights are supposed to do. This leads to n Õ i=1 p (xi) g  ®xi  ≥g n Õ i=1 p (xi) ®xi ! =⇒피(g (X)) ≥g (피(X)) (A.3) 502 APPENDIX Equation A.3 is Jensen’s inequality. A good mnemonic for it is: for a convex function, the expected value of the function is greater than or equal to the function of expected value. It holds for continuous random variables, too. A.4.2 Log sum inequality Suppose we have two sets of positive numbers a1, a2, · · · , an and b1, b2, · · · , bn. Let a = Ín i=1 ai and b = Ín i=1 bi. Given these, the log sum inequality theorem says, n Õ i=1 ailog  ai bi  ≥alog  a b  (A.4) To see why this is true, let’s carve out an informal proof. First let’s define g (x) = xlogx. This is a convex function because dg dx = dx dxlogx + x d (logx) dx = logx + x 1 x = logx + 1 =⇒d2g dx2 = 1 x > 0 (for positive x) Now, with that definition of g, n Õ i=1 ailog ai bi = n Õ i=1 bi g  ai bi  = b n Õ i=1 bi b g  ai bi  This last expression is a weighted sum of convex function outputs with the weights summing to 1 (since Ín i=1 bi b = 1). So, we can use equation 3.11, section 3.7. Then we get b n Õ i=1 bi b g  ai bi  ≥b g n Õ i=1 bi b ai bi ! = b g n Õ i=1 ai b ! = b g Ín i=1 ai b  = b g  a b  = a log  a b  A.5 Gamma functions and distribution To understand the gamma distribution, we need to understand the basic gamma func- tion. First let’s do an overview of the gamma function. A.5.1 Gamma function The gamma function is in some sense a generalization of the factorial. The factorial function is only defined for integers and is characterized by the basic equation n! = n (n −1)! The gamma function is defined by Γ (훼) = ∫∞ x=0 x(훼−1)e−xdx (A.5) Applying integration by parts to equation A.5, we get Γ (훼) = h x(훼−1) (−e−x) i∞ 0 − ∫∞ x=0 (훼−1) x(훼−2) (−e−x) dx = h x(훼−1) (−e−x) i∞ 0 + ∫∞ x=0 (훼−1) x(훼−2) (e−x) dx A.5 Gamma functions and distribution 503 The first term is zero. This is because lim x−→0 x(훼−1) ex = 0 lim x−→∞ x(훼−1) ex = 0 by repeated application of L’Hospital’s rule Hence, Γ (훼) = ∫∞ x=0 (훼−1) x(훼−2) (e−x) dx = (훼−1) Γ (훼−1) Thus, for integer values 훼= n, Γ (n) = n!. There are other equivalent definitions of the gamma function, but we will not discuss them here. Instead, let’s talk about the gamma distribution. A.5.2 Gamma distribution The probability density function for a random variable having a gamma distribution is a function with two parameters 훼and 훽: p (x|훼, 훽) = 훾(x; 훼, 훽) = 훽훼 Γ (훼) x(훼−1)e−훽x where 훼, 훽> 0, x ≥0 (A.6) It is not hard to see that this is a proper probability density function: ∫∞ x=0 p (x|훼, 훽) dx = 1 By substituting y = 훽x, we get p (x|훼, 훽) = 훽훼 Γ (훼) ∫y=∞ y=0  y 훽  (훼−1) e−y dy 훽= ∫∞ y=0 y(훼−1)e−ydy Γ (훼) = Γ (훼) Γ (훼) = 1 If 훼= 1, the gamma distribution reduces to p (x) = 훽 Γ (1) e−훽x = 훽e−훽x ( it can be shown that Γ (1) = 1 ) which is graphed in figure A.2a at several values of 훽. The gamma distribution has two terms x훼−1 and e−훽x that have somewhat opposite effects: the former increases with x, while the latter deceases with x. At smaller values of x, the former wins, and the product increases with x. But eventually, the exponential starts winning and pulls the product downward asymptotically toward zero. Thus the gamma distribution has a peak. Larger 훽results in taller peaks and a sharper decline toward zero. Larger 훼moves the peak further to the right. The expected value is 피(x) = 훼 훽, as illustrated in figure A.2. The expected value of the gamma distribution 피(x) = 훼 훽. This can be proved using a little trick so cool that it is worth discussing for that reason alone: 피(x) = ∫∞ x=0 xp (x|훼, 훽) dx = 훽훼 Γ (훼) ∫∞ x=0 xx(훼−1)e−훽xdx = 훽훼 Γ (훼) ∫∞ x=0 x훼e−훽xdx 504 APPENDIX Gamma distribution (a) Gamma distribution: 훼= 1, various 훽s Gamma distribution (b) Gamma distribution: 훽= 1, various 훼s Gamma distribution (c) Gamma distribution: 훼= 2, various 훽s Gamma distribution (d) Gamma distribution: 훽= 2, various 훼s Figure A.2 Graph of a gamma distribution for various values of 휶and 휷. Larger 휷results in taller peaks and a sharper decline toward zero. Larger 휶moves the peak to the right. The expected value is 피(x) = 휶 휷. But the gamma distribution p (x|훼+ 1, 훽) = 훽훼+1 Γ (훼+ 1) x(훼)e−훽x or x(훼)e−훽x = Γ (훼+ 1) 훽훼+1 p (x|훼+ 1, 훽) Using this, 피(x) = 훽훼 Γ (훼) Γ (훼+ 1) 훽훼+1 =1 z }| { ∫∞ x=0 p (x|훼+ 1, 훽) dx = 훽훼 Γ (훼) 훼Γ (훼) 훽훽훼 = 훼 훽 A.5 Gamma functions and distribution 505 MAXIMUM OF A GAMMA DISTRIBUTION To maximize the gamma probability density function p (휆|X) = 휆(훼n−1)e−훽n휆for a ran- dom variable 휆, we take the derivative and equate to zero: d d휆  휆훼−1e−훽휆 = 0 =⇒(훼−1) 휆훼−1e−훽휆+ 휆훼−1 (−훽) e−훽휆= 0 휆= 훼−1 훽 notations The symbol ℝstands for the field (set) of real numbers. The symbol ∈should be read as “belongs to the set.” Vectors are denoted via overhead arrows: for example, ®x. ®x ∈ℝn indicates that ®x is a vector with n elements, each of which is a real number. Matrices are denoted via uppercase symbols: for example, A. Sometimes Am,n is used to indicate a matrix with m rows and n columns. A ∈ℝm×n indicates that A is a matrix with m rows and n columns, each of whose elements is a real number. Vector and matrix transformations are indicated via superscript T: for exam- ple, ®xT or AT. Individual elements of a vector or matrix are denoted via subscript: for exam- ple, Ai j or ®xi. ®x, ®y denotes the inner product of the two vectors ®x and ®y. For finite- dimensional vectors, this is equivalent to ®xT ®y. ˆl denotes a unit vector: ∥ˆl∥= 1. The symbol ∃should be read as “there exists.” The symbol ∀should be read as “for all.” 507 index A AdaGrad algorithm 326–327 Adam optimizer algorithm 328–329 AlexNet 391 argmaxonehot function 307–308 arrays 53 asymptotically approaching zero 174 autoencoders 469, 478–481 autoencoder decoder, code listing 480–481 autoencoder encoder, code listing 480 autoencoder training, code listing 481 decoder, definition of 478 definition of 305, 377, 478 embedding an image 305 encoder and decoder neural networks, discussion of 479–480 encoder, definition of 478 hyperparameter, definition of 478 PCA and 481 reconstruction loss, definition of 478 representation learning, definition of 478 schematic representation of an autoencoder 479 as unsupervised 479 See also variational autoencoders (VAEs) average pooling, definition of 381 B backpropagation algorithm 286–294 algorithm for training a neural network 294–295 backpropagation algorithm on an arbitrary network of linear layers 290–294 definition of 286 evaluating on a simple MLP with a single neuron per layer 286 forward and backward propagation, code listing 289–290 forward pass, definition of 289 Hadamard product, definition of 292 performing min-max normalization in PyTorch, code listing 296 training a neural network in PyTorch 295–298 using an optimizer to update weights 298 Bayes’ theorem 194, 196–198, 448 Bernoulli distribution 188–189 binary classifiers 83, 88 binomial distribution 180–184 bounding box, definition of 385 C categorical variables, definition of 242 centroid, definition of 163 509 510 INDEX classification loss, definition of 423 classifiers binary classifiers 88 charted examples of good and bad decision boundaries 250 charts of cat-brain threat-model decision boundaries 247–249 as decision boundaries 84–85, 246–247 decision boundary as a hypersurface 249 definition of 12, 245 estimating a decision boundary 251 feature space 246 forming mental pictures of hyperspaces with 3D analogs 249 geometric depiction of a classification problem 85 as a hypersurface 85 input space 246 modeling the classifier, definition of 86 continuous random variable 152 continuous variables, definition of 242 convex and nonconvex functions convex curves and surfaces, three definitions of 110–112 convexity and the Taylor series 112–113 examples of convex functions 113 introduction to 109–110 convolution convolution layers 344, 380–381 convolution output size 356 description of 343–345 expressing convolution layers as matrix-vector multiplications 344 one-dimensional 345–356 three-dimensional 368–374 transposed 374–379 two-dimensional 356–368 convolution, one-dimensional 345–356 1D edge detection, code listing 355 1D local averaging convolution, code listing 354–355 convolution output size 356 curve edge detection via 1D convolution 350–351 curve smoothing via 1D convolution 350 detecting edges as a way to understand images 351 directly invoking the convolution function, code listing 356 edge, definition of 351 formula for generating a single output value in 1D convolution 349 graphical and algebraical view 345 how to visualize a 1D convolution 345 input, definition of 345 kernel, definition of 345 as matrix multiplication 351–354 output, definition of 345 padding, definition of 346–347 same (zero) padding 347, 352 setting the weights of a 1D kernel 354 stride, definition of 345–346 valid padding 347, 352 convolution, three-dimensional 368–374 3D convolution with custom weights, code listing 373–374 diagrams illustrating the spatio-temporal view of 3D convolution 370 generating a single output value in 3D convolution 370 how a kernel extracts motion information from video frames 371 how to visualize a 3D convolution 369 illustration of a 3D convolution motion detector 372 video as a 3D entity extending over a spatio-temporal volume 368 video motion detection via 3D convolution 370–371 convolution, transposed 374–379 2D convolution and its transpose 377 autoencoder, definition of 377 decoder, definition of 376 descriptor vector, definition of 375 embedding as an effective compression technique 377 embedding, definition of 375 encoder, definition of 375 end-to-end learning, definition of 377 fractionally strided convolution 374–375 illustration of a 1D convolution and its transpose 376 illustration of a 2D convolution and its transpose 377 output size 377 upsampling using transpose convolutions, code listing 378–379 why autoencoders need transposed convolution 375 INDEX 511 convolution, two-dimensional 356–368 2D convolution as matrix multiplication 366–368 2D convolution with custom weights 363–365 2D edge detection, code listing 366 2D local averaging convolution, code listing 365–366 comparing Euclidean distance to Manhattan distance 358 generating a single output value in 2D convolution 361–362 graphical and algebraic view 356–358 how to visualize a 2D convolution 358 image edge detection via 2D convolution 362–363 image smoothing via 2D convolution 362 image, definition of 356–357 input, definition of 359 kernel, definition of 359 output, definition of 359 padding, definition of 361 same (zero) padding 361 stride, definition of 360–361 two-dimensional neighborhoods not preserved by rasterization 358 valid padding 361 convolutional neural networks (CNNs) AlexNet 391 benefits of neural networks with multiple convolutional layers 388–391 bounding box, definition of 385 feature map, definition of 388 GoogLeNet 398 image classification, definition of 386 Inception_v1 architecture 397–401 LeNet architecture, components of 387–389 MNIST data set, sample images from 387 object detection, definition of 386 ResNet architecture 401–406 VGG (Visual Geometry Group) Net 391–397 covariance covariance as the multivariate analog of variance 165–167 covariance of a multivariate Gaussian distribution 178–180 variance, covariance, and standard deviation 164–165 zero-mean, unit-covariance Gaussian for the known prior 490–492 See also variance cross-entropy loss binary cross-entropy loss, code listing 305 definition of 303 Cybenko’s universal approximation theorem 261–262 D data imbalance, definition of 310–311 decision making 2–4 decoder, definition of 376, 478 deep learning, overview of 1–17 deep neural networks, definition of 260 dependent events 157–159 descriptor vector, definition of 375 determinants 499 differentiable step-like functions 273–276 graph of the derivatives of 1D sigmoid and tanh functions 276 Heaviside step function as not differentiable 273 sigmoid function and its properties 273–275 tanh function 275–276 dimensionality reduction, definition of 469 discriminative functions 252 document descriptor space 116–118 document retrieval problem 141–147 dot product and cosine of the angle between two vectors 497–498 downsampling, definition of 381 dropout 336–339 E eigenvalues and eigenvectors 62–65, 67–69, 72–73 encoder, definition of 375, 478 entropy 198–212 applying to continuous and multidimensional random variables 201 chain rule of conditional entropy 212 charts of entropies of peaked and flat distributions 202 charts of KLD between example distributions 211 computing the cross-entropy of a Gaussian, code listing 202 computing the entropy of a Gaussian distribution, code listing 204 computing the KLD, code listing 210 conditional entropy 210–212 512 INDEX entropy (continued) cross-entropy 204–207 definition of 200–201 entropy of Gaussians 203–204 examples of 199 geometrical intuition for entropy 201–203 Huffman encoding 200 Kullback–Leibler divergence (KLD) 207–210 prefix coding 200 quantifying the uncertainty associated with a chancy event 199 variable bit-rate coding 200 epoch, definition of 302 error. See loss function (error) evidence lower bound (ELBO) 488–490 expected value (mean) 162–163 expressive power 14, 16, 240 F Fast R-CNN architecture 429 Faster R-CNN, high level architecture 414 feature space, definition of 10 first-order approximation 101 fixed point, definition of 231 focal loss 310–312 frequentist paradigm 151 Frobenius norms, definition of 122–123 Fully Bayes estimation 448–453 Bayes’ theorem 448–449 Bayesian estimation with unknown mean, known variance, code listing 452–453 Bayesian estimation with unknown mean, unknown variance, code listing 459 Bayesian estimation with unknown variance, known mean, code listing 456 Bayesian inference 460–461 computing posterior probability using Bayesian inference, code listing 461 conjugate priors 454 estimating precision 464–466 estimating the mean and precision parameters 457–458 estimating the precision parameter when the mean is known 455– 456 Fully Bayesian inferencing 459–461 Gaussian, unknown mean, known precision 450–453 Gaussian, unknown precision, known mean 454 maximum a posteriori (MAP) estimation 448 maximum likelihood estimation 460 maximum likelihood parameter estimation (MLE) 448 MLE for Gaussian parameter values, recap of 449–450 multivariate Bayesian inferencing, unknown mean 463 multivariate Gaussian, unknown mean, known precision 461–463 multivariate, unknown precision, known mean 463–466 normal-gamma distribution 457–459 parameter estimation and belief injection 448–449 prior probability density 448 Wishart distribution 454, 463–464 fully connected layer 277 function family 86 function-fitting problem 8–9 G Gamma distribution 454–455, 503–505 Gamma function, overview of 502–503 Gaussian (normal) distribution 173–180 asymptotically approaching zero 174 bell-shaped curve 174 Bernoulli distribution 188–189 binomial distribution 180–184 categorical distribution and one-hot vectors 189–190 chart of a univariate Gaussian random probability density function 177 computing the variance of 499–501 covariance of a multivariate Gaussian distribution 178–180 expected value of a Bernoulli distribution 188–189 expected value of a binomial distribution 184–185 expected value of a categorical distribution 190–191 expected value of a Gaussian distribution 176–177 Gaussian probability density function 174 geometry of sampled point clouds 180 log probability of a Bernoulli distribution, code listing 188 INDEX 513 log probability of a binomial distribution, code listing 183–184 log probability of a multinomial distribution, code listing 186 log probability of a univariate normal distribution, code listing 175 mean and variance of a Bernoulli distribution, code listing 189 mean and variance of a multinomial distribution, code listing 187–188 mean and variance of a multivariate normal distribution, code listing 179 mean and variance of a univariate Gaussian, code listing 178 multinomial distribution 185–187 multivariate Gaussian 175–176 multivariate Gaussian point clouds and hyper-ellipses 180 outlier values 174 probability of a categorical distribution 190 variance of a Bernoulli distribution 189 variance of a binomial distribution 185 Gaussian mixture models (GMM) 215, 223–237 algorithm of GMM fit (MLE of GMM parameters) 236 charts of two-dimensional GMMs with circular and elliptical bases 226 classification via GMM 230 fixed point, definition of 231 Gaussian mixture model distribution 229 GMM fit, code listing 236–237 latent variables for class selection 227–229 maximum likelihood estimation of GMM parameters (GMM fit) 230–237 probability density function of the GMM 223–227 progression of maximum likelihood estimation for GMM parameters 232 generative functions 252 generative modeling, definition of 447–448 global minimum 303 GoogLeNet 398 gradients 95–99 becoming zero at the optimum 96 gradient example in 3D 98–99 gradient vectors and minimizing loss functions 89–90 introduction to 90–91 as the vector of all the partial derivatives 95 ground truth 6, 241 GT vector 301, 303 H Hadamard product, definition of 292 Hausdorff property, definition of 441–442 Heaviside step function 252–253, 273 Hessian matrix 101, 284 hidden layers 260 hinge loss function 312–314 histograms 152–153 homeomorphism 443–445 Huffman encoding 200 human labeling (human curation), definition of 6 hyperparameter, definition of 478 hyperplanes 85, 253–254 I image 83–84, 386 Inception_v1 architecture 397–401 description of its network-in-network paradigm 397–399 diagram of 398 GoogLeNet 398 implementing a dimensionality reduced Inception block 400–401 implementing a naive Inception block, code listing 399 inferencing 4, 6, 10, 241 input variables 242 inputs, normalizing 7 iterative training 8–9 J Jensen’s inequality theorem 501–502 joint probability, definition of 155 Jupyter Notebook 18–19, 22 K Kullback–Leibler divergence (KLD) 207–210 L L’Hospital’s rule 500 L1 regularization 333–334 L2 regularization 332–334 514 INDEX labeling 241, 273 latent or hidden variables/parameters 216, 228 latent semantic analysis (LSA) 118, 142–147 latent spaces 468–496 comparing discriminative and generative models 471– 472 considering the space of natural and digital images 469 dimensionality reduction 469, 475 dimensionality reduction using PCA, code listing 477–478 discriminative classifiers, definition of 471 generative classifiers 471–472 generative models, properties of 471–472 geometric view of 469–471 illustration of good and bad discriminative classifiers 472 latent space modeling briefly explained 470–471 latent vector, definition of 468 latent-space modeling, benefits and applications 472–473 linear latent space manifolds and PCA 474–478 manifold as capturing the essence of a common property 469 mapping from a 2D input space to a 1D latent space 482 observed vector, definition of 468 PCA as a special case of latent space representation 471 regularization as creating a more compact latent space 483 smoothness, continuity, and regularization of 481–483 steps involved in a PCA-based dimensionality reduction 475–477 two examples of latent subspaces, with planar and curved manifolds 470 learning rate (LR) 285, 315 LeNet architecture 387–389 implementing LeNet for image classification on MNIST, code listing 388–389 output feature map passed through two fully connected (FC) layers 388 PyTorch Lightning 406–411 subsampling (pooling) layers 388 tanh activation layer 388 three convolutional layers of 5 × 5 kernels 387–388 level contours 97 linear layers 277–281 algorithm for training a neural network 294–295 backpropagation algorithm 286–294 diagram of a complete multilayered neural network 278 forward and backward propagation, code listing 289–290 forward propagation of a single linear layer 280–281 forward propagation, code listing 281 fully connected layer 277 gradient descent and local minima 285–286 Hadamard product, definition of 292 Hessian matrix 284 learning rate 285 loss and its minimization 282–283 loss surface and gradient descent 283–286 as matrix-vector multiplication 277–280 mean squared error (MSE) function 282 MSE loss, code listing 283 never using test data for training 281 performing min-max normalization in PyTorch, code listing 296 training a neural network in PyTorch 295–298 training and backpropagation 281–282 tunable hyperparameter 285 using an optimizer to update weights 298 linear vs. nonlinear models 12–14 local minimum 303 local response normalization (LRN) layers 391 local translation invariance, definition of 381 logical functions 242–245 definition of 242 logical AND 243–244 logical NOT 244–245 logical OR 242–243 logical XOR 244–245 m-out-of-n trigger 244 multi-input logical AND 244 multi-input logical OR 244 log-sum inequality theorem 502 loss function (error) autoencoders, definition of 305 binary cross-entropy loss for image and vector mismatches 305–306 binary cross-entropy loss, code listing 305 INDEX 515 computing the gradient of the loss function 316 creating a custom neural network model, code listing 318 creating a custom PyTorch data set, code listing 317 cross-entropy loss, code listing 304 cross-entropy loss, definition of 303 data imbalance, definition of 310–311 definition of 89–91, 301 epoch, definition of 302 equation for describing a full neural network 301 focal loss 310–312 generating one training loop, code listing 319 global minimum 303 gradient vectors and minimizing loss functions 89–90 GT vector, definition of 301–302 local approximation for the loss function 99–100 local minimum 303 loss function and SGD optimizer, code listing 318 loss surfaces and their minimization 301–303 loss surfaces, description of 302–303 minimizing 83, 88–89 multi-dimensional loss functions 93–94 one-dimensional loss functions 91–93 output vector 303 prediction vector 303 regression loss, code listing 303 running the training loop num_epochs times, code listing 319 softmax function 306–310 total error, definition of 89 total training loss, definition of 301 using a squared error function 89 visualizing loss surfaces 97 M machine learning analogy to the human brain 5 cat brain model 7 chart of the 2D input point space for the cat brain model 11 classifier, definition of 12 collinearity as implying linear dependence 47 computing eigenvectors and eigenvalues 67 computing the simplified cat brain threat score model 11 defining the span of a set of vectors 47–48 dot product and the difference between two unit vectors 38–39 dot product of two vectors 29–30 eigenvalues and eigenvectors 62–65 eigenvectors and linear independence 65–66 entropy 198–212 equations for describing a multilayered neural network 16 estimating a threat score 7 example cat-brain dataset matrix 24 example training dataset 24 expressive power 14, 16 feature space, definition of 10 finding the axes of a hyperellipse 78–79 formula for transforming an arbitrary input value to a normalized value 7 from arbitrary input to the desired output during inferencing 5 generating the right outcome on never-before-seen data 5 generic multidimensional definition of linear transforms 51 geometric intuitions for dot product and vector length 36–37 geometrical view of 10–11 introduction to vectors via PyTorch 23 Kullback–Leibler divergence (KLD) 207–210 latent semantic analysis (LSA) 118 learning, definition of 5 linear dependence 46–47 linear systems with zero or near-zero determinants 55–57 linear vs. nonlinear models 12–14 list of problem solving stages 4 machine learning model error 34–36 matrix diagonalization 73–74 matrix powers using diagonalization 76–77 matrix-matrix multiplication 32–33 matrix-vector multiplication 31–32 matrix-vector multiplications as linear transforms 52–53 measuring the component of a vector along a coordinate axis 37–38 minimizing a quadratic form in machine learning problems 121 model estimation 8 516 INDEX machine learning (continued) multidimensional line and plane equations 42–46 multidimensional line equation 42–43 multidimensional planes 43–46 multilayered neural network, diagram of 15 natural language processing (NLP) 118 over-determined and under-determined linear systems 57–59 as a paradigm shift in computing 3 performing basic vector and matrix operations 26–28 principal component analysis (PCA) 118 producing Python code using Jupyter Notebook 22 quadratic form, definition of 118 regressor, definition of 12 retrieving documents that match a query phrase 140–147 role of matrices in 23 role of vectors in 19–21 sigmoid function, definition of 14 singular value decomposition (SVD) 130–140 solving linear systems without inversion via diagonalization 74–75 spectral decomposition of a symmetric matrix 77 squared error 34 sticking to any fixed coordinate system 22 supervised machine learning 194 supervised vs. unsupervised learning 4 symmetric matrices and orthogonal eigenvectors 66 target output 4 training data, definition of 4 training, definition of 5 transpose of matrix products 33 trying to model the unknown transformation function 6 unsupervised machine learning 193–194 using 3D analogues for higher dimensional spaces 22 using PyTorch code for vector manipulations 22 See also neural networks manifolds 438–443 applying calculus to a locally Euclidean property 440–441 bounded, compact, and precompact sets 443 definition of 438 d-manifold, definition of 440 example manifolds and non-manifolds in 1D and 2D 440 Hausdorff property, definition of 441–442 manifolds as locally Euclidean 440 mapping points from one manifold to another 439 neural networks and 438 open sets, closed sets, and boundaries 442 second countable property of manifolds 442–443 mathematical notations used throughout the text 506 matrices applying rotation matrices 69 basic vector and matrix operations in machine learning 26–28 converting a matrix into a vector via rasterization 84 data matrix columns as dimensions in the feature space 115 data matrix rows as representing feature vectors 115 example cat-brain dataset matrix 24 Frobenius norms, definition of 122–123 full-rank matrices, definition of 137 introducing matrices via PyTorch 25–26 inverting a matrix and computing its determinant 57 linear systems and matrix inverse 53–55 matrix and vector transpose 28–29 matrix diagonalization 73–74 matrix powers using diagonalization 76–77 matrix, definition of 23 matrix-matrix multiplication 32–33 matrix-vector multiplication 31–32 Moore Penrose pseudo-inverse of a matrix 59–62 orthogonal (rotation) matrices and their eigenvalues and eigenvectors 67–69 orthogonality and length-preservation 70–71 orthogonality of rotation matrices 71–72 rank of a matrix, definition of 137 representing digital images as matrices 25 role in machine learning 23 slicing and dicing matrices 26 solving an overdetermined system using the pseudo-inverse 62 spectral norms, definition of 122 INDEX 517 symmetric positive semidefinite matrices 121–122 transpose of matrix products 33 using linear algebraic tools to analyze matrix structures 115–116 max pooling, definition of 381 maximum a posteriori (MAP) estimation 448 maximum likelihood parameter estimation (MLE) 448 mean squared error (MSE) function 282 model architecture 4, 6, 8 model parameter estimation 213–222 estimating the model parameters from the unlabeled training data 213 examining the likelihood term 213 examining the prior probability term 213 Gaussian mixture models (GMMs) 215 Gaussian negative log-likelihood for training data, code listing 219–220 Gaussian negative log-likelihood with regularization, code listing 221–222 latent variables and evidence maximization 215–216 likelihood, evidence, and posterior and prior probabilities 213– 214 maximum a posteriori (MAP) parameter estimation and regularization 215 maximum likelihood estimate for a Gaussian, code listing 219 maximum likelihood parameter estimation (MLE) 214–215 maximum likelihood parameter estimation for Gaussians 216–218 minimizing MLE loss via gradient descent, code listing 220 using the log-likelihood trick 214 modeling inferencing 4 linear vs. nonlinear models 12–14 model architecture selection 86 model training 4, 8–10, 86 overall algorithm for training a supervised model 90 training error, definition of 86 trying to model the unknown transformation function 6 momentum 320–325 AdaGrad algorithm 326–327 Adam optimizer algorithm with bias correction 328–329 chart showing an overfitting of data points in a binary classifier 331 explanation of 320–321 L1 regularization 333–334 L2 regularization 332–334 momentum-based gradient descent 322 Nesterov accelerated gradients 322–325 overfitting and underfitting 330 regularization 330 root mean squared propagation (RMSProp) 327–328 viewing regularization as minimizing descriptor length 332 Multibox Single-Shot Detector (SSD) 435–436 multidimensional functions 93–95 multidimensional integral 161–162 N natural language processing (NLP) 118 Nesterov accelerated gradients 322–325 neural networks adjusting its architecture and parameter values 240 algorithm for training a neural network 294–295 backpropagation algorithm 286–294 categorical variables, definition of 242 charts of cat-brain threat-model decision boundaries 247–249 charts of good and bad decision boundaries 250 choosing an architecture 240 classifier functions 245–246 classifying into supervised and unsupervised neural networks 241 continuous variables, definition of 242 decision boundaries, definition of 246–247 decision boundary as a hypersurface 249 determining parameter values through training 240 diagram of a complete multilayered neural network 278 diagram of a multilayered neural network 15 differentiable step-like functions 273–276 discriminative functions 252 equations for describing a multilayered neural network 16 estimating a decision boundary 251 518 INDEX neural networks (continued) expressing real-world problems in target functions 240–242 expressive power, definition of 240 feature space 246 forming mental pictures of hyperspaces with 3D analogs 249 forward and backward propagation, code listing 289–290 fully connected layer 277 generative functions 252 gradient descent and local minima 285–286 ground truth 241 Hadamard product, definition of 292 Heaviside step function 252–253, 273 Hessian matrix 284 hyperplanes 253–254 inferencing 241 input space 246 input variables 242 labeling 241, 273 learning rate 285 linear layers 276–281 logical AND function 243–244 logical functions, definition of 242–245 logical NOT function 244–245 logical OR function 242–243 logical XOR function 244–245 making a probabilistic statement of output correctness 241 manual annotation 241 mean squared error (MSE) function 282 m-out-of-n trigger function 244 multi-input logical AND function 244 multi-input logical OR function 244 multilayer perceptrons (MLPs) 259–269 neuron, basic description of 240, 252 output variables 242 overview of 240–241 perceptrons 254–269 performing min-max normalization in PyTorch, code listing 296 sigmoid function and its properties 273–275 supervised neural networks 273 supervised training data 241 tanh function 275–276 target output 241 training a neural network in PyTorch 295–298 training data, definition of 272 tunable hyperparameter 285 using an optimizer to update weights 298 weights 241 See also machine learning neuron, description of 240, 252 non-maxima suppression (NMS) algorithm 425–426 O object detectors 411–436 anchors and their configurations, description of 415 assigning GT labels for each anchor box, code listing 420 assigning targets to anchor boxes 421–422 classification loss, definition of 423 classifier predicting an objectness value 417 contributions and improvements of Fast R-CNN 412–413 dealing with the imbalance between negative and positive anchors 421 Fast R-CNN and Rols 427–435 Fast R-CNN architecture 429 Fast R-CNN inference 433–434 Fast R-CNN loss function 431–432 Fast R-CNN RoI head, code listing 430–431 Faster R-CNN and its two core modules 412–413 Faster R-CNN, high level architecture 414 FCN of the RPN, code listing 418–419 Feature Pyramid Network (FPN) 436 FRCNN guidelines for assigning labels to anchor boxes 420 fully convolutional network (FCN) architecture 417–418 generating a target (GT) for an RPN 419–421 generating all anchors for a given image 417 generating anchors at a particular grid point, code listing 416 generating region proposals 424–425 Multibox Single-Shot Detector (SSD) 435–436 NMS of RoIs, code listing 427 non-maxima suppression (NMS) algorithm 425–426 other object-detection paradigms 435–436 R-CNN module 413–414 Region proposal network (RPN) 413–415 regression loss, definition of 423 INDEX 519 Rol pooling 428–429 RPN loss function 423–424 three stages in the R-CNN approach to object detection 411–412 training the Fast R-CNN 431 training the Faster R-CNN 434–435 You Only Look Once (YOLO) 435 observed vector, definition of 468 one-dimensional loss functions 91–93 optimization 314–316 AdaGrad algorithm 326–327 Adam optimizer algorithm with bias correction 328–329 Bayes’ theorem and the stochastic view of optimization 334–335 creating a custom neural network model, code listing 318 creating a custom PyTorch data set, code listing 317 definition of 301–302, 315 dropout 336–339 generating one training loop, code listing 319 L1 regularization 333–334 L2 regularization 332–334 learning rate (LR) 315 loss function and SGD optimizer, code listing 318 map optimization 335–336 MLE-based optimization 335 overfitting and underfitting 330 overfitting of data points in a binary classifier 331 random shuffling of training data after every epoch 315 regularization 330 root mean squared propagation (RMSProp) 327–328 running the training loop num_epochs times, code listing 319 stochastic gradient descent (SGD) 315–316 viewing regularization as minimizing descriptor length 332 output variables 242 output vector 303 P parameterized function, threat score 4 partial derivatives, definition of 94 perceptrons 254–269 classification and 254 code listing for 256 Cybenko’s universal approximation theorem 261–262 deep neural networks, definition of 260 definition of 254 generating 2D steps and waves with perceptrons 264–267 generating a 1D tower with perceptrons 262–264 hidden layers 260 introduction to modeling common logic gates with perceptrons 256 layering for organizing perceptrons into a neural network 260 MLP for a logical XOR function 259–260 MLPs for polygonal decision boundaries 268–269 modeling logical gates, code listing 258 multilayer perceptrons (MLPs) 259–269 multiple perceptrons 256 partitioning with a planar decision surface 255 perceptron for a logical AND function 256–257 perceptron for a logical NOT function 257–258 perceptron for a logical OR function 257–258 perceptrons and MLPs in 1D, code listing 267 perceptrons and MLPs in 2D, code listing 267–268 truth table for two-variable logical functions 261 pixel, definition of 83 pooling 381–383 prediction vector 303 prefix coding 200 principal component analysis (PCA) 118, 123–130 applying PCA on correlated and uncorrelated datasets 128 calculating the direction of maximum spread 125–127 dimensionality reduction via PCA 127–128 introduction to 123–125 limitations of PCA 129–130 linear latent space manifolds and PCA 474–478 520 INDEX principal component analysis (PCA) (continued) PCA and data compression 130 PCA computation, code listing 128–129 PCA on synthetic correlated data, code listing 129 PCA on synthetic nonlinearly correlated data, code listing 130 PCA on synthetic uncorrelated data 129 as a special case of latent space representation 471 use in JPEG 98 image compression techniques 130 probability density function (PDF) 152, 161, 198 probability distributions continuous random variable 152 definition of 153 discrete random variable 151 emphasizing the geometrical view of multivariate statistics 150 example graph for the weights of adults in Statsville 154 fitting probability distributions to specific groups of people 150 frequentist paradigm 151 loosely structured point distributions in high-dimensional spaces 149 probabilities as always less than or equal to 1 151 probability density 152 PyTorch distributions package 150, 162 random variable, definition of 151 semantic segmentation 150 using histograms to visualize discrete random variables 152–153 using probabilistic models in unsupervised and minimally supervised learning 150 using uppercase letters to denote random variables 152 variational auto encoders (VAEs) 150 See also probability theory probability theory asymptotically approaching zero 174 basic concepts of 154–155 bell-shaped curve 174 Bernoulli distribution 188–189 binomial distribution 180–184 Cartesian product 157 categorical distribution and one-hot vectors 189–190 centroid, definition of 163 chart of a univariate Gaussian random probability density function 177 chart of bivariate uniform random probability density function 173 conditional probability 196 continuous random variables and probability density 160–162 covariance as the multivariate analog of variance 165–167 covariance of a multivariate Gaussian distribution 178–180 dependent events and their joint probability distribution 157–159 dependent vs. independent variables 196 entropy 198–212 entropy, definition of 200–201 exhaustive and mutually exclusive events 154–155 expected value (mean) 162–164 expected value of a Bernoulli distribution 188–189 expected value of a binomial distribution 184–185 expected value of a categorical distribution 190–191 expected value of a function of a random variable 163 expected value of a Gaussian distribution 176–177 expected value of a linear combination of random variables 164 expected value of a uniform distribution 171 Gaussian (normal) distribution 173–180 Gaussian probability density function 174 geometry of sampled point clouds 180 graphical visualization of joint probability distributions 160 independent events 155 joint and marginal probability 194–196 joint probabilities and their distributions 155–157 Kullback–Leibler divergence (KLD) 207–210 log probability of a Bernoulli distribution, code listing 188 log probability of a binomial distribution, code listing 183–184 log probability of a multinomial distribution, code listing 186 log probability of a univariate normal distribution, code listing 175 INDEX 521 log probability of a univariate uniform random distribution, code listing 171 marginal probabilities 157 marginal probability for a variable 195 mean and variance of a Bernoulli distribution, code listing 189 mean and variance of a multinomial distribution, code listing 187–188 mean and variance of a multivariate normal distribution, code listing 179 mean and variance of a uniform random distribution, code listing 172 mean and variance of a univariate Gaussian, code listing 178 multidimensional integral 161–162 multinomial distribution 185–187 multivariate Gaussian 175–176 multivariate Gaussian point clouds and hyper-ellipses 180 outlier values 174 probabilities of impossible and certain events 154 probability density function (PDF) 16, 198 probability of a categorical distribution 190 product rule, definition of 155 properties of distributions 162–167 sample point distributions for dependent and independent variables159–160 sampling from a distribution 167–169 sum rule 195 uniform distributions as multivariate 173 uniform random distributions 170–171 variance and expected value 167 variance of a Bernoulli distribution 189 variance of a binomial distribution 185 variance of a uniform distribution 171–172 variance, covariance, and standard deviation 164–165 See also probability distributions product rule, definition of 155 Python code 18–19 applying PCA on correlated and uncorrelated datasets 128 computing LSA 145–146 computing LSA and SVD on a large dataset 146–147 computing PCA directly using SVD 139 dot product of two vectors 40 eigenvalues and eigenvectors of a rotation matrix 72 examining linear models 101–105 examining nonlinear models 105–107 finding the axes of a hyperellipse 79–80 introducing matrices via PyTorch 25–26 matrix diagonalization 74 matrix vector multiplication 40–41 orthogonality of rotation matrices 71–72 PCA computation, code listing 128–129 PCA on synthetic correlated data, code listing 129 PCA on synthetic nonlinearly correlated data, code listing 130 PCA on synthetic uncorrelated data 129 performing a matrix transpose 39–40 slicing and dicing matrices 26 solving linear systems via diagonalization 76 solving linear systems with SVD 137–139 spectral decomposition of a symmetric matrix 77–78 tensors and images in PyTorch 26 training a linear model for the cat brain 108 transpose of a matrix product 42 using for vector manipulations 22 PyTorch creating a custom PyTorch data set, code listing 317 inferencing a model, PyTorch Trainer code listing 411 introducing matrices via PyTorch 25–26 introducing vectors via PyTorch 23 MNIST data module, PyTorch DataModule code listing 407–408 performing min-max normalization in PyTorch, code listing 296 PyTorch code for solving linear systems with SVD 137–139 PyTorch distributions package 150, 162 tensors and images in PyTorch 26 training a neural network in PyTorch 295–298 using PyTorch code for vector manipu- lations 22 PyTorch Autograd 103 PyTorch DataLoader 316 PyTorch Lightning 406–411 DataModule component 407–408 implementing LeNet as a PyTorch Lightning module, code listing 408–410 inferencing a model, PyTorch Trainer code listing 411 LightningModule component 408–410 522 INDEX PyTorch Lightning (continued) MNIST data module, PyTorch DataModule code listing 407–408 Trainer component 410–411 Q quadratic forms, minimizing 118–121 quantitative estimation 2–3 quantitative inputs 4 R random variable 151–153, 160–162 rasterized vector, creating 84 reconstruction loss, definition of 478 rectified linear unit (ReLU) 392–394 regression loss, definition of 303, 423 regressors, definition of 2, 12 regularization 330 representation learning, definition of 478 ResNet architecture 401–406 components of the core architecture 403 examining how to solve the degradation problem 401–403 identity shortcut connection 402 implementing a basic skip connection block (BasicBlock) 403–404 PyTorch Lightning 406–411 ResidualConvBlock, code listing 405 ResNet-34, code listing 405–406 root mean squared propagation (RMSProp) 327–328 S semantic segmentation 150 sigmoid function 14, 273–276 singular value decomposition (SVD) 130–140 applying SVD by solving arbitrary linear systems 135–136 applying SVD to find the best low-rank approximation of a matrix 139–140 applying SVD via PCA computation 135 computing PCA directly using SVD 139 full-rank matrices, definition of 137 linear system as degenerate 137 PyTorch code for solving linear systems with SVD 137–139 rank of a matrix, definition of 137 SVD theorem 131–134 softmax function 306–314 spectral norms, definition of 122 standard deviation 164–165 stochastic gradient descent (SGD) 315, 317 stochastic mapping, definition of 484 supervised learning 4, 194 supervised neural networks 273 supervised training data 241 T tanh function 275–276 target functions 240–242 target output 4, 241 Taylor series 100–101, 112–113 tensors 25–26 term frequency (TF) 141 threat score 4, 7 thresholding 10, 12 torchvision package 397 total training loss, definition of 301 training data 4–6, 86, 272, 315 transposed convolution. See convolution, transposed tunable hyperparameter 285 U unit vector 36 unsupervised machine learning 193–194 V vanishing gradient problem 394 variable bit-rate coding 200 variance Bayesian estimation with unknown mean, known variance, code listing 452–453 Bayesian estimation with unknown mean, unknown variance, code listing 459 Bayesian estimation with unknown variance, known mean, code listing 456 computing the variance of a Gaussian distribution 499–501 covariance as the multivariate analog of variance 165–167 mean and variance of a Bernoulli distribution, code listing 189 mean and variance of a multinomial distribution, code listing 187–188 INDEX 523 mean and variance of a multivariate normal distribution, code listing 179 mean and variance of a univariate Gaussian, code listing 178 variance and expected value 167 variance of a Bernoulli distribution 189 variance of a binomial distribution 185 variance of a uniform distribution 171–172 variance, covariance, and standard deviation 164–165 See also covariance variational autoencoders (VAEs) 150, 241, 483–495 autoencoders vs. VAEs 494 comparing autoencoder-and VAE-reconstructed images on the MNIST data set 494 computing the reconstruction loss and KL divergence loss 485 differences between the learned latent spaces of the autoencoder and VAE 495 evidence lower bound (ELBO) 488–490 examples of high and low KL divergence loss 486 geometric overview of 483–484 KLD loss as regularizing the latent space 486 minimizing reconstruction loss as leading to ELBO maximization 490 physical significance of ELBO maximization 489 reparameterization trick, code listing 492 stochastic mapping as leading to latent-space smoothness 487 stochastic mapping, definition of 484 VAE decoder, code listing 493 VAE loss, code listing 494 VAE training, code listing 494 VAE training, losses, and inferencing 485–486 VAE, code listing 493 VAEs and Bayes’ theorem 487 zero-mean, unit-covariance Gaussian for the known prior 490–492 See also autoencoders vectors basic vector and matrix operations in machine learning 26–28 basis vectors 48 creating feature vectors that describe a document 20 defining the span of a set of vectors 47–48 definition of 19 describing a point’s position in a coordinate system 21 document feature vectors 141 dot product of two vectors 29–30 feature vector, definition of 19 geometric intuitions for vector length 36 geometric view of 21 as inputs to a machine learning system 84 introduction to vectors via PyTorch 23 linear transforms 49–51 mapping input points to output points in a high-dimensional space 21 matrix and vector transpose 28–29 minimal and complete basis 48–49 orthogonality of vectors 39 representing both inputs and outputs 19 representing the parameters of the model function 19 role of in machine learning 19–21 unit vector 36 vector spaces 48–49 VGG (Visual Geometry Group) Net 391–397 common structural elements of the VGG family of networks 391–392 convolutional backbone, code listing 395–396 graph of a 1D sigmoid function and its derivative 394 instantiating a VGG network from a specific config 397 rectified linear unit (ReLU) 392–394 removal of the local response normalization (LRN) layers 391 single convolutional block, code listing 395 torchvision package 397 use of smaller (3x3) convolution filters 391 vanishing gradient problem 394 VGG network, code listing 396 VGG-11 architecture diagram 393 W weights 4, 6, 241 Wishart distribution 454, 463–464 Krishnendu Chaudhury ● Foreword by Prith Banerjee ISBN-13: 978-1-61729-648-2 D iscover what’s going on inside the black box! To work with deep learning you’ll have to choose the right model, train it, preprocess your data, evaluate performance and accuracy, and deal with uncertainty and variability in the outputs of a deployed solution. Th is book takes you systemati- cally through the core mathematical concepts you’ll need as a working data scientist: vector calculus, linear algebra, and Bayesian inference, all from a deep learning perspective. Math and Architectures of Deep Learning teaches the math, the- ory, and programming principles of deep learning models laid out side by side, and then puts them into practice with well- annotated Python code. You’ll progress from algebra, calculus, and statistics all the way to state-of-the-art DL architectures taken from the latest research. What’s Inside ● Th e core design principles of neural networks ● Implementing deep learning with Python and PyTorch ● Regularizing and optimizing underperforming models Readers need to know Python and the basics of algebra and calculus. Krishnendu Chaudhury is co-founder and CTO of the AI startup Drishti Technologies. He previously spent a decade each at Google and Adobe. For print book owners, all ebook formats are free: https://www.manning.com/freebook Math and Architectures of Deep Learning DATA SCIENCE / DEEP LEARNING M A N N I N G “ Machine learning uses a cocktail of linear algebra, vector calculus, statistical analysis, and topology to represent, visualize, and manipulate points in high dimensional spaces. Th is book builds that foundation in an intuitive way–along with the PyTorch code you need to be a successful deep learning practitioner.” —Vineet Gupta Google Research “ A thorough explanation of the mathematics behind deep learning!” —Grigory Sapunov, Intento “ Deep learning in its full glory, with all its mathematical details. Th is is the book!” —Atul Saurav, Genworth Financial See first page
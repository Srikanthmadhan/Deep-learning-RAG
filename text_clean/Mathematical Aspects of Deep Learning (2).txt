MATHEMATICAL ASPECTS OF DEEP LEARNING In recent years the development of new classiï¬cation and regression algorithms based on deep learning has led to a revolution in the ï¬elds of artiï¬cial intelligence, machine learning, and data analysis. The development of a theoretical foundation to guarantee the success of these algorithms constitutes one of the most active and exciting research topics in applied mathematics. This book presents the current mathematical understanding of deep learning methods from the point of view of the leading experts in the ï¬eld. It serves both as a starting point for researchers and graduate students in computer science, mathematics, and statistics trying to get into the ï¬eld and as an invaluable reference for future research. philipp grohs is Professor of Applied Mathematics at the University of Vienna and Group Leader of Mathematical Data Science at the Austrian Academy of Sciences. gitta kutyniok is Bavarian AI Chair for Mathematical Foundations of Artiï¬cial Intelligence at Ludwig-Maximilians-UniversitÂ¨at MÂ¨unchen and Adjunct Professor for Machine Learning at the University of TromsÃ¸. MATHEMATICAL ASPECTS OF DEEP LEARNING Edited by PHILIPP GROHS UniversitÂ¨at Wien, Austria GITTA KUTYNIOK Ludwig-Maximilians-UniversitÂ¨at MÂ¨unchen University Printing House, Cambridge CB2 8BS, United Kingdom One Liberty Plaza, 20th Floor, New York, NY 10006, USA 477 Williamstown Road, Port Melbourne, VIC 3207, Australia 314-321, 3rd Floor, Plot 3, Splendor Forum, Jasola District Centre, New Delhi â€“ 110025, India 103 Penang Road, #05â€“06/07, Visioncrest Commercial, Singapore 238467 Cambridge University Press is part of the University of Cambridge. It furthers the Universityâ€™s mission by disseminating knowledge in the pursuit of education, learning, and research at the highest international levels of excellence. www.cambridge.org Information on this title: www.cambridge.org/9781316516782 DOI: 10.1017/9781009025096 Â© Cambridge University Press 2023 This publication is in copyright. Subject to statutory exception and to the provisions of relevant collective licensing agreements, no reproduction of any part may take place without the written permission of Cambridge University Press. First published 2023 Printed in the United Kingdom by TJ Books Limited, Padstow Cornwall A catalogue record for this publication is available from the British Library. ISBN 978-1-316-51678-2 Hardback Cambridge University Press has no responsibility for the persistence or accuracy of URLs for external or third-party internet websites referred to in this publication and does not guarantee that any content on such websites is, or will remain, accurate or appropriate. Contents Contributors page xiii Preface xv 1 The Modern Mathematics of Deep Learning Julius Berner, Philipp Grohs, Gitta Kutyniok and Philipp Petersen 1 1.1 Introduction 1 1.1.1 Notation 4 1.1.2 Foundations of Learning Theory 5 1.1.3 Do We Need a New Theory? 23 1.2 Generalization of Large Neural Networks 31 1.2.1 Kernel Regime 31 1.2.2 Norm-Based Bounds and Margin Theory 33 1.2.3 Optimization and Implicit Regularization 35 1.2.4 Limits of Classical Theory and Double Descent 38 1.3 The Role of Depth in the Expressivity of Neural Networks 41 1.3.1 Approximation of Radial Functions 41 1.3.2 Deep ReLU Networks 44 1.3.3 Alternative Notions of Expressivity 47 1.4 Deep Neural Networks Overcome the Curse of Dimensionality 49 1.4.1 Manifold Assumption 49 1.4.2 Random Sampling 51 1.4.3 PDE Assumption 53 1.5 Optimization of Deep Neural Networks 57 1.5.1 Loss Landscape Analysis 57 1.5.2 Lazy Training and Provable Convergence of Stochas- tic Gradient Descent 61 1.6 Tangible Eï¬€ects of Special Architectures 65 1.6.1 Convolutional Neural Networks 66 v vi Contents 1.6.2 Residual Neural Networks 68 1.6.3 Framelets and U-Nets 70 1.6.4 Batch Normalization 73 1.6.5 Sparse Neural Networks and Pruning 75 1.6.6 Recurrent Neural Networks 76 1.7 Describing the Features that a Deep Neural Network Learns 78 1.7.1 Invariances and the Scattering Transform 78 1.7.2 Hierarchical Sparse Representations 79 1.8 Eï¬€ectiveness in Natural Sciences 81 1.8.1 Deep Neural Networks Meet Inverse Problems 82 1.8.2 PDE-Based Models 84 2 Generalization in Deep Learning K. Kawaguchi, Y. Bengio, and L. Kaelbling 112 2.1 Introduction 112 2.2 Background 113 2.3 Rethinking Generalization 116 2.3.1 Consistency of Theory 118 2.3.2 Diï¬€erences in Assumptions and Problem Settings 119 2.3.3 Practical Role of Generalization Theory 121 2.4 Generalization Bounds via Validation 121 2.5 Direct Analyses of Neural Networks 122 2.5.1 Model Description via Deep Paths 123 2.5.2 Theoretical Insights via Tight Theory for Every Pair (P,S) 125 2.5.3 Probabilistic Bounds over Random Datasets 127 2.5.4 Probabilistic Bound for 0â€“1 Loss with Multi-Labels 130 2.6 Discussions and Open Problems 131 Appendix A Additional Discussions 133 A1 Simple Regularization Algorithm 133 A2 Relationship to Other Fields 135 A3 SGD Chooses Direction in Terms of Â¯w 135 A4 Simple Implementation of Two-Phase Training Procedure 136 A5 On Proposition 2.3 136 A6 On Extensions 137 Appendix B Experimental Details 137 Appendix C Proofs 138 C1 Proof of Theorem 2.1 139 C2 Proof of Corollary 2.2 139 Contents vii C3 Proof of Theorem 2.7 140 C4 Proof of Theorem 2.9 141 C5 Proof of Theorem 2.10 142 C6 Proof of Proposition 2.5 143 3 Expressivity of Deep Neural Networks Ingo GÃ¼hring, Mones Raslan, and Gitta Kutyniok 149 3.1 Introduction 149 3.1.1 Neural Networks 151 3.1.2 Goal and Outline of this Chapter 154 3.1.3 Notation 154 3.2 Shallow Neural Networks 155 3.2.1 Universality of Shallow Neural Networks 156 3.2.2 Lower Complexity Bounds 159 3.2.3 Upper Complexity Bounds 160 3.3 Universality of Deep Neural Networks 161 3.4 Approximation of Classes of Smooth Functions 163 3.5 Approximation of Piecewise Smooth Functions 167 3.6 Assuming More Structure 172 3.6.1 Hierachical Structure 172 3.6.2 Assumptions on the Data Manifold 174 3.6.3 Expressivity of Deep Neural Networks for Solutions of PDEs 175 3.7 Deep Versus Shallow Neural Networks 177 3.8 Special Neural Network Architectures and Activation Functions 180 3.8.1 Convolutional Neural Networks 180 3.8.2 Residual Neural Networks 184 3.8.3 Recurrent Neural Networks 185 4 Optimization Landscape of Neural Networks RenÃ© Vidal, Zhihui Zhu, and Benjamin D. Haeï¬€ele 200 4.1 Introduction 201 4.2 Basics of Statistical Learning 205 4.3 Optimization Landscape of Linear Networks 206 4.3.1 Single-Hidden-Layer Linear Networks with Squared Loss and Fixed Size Regularization 207 4.3.2 Deep Linear Networks with Squared Loss 212 4.4 Optimization Landscape of Nonlinear Networks 214 4.4.1 Motivating Example 215 4.4.2 Positively Homogeneous Networks 221 4.5 Conclusions 225 viii Contents 5 Explaining the Decisions of Convolutional and Recurrent Neural Networks Wojciech Samek, Leila Arras, Ahmed Osman, GrÃ©goire Montavon, Klaus-Robert MÃ¼ller 229 5.1 Introduction 229 5.2 Why Explainability? 231 5.2.1 Practical Advantages of Explainability 231 5.2.2 Social and Legal Role of Explainability 232 5.2.3 Theoretical Insights Through Explainability 232 5.3 From Explaining Linear Models to General Model Explain- ability 233 5.3.1 Explainability of Linear Models 233 5.3.2 Generalizing Explainability to Nonlinear Models 235 5.3.3 Short Survey on Explanation Methods 236 5.4 Layer-Wise Relevance Propagation 238 5.4.1 LRP in Convolutional Neural Networks 239 5.4.2 Theoretical Interpretation of the LRP Redistribution Process 242 5.4.3 Extending LRP to LSTM Networks 248 5.5 Explaining a Visual Question Answering Model 251 5.6 Discussion 258 6 Stochastic Feedforward Neural Networks: Universal Approxima- tion Thomas Merkh and Guido MontÃºfar 267 6.1 Introduction 268 6.2 Overview of Previous Works and Results 271 6.3 Markov Kernels and Stochastic Networks 273 6.3.1 Binary Probability Distributions and Markov Kernels 273 6.3.2 Stochastic Feedforward Networks 274 6.4 Results for Shallow Networks 276 6.4.1 Fixed Weights in the Output Layer 277 6.4.2 Trainable Weights in the Output Layer 278 6.5 Proofs for Shallow Networks 278 6.5.1 Fixed Weights in the Output Layer 279 6.5.2 Trainable Weights in the Second Layer 283 6.5.3 Discussion of the Proofs for Shallow Networks 285 6.6 Results for Deep Networks 286 6.6.1 Parameter Count 288 6.6.2 Approximation with Finite Weights and Biases 288 Contents ix 6.7 Proofs for Deep Networks 289 6.7.1 Notation 289 6.7.2 Probability Mass Sharing 290 6.7.3 Universal Approximation 293 6.7.4 Error Analysis for Finite Weights and Biases 296 6.7.5 Discussion of the Proofs for Deep Networks 298 6.8 Lower Bounds for Shallow and Deep Networks 299 6.8.1 Parameter Counting Lower Bounds 299 6.8.2 Minimum Width 301 6.9 A Numerical Example 302 6.10 Conclusion 306 6.11 Open Problems 307 7 Deep Learning as Sparsity-Enforcing Algorithms A. Aberdam and J. Sulam 314 7.1 Introduction 314 7.2 Related Work 316 7.3 Background 317 7.4 Multilayer Sparse Coding 320 7.4.1 MLâ€“SC Pursuit and the Forward Pass 321 7.4.2 MLâ€“SC: A Projection Approach 323 7.5 The Holistic Way 324 7.6 Multilayer Iterative Shrinkage Algorithms 327 7.6.1 Towards Principled Recurrent Neural Networks 329 7.7 Final Remarks and Outlook 332 8 The Scattering Transform Joan Bruna 338 8.1 Introduction 338 8.2 Geometric Stability 339 8.2.1 Euclidean Geometric Stability 340 8.2.2 Representations with Euclidean Geometric Stability 341 8.2.3 Non-Euclidean Geometric Stability 342 8.2.4 Examples 343 8.3 Scattering on the Translation Group 346 8.3.1 Windowed Scattering Transform 346 8.3.2 Scattering Metric and Energy Conservation 349 8.3.3 Local Translation Invariance and Lipschitz Continu- ity with Respect to Deformations 351 8.3.4 Algorithms 354 8.3.5 Empirical Analysis of Scattering Properties 357 x Contents 8.3.6 Scattering in Modern Computer Vision 362 8.4 Scattering Representations of Stochastic Processes 363 8.4.1 Expected Scattering 363 8.4.2 Analysis of Stationary Textures with Scattering 367 8.4.3 Multifractal Analysis with Scattering Moments 369 8.5 Non-Euclidean Scattering 371 8.5.1 Joint versus Separable Scattering 372 8.5.2 Scattering on Global Symmetry Groups 372 8.5.3 Graph Scattering 375 8.5.4 Manifold Scattering 383 8.6 Generative Modeling with Scattering 384 8.6.1 Suï¬ƒcient Statistics 384 8.6.2 Microcanonical Scattering Models 385 8.6.3 Gradient Descent Scattering Reconstruction 387 8.6.4 Regularising Inverse Problems with Scattering 389 8.6.5 Texture Synthesis with Microcanonical Scattering 391 8.7 Final Remarks 393 9 Deep Generative Models and Inverse Problems Alexandros G. Dimakis 400 9.1 Introduction 400 9.2 How to Tame High Dimensions 401 9.2.1 Sparsity 401 9.2.2 Conditional Independence 402 9.2.3 Deep Generative Models 403 9.2.4 GANs and VAEs 404 9.2.5 Invertible Generative Models 405 9.2.6 Untrained Generative Models 405 9.3 Linear Inverse Problems Using Deep Generative Models 406 9.3.1 Reconstruction from Gaussian Measurements 407 9.3.2 Optimization Challenges 409 9.3.3 Extending the Range of the Generator 410 9.3.4 Non-Linear Inverse Problems 410 9.3.5 Inverse Problems with Untrained Generative Priors 412 9.4 Supervised Methods for Inverse Problems 414 10 Dynamical Systems and Optimal Control Approach to Deep Learn- ing Weinan E, Jiequn Han, and Qianxiao Li 422 10.1 Introduction 422 10.1.1 The Problem of Supervised Learning 423 Contents xi 10.2 ODE Formulation 424 10.3 Mean-Field Optimal Control and Pontryaginâ€™s Maximum Principle 425 10.3.1 Pontryaginâ€™s Maximum Principle 426 10.4 Method of Successive Approximations 428 10.4.1 Extended Pontryagin Maximum Principle 428 10.4.2 The Basic Method of Successive Approximation 428 10.4.3 Extended Method of Successive Approximation 431 10.4.4 Discrete PMP and Discrete MSA 433 10.5 Future Work 435 11 Bridging Many-Body Quantum Physics and Deep Learning via Tensor Networks Yoav Levine, Or Sharir, Nadav Cohen and Amnon Shashua 439 11.1 Introduction 440 11.2 Preliminaries â€“ Many-Body Quantum Physics 442 11.2.1 The Many-Body Quantum Wave Function 443 11.2.2 Quantum Entanglement Measures 444 11.2.3 Tensor Networks 447 11.3 Quantum Wave Functions and Deep Learning Architectures 450 11.3.1 Convolutional and Recurrent Networks as Wave Functions 450 11.3.2 Tensor Network Representations of Convolutional and Recurrent Networks 453 11.4 Deep Learning Architecture Design via Entanglement Measures 453 11.4.1 Dependencies via Entanglement Measures 454 11.4.2 Quantum-Physics-Inspired Control of Inductive Bias 456 11.5 Power of Deep Learning for Wave Function Representations 460 11.5.1 Entanglement Scaling of Deep Recurrent Networks 461 11.5.2 Entanglement Scaling of Overlapping Convolutional Networks 463 11.6 Discussion 467 Contributors Aviad Aberdam Electrical Engineering Department, Technion â€“ Israel Institute of Technology, Haifa 32000, Israel Leila Arras Department of Artiï¬cial Intelligence, Fraunhofer Heinrich Hertz Institute, Einsteinufer 37, 10587 Berlin, Germany Julius Berner Faculty of Mathematics, University of Vienna, UniversitÃ¤tsring 1, 1010 Wien, Austria Yoshua Bengio University of Montreal, 2900 Edouard Montpetit Blvd, Montreal, QC H3T 1J4, Canada Joan Bruna Courant Institute and Center for Data Science, New York University, 251 Mercer Street, New York, NY 10012, USA Nadav Cohen Department of Computer Science and Applied Mathematics Weiz- mann Institute of Science Rehovot 7610001, Israel Alexandros G. Dimakis Department of Electrical and Computer Engineering, University of Texas, Austin TX 78712, USA Weinan E Department of Mathematics, PrincetonUniversity,Princeton,NJ08544â€“ 1000, USA. Philipp Grohs Faculty of Mathematics and Research Platform Data Science, University of Vienna, Oskar Morgenstern Platz 1, 1090 Wien, Austria Ingo GÃ¼hring Institut fÃ¼r Mathematik, Technische UniversitÃ¤t Berlin, StraÃŸe des 17. Juni 136, 10623 Berlin Benjamin D. Haeï¬€ele Mathematical Institute for Data Science, Department of Biomedical Engineering, Johns Hopkins University, 3400 N. Charles Street, Baltimore MD 21218, USA Jiequn Han Center for Computational Mathematics, Flatiron Institute, 162 5th Ave. New York, NY 10010, USA Leslie Pack Kaelbling Massachusetts Institute of Technology, Computer Science & Artiï¬cial Intelligence Laboratory, 32 Vassar St., Cambridge, MA 02139, USA xiii xiv Contributors Kenji Kawaguchi Massachusetts Institute of Technology, 77 Massachusetts Ave, Cambridge, MA 02139, USA Gitta Kutyniok Mathematisches Institut der UniversitÃ¤t MÃ¼nchen, Theresien- straÃŸe, 80333 MÃ¼nchen, Germany Yoav Levine School of Computer Science and Engineering, The Hebrew Univer- sity of Jerusalem, 91904 Jerusalem, Israel Qianxiao Li Department of Mathematics, National University of Singapore, 10 Lower Kent Ridge Road, Singapore 119076. Thomas Merkh Department of Mathematics and Department of Statistics, UCLA, Los Angeles, CA 90095, USA GrÃ©goire Montavon Institute of Software Engineering and Theoretical Computer Science, TU Berlin, D-10587 Berlin, Germany Guido MontÃºfar Department of Mathematics and Department of Statistics, UCLA, CA 90095, USA and Max Planck Institute for Mathematics in the Sciences, 04103 Leipzig, Germany Klaus-Robert MÃ¼ller Institute of Software Engineering and Theoretical Computer Science, TU Berlin, D-10587 Berlin, Germany Ahmed Osman Department of Artiï¬cial Intelligence, Fraunhofer Heinrich Hertz Institute, Einsteinufer 37, 10587 Berlin, Germany Philipp Petersen Faculty of Mathematics and Research Platform Data Science, University of Vienna, Oskar Morgenstern Platz 1, 1090 Wien, Austria Mones Raslan Institut fÃ¼r Mathematik, Technische UniversitÃ¤t Berlin, StraÃŸe des 17. Juni 136, 10623 Berlin, Germany Wojciech Samek Department of Artiï¬cial Intelligence, Fraunhofer Heinrich Hertz Institute, Einsteinufer 37, 10587 Berlin, Germany Or Sharir School of Computer Science and Engineering, The Hebrew University of Jerusalem, 91904 Jerusalem, Israel Amnon Shashua School of Computer Science and Engineering, The Hebrew Uni- versity of Jerusalem, 91904 Jerusalem, Israel Jeremias Sulam Biomedical Engineering Department & Mathematical Institute for Data Science, Johns Hopkins University, Homewood Campus, Baltimore MD 21218, USA RenÃ© Vidal Mathematical Institute for Data Science and Department of Biomed- ical Engineering, Johns Hopkins University, Clark 302B, 3400 N. Charles Street, Baltimore MD 21218, USA Zhihui Zhu Department of Electrical & Computer Engineering, University of Denver, 2155 E. Wesley Avenue, Denver CO 80208, USA Preface We currently are witnessing the spectacular success of â€œdeep learningâ€ in both science (for example, in astronomy, biology, and medicine) and the public sector, where autonomous vehicles and robots are already present in daily life. However, the development of a rigorous mathematical foundation for deep learning is at an early stage, and most of the related research is still empirically driven. At the same time, methods based on deep neural networks have already shown their impressive potential in mathematical research areas such as imaging sciences, inverse prob- lems, and the numerical analysis of partial diï¬€erential equations, sometimes far outperforming classical mathematical approaches for particular classes of problem. This book provides the ï¬rst comprehensive introduction to the subject, highlighting recent theoretical advances as well as outlining the numerous remaining research challenges. The model of a deep neural network is inspired by the structure of the human brain, with artiï¬cial neurons concatenated and arranged in layers, leading to an (ar- tiï¬cial feed-forward) neural network. Because of the structure of artiï¬cial neurons, the realization of such a neural network, i.e., the function it provides, consists of compositions of aï¬ƒne linear maps and (non-linear) activation functions Ï±: R â†’R. More precisely, the realization of a neural network with L layers, and N0, NL, and Nâ„“, â„“= 1,. . ., L âˆ’1, the number of neurons in the input, output, and â„“th hidden layer, as well as weight matrices and bias vectors, W(â„“) âˆˆRNâ„“Ã—Nâ„“âˆ’1 and b(â„“) âˆˆRNâ„“, respectively, is given by Î¦(x,Î¸) = W(L)Ï(W(Lâˆ’1) Â· Â· Â· Ï(W(1)x + b(1)) + Â· Â· Â· + b(Lâˆ’1)) + b(L), x âˆˆRN0 , with free parameters Î¸ =  (W(â„“), b(â„“)) L â„“=1. Given training data (z(i))m i=1 := ((x(i), y(i)))m i=1, which arise from a function g: RN0 â†’RNL, the parameters are then learned by xv xvi Preface minimizing the empirical risk 1 m m Ã• i=1 L(Î¦(Â·,Î¸), z(i)), with L a suitable loss function. This is commonly achieved by stochastic gradient descent, which is a variant of gradient descent accommodating the obstacle that the number of parameters and training samples is typically in the millions. The performance is then measured by the ï¬t of the trained neural network to a test data set. This leads to three main research directions in the theory of deep learning, namely: (1) expressivity, i.e., studying the error accrued in approximating g by the hypothesis class of deep neural networks; (2) optimization, which studies the algorithmic error using minimization of the empirical risk; and (3) generalization, which aims to understand the out-of-sample error. Expressivity is at present from a theoretical viewpoint the most advanced research direction; a current key question is the impact on the overall performance of various architectural components of neural networks, such as their depth. Optimization has recently seen intriguing new results. However, the main mystery of why stochastic gradient descent converges to good local minima despite the non-convexity of the problem is as yet unraveled. Finally, generalization is the direction that is the least explored so far, and a deep theoretical understanding of, for instance, why highly overparametrized models often do not overï¬t, is still out of reach. These core theoretical directions are complemented by others such as explainability, fairness, robustness, or safety â€“ sometimes summarized as the reliability of deep neural networks. Interestingly, basically the entire ï¬eld of mathematics, ranging from algebraic geometry through to approximation theory and then to stochastics is required to tackle these challenges, which often even demand the development of novel mathematics. And, in fact, at a rapidly increasing rate, mathematicians from all areas are joining the ï¬eld and contributing with their unique expertise. Apart from the development of a mathematical foundation of deep learning, deep learning has also a tremendous impact on mathematical approaches to other areas such as solving inverse problems or partial diï¬€erential equations. In fact, it is fair to say that the area of inverse problems, in particular imaging science, has already undergone a paradigm shift towards deep-learning-based approaches. The area of the numerical analysis of partial diï¬€erential equations has been slower to embrace these novel methodologies, since it was initially not evident what their advantage would be for this ï¬eld. However, by now there exist various results of both a numerical and a theoretical nature showing that deep neural networks are capable of beating the curse of dimensionality while providing highly ï¬‚exible and fast solvers. This observation has led to the fact that this area is also currently being Preface xvii swept by deep-learning-type approaches, requiring the development of a theoretical foundation as well. This book is the ï¬rst monograph in the literature to provide a comprehensive survey of the mathematical aspects of deep learning. Its potential readers could be researchers in the areas of applied mathematics, computer science, and statistics, or a related research area, or they could be graduate students seeking to learn about the mathematics of deep learning. The particular design of this volume ensures that it can serve as both a state-of-the-art reference for researchers as well as a textbook for students. The book contains 11 diverse chapters written by recognized leading experts from all over the world covering a large variety of topics. It does not assume any prior knowledge in the ï¬eld. The chapters are self-contained, covering the most recent research results in the respective topic, and can all be treated independently of the others. A brief summary of each chapter is given next. Chapter 1 provides a comprehensive introduction to the mathematics of deep learning, and serves as a background for the rest of the book. The chapter covers the key research directions within both the mathematical foundations of deep learning and deep learning approaches to solving mathematical problems. It also discusses why there is a great need for a new theory of deep learning, and provides an overview of the main future challenges. Chapter 2 provides a comprehensive introduction to generalization properties of deep learning, emphasizing the speciï¬c phenomena that are special to deep learning models. Towards analyzing the generalization behavior of deep neural networks, the authors then present generalization bounds based on validation datasets and an analysis of generalization errors based on training datasets. Chapter 3 surveys a recent body of work related to the expressivity of model classes of neural networks. The chapter covers results providing approximation rates for diverse function spaces as well as those shedding light on the question of why the depth of a neural network is important. The overview not only focuses on feed-forward neural networks, but also includes convolutional, residual, and recurrent ones. Chapter 4 presents recent advances concerning the algorithmic solution of op- timization problems that arise in the context of deep learning, in the sense of analyzing the optimization landscape of neural network training. A speciï¬c focus is on linear networks trained with a squared loss and without regularization as well as on deep networks with a parallel structure, positively homogeneous network mapping and regularization, and that have been trained with a convex loss. Chapter 5 summarizes recent approaches towards rendering deep-learning-based classiï¬cation decisions interpretable. It ï¬rst discusses the algorithmic and theoret- ical aspects of an approach called Layer-wise Relevance Propagation (LRP). This xviii Preface is a propagation-based method, allowing us to derive explanations of the decisions of a variety of ML models. The authors also demonstrate how this method can be applied to a complex model trained for the task of visual question answering. Chapter 6 introduces stochastic feed-forward neural networks, one prominent example of which is deep belief networks. The authors ï¬rst review existing ex- pressivity results for this class of networks. They then analyze the question of a universal approximation for shallow networks and present a uniï¬ed analysis for several classes of such deep networks. Chapter 7 explores connections between deep learning and sparsity-enforcing algorithms. More precisely, this chapter reviews and builds on previous work on a novel interpretation of deep neural networks from a sparsity viewpoint, namely as pursuit algorithms aiming for sparse representations, provided that the signals belong to a multilayer synthesis sparse model. The authors then present extensions of this conceptual approach and demonstrate the advantage of the resulting algorithms in a speciï¬c supervised learning setting, leading to an improvement of performance while retaining the number of parameters. Chapter 8 provides a comprehensive introduction of the scattering transform. The author presents both mathematical results, showing that geometric stability indeed plays a key role in deep learning representations, and applications to, for instance, computer vision. Also, more general group-invariant feature descriptors in terms of Lie groups and non-Euclidean domains are described. Chapter 9 focuses on the application of deep neural networks to solving inverse problems. The author provides an introduction to the use of generative deep learning models as priors in the regularization of inverse problems. Also, the speciï¬c setting of a compressed sensing problem is studied and both mathematical and numerical results in compressed sensing for deep generative models are presented. Chapter 10 introduces a reformulation of the training process for residual neural networks as well as a corresponding theory. More precisely, the dynamical sys- tems viewpoint regards the back-propagation algorithm as a simple consequence of variational equations in ordinary diï¬€erential equations, whereas the control the- ory viewpoint regards deep learning as one instance of mean-ï¬eld control where all agents share the same control. The authors ï¬nally introduce a new class of algorithms for deep learning as one application of these conceptual viewpoints. Chapter 11 illuminates the connections between tensor networks and convolu- tional neural networks. These are established by relating one of the current goals of the ï¬eld of many-body physics, namely the eï¬ƒcient representation of highly entangled many-particle quantum systems, to the area of deep learning. As one application of this framework, the authors derive a new entanglement-based deep learning design scheme which allows theoretical insight in a wide variety of cus- tomarily used network architectures. 1 The Modern Mathematics of Deep Learning Julius Berner, Philipp Grohs, Gitta Kutyniok and Philipp Petersen Abstract: We describe the new ï¬eld of the mathematical analysis of deep learn- ing. This ï¬eld emerged around a list of research questions that were not answered within the classical framework of learning theory. These questions concern: the outstanding generalization power of overparametrized neural networks, the role of depth in deep architectures, the apparent absence of the curse of dimensionality, a surprisingly successful optimization performance despite the non-convexity of the problem, understanding what features are learned, why deep architectures perform exceptionally well in physical problems, and which ï¬ne aspects of an architec- ture aï¬€ect the behavior of a learning task in which way. We present an overview of modern approaches that yield partial answers to these questions. For selected approaches, we describe the main ideas in more detail. 1.1 Introduction Deep learning has undoubtedly established itself as the outstanding machine learn- ing technique of recent times. This dominant position has been claimed through a series of overwhelming successes in widely diï¬€erent application areas. Perhaps the most famous application of deep learning, and certainly one of the ï¬rst where these techniques became state-of-the-art, is image classiï¬cation (LeCun et al., 1998; Krizhevsky et al., 2012; Szegedy et al., 2015; He et al., 2016). In this area, deep learning is nowadays the only method that is seriously considered. The prowess of deep learning classiï¬ers goes so far that they often outperform humans in image-labelling tasks (He et al., 2015). A second famous application area is the training of deep-learning-based agents to play board games or computer games, such as Atari games (Mnih et al., 2013). In this context, probably the most prominent achievement yet is the development of an algorithm that beat the best human player in the game of Go (Silver et al., 2016, 2017) â€“ a feat that was previously unthinkable owing to the extreme complexity 1 2 Berner et al. The Modern Mathematics of Deep Learning of this game. Moreover, even in multiplayer, team-based games with incomplete information, deep-learning-based agents nowadays outperform world-class human teams (Berner et al., 2019a; Vinyals et al., 2019). In addition to playing games, deep learning has also led to impressive break- throughs in the natural sciences. For example, it is used in the development of drugs (Ma et al., 2015), molecular dynamics (Faber et al., 2017), and in high-energy physics (Baldi et al., 2014). One of the most astounding recent breakthroughs in scientiï¬c applications is the development of a deep-learning-based predictor for the folding behavior of proteins (Senior et al., 2020). This predictor is the ï¬rst method to match the accuracy of lab-based methods. Finally, in the vast ï¬eld of natural language processing, which includes the subtasks of understanding, summarizing, and generating text, impressive advances have been made based on deep learning. Here, we refer to Young et al. (2018) for an overview. One technique that has recently stood out is based on a so-called transformer neural network (Bahdanau et al., 2015; Vaswani et al., 2017). This network structure has given rise to the impressive GPT-3 model (Brown et al., 2020) which not only creates coherent and compelling texts but can also produce code, such as that for the layout of a webpage according to some instructions that a user inputs in plain English. Transformer neural networks have also been successfully employed in the ï¬eld of symbolic mathematics (Saxton et al., 2018; Lample and Charton, 2019). In this chapter, we present and discuss the mathematical foundations of the success story outlined above. More precisely, our goal is to outline the newly emerging ï¬eld of the mathematical analysis of deep learning. To accurately describe this ï¬eld, a necessary preparatory step is to sharpen our deï¬nition of the term deep learning. For the purposes of this chapter, we will use the term in the following narrow sense: deep learning refers to techniques where deep neural networks1 are trained with gradient-based methods. This narrow deï¬nition helps to make this chapter more concise. We would like to stress, however, that we do not claim in any way that this is the best or the right deï¬nition of deep learning. Having ï¬xed a deï¬nition of deep learning, three questions arise concerning the aforementioned emerging ï¬eld of mathematical analysis of deep learning. To what extent is a mathematical theory necessary? Is it truly a new ï¬eld? What are the questions studied in this area? Let us start by explaining the necessity of a theoretical analysis of the tools de- scribed above. From a scientiï¬c perspective, the primary reason why deep learning should be studied mathematically is simple curiosity. As we will see throughout this chapter, many practically observed phenomena in this context are not explained 1 We will deï¬ne the term neural network later but, for now, we can view it as a parametrized family of functions with a diï¬€erentiable parametrization. 1.1 Introduction 3 theoretically. Moreover, theoretical insights and the development of a comprehen- sive theory often constitute the driving force underlying the development of new and improved methods. Prominent examples of mathematical theories with such an eï¬€ect are the theory of ï¬‚uid mechanics which is fundamental ingredient of the design of aircraft or cars, and the theory of information which aï¬€ects and shapes all modern digital communication. In the words of Vladimir Vapnik2: â€œNothing is more practical than a good theory,â€ (Vapnik, 2013, Preface). In addition to being interesting and practical, theoretical insight may also be necessary. Indeed, in many applications of machine learning, such as medical diagnosis, self-driving cars, and robotics, a signiï¬cant level of control and predictability of deep learning methods is mandatory. Also, in services such as banking or insurance, the technology should be controllable in order to guarantee fair and explainable decisions. Let us next address the claim that the ï¬eld of mathematical analysis of deep learn- ing is a newly emerging area. In fact, under the aforementioned deï¬nition of deep learning, there are two main ingredients of the technology: deep neural networks and gradient-based optimization. The ï¬rst artiï¬cial neuron was already introduced in McCulloch and Pitts (1943). This neuron was not trained but instead used to ex- plain a biological neuron. The ï¬rst multi-layered network of such artiï¬cial neurons that was also trained can be found in Rosenblatt (1958). Since then, various neural network architectures have been developed. We will discuss these architectures in detail in the following sections. The second ingredient, gradient-based optimiza- tion, is made possible by the observation that, owing to the graph-based structure of neural networks, the gradient of an objective function with respect to the param- eters of the neural network can be computed eï¬ƒciently. This has been observed in various ways: see Kelley (1960); Dreyfus (1962); Linnainmaa (1970); Rumelhart et al. (1986). Again, these techniques will be discussed in the upcoming sections. Since then, techniques have been improved and extended. As the rest of the chapter is spent reviewing these methods, we will keep the discussion of literature brief at this point. Instead, we refer to some overviews of the history of deep learning from various perspectives: LeCun et al. (2015); Schmidhuber (2015); Goodfellow et al. (2016); Higham and Higham (2019). Given the fact that the two main ingredients of deep neural networks have been around for a long time, one might expect that a comprehensive mathematical the- ory would have been developed that describes why and when deep-learning-based methods will perform well or when they will fail. Statistical learning theory (An- thony and Bartlett, 1999; Vapnik, 1999; Cucker and Smale, 2002; Bousquet et al., 2003; Vapnik, 2013) describes multiple aspects of the performance of general learning methods and in particular deep learning. We will review this theory in the 2 This claim can be found earlier in a non-mathematical context in the works of Kurt Lewin (1943). 4 Berner et al. The Modern Mathematics of Deep Learning context of deep learning in Â§1.1.2 below. Here, we focus on the classical, deep- learning-related results that we consider to be well known in the machine learning community. Nonetheless, the choice of these results is guaranteed to be subjective. We will ï¬nd that this classical theory is too general to explain the performance of deep learning adequately. In this context, we will identify the following questions that appear to be diï¬ƒcult to answer within the classical framework of learning the- ory: Why do trained deep neural networks not overï¬t on the training data despite the enormous power of the architecture? What is the advantage of deep compared to shallow architectures? Why do these methods seemingly not suï¬€er from the curse of dimensionality? Why does the optimization routine often succeed in ï¬nding good solutions despite the non-convexity, nonlinearity, and often non-smoothness of the problem? Which aspects of an architecture aï¬€ect the performance of the associated models and how? Which features of data are learned by deep architectures? Why do these methods perform as well as or better than specialized numerical tools in the natural sciences? The new ï¬eld of the mathematical analysis of deep learning has emerged around questions like those listed above. In the remainder of this chapter, we will collect some of the main recent advances towards answering these questions. Because this ï¬eld of the mathematical analysis of deep learning is incredibly active and new material is added at breathtaking speed, a brief survey of recent advances in this area is guaranteed to miss not only a couple of references but also many of the most essential ones. Therefore we do not strive for a complete overview but, instead, showcase several fundamental ideas on a mostly intuitive level. In this way, we hope to allow readers to familiarize themselves with some exciting concepts and provide a convenient entry-point for further studies. 1.1.1 Notation We denote by N the set of natural numbers, by Z the set of integers, and by R the ï¬eld of real numbers. For N âˆˆN, we denote by [N] the set {1,. . ., N}. For two functions f,g: X â†’[0,âˆ), we write f â‰²g if there exists a universal constant c such that f (x) â‰¤cg(x) for all x âˆˆX. In a pseudometric space (X, dX), we deï¬ne the ball of radius r âˆˆ(0,âˆ) around a point x âˆˆX by BdX r (x), or Br(x) if the pseudometric dX is clear from the context. By âˆ¥Â· âˆ¥p, p âˆˆ[1,âˆ], we denote the â„“p-norm, and by âŸ¨Â·,Â·âŸ©the Euclidean inner product of given vectors. By âˆ¥Â· âˆ¥op we denote the operator norm induced by the Euclidean norm and by âˆ¥Â· âˆ¥F the Frobenius norm of given matrices. For p âˆˆ[1,âˆ], s âˆˆ[0,âˆ), d âˆˆN, and X âŠ‚Rd, we denote by Ws,p(X) the Sobolevâ€“Slobodeckij space, which for s = 0 is just a Lebesgue space, i.e., W0,p(X) = Lp(X). For measurable spaces X and Y, we deï¬ne M(X,Y) to be the set of measurable functions from X to Y. We denote by Ë†g the 1.1 Introduction 5 Fourier transform3 of a tempered distribution g. For probabilistic statements, we will assume a suitable underlying probability space with probability measure I. For an X-valued random variable X, we denote by E[X] and V[X] its expectation and variance and by IX the image measure of X on X, i.e., IX(A) = I(X âˆˆA) for every measurable set A âŠ‚X. If possible, we use the corresponding lowercase letter to denote the realization x âˆˆX of the random variable X for a given outcome. We write Id for the d-dimensional identity matrix and, for a set A, we write 1A for the indicator function of A, i.e., 1A(x) = 1 if x âˆˆA and 1A(x) = 0 otherwise. 1.1.2 Foundations of Learning Theory Before we describe recent developments in the mathematical analysis of deep learn- ing methods, we will start by providing a concise overview of the classical math- ematical and statistical theory underlying machine learning tasks and algorithms that, in their most general form, can be formulated as follows. Deï¬nition 1.1 (Learning â€“ informal). Let X, Y, and Z be measurable spaces. In a learning task, one is given data in Z and a loss function L : M(X,Y) Ã— Z â†’R. The goal is to choose a hypothesis set F âŠ‚M(X,Y) and to construct a learning algorithm, i.e., a mapping A : Ã˜ mâˆˆN Zm â†’F, that uses training data s = (z(i))m i=1 âˆˆZm to ï¬nd a model fs = A(s) âˆˆF that performs well on the training data s and also generalizes to unseen data z âˆˆZ. Here, performance is measured via the loss function L and the corresponding loss L( fs, z) and, informally speaking, generalization means that the out-of-sample performance of fs at z behaves similarly to the in-sample performance on s. Deï¬nition 1.1 is deliberately vague on how to measure generalization perfor- mance. Later, we will often study the expected out-of-sample performance. To talk about expected performance, a data distribution needs to be speciï¬ed. We will revisit this point in Assumption 1.10 and Deï¬nition 1.11. For simplicity, we focus on one-dimensional supervised prediction tasks with input features in Euclidean space, as deï¬ned in the following. Deï¬nition 1.2 (Prediction task). In a prediction task, we have that Z B XÃ—Y, i.e., we are given training data s = ((x(i), y(i)))m i=1 that consist of input features x(i) âˆˆX and corresponding labels y(i) âˆˆY. For one-dimensional regression tasks with Y âŠ‚R, we consider the quadratic loss L( f,(x, y)) = ( f (x) âˆ’y)2 and, for binary 3 Respecting common notation, we will also use the hat symbol to denote the minimizer of the empirical risk bfs in Deï¬nition 1.8 but this clash of notation does not involve any ambiguity. 6 Berner et al. The Modern Mathematics of Deep Learning classiï¬cation tasks with Y = {âˆ’1,1}, we consider the 0â€“1 loss L( f,(x, y)) = 1(âˆ’âˆ,0)(y f (x)). We assume that our input features are in Euclidean space, i.e., X âŠ‚Rd with input dimension d âˆˆN. In a prediction task, we aim for a model fs : X â†’Y, such that, for unseen pairs (x, y) âˆˆX Ã— Y, fs(x) is a good prediction of the true label y. However, note that large parts of the presented theory can be applied to more general settings. Remark 1.3 (Learning tasks). Apart from straightforward extensions to multi- dimensional prediction tasks and other loss functions, we want to mention that unsupervised and semi-supervised learning tasks are often treated as prediction tasks. More precisely, one transforms unlabeled training data z(i) into features x(i) = T1(z(i)) âˆˆX and labels y(i) = T2(z(i)) âˆˆY using suitable transformations T1 : Z â†’X, T2 : Z â†’Y. In doing so, one asks for a model fs approximating the transformation T2 â—¦Tâˆ’1 1 : X â†’Y which is, for example, made in order to learn feature representations or invariances. Furthermore, one can consider density estimation tasks, where X = Z, Y B [0,âˆ], and F consists of probability densities with respect to some Ïƒ-ï¬nite ref- erence measure Âµ on Z. One then aims for a probability density fs that approxi- mates the density of the unseen data z with respect to Âµ. One can perform L2(Âµ)- approximation based on the discretization L( f, z) = âˆ’2 f (z)+âˆ¥f âˆ¥2 L2(Âµ) or maximum likelihood estimation based on the surprisal L( f, z) = âˆ’log( f (z)). In deep learning the hypothesis set F consists of realizations of neural networks Î¦a(Â·,Î¸), Î¸ âˆˆP, with a given architecture a and parameter set P. In practice, one uses the term neural network for a range of functions that can be represented by directed acyclic graphs, where the vertices correspond to elementary almost every- where diï¬€erentiable functions parametrizable by Î¸ âˆˆP and the edges symbolize compositions of these functions. In Â§1.6, we will review some frequently used ar- chitectures; in the other sections, however, we will mostly focus on fully connected feed-forward (FC) neural networks as deï¬ned below. Deï¬nition 1.4 (FC neural network). A fully connected feed-forward neural network is given by its architecture a = (N, Ï±), where L âˆˆN, N âˆˆNL+1, and Ï±: R â†’R. We refer to Ï± as the activation function, to L as the number of layers, and to N0, NL, and Nâ„“, â„“âˆˆ[L âˆ’1], as the number of neurons in the input, output, and â„“th hidden layer, respectively. We denote the number of parameters by P(N) B L Ã• â„“=1 Nâ„“Nâ„“âˆ’1 + Nâ„“ and deï¬ne the corresponding realization function Î¦a : RN0 Ã—RP(N) â†’RNL, which 1.1 Introduction 7 x1 x2 x3 Î¦(1) 1 Î¦(1) 2 Î¦(1) 3 Î¦(1) 4 x 7â†’W(1)x + b(1) Â¯Î¦(1) 1 Â¯Î¦(1) 2 Â¯Î¦(1) 3 Â¯Î¦(1) 4 Ï± Î¦(2) 1 Î¦(2) 2 Î¦(2) 3 Î¦(2) 4 Î¦(2) 5 Î¦(2) 6 x 7â†’W(2)x + b(2) Â¯Î¦(2) 1 Â¯Î¦(2) 2 Â¯Î¦(2) 3 Â¯Î¦(2) 4 Â¯Î¦(2) 5 Â¯Î¦(2) 6 Ï± Î¦a x 7â†’W(3)x + b(3) Figure 1.1 Graph (pale gray) and (pre-)activations of the neurons (white) of a deep fully con- nected feed-forward neural network Î¦a : R3 Ã— R53 7â†’R with architecture a = ((3, 4, 6, 1), Ï±) and parameters Î¸ = ((W (â„“), b(â„“))3 â„“=1. satisï¬es, for every input x âˆˆRN0 and parameters Î¸ = (Î¸(â„“))L â„“=1 = ((W(â„“), b(â„“)))L â„“=1 âˆˆ L ? â„“=1 (RNâ„“Ã—Nâ„“âˆ’1 Ã— RNâ„“)  RP(N) , that Î¦a(x,Î¸) = Î¦(L)(x,Î¸), where Î¦(1)(x,Î¸) = W(1)x + b(1), Â¯Î¦(â„“)(x,Î¸) = Ï± Î¦(â„“)(x,Î¸), â„“âˆˆ[L âˆ’1], and Î¦(â„“+1)(x,Î¸) = W(â„“+1) Â¯Î¦(â„“)(x,Î¸) + b(â„“+1), â„“âˆˆ[L âˆ’1], (1.1) and Ï± is applied componentwise. We refer to W(â„“) âˆˆRNâ„“Ã—Nâ„“âˆ’1 and b(â„“) âˆˆRNâ„“ as the weight matrices and bias vectors, and to Â¯Î¦(â„“) and Î¦(â„“) as the activations and pre-activations of the Nâ„“neurons in the â„“th layer. The width and depth of the architecture are given by âˆ¥Nâˆ¥âˆand L and we call the architecture deep if L > 2 and shallow if L = 2. The underlying directed acyclic graph of FC networks is given by compositions of the aï¬ƒne linear maps x 7â†’W(â„“)x + b(â„“), â„“âˆˆ[L], with the activation function Ï± intertwined; see Figure 1.1. Typical activation functions used in practice are variants of the rectiï¬ed linear unit (ReLU) given by Ï±R(x) B max{0, x} and sigmoidal functions Ï± âˆˆC(R) satisfying Ï±(x) â†’1 for x â†’âˆand Ï±(x) â†’0 for x â†’âˆ’âˆ, such as the logistic function Ï±Ïƒ(x) B 1/(1 + eâˆ’x) (often referred to as the sigmoid function). See also Table 1.1 for a comprehensive list of widely used activation functions. 8 Berner et al. The Modern Mathematics of Deep Learning Name Given as a function Plot of x âˆˆR by linear x Heaviside / step function 1(0,âˆ)(x) logistic / sigmoid 1 1+eâˆ’x rectiï¬ed linear unit (ReLU) max{0, x} power rectiï¬ed linear unit max{0, x}k for k âˆˆN parametric ReLU (PReLU) max{ax, x} for a â‰¥0, a , 1 exponential linear unit (ELU) xÂ·1[0,âˆ)(x)+ (ex âˆ’1)Â·1(âˆ’âˆ,0)(x) softsign x 1+|x| inverse square root linear unit xÂ·1[0,âˆ)(x)+ x âˆš 1+ax2 Â·1(âˆ’âˆ,0)(x) for a > 0 inverse square root unit x âˆš 1+ax2 for a > 0 tanh exâˆ’eâˆ’x ex+eâˆ’x arctan arctan(x) softplus ln(1 + ex) Gaussian eâˆ’x2/2 Table 1.1 List of commonly used activation functions. 1.1 Introduction 9 Remark 1.5 (Neural networks). If not further speciï¬ed, we will use the term (neural) network, or the abbreviation NN, to refer to FC neural networks. Note that many of the architectures used in practice (see Â§1.6) can be written as special cases of Deï¬nition 1.4 where, for example, speciï¬c parameters are prescribed by constants or shared with other parameters. Furthermore, note that aï¬ƒne linear functions are NNs with depth L = 1. We will also consider biasless NNs given by linear mappings without bias vector, i.e., b(â„“) = 0, â„“âˆˆ[L]. In particular, any NN can always be written without bias vectors by redeï¬ning x â†’ x 1  ; (W(â„“), b(â„“)) â†’ W(â„“) b(â„“) 0 1  ; â„“âˆˆ[L âˆ’1]; and (W(L), b(L)) â†’  W(L) b(L) . To enhance readability we will often not specify the underlying architecture a = (N, Ï±) or the parameters Î¸ âˆˆRP(N) but use the term NN to refer to the architecture as well as the realization functions Î¦a(Â·,Î¸): RN0 â†’RNL or Î¦a : RN0Ã—RP(N) â†’RNL. However, we want to emphasize that one cannot infer the underlying architecture or properties such as the magnitude of parameters solely from these functions, as the mapping (a,Î¸) 7â†’Î¦a(Â·,Î¸) is highly non-injective. As an example, we can set W(L) = 0, which implies Î¦a(Â·,Î¸) = b(L) for all architectures a = (N, Ï±) and all values of (W(â„“), b(â„“))Lâˆ’1 â„“=1 . In view of our considered prediction tasks in Deï¬nition 1.2, this naturally leads to the following hypothesis sets of neural networks. Deï¬nition 1.6 (Hypothesis sets of neural networks). Let a = (N, Ï±) be a NN archi- tecture with input dimension N0 = d, output dimension NL = 1, and measurable activation function Ï±. For regression tasks the corresponding hypothesis set is given by Fa =  Î¦a(Â·,Î¸): Î¸ âˆˆRP(N) and for classiï¬cation tasks by Fa,sgn =  sgn(Î¦a(Â·,Î¸)): Î¸ âˆˆRP(N) , where sgn(x) B ( 1, if x â‰¥0, âˆ’1, if x < 0. Note that we compose the output of the NN with the sign function in order to obtain functions mapping to Y = {âˆ’1,1}. This can be generalized to multi- dimensional classiï¬cation tasks by replacing the sign by an argmax function. Given a hypothesis set, a popular learning algorithm is empirical risk minimization (ERM), which minimizes the average loss on the given training data, as described in the next two deï¬nitions. 10 Berner et al. The Modern Mathematics of Deep Learning Deï¬nition 1.7 (Empirical risk). For training data s = (z(i))m i=1 âˆˆZm and a function f âˆˆM(X,Y), we deï¬ne the empirical risk by bRs( f ) B 1 m m Ã• i=1 L( f, z(i)). (1.2) Deï¬nition 1.8 (ERM learning algorithm). Given a hypothesis set F , an empirical risk-minimization algorithm Aerm chooses4 for training data s âˆˆZm a minimizer bfs âˆˆF of the empirical risk in F , i.e., Aerm(s) âˆˆargmin f âˆˆF bRs( f ). (1.3) Remark 1.9 (Surrogate loss and regularization). Note that, for classiï¬cation tasks, one needs to optimize over non-diï¬€erentiable functions with discrete outputs in (1.3). For an NN hypothesis set Fa,sgn one typically uses the corresponding hypothesis set for regression tasks Fa to ï¬nd an approximate minimizer bf surr s âˆˆFa of 1 m m Ã• i=1 Lsurr( f, z(i)), where Lsurr : M(X,R) Ã— Z â†’R is a surrogate loss guaranteeing that sgn( bf surr s ) âˆˆ argminf âˆˆFa,sgn bRs( f ). A frequently used surrogate loss is the logistic loss,5 given by Lsurr( f, z) = log  1 + eâˆ’yf (x) . In various learning tasks one also adds regularization terms to the minimization problem in (1.3), such as penalties on the norm of the parameters of the NN, i.e., min Î¸ âˆˆRP(N) bRs(Î¦a(Â·,Î¸)) + Î±âˆ¥Î¸âˆ¥2 2, where Î± âˆˆ(0,âˆ) is a regularization parameter. Note that in this case the minimizer depends on the chosen parameters Î¸ and not only on the realization function Î¦a(Â·,Î¸); see also Remark 1.5. Coming back to our initial, informal description of learning in Deï¬nition 1.1, we have now outlined potential learning tasks in Deï¬nition 1.2, NN hypothesis sets in Deï¬nition 1.6, a metric for the in-sample performance in Deï¬nition 1.7, and a 4 For simplicity, we assume that the minimum is attained; this is the case, for instance, if F is a compact topological space on which bRs is continuous. Hypothesis sets of NNs F(N ,Ï±) constitute a compact space if, for example, one chooses a compact parameter set P âŠ‚RP(N) and a continuous activation function Ï±. One could also work with approximate minimizers: see Anthony and Bartlett (1999). 5 This can be viewed as cross-entropy between the label y and the output of f composed with a logistic function Ï±Ïƒ. In a multi-dimensional setting one can replace the logistic function with a softmax function. 1.1 Introduction 11 corresponding learning algorithm in Deï¬nition 1.8. However, we are still lacking a mathematical concept to describe the out-of-sample (generalization) performance of our learning algorithm. This question has been intensively studied in the ï¬eld of statistical learning theory; see Â§1.1 for various references. In this ï¬eld one usually establishes a connection between the unseen data z and the training data s = (z(i))m i=1 by imposing that z and z(i), i âˆˆ[m], are realizations of independent samples drawn from the same distribution. Assumption 1.10 (Independent and identically distributed data). We assume that z(1),. . ., z(m), z are realizations of i.i.d. random variables Z(1),. . ., Z(m), Z. In this formal setting, we can compute the average out-of-sample performance of a model. Recall from our notation in Â§1.1.1 that we denote by IZ the image measure of Z on Z, which is the underlying distribution of our training data S = (Z(i))m i=1 âˆ¼Im Z and unknown data Z âˆ¼IZ. Deï¬nition 1.11 (Risk). For a function f âˆˆM(X,Y), we deï¬ne6 the risk by R( f ) B E  L( f, Z)  = âˆ« Z L( f, z) dIZ(z). (1.4) Deï¬ning S B (Z(i))m i=1, the risk of a model fS = A(S) is thus given by R( fS) = E  L( fS, Z)|S  . For prediction tasks, we can write Z = (X,Y) such that the input features and labels are given by an X-valued random variable X and a Y-valued random variable Y, respectively. Note that for classiï¬cation tasks the risk equals the probability of misclassiï¬cation R( f ) = E[1(âˆ’âˆ,0)(Y f (X))] = I[ f (X) , Y]. For noisy data, there might be a positive lower bound on the risk, i.e., an irre- ducible error. If the lower bound on the risk is attained, one can also deï¬ne the notion of an optimal solution to a learning task. Deï¬nition 1.12 (Bayes-optimal function). A function f âˆ—âˆˆM(X,Y) achieving the smallest risk, the so-called Bayes risk Râˆ—B inf f âˆˆM(X,Y) R( f ), is called a Bayes-optimal function. 6 Note that this requires z 7â†’L(f , z) to be measurable for every f âˆˆM(X, Y), which is the case for our considered prediction tasks. 12 Berner et al. The Modern Mathematics of Deep Learning For the prediction tasks in Deï¬nition 1.2, we can represent the risk of a func- tion with respect to the Bayes risk and compute the Bayes-optimal function; see, e.g., Cucker and Zhou (2007, Propositions 1.8 and 9.3). Lemma 1.13 (Regression and classiï¬cation risk). For a regression task with V[Y] < âˆ, the risk can be decomposed as follows: R( f ) = E  ( f (X) âˆ’E[Y|X])2 + Râˆ—, f âˆˆM(X,Y), (1.5) which is minimized by the regression function f âˆ—(x) = E[Y|X = x]. For a classiï¬- cation task, the risk can be decomposed as R( f ) = E  |E[Y|X]|1(âˆ’âˆ,0)(E[Y|X] f (X))  + Râˆ—, f âˆˆM(X,Y), (1.6) which is minimized by the Bayes classiï¬er f âˆ—(x) = sgn(E[Y|X = x]). As our model fS depends on the random training data S, the risk R( fS) is a random variable and we might aim7 for R( fS) to be small with high probability or in expectation over the training data. The challenge for the learning algorithm A is to minimize the risk by using only training data, without knowing the underlying distribution. One can even show that for every learning algorithm there exists a distribution where convergence of the expected risk of fS to the Bayes risk is arbitrarily slow with respect to the number of samples m (Devroye et al., 1996, Theorem 7.2). Theorem 1.14 (No free lunch). Let am âˆˆ(0,âˆ), m âˆˆN, be a monotonically decreasing sequence with a1 â‰¤1/16. Then for every learning algorithm A of a classiï¬cation task there exists a distribution IZ such that for every m âˆˆN and training data S âˆ¼Im Z it holds true that E  R(A(S))  â‰¥Râˆ—+ am. Theorem 1.14 shows the non-existence of a universal learning algorithm for every data distribution IZ and shows that useful bounds must necessarily be accompanied by a priori regularity conditions on the underlying distribution IZ. Such prior knowledge can then be incorporated into the choice of the hypothesis set F . To illustrate this, let f âˆ— F âˆˆargminf âˆˆF R( f ) be a best approximation in F , such that we can bound the error R( fS) âˆ’Râˆ— = R( fS) âˆ’bRS( fS) + bRS( fS) âˆ’bRS( f âˆ— F) + bRS( f âˆ— F) âˆ’R( f âˆ— F) + R( f âˆ— F) âˆ’Râˆ— â‰¤Îµopt + 2Îµgen + Îµapprox (1.7) 7 In order to make probabilistic statements on R(fS) we assume that R(fS) is a random variable, i.e., measurable. This is, for example, the case if F constitutes a measurable space and s 7â†’A(s) and f â†’R |F are measurable. 1.1 Introduction 13 Figure 1.2 Illustration of the errors (A)â€“(C) in the decomposition (1.7). It shows the exemplary risk bR (blue) and the empirical risk bRs (red) with respect to the projected space of measurable functions M(X, Y). Note that the empirical risk and thus Îµgen and Îµopt depend on the realization s = (z(i))m i=1 of the training data S âˆ¼Im Z . by (A) an optimization error Îµopt B bRS( fS) âˆ’bRS( bfS) â‰¥bRS( fS) âˆ’bRS( f âˆ— F), with bfS as in Deï¬nition 1.8, (B) a (uniform8) generalization error Îµgen B sup f âˆˆF |R( f ) âˆ’bRS( f )| â‰¥max{R( fS) âˆ’bRS( fS), bRS( f âˆ— F) âˆ’R( f âˆ— F)}, and (C) an approximation error Îµapprox B R( f âˆ— F) âˆ’Râˆ—, see also Figure 1.2. The approximation error decreases when the hypothesis set is enlarged, but taking F = M(X,Y) prevents control of the generalization error; see also Theorem 1.14. This suggests a sweet-spot for the complexity of our hypothesis set F and is usually referred to as the biasâ€“variance trade-oï¬€; see also Figure 1.4 below. In the next sections, we will sketch mathematical ideas to tackle each of the errors in (A)â€“(C) in the context of deep learning. Observe that we bound the generalization and optimization errors with respect to the empirical risk bRS and its minimizer bfS, motivated by the fact that in deep-learning-based applications one typically tries to minimize variants of bRS. Optimization The ï¬rst error in the decomposition of (1.7) is the optimization error: Îµopt. This error is primarily inï¬‚uenced by the numerical algorithm A that is used to ï¬nd the model fs in a hypothesis set of NNs for given training data s âˆˆZm. We will focus on the typical setting, where such an algorithm tries to approximately minimize 8 Although this uniform deviation can be a coarse estimate it is frequently used in order to allow for the application of uniform laws of large numbers from the theory of empirical processes. 14 Berner et al. The Modern Mathematics of Deep Learning the empirical risk bRs. While there are many conceivable methods to solve this minimization problem, by far the most common are gradient-based methods. The main reason for the popularity of gradient-based methods is that for FC networks as in Deï¬nition 1.4, the accurate and eï¬ƒcient computation of pointwise derivatives âˆ‡Î¸Î¦a(x,Î¸) is possible by means of automatic diï¬€erentiation, a speciï¬c form of which is often referred to as the backpropagation algorithm (Kelley, 1960; Dreyfus, 1962; Linnainmaa, 1970; Rumelhart et al., 1986; Griewank and Walther, 2008). This numerical scheme is also applicable in general settings, such as those where the architecture of the NN is given by a general directed acyclic graph. Using these pointwise derivatives, one usually attempts to minimize the empirical risk bRs by updating the parameters Î¸ according to a variant of stochastic gradient descent (SGD), which we shall review below in a general formulation. Algorithm 1.1 Stochastic gradient descent Input: Diï¬€erentiable function r : Rp â†’R, sequence of step sizes Î·k âˆˆ(0,âˆ), k âˆˆ[K], Rp-valued random variable Î˜(0). Output: Sequence of Rp-valued random variables (Î˜(k))K k=1. for k = 1,. . .,K do Let D(k) be a random variable such that E[D(k)|Î˜(kâˆ’1)] = âˆ‡r(Î˜(kâˆ’1)) Set Î˜(k) B Î˜(kâˆ’1) âˆ’Î·kD(k) end for If D(k) is chosen deterministically in Algorithm 1.1, i.e., D(k) = âˆ‡r(Î˜(kâˆ’1)), then the algorithm is known as gradient descent. To minimize the empirical loss, we apply SGD with r : RP(N) â†’R set to r(Î¸) = bRs(Î¦a(Â·,Î¸)). More concretely, one might choose a batch-size mâ€² âˆˆN with mâ€² â‰¤m and consider the iteration Î˜(k) B Î˜(kâˆ’1) âˆ’Î·k mâ€² Ã• zâˆˆSâ€² âˆ‡Î¸L(Î¦a(Â·,Î˜(kâˆ’1)), z), (1.8) where Sâ€² is a so-called mini-batch of size |Sâ€²| = mâ€² chosen uniformly9 at random from the training data s. The sequence of step sizes (Î·k)kâˆˆN is often called the learning rate in this context. Stopping at step K, the output of a deep learning algorithm A is then given by fs = A(s) = Î¦a(Â·, Â¯Î¸), 9 We remark that in practice one typically picks Sâ€² by selecting a subset of training data in such a way to cover the full training data after one epoch of âŒˆm/mâ€²âŒ‰many steps. This, however, does not necessarily yield an unbiased estimator D(k) of âˆ‡Î¸r(Î˜(kâˆ’1)) given Î˜(kâˆ’1). 1.1 Introduction 15 where Â¯Î¸ can be chosen to be the realization of the last parameter Î˜(K) of (1.8) or a convex combination of (Î˜(k))K k=1 such as the mean. Algorithm 1.1 was originally introduced in Robbins and Monro (1951) in the context of ï¬nding the root of a nondecreasing function from noisy measurements. Shortly afterwards this idea was applied to ï¬nd the unique global minimum of a Lipschitz-regular function that has no ï¬‚at regions away from the minimum (Kiefer and Wolfowitz, 1952). In some regimes, we can guarantee the convergence of SGD at least in expecta- tion. See Nemirovsky and Yudin (1983), Nemirovski et al. (2009), Shalev-Shwartz et al. (2009), Shapiro et al. (2014, Section 5.9), Shalev-Shwartz and Ben-David (2014, Chapter 14). One prototypical convergence guarantee that is found in the aforementioned references in various forms is stated below. Theorem 1.15 (Convergence of SGD). Let p,K âˆˆN and let r : Rp âŠƒB1(0) â†’R be diï¬€erentiable and convex. Further, let (Î˜(k))K k=1 be the output of Algorithm 1.1 with initialization Î˜(0) = 0, step sizes Î·k = Kâˆ’1/2, k âˆˆ[K], and random variables (D(k))K k=1 satisfying âˆ¥D(k)âˆ¥2 â‰¤1 almost surely for all k âˆˆ[K]. Then E[r( Â¯Î˜)] âˆ’r(Î¸âˆ—) â‰¤ 1 âˆš K , where Â¯Î˜ B 1 K ÃK k=1 Î˜(k) and Î¸âˆ—âˆˆargminÎ¸ âˆˆB1(0) r(Î¸). Theorem 1.15 can be strengthened to yield a faster convergence rate if the convexity is replaced by strict convexity. If r is not convex then convergence to a global minimum cannot in general be guaranteed. In fact, in that case, stochastic gradient descent may converge to a local, non-global minimum; see Figure 1.3 for an example. Moreover, gradient descent, i.e., the deterministic version of Algorithm 1.1, will stop progressing if at any point the gradient of r vanishes. This is the case in every stationary point of r. A stationary point is either a local minimum, a local maximum, or a saddle point. One would expect that if the direction of the step D(k) in Algorithm 1.1 is not deterministic then random ï¬‚uctuations may allow the iterates to escape saddle points. Indeed, results guaranteeing convergence to local minima exist under various conditions on the type of saddle points that r admits (Nemirovski et al., 2009; Ghadimi and Lan, 2013; Ge et al., 2015; Lee et al., 2016; Jentzen et al., 2020). In addition, many methods that improve convergence by, for example, introducing more elaborate step-size rules or a momentum term have been established. We shall not review these methods here, but instead refer to Goodfellow et al. (2016, Chapter 8) for an overview. 16 Berner et al. The Modern Mathematics of Deep Learning Figure 1.3 Examples of the dynamics of gradient descent (four panels on the left) and stochastic gradient descent (four panels on the right) for an objective function with one non-global minimum next to the global minimum. We see that depending on the initial condition and also on ï¬‚uctuations in the stochastic part of SGD the algorithm can fail or succeed in ï¬nding the global minimum. Approximation Generally speaking, NNs, even FC NNs (see Deï¬nition 1.4) with only L = 2 layers, are universal approximators, meaning that under weak conditions on the activation function Ï± they can approximate any continuous function on a compact set up to arbitrary precision (Cybenko, 1989; Funahashi, 1989; Hornik et al., 1989; Leshno et al., 1993). Theorem 1.16 (Universal approximation theorem). Let d âˆˆN, let K âŠ‚Rd be compact, and let Ï± âˆˆLâˆ loc(R) be an activation function such that the closure of the points of discontinuity of Ï± is a Lebesgue null set. Further let eF B Ã˜ nâˆˆN F((d,n,1),Ï±) be the corresponding set of two-layer NN realizations. Then it follows that C(K) âŠ‚ cl(eF ) (where closure is taken with respect to the topology induced by the Lâˆ(K)- norm) if and only if there does not exist a polynomial p: R â†’R with p = Ï± almost everywhere. The theorem can be proven by the Hahnâ€“Banach theorem, which implies that eF being dense in some real normed vector space S is equivalent to the following condition: for all non-trivial functionals F âˆˆSâ€² \ {0} from the topological dual space of S there exist parameters w âˆˆRd and b âˆˆR such that F(Ï±(âŸ¨w,Â·âŸ©+ b)) , 0. 1.1 Introduction 17 In the case S = C(K) we have by the Rieszâ€“Markovâ€“Kakutani representation theorem that Sâ€² is the space of signed Borel measures on K; see Rudin (2006). Therefore, Theorem 1.16 holds if Ï± is such that, for a signed Borel measure Âµ, âˆ« K Ï±(âŸ¨w, xâŸ©+ b) dÂµ(x) = 0 (1.9) for all w âˆˆRd and b âˆˆR implies that Âµ = 0. An activation function Ï± satisfying this condition is called discriminatory. It is not hard to see that any sigmoidal Ï± is discriminatory. Indeed, assume that Ï± satisï¬es (1.9) for all w âˆˆRd and b âˆˆR. Since for every x âˆˆRd it follows that Ï±(ax + b) â†’1(0,âˆ)(x) + Ï±(b)1{0}(x) for a â†’âˆ, we conclude by superposition and passing to the limit that for all c1,c2 âˆˆR and w âˆˆRd, b âˆˆR, âˆ« K 1[c1,c2](âŸ¨w, xâŸ©+ b) dÂµ(x) = 0. Representing the exponential function x 7â†’eâˆ’2Ï€ix as the limit of sums of elemen- tary functions yields that âˆ« K eâˆ’2Ï€i(âŸ¨w,xâŸ©+b) dÂµ(x) = 0 for all w âˆˆRd, b âˆˆR. Hence, the Fourier transform of Âµ vanishes, which implies that Âµ = 0. Theorem 1.16 addresses the uniform approximation problem on a general com- pact set. If we are given a ï¬nite number of points and care about good approximation only at these points, then one can ask if this approximation problem is potentially simpler. Below we see that, if the number of neurons is larger than or equal to the number of data points, then one can always interpolate, i.e., exactly ï¬t the data to a given ï¬nite number of points. Proposition 1.17 (Interpolation). Let d,m âˆˆN, let x(i) âˆˆRd, i âˆˆ[m], with x(i) , x(j) for i , j, let Ï± âˆˆC(R), and assume that Ï± is not a polynomial. Then, there exist parameters Î¸(1) âˆˆRmÃ—d Ã— Rm with the following property. For every k âˆˆN and every sequence of labels y(i) âˆˆRk, i âˆˆ[m], there exist parameters Î¸(2) = (W(2),0) âˆˆ RkÃ—m Ã— Rk for the second layer of the NN architecture a = ((d,m, k), Ï±) such that Î¦a(x(i),(Î¸(1),Î¸(2))) = y(i), i âˆˆ[m]. We sketch the proof as follows. First, note that Theorem 1.16 also holds for functions g âˆˆC(K,Rm) with multi-dimensional output if we approximate each one-dimensional component x 7â†’(g(x))i and stack the resulting networks. Second, one can add an additional row containing only zeros to the weight matrix W(1) of the approximating neural network as well as an additional entry to the vector b(1). The eï¬€ect of this is that we obtain an additional neuron with constant output. Since Ï± , 0, we can choose b(1) such that the output of this neuron is not zero. Therefore, we can include the bias vector b(2) of the second layer in the weight matrix W(2); see also Remark 1.5. Now choose g âˆˆC(Rm,Rm) to be a function satisfying 18 Berner et al. The Modern Mathematics of Deep Learning g(x(i)) = e(i), i âˆˆ[m], where e(i) âˆˆRm denotes the ith standard basis vector. By the discussion above, there exists a neural network architecture Ëœa = ((d,n,m), Ï±) and parameters ËœÎ¸ = ((e W(1), Ëœb(1)),(e W(2),0)) such that âˆ¥Î¦ Ëœa(Â·, ËœÎ¸) âˆ’gâˆ¥Lâˆ(K) < 1 m, (1.10) where K is a compact set with x(i) âˆˆK, i âˆˆ[m]. Let us abbreviate the output of the activations in the ï¬rst layer evaluated at the input features by eA B  Ï±(e W(1)(x(1)) + Ëœb(1))) Â· Â· Â· Ï±(e W(1)(x(m)) + Ëœb(1)))  âˆˆRnÃ—m. (1.11) The equivalence of the max and operator norm together with (1.10) establish that âˆ¥e W(2) eA âˆ’Imâˆ¥op â‰¤m max i,jâˆˆ[m] (e W(2) eA âˆ’Im)i,j = m max jâˆˆ[m] âˆ¥Î¦ Ëœa(x(j), ËœÎ¸) âˆ’g(x(j))âˆ¥âˆ< 1, where Im denotes the m Ã— m identity matrix. Thus, the matrix e W(2) eA âˆˆRmÃ—m needs to have full rank and we can extract m linearly independent rows from eA, resulting in an invertible matrix A âˆˆRmÃ—m. Now, we deï¬ne the desired parameters Î¸(1) for the ï¬rst layer by extracting the corresponding rows from e W(1) and Ëœb(1) and the parameters Î¸(2) of the second layer by W(2) B  y(1)c . . . y(m) Aâˆ’1 âˆˆRkÃ—m. This proves that with any discriminatory activation function we can interpolate arbitrary training data (x(i), y(i)) âˆˆRd Ã— Rk, i âˆˆ[m], using a two-layer NN with m hidden neurons, i.e., O(m(d + k)) parameters. One can also ï¬rst project the input features onto a one-dimensional line where they are separated and then apply Proposition 1.17 with d = 1. For nearly all activation functions, this argument shows that a three-layer NN with only O(d+mk) parameters can interpolate arbitrary training data.10 Beyond interpolation results, one can obtain a quantitative version of Theo- rem 1.16 if one knows additional regularity properties of the Bayes optimal function f âˆ—, such as its smoothness, compositionality, and symmetries. For surveys on such results, we refer the reader to DeVore et al. (2021) and Chapter 3 in this book. For instructive purposes we review one such result, which can be found in Mhaskar (1996, Theorem 2.1), next. Theorem 1.18 (Approximation of smooth functions). Let d, k âˆˆN and p âˆˆ[1,âˆ]. Further, let Ï± âˆˆCâˆ(R) and assume that Ï± is not a polynomial. Then there exists a constant c âˆˆ(0,âˆ) with the following property. For every n âˆˆN there exist 10 To avoid the m Ã— d weight matrix (without using shared parameters as in Zhang et al., 2017) one interjects an approximate one-dimensional identity (Petersen and Voigtlaender, 2018, Deï¬nition 2.5), which can be arbitrarily well approximated by a NN with architecture a = ((1, 2, 1), Ï±), given that Ï±â€²(Î») , 0 for some Î» âˆˆR; see (1.12) below. 1.1 Introduction 19 parameters Î¸(1) âˆˆRnÃ—dÃ—Rn for the ï¬rst layer of the NN architecture a = ((d,n,1), Ï±) such that for every g âˆˆWk,p((0,1)d) it holds true that inf Î¸(2)âˆˆR1Ã—nÃ—R âˆ¥Î¦a(Â·,(Î¸(1),Î¸(2))) âˆ’gâˆ¥Lp((0,1)d) â‰¤cnâˆ’d/kâˆ¥gâˆ¥W k, p((0,1)d). Theorem 1.18 shows that NNs achieve the same optimal approximation rates that, for example, spline-based approximation yields for smooth functions. The idea be- hind this theorem is based on a strategy that is employed repeatedly throughout the literature. The strategy involves the re-approximation of classical approximation methods by the use of NNs, thereby transferring the approximation rates of these methods to NNs. In the example of Theorem 1.18, approximation by polynomials is used. Thanks to the non-vanishing derivatives of the activation function,11 one can approximate every univariate polynomial via divided diï¬€erences of the acti- vation function. Speciï¬cally, accepting unbounded parameter magnitudes, for any activation function Ï±: R â†’R which is p-times diï¬€erentiable at some point Î» âˆˆR with Ï±(p)(Î») , 0, one can approximate the monomial x 7â†’xp on a compact set K âŠ‚R up to arbitrary precision by a ï¬xed-size NN via rescaled pth-order diï¬€erence quotients as lim hâ†’0 sup xâˆˆK p Ã• i=0 (âˆ’1)i  p i  hp Ï±(p)(Î») Ï± (p/2 âˆ’i)hx + Î» âˆ’xp = 0. (1.12) Let us end this subsection by clarifying the connection of the approximation results above to the error decomposition of (1.7). Consider, for simplicity, a re- gression task with quadratic loss. Then, the approximation error Îµapprox equals a common L2-error: Îµapprox = R( f âˆ— F) âˆ’Râˆ—(âˆ—)= âˆ« X ( f âˆ— F(x) âˆ’f âˆ—(x))2 dIX(x) (âˆ—)= min f âˆˆF âˆ¥f âˆ’f âˆ—âˆ¥2 L2(IX) â‰¤min f âˆˆF âˆ¥f âˆ’f âˆ—âˆ¥2 Lâˆ(X), where the identities marked by (âˆ—) follow from Lemma 1.13. Hence, Theorem 1.16 postulates that Îµapprox â†’0 for increasing NN sizes, whereas Theorem 1.18 addi- tionally explains how fast Îµapprox converges to 0. Generalization Towards bounding the generalization error Îµgen = supf âˆˆF |R( f ) âˆ’bRS( f )|, one ob- serves that, for every f âˆˆF , Assumption 1.10 ensures that L( f, Z(i)), i âˆˆ[m], 11 The Baire category theorem ensures that for a non-polynomial Ï± âˆˆCâˆ(R) there exists Î» âˆˆR with Ï±(p)(Î») , 0 for all p âˆˆN; see, e.g., Donoghue (1969, Chapter 10). 20 Berner et al. The Modern Mathematics of Deep Learning are i.i.d. random variables. Thus, one can make use of concentration inequalities to bound the deviation of the empirical risk bRS( f ) = 1 m Ãm i=1 L( f, Z(i)) from its expectation R( f ). For instance, assuming boundedness12 of the loss, Hoeï¬€dingâ€™s inequality(Hoeï¬€ding, 1963) and a union bound directly imply the following gener- alization guarantee for countable, weighted hypothesis sets F ; see, e.g., Bousquet et al. (2003). Theorem 1.19 (Generalization bound for countable, weighted hypothesis sets). Let m âˆˆN, Î´ âˆˆ(0,1) and assume that F is countable. Further, let p be a probability distribution on F and assume that L( f, Z) âˆˆ[0,1] almost surely for every f âˆˆF . Then with probability 1 âˆ’Î´ (with respect to repeated sampling of Im Z -distributed training data S) it holds true for every f âˆˆF that |R( f ) âˆ’bRS( f )| â‰¤ r ln(1/p( f )) + ln(2/Î´) 2m . While the weighting p needs to be chosen before seeing the training data, one could incorporate prior information on the learning algorithm A. For ï¬nite hy- pothesis sets without prior information, setting p( f ) = 1/|F | for every f âˆˆF , Theorem 1.19 implies that, with high probability, Îµgen â‰² r ln(|F |) m . (1.13) Again, one notices that, in line with the biasâ€“variance trade-oï¬€, the generalization bound increases with the size of the hypothesis set |F |. Although in practice the parameters Î¸ âˆˆRP(N) of a NN are discretized according to ï¬‚oating-point arithmetic, the corresponding quantities |Fa| or |Fa,sgn| would be huge and we need to ï¬nd a replacement for the ï¬niteness condition. We will focus on binary classiï¬cation tasks and present a main result of VC theory, which to a great extent is derived from the work of Vladimir Vapnik and Alexey Chervonenkis (1971). While in (1.13) we counted the number of functions in F , we now reï¬ne this analysis to count the number of functions in F , restricted to a ï¬nite subset of X, given by the growth function growth(m, F ) B max (x(i))m i=1âˆˆXm |{ f |(x(i))m i=1 : f âˆˆF }|. The growth function can be interpreted as the maximal number of classiï¬ca- tion patterns in {âˆ’1,1}m which functions in F can realize on m points; thus 12 Note that for our classiï¬cation tasks in Deï¬nition 1.2 it follows that L(f , Z) âˆˆ{0, 1} for every f âˆˆF. For the regression tasks, one typically assumes boundedness conditions, such as |Y | â‰¤c and supf âˆˆF |f (X)| â‰¤c almost surely for some c âˆˆ(0, âˆ), which yields that supf âˆˆF |L(f , Z)| â‰¤4c2. 1.1 Introduction 21 growth(m, F ) â‰¤2m. The asymptotic behavior of the growth function is determined by a single intrinsic dimension of our hypothesis set F , the so-called VC-dimension VCdim(F ) B sup  m âˆˆN âˆª{0}: growth(m, F ) = 2m , which deï¬nes the largest number of points such that F can realize any classiï¬cation pattern; see, e.g.,Anthony and Bartlett (1999), Bousquet et al. (2003). There exist various results on the VC-dimensions of NNs with diï¬€erent activation functions; see, for instance, Baum and Haussler (1989), Karpinski and Macintyre (1997), Bartlett et al. (1998), Sakurai (1999). We present the result of Bartlett et al. (1998) for piecewise polynomial activation functions Ï±. It establishes a bound on the VC- dimension of hypothesis sets of NNs for classiï¬cation tasks F(N,Ï±),sgn that scales, up to logarithmic factors, linearly in the number of parameters P(N) and quadratically in the number of layers L. Theorem 1.20 (VC-dimension of neural network hypothesis sets). Let Ï± be a piecewise polynomial activation function. Then there exists a constant c âˆˆ(0,âˆ) such that for every L âˆˆN and N âˆˆNL+1, VCdim(F(N,Ï±),sgn) â‰¤c P(N)L log(P(N)) + P(N)L2. Given (x(i))m i=1 âˆˆXm, there exists a partition of RP(N) such that Î¦(x(i),Â·), i âˆˆ[m], are polynomials on each region of the partition. The proof of Theorem 1.20 is based on bounding the number of such regions and the number of classiï¬cation patterns of a set of polynomials. A ï¬nite VC-dimension ensures the following generalization bound (Talagrand, 1994; Anthony and Bartlett, 1999): Theorem 1.21 (VC-dimension generalization bound). There exists a constant c âˆˆ (0,âˆ) with the following property. For every classiï¬cation task as in Deï¬nition 1.2, every Z-valued random variable Z, and every m âˆˆN, Î´ âˆˆ(0,1), then, with probability 1 âˆ’Î´ (with respect to the repeated sampling of Im Z -distributed training data S), it follows that sup f âˆˆF |R( f ) âˆ’bRS( f )| â‰¤c r VCdim(F ) + log(1/Î´)) m . In summary, using NN hypothesis sets F(N,Ï±),sgn with a ï¬xed depth and piecewise polynomial activation Ï± for a classiï¬cation task, with high probability it follows that Îµgen â‰² r P(N) log(P(N)) m . (1.14) In the remainder of this section we will sketch a proof of Theorem 1.21 and, in 22 Berner et al. The Modern Mathematics of Deep Learning doing so, present further concepts and complexity measures connected with gener- alization bounds. We start by observing that McDiarmidâ€™s inequality (McDiarmid, 1989) ensures that Îµgen is sharply concentrated around its expectation, i.e., with probability 1 âˆ’Î´ it holds true that13 Îµgen âˆ’E  Îµgen â‰² r log(1/Î´) m . (1.15) To estimate the expectation of the uniform generalization error we employ a sym- metrization argument (GinÃ© and Zinn, 1984). Deï¬ne G B L â—¦F B {L( f,Â·): f âˆˆ F }, let eS = (eZ(i))m i=1 âˆ¼Im Z be a test data set that is independent of S, and note that R( f ) = E[bReS( f )]. By properties of the conditional expectation and Jensenâ€™s inequality it follows that E  Îµgen = E h sup f âˆˆF |R( f ) âˆ’bRS( f )| i = E h sup gâˆˆG 1 m m Ã• i=1 E  g(eZ(i)) âˆ’g(Z(i))|S  i â‰¤E h sup gâˆˆG 1 m m Ã• i=1 g(eZ(i)) âˆ’g(Z(i)) i = E h sup gâˆˆG 1 m m Ã• i=1 Ï„i  g(eZ(i)) âˆ’g(Z(i)) i â‰¤2E h sup gâˆˆG 1 m m Ã• i=1 Ï„ig(Z(i)) i , where we have used that multiplications with Rademacher variables (Ï„1,. . .,Ï„m) âˆ¼ U({âˆ’1,1}m) only amount to interchanging Z(i) with eZ(i), which has no eï¬€ect on the expectation since Z(i) and eZ(i) have the same distribution. The quantity Rm(G) B E h sup gâˆˆG 1 m m Ã• i=1 Ï„ig(Z(i)) i is called the Rademacher complexity14 of G. One can also prove a corresponding lower bound (van der Vaart and Wellner, 1997), i.e., Rm(G) âˆ’ 1 âˆšm â‰²E  Îµgen â‰²Rm(G). (1.16) Now we use a chaining method to bound the Rademacher complexity of F by cov- ering numbers on diï¬€erent scales. Speciï¬cally, Dudleyâ€™s entropy integral (Dudley, 13 For precise conditions to ensure that the expectation of Îµgen is well deï¬ned, we refer readers to van der Vaart and Wellner (1997), Dudley (2014). 14 Due to our decomposition in (1.7), we want to uniformly bound the absolute value of the diï¬€erence between the risk and the empirical risk. It is also common just to bound supf âˆˆF R(f ) âˆ’bRS(f ) leading to a deï¬nition of the Rademacher complexity without the absolute values, which can be easier to deal with. 1.1 Introduction 23 1967; Ledoux and Talagrand, 1991) implies that Rm(G) â‰²E h âˆ«âˆ 0 r log NÎ±(G, dS) m dÎ± i , (1.17) where NÎ±(G, dS) B inf n |G| : G âŠ‚G, G âŠ‚ Ã˜ gâˆˆG BdS Î± (g) o denotes the covering number with respect to the (random) pseudometric given by dS( f,g) = d(Z(i))m i=1( f,g) B v t 1 m m Ã• i=1   f (Z(i)) âˆ’g(Z(i))2. For the 0â€“1 loss L( f, z) = 1(âˆ’âˆ,0)(y f (x)) = (1 âˆ’f (x)y)/2, we can get rid of the loss function using the fact that NÎ±(G, dS) = N2Î±(F, d(X(i))m i=1). (1.18) The proof is completed by combining the inequalities in (1.15), (1.16), (1.17) and (1.18) with a result of David Haussler (1995) which shows that, for Î± âˆˆ(0,1), we have log(NÎ±(F, d(X(i))m i=1)) â‰²VCdim(F ) log(1/Î±). (1.19) We remark that this resembles a typical behavior of covering numbers. For instance, the logarithm of the covering number log(NÎ±(M)) of a compact d- dimensional Riemannian manifold M essentially scales as d log(1/Î±). Finally, note that there exists a bound similar to the one in (1.19) for bounded regression tasks that makes use of the so-called fat-shattering dimension (Mendelson and Vershynin, 2003, Theorem 1). 1.1.3 Do We Need a New Theory? Despite the already substantial insight that the classical theories provide, a lot of open questions remain. We will outline these questions below. The remainder of this chapter then collects modern approaches to explain the following issues. Why do large neural networks not overï¬t? In Â§1.1.2, we have observed that three-layer NNs with commonly used activation functions and only O(d + m) parameters can interpolate any training data (x(i), y(i)) âˆˆRd Ã—R, i âˆˆ[m]. While this speciï¬c representation might not be found in practice (Zhang et al., 2017), indeed trained convolutional15 NNs with ReLU activation function and about 1.6 million 15 The basic deï¬nition of a convolutional NN will be given in Â§1.6. In Zhang et al. (2017) more elaborate versions such as an inception architecture (Szegedy et al., 2015) are employed. 24 Berner et al. The Modern Mathematics of Deep Learning parameters to achieve zero empirical risk on m = 50,000 training images of the CIFAR10 dataset (Krizhevsky and Hinton, 2009) with 32 Ã— 32 pixels per image, i.e., d = 1,024. For such large NNs, generalization bounds scaling with the number of parameters P(N) as the VC-dimension bound in (1.14) are vacuous. However, these workers observed close to state-of-the-art generalization performance.16 Generally speaking, NNs are observed in practice to generalize well despite hav- ing more parameters than training samples (usually referred to as overparametriza- tion) and approximately interpolating the training data (usually referred to as over- ï¬tting). As we cannot perform any better on the training data, there is no trade-oï¬€ between the ï¬t to training data and the complexity of the hypothesis set F hap- pening, seemingly contradicting the classical biasâ€“variance trade-oï¬€of statistical learning theory. This is quite surprising, especially given the following additional empirical observations in this regime, see Neyshabur et al. (2014, 2017), Zhang et al. (2017), Belkin et al. (2019b), Nakkiran et al. (2020): (i) Zero training error on random labels: Zero empirical risk can also be achieved for random labels using the same architecture and training scheme with only slightly increased training time. This suggests that the considered hypothesis set of NNs F can ï¬t arbitrary binary labels, which would imply that VCdim(F ) â‰ˆm or Rm(F ) â‰ˆ1, rendering our uniform generalization bounds in Theorem 1.21 and in (1.16) vacuous. (ii) Lack of explicit regularization: The test error depends only mildly on explicit regularization, such as norm-based penalty terms or dropout (see GÃ©ron, 2017, for an explanation of diï¬€erent regularization methods). As such regularization methods are typically used to decrease the complexity of F , one might ask if there is any implicit regularization (see Figure 1.4), constraining the range of our learning algorithm A to some smaller, potentially data-dependent, subset, i.e., A(s) âˆˆeFs âŠŠF . (iii) Dependence on the initialization: The same NN trained to zero empirical risk but starting from diï¬€erent initializations can exhibit diï¬€erent test errors. This indicates that properties of the local minimum at fs to which gradient descent converges might be correlated with its generalization. (iv) Interpolation of noisy training data: One still observes low test error when training up to approximately zero empirical risk using a regression (or surrogate) loss on noisy training data. This is particularly interesting, as the noise is captured by the model but seems not to hurt the generalization performance. 16 In practice one usually cannot measure the risk R(fs) and instead one evaluates the performance of a trained model fs by bR Ëœs(fs) using test data Ëœs, i.e., realizations of i.i.d. random variables distributed according to IZ and drawn independently of the training data. In this context one often calls Rs(fs) the training error and R Ëœs(fs) the test error. 1.1 Introduction 25 Figure 1.4 The left plot (and its semi-log inset) shows the median and interquartile range of the test and training errors of ten independent linear regressions with m = 20 samples, polynomial input features X = (1, Z, . . . , Z d) of degree d âˆˆ[40], and labels Y = f âˆ—(Z) + Î½, where Z âˆ¼U([âˆ’0.5, 0.5]), f âˆ—is a polynomial of degree three, and Î½ âˆ¼N(0, 0.01). This clearly reï¬‚ects the classical âˆª-shaped biasâ€“variance curve with a sweet-spot at d = 3 and drastic overï¬tting beyond the interpolation threshold at d = 20. However, the middle plot shows that we can control the complexity of our hypothesis set of linear models by restricting the Euclidean norm of their parameters using ridge regression with a small regularization parameter Î± = 10âˆ’3, i.e., minimizing the regularized empirical risk 1 m Ãm i=1(Î¦(X(i), Î¸) âˆ’Y(i))2 + Î±âˆ¥Î¸ âˆ¥2 2, where Î¦(Â·, Î¸) = âŸ¨Î¸, Â·âŸ©. Corresponding examples of bfs are depicted in the right plot. (v) Further overparametrization improves generalization performance: Further in- creasing the NN size can lead to even lower test error. Together with the previous item, this might require a diï¬€erent treatment of models that are complex enough to ï¬t the training data. According to the traditional lore â€œThe training error tends to decrease whenever we increase the model complexity; that is, whenever we ï¬t the data harder. However with too much ï¬tting, the model adapts itself too closely to the training data, and will not generalize well (i.e., it will have a large test error)â€, (Hastie et al., 2001). While this ï¬‚awlessly describes the situation for certain machine learning tasks (see Figure 1.4), it seems not to be directly applicable here. In summary, these observations suggest that the generalization performance of NNs depends on an interplay of the data distribution IZ with properties of the learning algorithm A, such as the optimization procedure and its range. In particular, clas- sical uniform bounds as in Item (B) on page13 of our error decomposition might deliver insuï¬ƒcient explanation; see also Nagarajan and Kolter (2019). The mis- match between the predictions of classical theory and the practical generalization performance of deep NNs is often referred to as the generalization puzzle. In Â§1.2 we will present possible explanations for this phenomenon. What is the role of depth? We saw in Â§1.1.2 that NNs can closely approximate every function if they are suï¬ƒciently wide (Cybenko, 1989; Funahashi, 1989; Hornik et al., 1989). There are additional classical results that even provide a trade- oï¬€between the width and the approximation accuracy (Chui et al., 1994; Mhaskar, 26 Berner et al. The Modern Mathematics of Deep Learning 1996; Maiorov and Pinkus, 1999). In these results, the central concept is the width of a NN. In modern applications, however, at least as much focus if not more lies on the depth of the underlying architectures, which can have more than 1000 layers (He et al., 2016). After all, the depth of NNs is responsible for the name â€œdeep learningâ€. This consideration begs the question of whether there is a concrete mathemati- cally quantiï¬able beneï¬t of deep architectures over shallow NNs. Indeed, we will see the eï¬€ects of depth at many places throughout this chapter. However, one as- pects of deep learning that is most clearly aï¬€ected by deep architectures is the approximation-theoretical aspect. In this framework, we will discuss in Â§1.3 multi- ple approaches that describe the eï¬€ect of depth. Why do neural networks perform well in very high-dimensional environments? We saw in Â§1.1.2 and will see in Â§1.3 that, from the perspective of approximation theory, deep NNs match the performance of the best classical approximation tool in virtually every task. In practice, we observe something that is even more astounding. In fact, NNs seem to perform incredibly well on tasks that no classical, non- specialized approximation method can even remotely handle. The approximation problem that we are talking about here is that of approximation of high-dimensional functions. Indeed, the classical curse of dimensionality (Bellman, 1952; Novak and WoÅºniakowski, 2009) postulates that essentially every approximation method deteriorates exponentially fast with increasing dimension. For example, for the uniform approximation error of 1-Lipschitz continuous functions on a d-dimensional unit cube in the uniform norm, we have a lower bound of â„¦(pâˆ’1/d), for p â†’âˆ, when approximating with a continuous scheme17 of p free parameters (DeVore, 1998). On the other hand, in most applications the input dimensions are massive. For example, the following datasets are typically used as benchmarks in image clas- siï¬cation problems: MNIST (LeCun et al., 1998) with 28 Ã— 28 pixels per image, CIFAR-10/CIFAR-100 (Krizhevsky and Hinton, 2009) with 32 Ã— 32 pixels per im- age, and ImageNet (Deng et al., 2009; Krizhevsky et al., 2012), which contains high-resolution images that are typically down-sampled to 256 Ã— 256 pixels. Nat- urally, in real-world applications, the input dimensions may well exceed those of these test problems. However, already for the simplest of the test cases above, the input dimension is d = 784. If we use d = 784 in the aforementioned lower bound for the approximation of 1-Lipschitz functions, then we require O(Îµâˆ’784) parameters 17 One can achieve better rates at the cost of discontinuous (with respect to the function to be approximated) parameter assignment. This can be motivated by the use of space-ï¬lling curves. In the context of NNs with piecewise polynomial activation functions, a rate of pâˆ’2/d can be achieved by very deep architectures (Yarotsky, 2018a; Yarotsky and Zhevnerchuk, 2020). 1.1 Introduction 27 to achieve a uniform error of Îµ âˆˆ(0,1). Even for moderate Îµ this value will quickly exceed the storage capacity of any conceivable machine in this universe. Consid- ering the aforementioned curse of dimensionality, it is puzzling to see that NNs perform adequately in this regime. In Â§1.4, we describe three approaches that oï¬€er explanations as to why deep NN-based approximation is not rendered meaningless in the context of high-dimensional input dimensions. Why does stochastic gradient descent converge to good local minima despite the non-convexity of the problem? As mentioned in Â§1.1.2, a convergence guarantee of stochastic gradient descent to a global minimum can typically be given only if the underlying objective function admits some form of convexity. However, the empirical risk of a NN, i.e., bRs(Î¦(Â·,Î¸)), is typically not a convex function with respect to the parameters Î¸. For a simple intuitive explanation of why this function fails to be convex, it is instructive to consider the following example. Example 1.22. Consider the NN Î¦(x,Î¸) = Î¸1Ï±R(Î¸3x + Î¸5) + Î¸2Ï±R(Î¸4x + Î¸6), Î¸ âˆˆR6, x âˆˆR, with the ReLU activation function Ï±R(x) = max{0, x}. It is not hard to see that the two parameter values Î¸ = (1,âˆ’1,1,1,1,0) and Â¯Î¸ = (âˆ’1,1,1,1,0,1) produce the same realization function,18 i.e., Î¦(Â·,Î¸) = Î¦(Â·, Â¯Î¸). However, since (Î¸ + Â¯Î¸)/2 = (0,0,1,1,1/2,1/2), we conclude that Î¦(Â·,(Î¸ + Â¯Î¸)/2) = 0. Clearly, for the data s = ((âˆ’1,0),(1,1)), we now have that bRs(Î¦(Â·,Î¸)) = bRs(Î¦(Â·, Â¯Î¸)) = 0 and bRs  Î¦(Â·,(Î¸ + Â¯Î¸)/2) = 1 2, showing the non-convexity of bRs. Given this non-convexity, Algorithm 1.1 faces serious challenges. First, there may exist multiple suboptimal local minima. Second, the objective function may exhibit saddle points, some of which may be of higher order, i.e., the Hessian vanishes. Finally, even if no suboptimal local minima exist, there may be extensive areas of the parameter space where the gradient is very small, so that escaping these regions can take a very long time. These issues are not mere theoretical possibilities, but will almost certainly arise in practice. For example, Auer et al. (1996) and Safran and Shamir (2018) showed the existence of many suboptimal local minima in typical learning tasks. Moreover, for ï¬xed-sized NNs, it was shown by Berner et al. (2019b) and Petersen et al. (2020) that, with respect to Lp-norms, the set of NNs is generally very non-convex and 18 This corresponds to interchanging the two neurons in the hidden layer. In general the realization function of an FC NN is invariant under permutations of the neurons in a given hidden layer. 28 Berner et al. The Modern Mathematics of Deep Learning Figure 1.5 Two-dimensional projection of the loss landscape of a neural network with four layers and ReLU activation function on four diï¬€erent scales. From upper left to lower right, we zoom into the global minimum of the landscape. non-closed. Moreover, the map Î¸ 7â†’Î¦a(Â·,Î¸) is not a quotient map, i.e., it is not continuously invertible when its non-injectivity is taken into account. Furthermore, in various situations, ï¬nding the global optimum of the minimization problem has been shown to be NP-hard in general (Blum and Rivest, 1989; Judd, 1990; Å Ã­ma, 2002). In Figure 1.5 we show the two-dimensional projection of a loss landscape, i.e., a projection of the graph of the function Î¸ 7â†’bRs(Î¦(Â·,Î¸)). It is apparent from the visualization that the problem exhibits more than one minimum. We also want to add that in practice one neglects the fact that the loss is only almost everywhere diï¬€erentiable in the case of piecewise-smooth activation functions, such as the ReLU, although one could resort to subgradient methods (Kakade and Lee, 2018). In view of these considerations, the classical framework presented in Â§1.1.2 oï¬€ers no explanation as to why deep learning works in practice. Indeed, in the survey of Orr and MÃ¼ller (1998, Section 1.4) the state of the art in 1998 was summarized by the following assessment: â€œThere is no formula to guarantee that (1) the NN 1.1 Introduction 29 will converge to a good solution, (2) convergence is swift, or (3) convergence even occurs at all.â€ Nonetheless, in applications, not only would an explanation of when and why SGD converges be extremely desirable, convergence is also quite often observed even though there is little theoretical explanation for it in the classical set-up. In Â§1.5 we collect modern approaches explaining why and when convergence occurs and can be guaranteed. Which aspects of a neural network architecture aï¬€ect the performance of deep learning? In the introduction to classical approaches to deep learning above, we saw that, in classical results such as in Theorem 1.18, the eï¬€ect of only a few aspects of the NN architectures are considered. In Theorem 1.18 only the impact of the width of the NN was studied. In further approximation theorems below, for example, in Theorems 1.23 and 1.25, we will additionally have a variable depth of NNs. However, for deeper architectures, there are many additional aspects of the architecture that could potentially aï¬€ect the performance of the model for the associated learning task. For example, even for a standard FC NN with L layers as in Deï¬nition 1.4, there is a lot of ï¬‚exibility in choosing the number of neurons (N1,. . ., NLâˆ’1) âˆˆNLâˆ’1 in the hidden layers. One would expect that certain choices aï¬€ect the capabilities of the NNs considerably and that some choices are preferable to others. Note that one aspect of the neural network architecture that can have a profound eï¬€ect on performance, especially regarding the approximation-theoretic aspects of performance, is the choice of the activation function. For example, in Maiorov and Pinkus (1999) and Yarotsky (2021) activation functions were found that allow the uniform approximation of continuous functions to arbitrary accu- racy with ï¬xed-size neural networks. In what follows we will focus, however, on architectural aspects other than the activation function. In addition, practitioners have invented an immense variety of NN architectures for speciï¬c problems. These include NNs with convolutional blocks (LeCun et al., 1998), with skip connections (He et al., 2016), sparse connections (Zhou et al., 2016; Bourely et al., 2017), batch normalization blocks (Ioï¬€e and Szegedy, 2015), and many more. Furthermore, for sequential data, recurrent connections have been used (Rumelhart et al., 1986) and these have often had forget mechanisms (Hochreiter and Schmidhuber, 1997) or other gates (Cho et al., 2014) included in their architectures. The choice of an appropriate NN architecture is essential to the success of many deep learning tasks. This is so important that frequently an architecture search is applied to ï¬nd the most suitable one (Zoph and Le, 2017; Pham et al., 2018). In most cases, though, the design and choice of the architecture is based on the intuition of the practitioner. Naturally, from a theoretical point of view, this situation is not satisfactory. 30 Berner et al. The Modern Mathematics of Deep Learning Instead, it would be highly desirable to have a mathematical theory guiding the choice of NN architectures. More concretely, one would wish for mathematical theorems that identify those architectures that would work for a speciï¬c problem and those that would yield suboptimal results. In Â§1.6, we discuss various results that explain theoretically quantiï¬able eï¬€ects of certain aspects, or building blocks, of NN architectures. Which features of data are learned by deep architectures? It is commonly believed that the neurons of NNs constitute feature extractors at diï¬€erent levels of abstraction that correspond to the layers. This belief is partially grounded in experimental evidence as well as by drawing connections to the human visual cortex; see Goodfellow et al. (2016, Chapter 9.10). Understanding the features that are learned can be linked, in a way, to under- standing the reasoning with which a NN-based model ended up with its result. Therefore, analyzing the features that a NN learns constitutes a data-aware ap- proach to understanding deep learning. Naturally, this falls outside of the scope of the classical theory, which is formulated in terms of optimization, generalization, and approximation errors. One central obstacle towards understanding these features theoretically is that, at least for practical problems, the data distribution is unknown. However, one often has partial knowledge. One example is that in image classiï¬cation it appears reasonable to assume that any classiï¬er is translation and rotation invariant as well as invariant under small deformations. In this context, it is interesting to understand under which conditions trained NNs admit the same invariances. Biological NNs such as the visual cortex are believed to have evolved in a way that is based on sparse multiscale representations of visual information (Olshausen and Field, 1996). Again, a fascinating question is whether NNs trained in practice can be shown to favor such multiscale representations based on sparsity or whether the architecture is theoretically linked to sparse representations. We will discuss various approaches studying the features learned by neural networks in Â§1.7. Are neural networks capable of replacing highly specialized numerical algo- rithms in natural sciences? Shortly after their successes in various data-driven tasks in data science and AI applications, NNs started to be used also as a numerical ansatz for solving highly complex models from the natural sciences that could be combined with data-driven methods. This is per se not very surprising as many such models can be formulated as optimization problems where the commonly used deep learning paradigm can be directly applied. What might be considered surprising is that this approach seems to be applicable to a wide range of problems which had previously been tackled by highly specialized numerical methods. 1.2 Generalization of Large Neural Networks 31 Particular successes include the data-drivensolutionof ill-posedinverseproblems (Arridge et al., 2019) which has, for example, led to a fourfold speedup in MRI scantimes (Zbontar et al., 2018) igniting the research project fastmri.org. Deep- learning-based approaches have also been very successful in solving a vast array of partial diï¬€erential equation (PDE) models, especially in the high-dimensional regime (E and Yu, 2018; Raissi et al., 2019; Hermann et al., 2020; Pfau et al., 2020) where most other methods would suï¬€er from the curse of dimensionality. Despite these encouraging applications, the foundational mechanisms governing their workings and limitations are still not well understood. In Â§Â§1.4.3 and 1.8 we discuss some theoretical and practical aspects of deep learning methods applied to the solution of inverse problems and PDEs. 1.2 Generalization of Large Neural Networks In the following we will shed light on the generalization puzzle of NNs as described in Â§1.1.3. We focus on four diï¬€erent lines of research which, even so, do not cover the wide range of available results. In fact, we had to omit a discussion of a multitude of important works, some of which we reference in the following paragraph. First, let us mention extensions of the generalization bounds presented in Â§1.1.2 that make use of local Rademacher complexities (Bartlett et al., 2005) or that drop assumptions on boundedness or rapidly decaying tails (Mendelson, 2014). Furthermore, there are approaches to generalization which do not focus on the hypothesis set F , i.e., the range of the learning algorithm A, but on the way in which A chooses its model fs. For instance, one can assume that fs does not depend too strongly on each individual sample (algorithmic stability: Bousquet and Elisseeï¬€, 2002, Poggio et al., 2004), but only on a subset of the samples (compression bounds: Arora et al., 2018b), or that it satisï¬es local properties (algorithmic robustness: Xu and Mannor, 2012). Finally, we refer the reader to Jiang et al. (2020) and the references mentioned therein for an empirical study of various measures related to generalization. Note that many results on the generalization capabilities of NNs can still only be proven in simpliï¬ed settings, for example for deep linear NNs, i.e., Ï±(x) = x, or basic linear models, i.e., one-layer NNs. Thus, we start by emphasizing the connection of deep, nonlinear NNs to linear models (operating on features given by a suitable kernel) in the inï¬nite-width limit. 1.2.1 Kernel Regime We consider a one-dimensional prediction setting where the loss L( f,(x, y)) de- pends on x âˆˆX only through f (x) âˆˆY, i.e., there exists a function â„“: Y Ã— Y â†’R 32 Berner et al. The Modern Mathematics of Deep Learning such that L( f,(x, y)) = â„“( f (x), y). For instance, in the case of quadratic loss we have that â„“(Ë†y, y) = (Ë†y âˆ’y)2. Further, let Î¦ be a NN with architecture (N, Ï±) = ((d, N1,. . ., NLâˆ’1,1), Ï±) and let Î˜0 be a RP(N)-valued random variable. For simplicity, we evolve the parameters of Î¦ according to the continuous version of gradient descent, so-called gradient ï¬‚ow, given by dÎ˜(t) dt = âˆ’âˆ‡Î¸ bRs(Î¦(Â·,Î˜(t))) = âˆ’1 m m Ã• i=1 âˆ‡Î¸Î¦(x(i),Î˜(t))Di(t), Î˜(0) = Î˜0, (1.20) where Di(t) B âˆ‚â„“(Ë†y, y(i)) âˆ‚Ë†y |Ë†y=Î¦(x(i),Î˜(t)) is the derivative of the loss with respect to the prediction at input feature x(i) at time t âˆˆ[0,âˆ). The chain rule implies the following dynamics of the NN realization dÎ¦(Â·,Î˜(t)) dt = âˆ’1 m m Ã• i=1 KÎ˜(t)(Â·, x(i))Di(t) (1.21) and of its empirical risk dbRs(Î¦(Â·,Î˜(t)) dt = âˆ’1 m2 m Ã• i=1 m Ã• j=1 Di(t)KÎ˜(t)(x(i), x(j))Dj(t), (1.22) where KÎ¸, Î¸ âˆˆRP(N), is the so-called neural tangent kernel (NTK): KÎ¸ : Rd Ã— Rd â†’R, KÎ¸(x1, x2) =  âˆ‡Î¸Î¦(x1,Î¸)Tâˆ‡Î¸Î¦(x2,Î¸). (1.23) Now let Ïƒw,Ïƒb âˆˆ(0,âˆ) and assume that the initialization Î˜0 consists of indepen- dent entries, where entries corresponding to the weight matrix and bias vector in the â„“th layer follow a normal distribution with zero mean and variances Ïƒ2 w/Nâ„“and Ïƒ2 b, respectively. Under weak assumptions on the activation function, the central limit theorem implies that the pre-activations converge to i.i.d. centered Gaussian processes in the inï¬nite-width limit N1,. . ., NLâˆ’1 â†’âˆ; see Lee et al. (2018) and Matthews et al. (2018). Similarly, KÎ˜0 also converges to a deterministic kernel Kâˆ which stays constant in time and depends only on the activation function Ï±, the depth L, and the initialization parameters Ïƒw and Ïƒb (Jacot et al., 2018; Arora et al., 2019b; Yang, 2019; Lee et al., 2020). Thus, within the inï¬nite width limit, gradient ï¬‚ow on the NN parameters as in (1.20) is equivalent to functional gradient ï¬‚ow in the reproducing kernel Hilbert space (HKâˆ, âˆ¥Â· âˆ¥Kâˆ) corresponding to Kâˆ; see (1.21). 1.2 Generalization of Large Neural Networks 33 By (1.22), the empirical risk converges to a global minimum as long as the kernel evaluated at the input features, Â¯KâˆB (Kâˆ(x(i), x(j)))m i,j=1 âˆˆRmÃ—m, is positive deï¬nite (see, e.g., Jacot et al., 2018, Du et al., 2019, for suitable conditions) and the â„“(Â·, y(i)) are convex and lower bounded. For instance, in the case of quadratic loss the solution of (1.21) is then given by Î¦(Â·,Î˜(t)) = C(t)(y(i))m i=1 +  Î¦(Â·,Î˜0) âˆ’C(t)(Î¦(x(i),Î˜0))m i=1 , (1.24) where C(t) :=  (Kâˆ(Â·, x(i)))m i=1 T( Â¯Kâˆ)âˆ’1(Im âˆ’eâˆ’2 Â¯Kâˆt/m). As the initial realization Î¦(Â·,Î˜0) constitutes a centered Gaussian process, the second term in (1.24) follows a normal distribution with zero mean at each input. In the limit t â†’âˆ, its variance vanishes on the input features x(i), i âˆˆ[m], and the ï¬rst term converges to the minimum kernel-norm interpolator, i.e., to the solution of min f âˆˆHKâˆâˆ¥f âˆ¥Kâˆ s.t. f (x(i)) = y(i). Therefore, within the inï¬nite-width limit, the generalization properties of the NN could be described by the generalization properties of the minimizer in the repro- ducing kernel Hilbert space corresponding to the kernel Kâˆ(Belkin et al., 2018; Liang and Rakhlin, 2020; Liang et al., 2020; Ghorbani et al., 2021; Li, 2021). This so-called lazy training, where a NN essentially behaves like a linear model with respect to the nonlinear features x 7â†’âˆ‡Î¸Î¦(x,Î¸), can already be observed in the non-asymptotic regime; see also Â§1.5.2. For suï¬ƒciently overparametrized (P(N) â‰«m) and suitably initialized models, one can show that KÎ¸(0) is close to Kâˆat initialization and KÎ¸(t) stays close to KÎ¸(0) throughout training; see Du et al. (2018b, 2019), Arora et al. (2019b), and Chizat et al. (2019). The dynamics of the NN under gradient ï¬‚ow in (1.21) and (1.22) can thus be approximated by the dynamics of the linearization of Î¦ at initialization Î˜0, given by Î¦lin(Â·,Î¸) B Î¦(Â·,Î˜0) + âŸ¨âˆ‡Î¸Î¦(Â·,Î˜0),Î¸ âˆ’Î˜0âŸ©, (1.25) which motivates studying the behavior of linear models in the overparametrized regime. 1.2.2 Norm-Based Bounds and Margin Theory For piecewise linear activation functions, one can improve upon the VC-dimension bounds in Theorem 1.20 and show that, up to logarithmic factors, the VC-dimension is asymptotically bounded both above and below by P(N)L; see Bartlett et al. (2019). The lower bound shows that the generalization bound in Theorem 1.21 can be non- vacuous only if the number of samples m scales at least linearly with the number of NN parameters P(N). However, the heavily overparametrized NNs used in practice seem to generalize well outside of this regime. 34 Berner et al. The Modern Mathematics of Deep Learning One solution is to bound other complexity measures of NNs, taking into account various norms on the parameters, and avoid the direct dependence on the num- ber of parameters (Bartlett, 1998). For instance, we can compute bounds on the Rademacher complexity of NNs with positively homogeneous activation function, where the Frobenius norm of the weight matrices is bounded; see also Neyshabur et al. (2015). Note that, for instance, the ReLU activation is positively homogeneous, i.e., it satisï¬es that Ï±R(Î»x) = Î»Ï±R(x) for all x âˆˆR and Î» âˆˆ(0,âˆ). Theorem 1.23 (Rademacher complexity of neural networks). Let d âˆˆN, assume that X = B1(0) âŠ‚Rd, and let Ï± be a positively homogeneous activation function with Lipschitz constant 1. We deï¬ne the set of all biasless NN realizations with depth L âˆˆN, output dimension 1, and Frobenius norm of the weight matrices bounded by C âˆˆ(0,âˆ) as eFL,C B  Î¦(N,Ï±)(Â·,Î¸): N âˆˆNL+1, N0 = d, NL = 1, Î¸ = ((W(â„“),0))L â„“=1 âˆˆRP(N), âˆ¥W(â„“)âˆ¥F â‰¤C . Then for every m âˆˆN it follows that Rm(eFL,C) â‰¤C(2C)Lâˆ’1 âˆšm . The factor 2Lâˆ’1, depending exponentially on the depth, can be reduced to âˆš L or completely omitted by invoking the spectral norm of the weight matrices (Golowich et al., 2018). Further, observe that for L = 1, i.e., linear classiï¬ers with bounded Euclidean norm, this bound is independent of the input dimension d. Together with (1.16), this motivates why the regularized linear model in Figure 1.4 did perform well in the overparametrized regime. The proof of Theorem 1.23 is based on the contractionpropertyoftheRademacher complexity (Ledoux and Talagrand, 1991), which establishes that Rm(Ï± â—¦eFâ„“,C) â‰¤2Rm(eFâ„“,C), â„“âˆˆN. We can iterate this together with the fact that for every Ï„ âˆˆ{âˆ’1,1}m, and x âˆˆRNâ„“âˆ’1 it follows that sup âˆ¥W (â„“) âˆ¥F â‰¤C m Ã• i=1 Ï„i Ï±(W(â„“)x) 2 = C sup âˆ¥w âˆ¥2 â‰¤1 m Ã• i=1 Ï„i Ï±(âŸ¨w, xâŸ©) . In summary, we have established that Rm(eFL,C) = C mE h sup f âˆˆeFLâˆ’1,C m Ã• i=1 Ï„i Ï±( f (X(i))) 2 i â‰¤C(2C)Lâˆ’1 m E h m Ã• i=1 Ï„iX(i) 2 i , which by Jensenâ€™s inequality yields the claim. 1.2 Generalization of Large Neural Networks 35 Recall that for classiï¬cation problems one typically minimizes a surrogate loss Lsurr; see Remark 1.9. This suggests that there could be a trade-oï¬€happening between the complexity of the hypothesis class Fa and the underlying regression ï¬t, i.e., the margin M( f, z) B y f (x) by which a training example z = (x, y) has been classiï¬ed correctly by f âˆˆFa; see Bartlett et al. (2017), Neyshabur et al. (2018), and Jiang et al. (2019). For simplicity, let us focus on the ramp-function surrogate loss with conï¬dence Î³ > 0, i.e., Lsurr Î³ ( f, z) B â„“Î³(M( f, z)), where â„“Î³(t) B 1(âˆ’âˆ,Î³](t) âˆ’t Î³ 1[0,Î³](t), t âˆˆR. Note that the ramp function â„“Î³ is 1/Î³-Lipschitz continuous. Using McDiarmidâ€™s inequality and a symmetrization argument similar to the proof of Theorem 1.21, combined with the contraction property of the Rademacher complexity, yields the following bound on the probability of misclassiï¬cation. With probability 1 âˆ’Î´ for every f âˆˆFa we have I[sgn( f (X)) , Y] â‰¤E  Lsurr Î³ ( f, Z)  â‰²1 m m Ã• i=1 Lsurr Î³ ( f, Z(i)) + Rm(Lsurr Î³ â—¦Fa) + r ln(1/Î´) m â‰²1 m m Ã• i=1 1(âˆ’âˆ,Î³)(Y(i) f (X(i))) + Rm(M â—¦Fa) Î³ + r ln(1/Î´) m = 1 m m Ã• i=1 1(âˆ’âˆ,Î³)(Y(i) f (X(i))) + Rm(Fa) Î³ + r ln(1/Î´) m . This shows the trade-oï¬€between the complexity of Fa measured by Rm(Fa) and the fraction of training data classiï¬ed correctly with a margin of at least Î³. In particular this suggests, that (even if we classify the training data correctly with respect to the 0â€“1 loss) it might be beneï¬cial to increase the complexity of Fa further, in order to simultaneously increase the margins by which the training data has been classiï¬ed correctly and thus obtain a better generalization bound. 1.2.3 Optimization and Implicit Regularization The optimization algorithm, which is usually a variant of SGD, seems to play an important role in generalization performance. Potential indicators for good gener- alization performance are high speed of convergence (Hardt et al., 2016) or ï¬‚atness of the local minimum to which SGD converges, which can be characterized by the magnitude of the eigenvalues of the Hessian (or approximately by the robustness of the minimizer to adversarial perturbations on the parameter space); see Keskar et al. 36 Berner et al. The Modern Mathematics of Deep Learning (2017). In Dziugaite and Roy (2017) and Neyshabur et al. (2017) generalization bounds depending on a concept of ï¬‚atness are established by employing a PAC- Bayesian framework, which can be viewed as a generalization of Theorem 1.19; see McAllester (1999). Further, one can also unite ï¬‚atness and norm-based bounds by the Fisherâ€“Rao metric of information geometry (Liang et al., 2019). Let us motivate the link between generalization and ï¬‚atness in the case of simple linear models: We assume that our model takes the form âŸ¨Î¸,Â·âŸ©, Î¸ âˆˆRd, and we will use the abbreviations r(Î¸) B bRs(âŸ¨Î¸,Â·âŸ©) and Î³(Î¸) B min iâˆˆ[m] M(âŸ¨Î¸,Â·âŸ©, z(i)) = min iâˆˆ[m] y(i)âŸ¨Î¸, x(i)âŸ© throughout this subsection to denote the empirical risk and the margin for given training data s = ((x(i), y(i)))m i=1. We assume that we are solving a classiï¬cation task with the 0â€“1 loss and that our training data is linearly separable. This means that there exists a minimizer Ë†Î¸ âˆˆRd such that r( Ë†Î¸) = 0. We observe that Î´-robustness in the sense that max Î¸ âˆˆBÎ´(0) r( Ë†Î¸ + Î¸) = r( Ë†Î¸) = 0 implies that 0 < min iâˆˆ[m] y(i)D Ë†Î¸ âˆ’Î´y(i) x(i) âˆ¥x(i)âˆ¥2 , x(i)E â‰¤Î³( Ë†Î¸) âˆ’Î´ min iâˆˆ[m] âˆ¥x(i)âˆ¥2 ; see also Poggio et al. (2017a). This lower bound on the margin Î³( Ë†Î¸) then ensures generalization guarantees, as described in Â§1.2.2. Even without explicit19 control on the complexity of Fa, there do exist results showing that SGD acts as an implicit regularization Neyshabur et al. (2014). This is motivated by linear models where SGD converges to the minimal Euclidean norm solution for a quadratic loss and in the direction of the hard-margin support vector machine solution for the logistic loss on linearly separable data (Soudry et al., 2018). Note that convergence to minimum-norm or maximum-margin solutions in particular decreases the complexity of our hypothesis set and thus improves generalization bounds; see Â§1.2.2. While we have seen this behavior of gradient descent for linear regression already in the more general context of kernel regression in Â§1.2.1, we want to motivate the corresponding result for classiï¬cation tasks as follows. We focus on the exponential surrogate loss Lsurr( f, z) = â„“(M( f, z)) = eâˆ’y f (x) with â„“(z) = eâˆ’z, but similar observations can be made for the logistic loss deï¬ned in Remark 1.9. We assume 19 Note also that diï¬€erent architectures can exhibit vastly diï¬€erent inductive biases (Zhang et al., 2020) and also that, within an architecture, diï¬€erent parameters have diï¬€erent degrees of importance; see Frankle and Carbin (2018), Zhang et al. (2019), and Proposition 1.29. 1.2 Generalization of Large Neural Networks 37 that the training data is linearly separable, which guarantees the existence of Ë†Î¸ , 0 with Î³( Ë†Î¸) > 0. Then for every linear model âŸ¨Î¸,Â·âŸ©, Î¸ âˆˆRd, it follows that Ë†Î¸,âˆ‡Î¸r(Î¸)âŸ©= 1 m m Ã• i=1 â„“â€²(y(i)âŸ¨Î¸, x(i)âŸ©) | {z } <0 y(i)âŸ¨Ë†Î¸, x(i)âŸ© | {z } >0 . A critical point âˆ‡Î¸r(Î¸) = 0 can therefore be approached if and only if for every i âˆˆ[m] we have â„“â€²(y(i)âŸ¨Î¸, x(i)âŸ©) = âˆ’eâˆ’y(i) âŸ¨Î¸,x(i)âŸ©â†’0, which is equivalent to âˆ¥Î¸âˆ¥2 â†’âˆand Î³(Î¸) > 0. Let us now deï¬ne rÎ²(Î¸) B â„“âˆ’1(r(Î²Î¸)) Î² , Î¸ âˆˆRd, Î² âˆˆ(0,âˆ), and observe that rÎ²(Î¸) = âˆ’log(r(Î²Î¸)) Î² â†’Î³(Î¸), Î² â†’âˆ. (1.26) Owing to this property, rÎ² is often referred to as the smoothed margin (Lyu and Li, 2019; Ji and Telgarsky, 2019b). We evolve Î¸ according to gradient ï¬‚ow with respect to the smoothed margin r1, i.e., dÎ¸(t) dt = âˆ‡Î¸r1(Î¸(t)) = âˆ’ 1 r(Î¸(t))âˆ‡Î¸r(Î¸(t)), which produces the same trajectory as gradient ï¬‚ow with respect to the empirical risk r under a rescaling of the time t. Looking at the evolution of the normalized parameters ËœÎ¸(t) = Î¸(t)/âˆ¥Î¸(t)âˆ¥2, the chain rule establishes that d ËœÎ¸(t) dt = P ËœÎ¸(t) âˆ‡Î¸rÎ²(t)( ËœÎ¸(t)) Î²(t) with Î²(t) B âˆ¥Î¸(t)âˆ¥2 and PÎ¸ B Id âˆ’Î¸Î¸T, Î¸ âˆˆRd. This shows that the normalized parameters perform projected gradient ascent with respect to the function rÎ²(t), which converges to the margin thanks to (1.26) and the fact that Î²(t) = âˆ¥Î¸(t)âˆ¥2 â†’âˆwhen approaching a critical point. Thus, during gradient ï¬‚ow, the normalized parameters implicitly maximize the margin. See Gunasekar et al. (2018a), Gunasekar et al. (2018b), Lyu and Li (2019), Nacson et al. (2019), Chizat and Bach (2020), and Ji and Telgarsky (2020) for a precise analysis and various extensions, for example, to homogeneous or two-layer NNs and other optimization geometries. To illustrate one particular research direction, we now present a result by way of example. Let Î¦ = Î¦(N,Ï±) be a biasless NN with parameters Î¸ = ((W(â„“),0))L â„“=0 and output dimension NL = 1. For given input features x âˆˆRN0, the gradient 38 Berner et al. The Modern Mathematics of Deep Learning âˆ‡W (â„“)Î¦ = âˆ‡W (â„“)Î¦(x,Î¸) âˆˆRNâ„“âˆ’1Ã—Nâ„“with respect to the weight matrix in the â„“th layer satisï¬es that âˆ‡W (â„“)Î¦ = Ï±(Î¦(â„“âˆ’1)) âˆ‚Î¦ âˆ‚Î¦(â„“+1) âˆ‚Î¦(â„“+1) âˆ‚Î¦(â„“) = Ï±(Î¦(â„“âˆ’1)) âˆ‚Î¦ âˆ‚Î¦(â„“+1)W(â„“+1) diag  Ï±â€²(Î¦(â„“)), where the pre-activations (Î¦(â„“))L â„“=1 are as in (1.1). Evolving the parameters ac- cording to gradient ï¬‚ow as in (1.20) and using an activation function Ï± with Ï±(x) = Ï±â€²(x)x, such as the ReLU, this implies that diag  Ï±â€²(Î¦(â„“))W(â„“)(t) dW(â„“)(t) dt T = dW(â„“+1)(t) dt T W(â„“+1)(t) diag  Ï±â€²(Î¦(â„“)). (1.27) Note that this ensures the conservation of balancedness between the weight matrices of adjacent layers, i.e., d dt  âˆ¥W(â„“+1)(t)âˆ¥2 F âˆ’âˆ¥W(â„“)(t)âˆ¥2 F  = 0, see Du et al. (2018a). Furthermore, for deep linear NNs, i.e., Ï±(x) = x, the property in (1.27) implies conservation of alignment of the left and right singular spaces W(â„“) and W(â„“+1). This can then be used to show the implicit preconditioning and convergence of gradient descent (Arora et al., 2018a, 2019a) and that, under addi- tional assumptions, gradient descent converges to a linear predictor that is aligned with the maximum margin solution (Ji and Telgarsky, 2019a). 1.2.4 Limits of Classical Theory and Double Descent There is ample evidence that classical tools from statistical learning theory alone, such as Rademacher averages, uniform convergence, or algorithmic stability, may be unable to explain the full generalization capabilities of NNs (Zhang et al., 2017; Nagarajan and Kolter, 2019). It is especially hard to reconcile the classical biasâ€“ variance trade-oï¬€with the observation of good generalization performance when achieving zero empirical risk on noisy data using a regression loss. On top of that, this behavior of overparametrized models in the interpolation regime turns out not to be unique to NNs. Empirically, one observes for various methods (decision trees, random features, linear models) that the test error decreases even below the sweet-spot in the âˆª-shaped biasâ€“variance curve when the number of parameters is increased further (Belkin et al., 2019b; Geiger et al., 2020; Nakkiran et al., 2020). This is often referred to as the double descent curve or benign overï¬tting; see Figure 1.6. For special cases, for example linear regression or random feature regression, such behavior can even be proven; see Hastie et al. (2019), Mei and Montanari (2019), Bartlett et al. (2020), Belkin et al. (2020), and Muthukumar et al. (2020). 1.2 Generalization of Large Neural Networks 39 Figure 1.6 This illustration shows the classical, underparametrized regime in green, where the âˆª-shaped curve depicts the biasâ€“variance trade-oï¬€as explained in Â§1.1.2. Starting with a complexity of our algorithm A larger than the interpolation threshold we can achieve zero empirical risk bRs(fs) (the training error), where fs = A(s). Within this modern interpolation regime, the risk R(fs) (the test error) might be even lower than at the classical sweet spot. Whereas complexity(A) traditionally refers to the complexity of the hypothesis set F, there is evidence that the optimization scheme and the data also inï¬‚uence the complexity, leading to deï¬nitions such as complexity(A) B max  m âˆˆN: E  bRS(A(S))  â‰¤Îµ with S âˆ¼Im Z , for suitable Îµ > 0 (Nakkiran et al., 2020). This illustration is based on Belkin et al. (2019b). In the following we analyze this phenomenon in the context of linear regression. Speciï¬cally, we focus on a prediction task with quadratic loss, input features given by a centered Rd-valued random variable X, and labels given by Y = âŸ¨Î¸âˆ—, XâŸ©+ Î½, where Î¸âˆ—âˆˆRd and Î½ is a centered random variable that is independent of X. For training data S = ((X(i),Y(i)))m i=1, we consider the empirical risk minimizer bfS = âŸ¨Ë†Î¸,Â·âŸ©with minimum Euclidean norm of its parameters Ë†Î¸ or, equivalently, we can consider the limit of gradient ï¬‚ow with zero initialization. Using (1.5) and a biasâ€“variance decomposition we can write E[R( bfS)|(X(i))m i=1] âˆ’Râˆ—= E[âˆ¥bfS âˆ’f âˆ—âˆ¥L2(IX)|(X(i))m i=1] = (Î¸âˆ—)T P E[XXT]PÎ¸âˆ—+ E[Î½2] Tr Î£+ E[XXT], where Î£ B Ãm i=1 X(i)(X(i))T, Î£+ denotes the Mooreâ€“Penrose inverse of Î£, and P B Id âˆ’Î£+Î£ is the orthogonal projector onto the kernel of Î£. For simplicity, we focus on the variance Tr Î£+ E[XXT], which can be viewed as the result of setting Î¸âˆ—= 0 and E[Î½2] = 1. Assuming that X has i.i.d. entries with unit variance and bounded ï¬fth moment, the distribution of the eigenvalues of 1 mÎ£+ in the limit d,m â†’âˆwith d m â†’Îº âˆˆ(0,âˆ) can be described via the Marchenkoâ€“Pastur law. 40 Berner et al. The Modern Mathematics of Deep Learning 0 50 100 150 d 0.0 0.5 1.0 variance Figure 1.7 The expected variance of the linear regression in (1.29) with d âˆˆ[150] and Xi âˆ¼ U({âˆ’1, 1}), i âˆˆ[150], where Xi = X1 for i âˆˆ{10, . . . , 20} âˆª{30, . . . , 50} and all other coordinates are independent. Therefore, the asymptotic variance can be computed explicitly as Tr Î£+ E[XXT] â†’1 âˆ’max{1 âˆ’Îº,0} |1 âˆ’Îº| for d,m â†’âˆ with d m â†’Îº, almost surely; see Hastie et al. (2019). This shows that despite interpolating the data we can decrease the risk in the overparametrized regime Îº > 1. In the limit d,m â†’âˆ, such benign overï¬tting can also be shown for more general settings (including lazy training of NNs), some of which even achieve their optimal risk in the overparametrized regime (Mei and Montanari, 2019; Montanari and Zhong, 2020; Lin and Dobriban, 2021). For normally distributed input features X such that E[XXT] has rank larger than m, one can also compute the behavior of the variance in the non-asymptomatic regime (Bartlett et al., 2020). Deï¬ne kâˆ—:= min n k â‰¥0: Ã i>k Î»i Î»k+1 â‰¥cm o , (1.28) where Î»1 â‰¥Î»2 â‰¥Â· Â· Â· â‰¥Î»d â‰¥0 are the eigenvalues of E[XXT] in decreasing order and c âˆˆ(0,âˆ) is a universal constant. Assuming that kâˆ—/m is suï¬ƒciently small, with high probability we have Tr Î£+ E[XXT] â‰ˆkâˆ— m + m Ã i>kâˆ—Î»2 i (Ã i>kâˆ—Î»i)2 . This precisely characterizes the regimes for benign overï¬tting in terms of the eigenvalues of the covariance matrix E[XXT]. Furthermore, it shows that adding new input feature coordinates and thus increasing the number of parameters d can lead to either an increase or a decrease in the risk. 1.3 The Role of Depth in the Expressivity of Neural Networks 41 To motivate this phenomenon, which is considered in much more depth in Chen et al. (2020), let us focus on a single sample m = 1 and features X that take values in X = {âˆ’1,1}d. Then it follows that Î£+ = X(1)(X(1))T âˆ¥X(1)âˆ¥4 = X(1)(X(1))T d2 and thus E  Tr Î£+E[XXT] = 1 d2 E  XXT  2 F. (1.29) In particular, this shows that by incrementing the input feature dimensions via d 7â†’d + 1 one can increase or decrease the risk depending on the correlation of the coordinate Xd+1 with respect to the previous coordinates (Xi)d i=1; see also Figure 1.7. Generally speaking, overparametrization and the perfect ï¬tting of noisy data does not exclude good generalization performance; see also Belkin et al. (2019a). How- ever, the risk crucially depends on the data distribution and the chosen algorithm. 1.3 The Role of Depth in the Expressivity of Neural Networks The approximation-theoretic aspect of a NN architecture, which is responsible for the approximation component Îµapprox B R( f âˆ— F)âˆ’Râˆ—of the error R( fS)âˆ’Râˆ—in (1.7), is probably one of the most well-studied parts of the deep learning pipe-line. The achievable approximation error of an architecture directly describes the power of the architecture. As mentioned in Â§1.1.3, many classical approaches study the approximation theory of NNs with only a few layers, whereas modern architectures are typically very deep. A ï¬rst observation about the eï¬€ect of depth is that it can often compensate for insuï¬ƒcient width. For example, in the context of the universal approximation theorem, it has been shown that very narrow NNs are still universal if, instead of increasing the width, the number of layers can be chosen arbitrarily (Hanin and Sellke, 2017; Hanin, 2019; Kidger and Lyons, 2020). However, if the width of a NN falls below a critical number, then the universality will no longer hold. Below, we discuss three additional observations that shed light on the eï¬€ect of depth on the approximation capacities, or alternative notions of expressivity, of NNs. 1.3.1 Approximation of Radial Functions One technique to study the impact of depth relies on the construction of speciï¬c functions which can be well approximated by NNs of a certain depth, but require 42 Berner et al. The Modern Mathematics of Deep Learning signiï¬cantly more parameters when approximated to the same accuracy by NNs of smaller depth. In the following we present one example of this type of approach, which can be found in Eldan and Shamir (2016). Theorem 1.24 (Power of depth). Let Ï± âˆˆ{Ï±R, Ï±Ïƒ,1(0,âˆ)} be the ReLU, the logistic, or the Heaviside function. Then there exist constants c,C âˆˆ(0,âˆ) with the following property. For every d âˆˆN with d â‰¥C there exist a probability measure Âµ on Rd, a three-layer NN architecture a = (N, Ï±) = ((d, N1, N2,1), Ï±) with âˆ¥Nâˆ¥âˆâ‰¤Cd5, and corresponding parameters Î¸âˆ—âˆˆRP(N) with âˆ¥Î¸âˆ—âˆ¥âˆâ‰¤CdC and âˆ¥Î¦a(Â·,Î¸âˆ—)âˆ¥Lâˆ(Rd) â‰¤ 2 such that for every n â‰¤cecd we have inf Î¸ âˆˆRP((d,n,1)) âˆ¥Î¦((d,n,1),Ï±)(Â·,Î¸) âˆ’Î¦a(Â·,Î¸âˆ—)âˆ¥L2(Âµ) â‰¥c. In fact, the activation function in Theorem 1.24 is required to satisfy only mild conditions and the result holds, for instance, also for more general sigmoidal func- tions. The proof of Theorem 1.24 is based on the construction of a suitable radial function g: Rd â†’R, i.e., g(x) = Ëœg(âˆ¥xâˆ¥2 2) for some Ëœg: [0,âˆ) â†’R, which can be eï¬ƒciently approximated by three-layer NNs but for which approximation by only a two-layer NN requires exponentially large complexity, i.e., a width that is exponential in d. The ï¬rst observation of Eldan and Shamir (2016) was that g can typically be well approximated on a bounded domain by a three-layer NN, if Ëœg is Lipschitz continuous. Indeed, for the ReLU activation function it is not diï¬ƒcult to show that, emulating a linear interpolation, one can approximate a univariate C-Lipschitz function uniformly on [0,1] up to precision Îµ by a two-layer architecture of width O(C/Îµ). The same holds for smooth, non-polynomial activation functions, owing to Theorem 1.18. This implies that the squared Euclidean norm, as a sum of d univariate functions, i.e., [0,1]d âˆ‹x 7â†’Ãd i=1 x2 i , can be approximated up to precision Îµ by a two-layer architecture of width O(d2/Îµ). Moreover, this shows that the third layer can eï¬ƒciently approximate Ëœg, establishing the approximation of g on a bounded domain up to precision Îµ using a three-layer architecture with the number of parameters polynomial in d/Îµ. The second step of Eldan and Shamir (2016) was to choose g in such a way that the realization of any two-layer neural network Î¦ = Î¦((d,n,1),Ï±)(Â·,Î¸) with width n and not exponential in d is on average (with respect to the probability measure Âµ) a constant distance away from g. Their argument is heavily based on ideas from Fourier analysis and will be outlined below. In this context, let us recall that we denote by Ë†f the Fourier transform of a suitable function, or, more generally, tempered distribution. f . Assuming that the square root Ï• of the density function associated with the probability measure Âµ, as well as Î¦ and g, are well behaved, the Plancherel theorem 1.3 The Role of Depth in the Expressivity of Neural Networks 43 Figure 1.8 This illustration shows the largest possible support (blue) of c Î¦Ï•, where Ë†Ï• = 1Br (0) and Î¦ is a shallow neural network with architecture N = (2, 4, 1) and weight matrix W (1) = [w1 Â· Â· Â· w4]T in the ï¬rst layer. Any radial function with too much of its L2-mass located at high frequencies (indicated in red) cannot be well approximated by Î¦Ï•. yields âˆ¥Î¦ âˆ’gâˆ¥2 L2(Âµ) = âˆ¥Î¦Ï• âˆ’gÏ•âˆ¥2 L2(Rd) = c Î¦Ï• âˆ’c gÏ• 2 L2(Rd). (1.30) Next, the speciï¬c structure of two-layer NNs is used, which implies that for every j âˆˆ[n] there exists wj âˆˆRd with âˆ¥wjâˆ¥2 = 1 and Ï±j : R â†’R (subsuming the activation function Ï±, the norm of wj, and the remaining parameters corresponding to the jth neuron in the hidden layer) such that Î¦ is of the form Î¦ = n Ã• j=1 Ï±j(âŸ¨wj,Â·âŸ©) = n Ã• j=1 (Ï±j âŠ—1Rdâˆ’1) â—¦Rwj . (1.31) The second equality follows by viewing the action of the jth neuron as a tensor product of Ï±j and the indicator function 1Rdâˆ’1(x) = 1, x âˆˆRdâˆ’1, composed with a d-dimensional rotation Rwj âˆˆSO(d) which maps wj to the ï¬rst standard basis vector e(1) âˆˆRd. Noting that the Fourier transform respects linearity, rotations, and tensor products, we can compute Ë†Î¦ = n Ã• j=1 ( Ë†Ï±j âŠ—Î´Rdâˆ’1) â—¦Rwj, where Î´Rdâˆ’1 denotes the Dirac distribution on Rdâˆ’1. In particular, the support of Ë†Î¦ has a particular star-like shape, namely Ãn j=1 span{wj}, which represent lines passing through the origin. Now we choose Ï• to be the inverse Fourier transform of the indicator function of a ball Br(0) âŠ‚Rd with vol(Br(0)) = 1, ensuring that Ï•2 is a valid probability 44 Berner et al. The Modern Mathematics of Deep Learning density for Âµ as Âµ(Rd) = âˆ¥Ï•2âˆ¥L1(Rd) = âˆ¥Ï•âˆ¥2 L2(Rd) = âˆ¥Ë†Ï•âˆ¥2 L2(Rd) = âˆ¥1Br (0)âˆ¥2 L2(Rd) = 1. Using the convolution theorem, this choice of Ï• yields that supp( c Î¦Ï•) = supp( Ë†Î¦ âˆ—Ë†Ï•) âŠ‚ n Ã˜ j=1  span{wj} + Br(0) . Thus the lines passing through the origin are enlarged to tubes. It is this particular shape which allows the construction of some g such that âˆ¥c Î¦Ï• âˆ’c gÏ•âˆ¥2 L2(Rd) can be suitably lower bounded; see also Figure 1.8. Intriguingly, the peculiar behavior of high-dimensional sets now comes into play. Owing to the well-known concentration of measure principle, the variable n needs to be exponentially large for the set Ãn j=1  span{wj} + Br(0) not to be sparse. If it is smaller, one can construct a function g such that the main energy content of c gÏ• has a certain distance from the origin, yielding a lower bound for âˆ¥c Î¦Ï• âˆ’c gÏ•âˆ¥2 and hence âˆ¥Î¦ âˆ’gâˆ¥2 L2(Âµ); see (1.30). One key technical problem is the fact that such a behavior for Ë†g does not immediately imply a similar behavior for c gÏ•, requiring a quite delicate construction of g. 1.3.2 Deep ReLU Networks Perhaps for no activation function is the eï¬€ect of depth clearer than for the ReLU activation function Ï±R(x) = max{0, x}. We refer to the corresponding NN architec- tures (N, Ï±R) as ReLU (neural) networks (ReLU NNs). A two-layer ReLU NN with one-dimensional input and output is a function of the form Î¦(x) = n Ã• i=1 w(2) i Ï±R(w(1) i x + b(1) i ) + b(2), x âˆˆR, where w(1) i ,w(2) i , b(1) i , b(2) âˆˆR for i âˆˆ[n]. It is not hard to see that Î¦ is a continuous piecewise aï¬ƒne linear function. Moreover, Î¦ has at most n + 1 aï¬ƒne linear pieces. On the other hand, notice that the hat function h: [0,1] â†’[0,1], x 7â†’2Ï±R(x) âˆ’4Ï±R(x âˆ’1 2) = ( 2x, if 0 â‰¤x < 1 2, 2(1 âˆ’x), if 1 2 â‰¤x â‰¤1, (1.32) is a NN with two layers and two neurons. Telgarsky (2015) observed that the n-fold convolution hn(x) B h â—¦Â· Â· Â· â—¦h produces a sawtooth function with 2n spikes. In particular, hn admits 2n aï¬ƒne linear pieces with only 2n many neurons. In this case, we see that deep ReLU NNs are in some sense exponentially more eï¬ƒcient in generating aï¬ƒne linear pieces. 1.3 The Role of Depth in the Expressivity of Neural Networks 45 0 1 8 2 8 3 8 4 8 5 8 6 8 7 8 1 0 1 32 2 32 3 32 4 32 5 32 6 32 7 32 8 32 g I1 g I1 I2 I1 g I2 I3 I2 Figure 1.9 Interpolation In of [0, 1] âˆ‹x 7â†’g(x) B x âˆ’x2 on 2n + 1 equidistant points, which can be represented as a sum In = Ãn k=1 Ik âˆ’Ikâˆ’1 = Ãn k=1 hk/22k of n sawtooth functions. Each sawtooth function hk = hkâˆ’1 â—¦h in turn can be written as a k-fold composition of a hat function h. This illustration is based on ElbrÃ¤chter et al. (2019). Moreover, it was noted in Yarotsky (2017) that the diï¬€erence in interpolations of [0,1] âˆ‹x 7â†’x âˆ’x2 at 2n + 1 and 2nâˆ’1 + 1 equidistant points equals the scaled sawtooth function hn/22n; see Figure 1.9. This permits eï¬ƒcient implementation of approximative squaring and, by polarization, also of approximate multiplication using ReLU NNs. Composing these simple functions one can approximate localized Taylor polynomials and thus smooth functions; see Yarotsky (2017). We state below a generalization (GÃ¼hring et al., 2020) of Yarotskyâ€™s result which includes more general norms, but which for p = âˆand s = 0 coincides with his original result. Theorem 1.25 (Approximation of Sobolev-regular functions). Let d, k âˆˆN with k â‰¥2, let p âˆˆ[1,âˆ], s âˆˆ[0,1], B âˆˆ(0,âˆ), and let Ï± be a piecewise-linear activation function with at least one break point. Then there exists a constant c âˆˆ(0,âˆ) with the following property. For every Îµ âˆˆ(0,1/2) there exists a NN architecture a = (N, Ï±) with P(N) â‰¤cÎµâˆ’d/(kâˆ’s) log(1/Îµ) such that for every function g âˆˆWk,p((0,1)d) with âˆ¥gâˆ¥W k, p((0,1)d) â‰¤B we have inf Î¸ âˆˆRP(N) âˆ¥Î¦a(Î¸,Â·) âˆ’gâˆ¥W s, p((0,1)d) â‰¤Îµ. The ability of deep ReLU neural networks to emulate multiplication has also been employed to reapproximate wide ranges of high-order ï¬nite-element spaces. In Opschoor et al. (2020) and Marcati et al. (2020) it was shown that deep ReLU neural networks are capable of achieving the approximation rates of hp-ï¬nite-element methods. Concretely, this means that for piecewise analytic functions, which appear, for example, as solutions of elliptic boundary and eigenvalue problems with analytic data, exponential approximation rates can be achieved. In other words, the number 46 Berner et al. The Modern Mathematics of Deep Learning depth width Figure 1.10 Standard feed-forward neural network. For certain approximation results, depth and width need to be in a ï¬xed relationship to achieve optimal results. of parameters of neural networks needed to approximate such a function in the W1,2-norm up to an error of Îµ is logarithmic in Îµ. Theorem 1.25 requires the depth of the NN to grow. In fact, it can be shown that the same approximation rate cannot be achieved with shallow NNs. Indeed, there exists a certain optimal number of layers, and if the architecture has fewer layers than optimal then the NNs need to have signiï¬cantly more parameters to achieve the same approximation ï¬delity. This has been observed in many diï¬€erent settings in Liang and Srikant (2017), Safran and Shamir (2017), Yarotsky (2017), Petersen and Voigtlaender (2018), andElbrÃ¤chter et al. (2019). We state here Yarotskyâ€™s result: Theorem 1.26 (Depthâ€“width approximation trade-oï¬€). Let d, L âˆˆN with L â‰¥2 and let g âˆˆC2([0,1]d) be a function that is not aï¬ƒne linear. Then there exists a constant c âˆˆ(0,âˆ) with the following property. For every Îµ âˆˆ(0,1) and every ReLU NN architecture a = (N, Ï±R) = ((d, N1,. . ., NLâˆ’1,1), Ï±R) with L layers and âˆ¥Nâˆ¥1 â‰¤cÎµâˆ’1/(2(Lâˆ’1)) neurons it follows that inf Î¸ âˆˆRP(N) âˆ¥Î¦a(Â·,Î¸) âˆ’gâˆ¥Lâˆ([0,1]d) â‰¥Îµ. This results is based on the observation that ReLU NNs are piecewise aï¬ƒne linear. The number of pieces they admit is linked to their capacity of approximating functions that have non-vanishing curvature. Using a construction similar to the example at the beginning of this subsection, it can be shown that the number of pieces that can be generated using an architecture ((1, N1,. . ., NLâˆ’1,1), Ï±R) scales roughly as ÃLâˆ’1 â„“=1 Nâ„“. In the framework of the aforementioned results, we can speak of a depthâ€“width trade-oï¬€; see also Figure 1.10. A ï¬ne-grained estimate of achievable rates for freely varying depths was also established in Shen (2020). 1.3 The Role of Depth in the Expressivity of Neural Networks 47 1.3.3 Alternative Notions of Expressivity Conceptual approaches to studying the approximation power of deep NNs beyond the classical approximation framework usually aim to relate structural properties of the NN to the â€œrichnessâ€ of the set of possibly expressed functions. One early result in this direction was by MontÃºfar et al. (2014) who described bounds on the number of aï¬ƒne linear regions of a ReLU NN Î¦(N,Ï±R)(Â·,Î¸). In a simpliï¬ed setting, we already saw estimates on the number of aï¬ƒne linear pieces at the beginning of Â§1.3.2. Aï¬ƒne linear regions can be deï¬ned as the connected components of RN0\H, where H is the set of non-diï¬€erentiable parts of the realization20 Î¦(N,Ï±R)(Â·,Î¸). A reï¬ned analysis on the number of such regions was conducted, for example, by Hinz and van de Geer (2019). It was found that deep ReLU neural networks can exhibit signiï¬cantly more of such regions than of their shallow counterparts. The reason for this eï¬€ectiveness of depth is described by the following analogy. Through the ReLU each neuron Rd âˆ‹x 7â†’Ï±R(âŸ¨x,wâŸ©+ b), w âˆˆRd, b âˆˆR, splits the space into two aï¬ƒne linear regions separated by the hyperplane {x âˆˆRd : âŸ¨x,wâŸ©+ b = 0}. (1.33) A shallow ReLU NN Î¦((d,n,1),Ï±R)(Â·,Î¸) with n neurons in the hidden layer therefore produces a number of regions deï¬ned through n hyperplanes. Using classical bounds on the number of regions deï¬ned through hyperplane arrangements (Zaslavsky, 1975), one can bound the number of aï¬ƒne linear regions by Ãd j=0  n j . Deepening the neural networks then corresponds to a certain folding of the input space. Through this interpretation it can be seen that composing NNs can lead to a multiplication of the number of regions of the individual NNs, resulting in an exponential eï¬ƒciency of deep neural networks in generating aï¬ƒne linear regions.21 This approach was further developed in Raghu et al. (2017) to a framework to study expressivity that to some extent allows to include the training phase. One central object studied in Raghu et al. (2017) are so-called trajectory lengths. In this context, one analyzes how the length of a non-constant curve in the input space changes in expectation through the layers of a NN. The authors found an exponential dependence of the expected curve length on the depth. Let us motivate this in the 20 One can also study the potentially larger set of activation regions given by the connected components of RN0 \   ÃLâˆ’1 â„“=1 ÃNâ„“ i=1 Hi,â„“ , where Hi,â„“B {x âˆˆRN0 : Î¦(â„“) i (x, Î¸) = 0}, with Î¦(â„“) i as in (1.1), is the set of non-diï¬€erentiable parts of the activation of the ith neuron in the â„“th layer. In contrast with the linear regions, the activation regions are necessarily convex (Raghu et al., 2017; Hanin and Rolnick, 2019). 21 However, to exploit this eï¬ƒciency with respect to the depth, one requires highly oscillating pre-activations snd this in turn can only be achieved with a delicate selection of parameters. In fact, it can be shown that through random initialization the expected number of activation regions per unit cube depends mainly on the number of neurons in the NN, rather than its depth (Hanin and Rolnick, 2019). 48 Berner et al. The Modern Mathematics of Deep Learning input curve no hidden layer 3 hidden layers 10 hidden layers Figure 1.11 Shape of the trajectory t 7â†’Î¦((2,n,...,n,2),Ï±R)(Î³(t), Î¸) of the output of a randomly initialized network with 0, 3, or 10 hidden layers. The input curve Î³ is the circle given in the leftmost image. The hidden layers have n = 20 neurons and the variance of the initialization is taken as 4/n. special case of a ReLU NN with architecture a = ((N0,n,. . .,n, NL), Ï±R) and depth L âˆˆN. Given a non-constant continuous curve Î³ : [0,1] â†’RN0 in the input space, the length of the trajectory in the â„“th layer of the NN Î¦a(Â·,Î¸) is then given by Length( Â¯Î¦(â„“)(Î³(Â·),Î¸)), â„“âˆˆ[L âˆ’1], where Â¯Î¦(â„“)(Â·,Î¸) is the activation in the â„“th layer; see (1.1). Here the length of the curve is well deï¬ned since Â¯Î¦(â„“)(Â·,Î¸)) is continuous and therefore Â¯Î¦(â„“)(Î³(Â·),Î¸) is continuous. Now, let the parameters Î˜1 of the NN Î¦a be initialized independently in such a way that the entries corresponding to the weight matrices and bias vectors follow a normal distribution with zero mean and variances 1/n and 1, respectively. It is not hard to see, for example by Proposition 1.17, that the probability that Â¯Î¦(â„“)(Â·,Î˜1) will map Î³ to a non-constant curve is positive and hence, for ï¬xed â„“âˆˆ[L âˆ’1], E  Length( Â¯Î¦(â„“)(Î³(Â·),Î˜1))  = c > 0. Let Ïƒ âˆˆ(0,âˆ) and consider a second initialization Î˜Ïƒ, where we have changed the variances of the entries corresponding to the weight matrices and bias vectors to Ïƒ2/n and Ïƒ2, respectively. Recall that the ReLU is positively homogeneous, i.e., we have that Ï±R(Î»x) = Î»Ï±R(x) for all Î» âˆˆ(0,âˆ). Then it is clear that Â¯Î¦(â„“)(Â·,Î˜Ïƒ) âˆ¼Ïƒâ„“Â¯Î¦(â„“)(Â·,Î˜1), i.e., the activations corresponding to the two initialization strategies are identically distributed up to the factor Ïƒâ„“. Therefore, we immediately conclude that E  Length( Â¯Î¦(â„“)(Î³(Â·),Î˜Ïƒ))  = Ïƒâ„“c. This shows that the expected trajectory length depends exponentially on the depth of the NN, which is in line with the behavior of other notions of expressivity (Poole 1.4 Deep Neural Networks Overcome the Curse of Dimensionality 49 M Figure 1.12 Illustration of a one-dimensional manifold M embedded in R3. For every point x âˆˆM there exists a neighborhood in which the manifold can be linearly projected onto its tangent space at x such that the corresponding inverse function is diï¬€erentiable. et al., 2016). In Raghu et al. (2017) this result is also extended to a tanh activation function and the constant c is more carefully resolved. Empirically one also ï¬nds that the shapes of the trajectories become more complex in addition to becoming longer on average; see Figure 1.11. 1.4 Deep Neural Networks Overcome the Curse of Dimensionality In Â§1.1.3, one of the main puzzles of deep learning that we identiï¬ed was the surprising performance of deep architectures on problems where the input dimen- sions are very high. This performance cannot be explained in the framework of classical approximation theory, since such results always suï¬€er from the curse of dimensionality (Bellman, 1952; DeVore, 1998; Novak and WoÅºniakowski, 2009). In this section, we present three approaches that oï¬€er explanations of this phe- nomenon. As before, we have had to omit certain ideas which have been very inï¬‚uential in the literature to keep the length of this section under control. In par- ticular, an important line of reasoning is that functions to be approximated often have compositional structures which NNs may approximate very well, as reviewed in Poggio et al. (2017b). Note that also a suitable feature descriptor, factoring out invariances, might lead to a signiï¬cantly reduced eï¬€ective dimension; see Â§1.7.1. 1.4.1 Manifold Assumption A ï¬rst remedy for the high-dimensional curse of dimensionality is what we call the manifold assumption. Here it is assumed that we are trying to approximate a 50 Berner et al. The Modern Mathematics of Deep Learning function g: Rd âŠƒX â†’R, where d is very large. However, we are not seeking to optimize with respect to the uniform norm or a regular Lp space; instead, we consider a measure Âµ which is supported on a dâ€²-dimensional manifold M âŠ‚X. Then the error is measured in the Lp(Âµ)-norm. Here we consider the case where dâ€² â‰ªd. This setting is appropriate if the data z = (x, y) of a prediction task is generated from a measure supported on M Ã— R. This set-up or generalizations thereof was fundamental in Chui and Mhaskar (2018), Shaham et al. (2018), Chen et al. (2019), Schmidt-Hieber (2019), Cloninger and Klock (2020), Nakada and Imaizumi (2020). Let us outline an example-based approach, where we consider locally Ck-regular functions and NNs with ReLU activation functions below. (i) The regularity of g on the manifold is described. Naturally, we need to quantify the regularity of the function g restricted to M in an adequate way. The typical approach would be to make a deï¬nition via local coordinate charts. If we assume that M is an embedded submanifold of X, then locally, i.e., in a neighborhood of a point x âˆˆM, the orthogonal projection of M onto the dâ€²-dimensional tangent spaceTxM is a diï¬€eomorphism. The situation is depicted in Figure 1.12. Assuming M to be compact, we can choose a ï¬nite set of open balls (Ui)p i=1 that cover M and on which the local projections Î³i onto the respective tangent spaces as described above exists and are diï¬€eomorphisms. Now we can deï¬ne the regularity of g via classical regularity. In this example, we say that g âˆˆCk(M) if g â—¦Î³âˆ’1 i âˆˆCk(Î³i(M âˆ©Ui)) for all i âˆˆ[p]. (ii) Localization and charts are constructed via neural networks. According to the construction of local coordinate charts in Step (i), we can write g as follows: g(x) = p Ã• i=1 Ï†i(x)  g â—¦Î³âˆ’1 i (Î³i(x))  C p Ã• i=1 Ëœgi(Î³i(x), Ï†i(x)), x âˆˆM, (1.34) where Ï†i is a partition of unity such that supp(Ï†i) âŠ‚Ui. Note that Î³i is a linear map, hence representable by a one-layer NN. Since multiplication is a smooth operation, we have that if g âˆˆCk(M) then Ëœgi âˆˆCk(Î³i(M âˆ©Ui) Ã— [0,1]). The partition of unity Ï†i needs to be emulated by NNs. For example, if the activation function is the ReLU, then such a partition can be eï¬ƒciently constructed. Indeed, in He et al. (2020) it was shown that such NNs can represent linear ï¬nite elements exactly with ï¬xed-size NNs, and hence a partition of unity subordinate to any given covering of M can be constructed. (iii) A classical approximation result is used on the localized functions. By some 1.4 Deep Neural Networks Overcome the Curse of Dimensionality 51 form of Whitneyâ€™s extension theorem (Whitney, 1934), we can extend each Ëœgi to a function Â¯gi âˆˆCk(X Ã— [0,1]) which by classical results can be approximated up to an error of Îµ > 0 by NNs of size O(Îµâˆ’(dâ€²+1)/k) for Îµ â†’0; see Mhaskar (1996), Yarotsky (2017), and Shaham et al. (2018). (iv) The compositionality of neural networks is used to build the ï¬nal network. We have seen that every component in the representation (1.34), i.e., Ëœgi, Î³i, and Ï†i, can be eï¬ƒciently represented by NNs. In addition, composition and summation are operations which can directly be implemented by NNs through increasing their depth and widening their layers. Hence (1.34) is eï¬ƒciently â€“ i.e., with a rate depending only on dâ€² instead of the potentially much larger d â€“ approximated by a NN. Overall, we see that NNs are capable of learning local coordinate transformations and therefore of reducing the complexity of a high-dimensional problem to the underlying low-dimensional problem given by the data distribution. 1.4.2 Random Sampling As early as 1992, Andrew Barron showed that, under certain seemingly very nat- ural assumptions on the function to be approximated, a dimension-independent approximation rate by NNs can be achieved (Barron, 1992, 1993). Speciï¬cally, the assumption is formulated as a condition on the Fourier transform of a function, and the result is as follows. Theorem 1.27 (Approximation of Barron-regular functions). Let Ï±: R â†’R be the ReLU or a sigmoidal function. Then there exists a constant c âˆˆ(0,âˆ) with the following property. For every d,n âˆˆN, every probability measure Âµ supported on B1(0) âŠ‚Rd, and every g âˆˆL1(Rd) with Cg B âˆ« Rd âˆ¥Î¾âˆ¥2| Ë†g(Î¾)| dÎ¾ < âˆit follows that inf Î¸ âˆˆRP((d,n,1)) âˆ¥Î¦((d,n,1),Ï±)(Â·,Î¸) âˆ’gâˆ¥L2(Âµ) â‰¤c âˆšnCg . Note that the L2-approximation error can be replaced by an Lâˆ-estimate over the unit ball at the expense of a factor of the order of âˆš d on the right-hand side. The key idea behind Theorem 1.27 is the following application of the law of large numbers. First, we observe that, as per the assumption, g can be represented via the 52 Berner et al. The Modern Mathematics of Deep Learning inverse Fourier transform as g âˆ’g(0) = âˆ« Rd Ë†g(Î¾)(e2Ï€iâŸ¨Â·,Î¾âŸ©âˆ’1) dÎ¾ = Cg âˆ« Rd 1 âˆ¥Î¾âˆ¥2 (e2Ï€iâŸ¨Â·,Î¾ âŸ©âˆ’1) 1 Cg âˆ¥Î¾âˆ¥2 Ë†g(Î¾) dÎ¾ = Cg âˆ« Rd 1 âˆ¥Î¾âˆ¥2 (e2Ï€iâŸ¨Â·,Î¾ âŸ©âˆ’1) dÂµg(Î¾), (1.35) where Âµg is a probability measure. Then it was further shown by Barron (1992) that there exist (Rd Ã— R)-valued random variables (Î,eÎ) such that (1.35) can be written as g(x) âˆ’g(0) = Cg âˆ« Rd 1 âˆ¥Î¾âˆ¥2 (e2Ï€iâŸ¨x,Î¾âŸ©âˆ’1) dÂµg(Î¾) = CgE  Î“(Î,eÎ)(x)  , x âˆˆRd, (1.36) where for every Î¾ âˆˆRd, ËœÎ¾ âˆˆR, the function Î“(Î¾, ËœÎ¾): Rd â†’R is given by Î“(Î¾, ËœÎ¾) B s(Î¾, ËœÎ¾)(1(0,âˆ)(âˆ’âŸ¨Î¾/âˆ¥Î¾âˆ¥2,Â·âŸ©âˆ’ËœÎ¾) âˆ’1(0,âˆ)(âŸ¨Î¾/âˆ¥Î¾âˆ¥2,Â·âŸ©âˆ’ËœÎ¾)) with s(Î¾, ËœÎ¾) âˆˆ{âˆ’1,1}. Now, let ((Î(i),eÎ(i)))iâˆˆN be i.i.d. random variables with (Î(1),eÎ(1)) âˆ¼(Î,eÎ). Then BienaymÃ©â€™s identity and Fubiniâ€™s theorem establish that E  g âˆ’g(0) âˆ’Cg n n Ã• i=1 Î“(Î(i),eÎ(i)) 2 L2(Âµ)  = âˆ« B1(0) V " Cg n n Ã• i=1 Î“(Î(i),eÎ(i))(x) # dÂµ(x) = C2 g âˆ« B1(0) V  Î“(Î,eÎ)(x)  dÂµ(x) n â‰¤(2Ï€Cg)2 n , (1.37) where the last inequality follows from combining (1.36) with the fact that |e2Ï€iâŸ¨x,Î¾âŸ©âˆ’1|/âˆ¥Î¾âˆ¥2 â‰¤2Ï€, x âˆˆB1(0). This implies that there exists a realization ((Î¾(i), ËœÎ¾(i)))iâˆˆN of the random variables ((Î(i),eÎ(i)))iâˆˆN that achieves an L2-approximation error of nâˆ’1/2. Therefore, it re- mains to show that NNs can well approximate the functions ((Î“(Î¾(i), ËœÎ¾(i)))iâˆˆN. Now it is not hard to see that the function 1(0,âˆ) and hence functions of the form Î“(Î¾, ËœÎ¾), Î¾ âˆˆRd, ËœÎ¾ âˆˆR, can be arbitrarily well approximated with a ï¬xed-size, two-layer NN having a sigmoidal or ReLU activation function. Thus, we obtain an approximation rate of nâˆ’1/2 when approximating functions with one ï¬nite Fourier moment by two-layer NNs with n hidden neurons. It was pointed out in the dissertation of Emmanuel CandÃ¨s (1998) that the 1.4 Deep Neural Networks Overcome the Curse of Dimensionality 53 approximation rate of NNs for Barron-regular functions is also achievable by n- term approximation with complex exponentials, asis apparentbyconsidering (1.35). However, for deeper NNs, the results also extend to high-dimensional non-smooth functions, where Fourier-based methods are certain to suï¬€er from the curse of dimensionality (Caragea et al., 2020). In addition, the random sampling idea above was extended in E et al. (2019d, 2020), and E and Wojtowytsch (2020b,c) to facilitate the dimension-independent approximation of vastly more general function spaces. Basically, the idea is to use (1.36) as an inspiration and deï¬ne a generalized Barron space as comprising all functions that may be represented as E  1(0,âˆ)(âŸ¨Î,Â·âŸ©âˆ’eÎ)  for any random variable (Î,eÎ). In this context, deep and compositional versions of Barron spaces were introduced and studied in Barron and Klusowski (2018), E et al. (2019a), and E and Wojtowytsch (2020a), which considerably extend the original theory. 1.4.3 PDE Assumption Another structural assumption that leads to the absence of the curse of dimen- sionality in some cases is that the function we are trying to approximate is given as the solution to a partial diï¬€erential equation. It is by no means clear that this assumption leads to approximation without the curse of dimensionality, since most standard methods, such as ï¬nite elements, sparse grids, or spectral methods, typi- cally do suï¬€er from the curse of dimensionality. This is not merely an abstract theoretical problem. Very recently, Al-Hamdani et al. (2020) showed that two diï¬€erent gold standard methods for solving the multi- electron SchrÃ¶dinger equation produce completely diï¬€erent interaction energy pre- dictions when applied to large delocalized molecules. Classical numerical repre- sentations are simply not expressive enough to represent accurately complicated high-dimensional structures such as wave functions with long-range interactions. Interestingly, there exists an emerging body of work that shows that NNs do not suï¬€er from these shortcomings and enjoy superior expressivity properties as compared to standard numerical representations. Such results include, for example, Grohs et al. (2021), Gonon and Schwab (2020), and Hutzenthaler et al. (2020) for (linear and semilinear) parabolic evolution equations, ElbrÃ¤chter et al. (2019) for stationary elliptic PDEs, Grohs and Herrmann (2021) for nonlinear Hamiltonâ€“ Jacobiâ€“Bellman equations, and Kutyniok et al. (2019) for parametric PDEs. In all these cases, the absence of the curse of dimensionality in terms of the theoretical approximation power of NNs could be rigorously established. 54 Berner et al. The Modern Mathematics of Deep Learning One way to prove such results is via stochastic representations of the PDE solutions, as well as associated sampling methods. We illustrate the idea for the simple case of linear Kolmogorov PDEs; that is, the problem of representing the function g: Rd Ã— [0,âˆ) â†’R satisfying22 âˆ‚g âˆ‚t (x,t) = 1 2Tr Ïƒ(x,t)[Ïƒ(x,t)]âˆ—âˆ‡2 xg(x,t) + âŸ¨Âµ(x,t),âˆ‡xg(x,t)âŸ©, g(x,0) = Ï•(x), (1.38) where the functions Ï•: Rd â†’R (initial condition) and Ïƒ: Rd â†’RdÃ—d, Âµ: Rd â†’Rd (coeï¬ƒcient functions) are continuous and satisfy suitable growth conditions. A stochastic representation of g is given via the Ito processes (Sx,t)t â‰¥0 satisfying dSx,t = Âµ(Sx,t)dt + Ïƒ(Sx,t)dBt, Sx,0 = x, (1.39) where (Bt)t â‰¥0 is a d-dimensional Brownian motion. Then g is described via the Feynmanâ€“Kac formula, which states that g(x,t) = E[Ï•(Sx,t)], x âˆˆRd, t âˆˆ[0,âˆ). (1.40) Roughly speaking, a NN approximation result can be proven by ï¬rst approximating, via the law of large numbers, as follows: g(x,t) = E[Ï•(Sx,t)] â‰ˆ1 n n Ã• i=1 Ï•(S(i) x,t), (1.41) where (S(i) x,t)n i=1 are i.i.d. random variables with S(1) x,t âˆ¼Sx,t. Care has to be taken to establish such an approximation uniformly in the computational domain, for exam- ple, for every (x,t) in the unit cube [0,1]d Ã— [0,1]; see (1.37) for a similar estimate and Grohs et al. (2021) and Gonon and Schwab (2020) for two general approaches to ensure this property. Aside from this issue, (1.41) represents a standard Monte Carlo estimator which can be shown to be free of the curse of dimensionality. As a next step, one needs to establish that realizations of the processes (x,t) 7â†’ Sx,t can be eï¬ƒciently approximated by NNs. This can be achieved by emulating a suitable time-stepping scheme for the SDE (1.39) by NNs; this, roughly speaking, can be done without incurring the curse of dimensionality whenever the coeï¬ƒcient functions Âµ,Ïƒ can be approximated by NNs without incurring the curse of dimen- sionality and when some growth conditions hold true. In a ï¬nal step one assumes that the initial condition Ï• can be approximated by NNs without incurring the curse 22 The natural solution concept to this type of PDEs is the viscosity solution concept, a thorough study of which can be found in Hairer et al. (2015). 1.4 Deep Neural Networks Overcome the Curse of Dimensionality 55 of dimensionality which, by the compositionality of NNs and the previous step, directly implies that realizations of the processes (x,t) 7â†’Ï•(Sx,t) can be approxi- mated by NNs without incurring the curse of dimensionality. By (1.41) this implies a corresponding approximation result for the solution of the Kolmogorov PDE g in (1.38). Informally, we have discovered a regularity result for linear Kolmogorov equa- tions, namely that (modulo some technical conditions on Âµ,Ïƒ), the solution g of (1.38) can be approximated by NNs without incurring the curse of dimension- ality whenever the same holds true for the initial condition Ï•, as well as for the coeï¬ƒcient functions Âµ and Ïƒ. In other words, the property of being approximable by NNs without curse of dimensionality is preserved under the ï¬‚ow induced by the PDE (1.38). Some comments are in order. Assumption on the initial condition. One may wonder if the assumption that the initial condition Ï• can be approximated by NNs without incurring the curse of dimensionality is justiï¬ed. This is at least the case in many applications in compu- tational ï¬nance where the function Ï• typically represents an option pricing formula and (1.38) represents the famous Blackâ€“Scholes model. It turns out that nearly all common option pricing formulas are constructed from iterative applications of lin- ear maps and maximum/minimum functions â€“ in other words, in many applications in computational ï¬nance, the initial condition Ï• can be exactly represented by a small ReLU NN. Generalization and optimization error. The Feynmanâ€“Kac representation (1.40) directly implies that g(Â·,t) can be computed as the Bayes optimal function of a regression task with input features X âˆ¼U([0,1]d) and labels Y = Ï•(SX,t), which allows for an analysis of the generalization error as well as implementations based on ERM algorithms (Beck et al., 2021; Berner et al., 2020a). While it is in principle possible to analyze the approximation and generalization errors, the analysis of the computational cost and/or convergence of the correspond- ing SGD algorithms is completely open. Some promising numerical results exist â€“ see, for instance, Figure 1.13 â€“ but the stable training of NNs approximating PDEs to very high accuracy (which is needed in several applications such as quantum chemistry) remains very challenging. Recent work (Grohs and Voigtlaender, 2021) has even proved several impossibility results in that direction. Extensions and abstract idea. Similar techniques may be used to prove expres- sivity results for nonlinear PDEs, for example, using nonlinear Feynmanâ€“Kac-type representations of Pardoux and Peng (1992) in place of (1.40) and multilevel Picard sampling algorithms of E et al. (2019c) in place of (1.41). 56 Berner et al. The Modern Mathematics of Deep Learning 101 102 input dimension 107 108 109 1010 1011 x cx2.36 #parameters avg. #steps Â± 2 std. Figure 1.13 Computational complexity as the number of neural network parameters times the number of SGD steps needed to solve heat equations of varying dimensions up to a speciï¬ed precision. According to the ï¬t above, the scaling is polynomial in the dimension (Berner et al., 2020b). We can also formulate the underlying idea in an abstract setting (a version of which has also been used in Â§1.4.2). Assume that a high-dimensional function g: Rd â†’R admits a probabilistic representation of the form g(x) = E[Yx], x âˆˆRd, (1.42) for some random variable Yx which can be approximated by an iterative scheme Y(L) x â‰ˆYx and Y(â„“) x = Tâ„“(Y(â„“âˆ’1) x ), â„“= 1,. . ., L, with dimension-independent convergence rate. If we can approximate realizations of the initial mapping x 7â†’Y0 x and the maps Tâ„“, â„“âˆˆ[L], by NNs and if the numerical scheme is stable enough, then we can also approximate Y(L) x using compositionality. Emulating a uniform Monte-Carlo approximator of (1.42) then leads to approximation results for g without the curse of dimensionality. In addition, one can choose a Rd-valued random variable X as input features and deï¬ne the corresponding labels by YX to obtain a prediction task, which can be solved by means of ERM. Other methods. There exist a number of additional works related to the approxi- mation capacities of NNs for high-dimensional PDEs, for example, ElbrÃ¤chter et al. (2018), Li et al. (2019a), and Schwab and Zech (2019). In most of these works, the proof technique consists of emulating an existing method that does not suï¬€er from the curse of dimensionality. For instance, in the case of ï¬rst-order transport equa- tions, one can show in some cases that NNs are capable of emulating the method of characteristics, which then also yields approximation results that are free of the curse of dimensionality (Laakmann and Petersen, 2021). 1.5 Optimization of Deep Neural Networks 57 1.5 Optimization of Deep Neural Networks We recall from Â§Â§1.1.3 and 1.1.2 that the standard algorithm to solve the empirical risk minimization problem over the hypothesis set of NNs is stochastic gradient descent. This method would be guaranteed to converge to a global minimum of the objective if the empirical risk were convex, viewed as a function of the NN parameters. However, this function is severely non-convex; it may exhibit (higher- order) saddle points, seriously suboptimal local minima, and wide ï¬‚at areas where the gradient is very small. On the other hand, in applications, an excellent performance of SGD is observed. This indicates that the trajectory of the optimization routine somehow misses sub- optimal critical points and other areas that may lead to slow convergence. Clearly, the classical theory does not explain this performance. Below we describe using examples some novel approaches that give partial explanations of this success. In keeping with the ï¬‚avor of this chapter, the aim of this section is to present some selected ideas rather than giving an overview of the literature. To give at least some detail about the underlying ideas and to keep the length of this section reasonable, a selection of results has had to be made and some ground-breaking results have had to be omitted. 1.5.1 Loss Landscape Analysis Given a NN Î¦(Â·,Î¸) and training data s âˆˆZm, the function Î¸ 7â†’r(Î¸) B bRs(Î¦(Â·,Î¸)) describes, in a natural way through its graph, a high-dimensional surface. This surface may have regions associated with lower values of bRs which resemble valleys of a landscape if they are surrounded by regions of higher values. The analysis of the topography of this surface is called loss landscape analysis. Below we shall discuss a couple of approaches that yield deep insights into the shape of such a landscape. Spin glass interpretation. One of the ï¬rst discoveries about the shape of the loss landscape comes from deep results in statistical physics. The Hamiltonian of the spin glass model is a random function on the (n âˆ’1)-dimensional sphere of radius âˆšn. Making certain simplifying assumptions, it was shown in Choromanska et al. (2015a) that the loss associated with a NN with random inputs can be considered as the Hamiltonian of a spin glass model, where the inputs of the model are the parameters of the NN. This connection has far-reaching implications for the loss landscape of NNs because of the following surprising property of the Hamiltonian of spin glass models. Consider the critical points of the Hamiltonian, and associate with each 58 Berner et al. The Modern Mathematics of Deep Learning Index Loss No negative curvature at globally minimal risk. Critical points with high risk are unstable. 0 0.25 0.5 Figure 1.14 The distribution of critical points of the Hamiltonian of a spin glass model. point an index that denotes the percentage of the eigenvalues of the Hessian at that point which are negative. This index corresponds to the relative number of directions in which the loss landscape has negative curvature. Then, with high probability, a picture like that in Figure 1.14 emerges (Auï¬ƒnger et al., 2013). More precisely, the further away from the optimal loss we are, the more unstable the critical points become. Conversely, if one ï¬nds oneself in a local minimum, it is reasonable to assume that the loss is close to the global minimum. While some of the assumptions establishing the connection between the spin glass model and NNs are unrealistic in practice (Choromanska et al., 2015b), the theoretical distribution of critical points in Figure 1.14 is visible in many practical applications (Dauphin et al., 2014). Paths and level sets. Another line of research is to understand the loss landscape by analyzing paths through the parameter space, in particular, the existence of paths in parameter space such that the associated empirical risks are monotone along the path. Should there exist a path of non-increasing empirical risk from every point to the global minimum, then we can be certain that no non-global minimum exists, since no such path could escape such a minimum. An even stronger result holds: the existence of such paths shows that the loss landscape has connected level sets (Freeman and Bruna, 2017; Venturi et al., 2019). A crucial ingredient of the analysis of such paths is linear substructures. Consider a biasless two-layer NN Î¦ of the form Rd âˆ‹x 7â†’Î¦(x,Î¸) B n Ã• j=1 Î¸(2) j Ï±  Î¸(1) j , x 1   , (1.43) where Î¸(1) j âˆˆRd+1 for j âˆˆ[n], Î¸(2) âˆˆRn, Ï± is a Lipschitz continuous activation function, and we have augmented the vector x by a constant 1 in the last coordinate 1.5 Optimization of Deep Neural Networks 59 Î¦(Â·,Î¸min) Î¦(Â·,Î¸) eFË†Î¸(1) Î¦(Â·,Î¸âˆ—) Î³1 Figure 1.15 Construction of a path from an initial point Î¸ to the global minimum Î¸min that does not have signiï¬cantly higher risk than the initial point along the way. We depict here the landscape as a function of the neural network realizations rather than of their parametrizations, so that this landscape is convex. as outlined in Remark 1.5. If we consider Î¸(1) to be ï¬xed then it is clear that the space eFÎ¸(1) B {Î¦(Â·,Î¸): Î¸ = (Î¸(1),Î¸(2)), Î¸(2) âˆˆRn} (1.44) is a linear space. If the risk23 is convex, as is the case for the widely used quadratic or logistic losses, then this implies that Î¸(2) 7â†’r  (Î¸(1),Î¸(2)) is a convex map and hence, for every parameter set P âŠ‚Rn this map assumes its maximum on âˆ‚P. Therefore, within the vast parameter space, there are many paths one may travel upon that do not increase the risk above the risk of the start and end points. This idea was used in, for example, Freeman and Bruna (2017) in a way indicated by the following simple sketch. Assume that, for two parameters Î¸ and Î¸min, there exists a linear subspace of NNs eFË†Î¸(1) such that there are paths Î³1 and Î³2 connecting Î¦(Â·,Î¸) and Î¦(Â·,Î¸min) respectively to eFË†Î¸(1). Further, assume that these paths are such that, along them, the risk does not signiï¬cantly exceed max{r(Î¸),r(Î¸min)}. Figure 1.15 shows a visualization of these paths. In this case, a path from Î¸ to Î¸min not signiï¬cantly exceeding r(Î¸) along the way is found by concatenating the path Î³1, a path along eFË†Î¸(1), and the path Î³2. By the previous discussion, we know that only Î³1 and Î³2 determine the extent to which the combined path exceeds r(Î¸) along its way. Hence, we need to ask about the existence of aneFË†Î¸(1) that facilitates the construction of appropriate Î³1 and Î³2. To understand why a good choice of eFË†Î¸(1), such that the risk along Î³1 and Î³2 will 23 As most statements in this subsection are valid for the empirical risk r(Î¸) = bRs(Î¦(Â·, Î¸)) as well as the risk r(Î¸) = R(Î¦(Â·, Î¸)), given a suitable data distribution of Z, we will just call r the risk. 60 Berner et al. The Modern Mathematics of Deep Learning not rise much higher than r(Î¸), is likely to be possible we set24 Ë†Î¸(1) j B ( Î¸(1) j for j âˆˆ[n/2], (Î¸(1) min)j for j âˆˆ[n] \ [n/2]. (1.45) In other words, the ï¬rst half of Ë†Î¸(1) is constructed from Î¸(1) and the second from Î¸(1) min. If Î¸(1) j , j âˆˆ[N], are realizations of random variables distributed uniformly on the d-dimensional unit sphere, then, by invoking standard covering bounds of spheres (e.g., Corollary 4.2.13 of Vershynin, 2018), we expect that, for Îµ > 0 and a suï¬ƒciently large number of neurons n, the vectors (Î¸(1) j )n/2 j=1 already Îµ-approximate all vectors (Î¸(1) j )n j=1. Replacing all vectors (Î¸(1) j )n j=1 by their nearest neighbor in (Î¸(1) j )n/2 j=1 can be done using a linear path in the parameter space, and, given that r is locally Lipschitz continuous and âˆ¥Î¸(2)âˆ¥1 is bounded, this operation will not increase the risk by more than O(Îµ). We denote the vector resulting from this replacement procedure by Î¸(1) âˆ—. Since for all j âˆˆ[n] \ [n/2] we now have that Ï±  (Î¸(1) âˆ—)j, Â· 1   âˆˆ  Ï±  (Î¸(1) âˆ—)k, Â· 1   : k âˆˆ[n/2]  , there exists a vector Î¸(2) âˆ— with (Î¸(2) âˆ—)j = 0, j âˆˆ[n] \ [n/2], so that Î¦(Â·,(Î¸(1) âˆ—,Î¸(2))) = Î¦(Â·,(Î¸(1) âˆ—,Î»Î¸(2) âˆ—+ (1 âˆ’Î»)Î¸(2))), Î» âˆˆ[0,1]. In particular, this path does not change the risk between (Î¸(1) âˆ—,Î¸(2)) and (Î¸(1) âˆ—,Î¸(2) âˆ—). Now, since (Î¸(2) âˆ—)j = 0 for j âˆˆ[n]\[n/2], the realization Î¦(Â·,(Î¸(1) âˆ—,Î¸(2) âˆ—)) is computed by a subnetwork consisting of the ï¬rst n/2 hidden neurons, and we can replace the parameters corresponding to the other neurons without any eï¬€ect on the realization function. Speciï¬cally, we have Î¦(Â·,(Î¸(1) âˆ—,Î¸(2) âˆ—)) = Î¦(Â·,(Î» Ë†Î¸(1) + (1 âˆ’Î»)Î¸(1) âˆ—,Î¸(2) âˆ—)), Î» âˆˆ[0,1], yielding a path of constant risk between (Î¸(1) âˆ—,Î¸(2) âˆ—) and ( Ë†Î¸(1),Î¸(2) âˆ—). Connecting these paths completes the construction of Î³1 and shows that the risk along Î³1 does not exceed that at Î¸ by more than O(Îµ). Of course, Î³2 can be constructed in the same way. The entire construction is depicted in Figure 1.15. Overall, this derivation shows that for suï¬ƒciently wide NNs (appropriately ran- domly initialized) it is very likely possible to connect a random parameter value to the global minimum with a path which along the way does not need to climb much higher than the initial risk. In Venturi et al. (2019), a similar approach is taken and the convexity in the last layer is used. However, the authors invoke the concept of intrinsic dimension to 24 We assume, without loss of generality, that n is a multiple of 2. 1.5 Optimization of Deep Neural Networks 61 solve elegantly the nonlinearity of r((Î¸(1),Î¸(2))) with respect to Î¸(1). Also Safran and Shamir (2016) had already constructed a path of decreasing risk from random initializations. The idea here is that if one starts at a point of suï¬ƒciently high risk, one can always ï¬nd a path to the global optimum with strictly decreasing risk. The intriguing insight behind this result is that if the initialization is suï¬ƒciently bad, i.e., worse than that of a NN outputting only zero, then there exist two operations that inï¬‚uence the risk directly. Multiplying the last layer with a number smaller than 1 will decrease the risk, whereas choosing a number larger than 1 will increase it. Using this tuning mechanism, any given potentially non-monotone path from the initialization to the global minimum can be modiï¬ed so that it is strictly mono- tonically decreasing. In a similar spirit, Nguyen and Hein (2017) showed that if a deep NN has a layer with more neurons than training data points, then under cer- tain assumptions the training data will typically be mapped to linearly independent points in that layer. Of course, this layer could then be composed with a linear map that maps the linearly independent points to any desirable output, in particular one that achieves vanishing empirical risk; see also Proposition 1.17. As in the case of two-layer NNs, the previous discussion on linear paths shows immediately that in this situation a monotone path to the global minimum exists. 1.5.2 Lazy Training and Provable Convergence of Stochastic Gradient Descent When training highly overparametrized NNs, one often observes that the parameters of the NNs barely change during training. In Figure 1.16, we show the relative distance traveled by the parameters through the parameter space during the training of NNs of varying numbers of neurons per layer. The eï¬€ect described above has been observed repeatedly and has been explained theoretically: see e.g.,Du et al. (2018b, 2019), Li and Liang (2018), Allen-Zhu et al. (2019), and Zou et al. (2020). In Â§1.2.1, we have already given a high-level overview and, in particular, we discussed the function space perspective of this phenomenon in the inï¬nite-width limit. Below we present a short and highly simpliï¬ed derivation of this eï¬€ect and show how it leads to the provable convergence of gradient descent for suï¬ƒciently overparametrized deep NNs. A simple learning model. We consider again the simple NN model of (1.43) with a smooth activation function Ï± which is not aï¬ƒne linear. For a quadratic loss and training data s = ((x(i), y(i)))m i=1 âˆˆ(Rd Ã— R)m, where xi , xj for all i , j, the empirical risk is given by r(Î¸) = bRs(Î¸) = 1 m m Ã• i=1 (Î¦(x(i),Î¸) âˆ’y(i))2. 62 Berner et al. The Modern Mathematics of Deep Learning Figure 1.16 Four networks with architecture ((1, n, n, 1), Ï±R) and n âˆˆ{20, 100, 500, 2500} neurons per hidden layer were trained by gradient descent to ï¬t the four points shown in the top right ï¬gure as black dots. We depict on the top left the relative Euclidean distance of the parameters from the initialization through the training process. In the top right, we show the ï¬nal trained NNs. On the bottom we show the behavior of the training error. Let us further assume that Î˜(1) j âˆ¼N(0,1/n)d+1, j âˆˆ[n], and Î˜(2) j âˆ¼N(0,1/n), j âˆˆ[n], are independent random variables. A peculiar kernel. Next we would like to understand what the gradient âˆ‡Î¸r(Î˜) looks like, with high probability, over the initialization Î˜ = (Î˜(1),Î˜(2)). As with Equation (1.22), by restricting the gradient to Î¸(2) and applying the chain rule, we have that âˆ¥âˆ‡Î¸r(Î˜)âˆ¥2 2 â‰¥4 m2 m Ã• i=1 âˆ‡Î¸(2)Î¦(x(i),Î˜)(Î¦(x(i),Î˜) âˆ’y(i)) 2 2 = 4 m2  (Î¦(x(i),Î˜) âˆ’y(i))m i=1 T Â¯KÎ˜(Î¦(x(j),Î˜) âˆ’y(j))m j=1, (1.46) 1.5 Optimization of Deep Neural Networks 63 where Â¯KÎ˜ is a random RmÃ—m-valued kernel given by ( Â¯KÎ˜)i,j B  âˆ‡Î¸(2)Î¦(x(i),Î˜)Tâˆ‡Î¸(2)Î¦(x(j),Î˜), i, j âˆˆ[m]. This kernel is closely related to the neural tangent kernel in (1.23) evaluated at the features (x(i))m i=1 and the random initialization Î˜. It is a slightly simpliï¬ed version thereof because, in (1.23), the gradient is taken with respect to the full vector Î¸. This can also be regarded as the kernel associated with a random features model (Rahimi et al., 2007). Note that for our two-layer NN we have that  âˆ‡Î¸(2)Î¦(x,Î˜) k = Ï±  Î˜(1) k , x 1   , x âˆˆRd, k âˆˆ[n]. (1.47) Thus, we can write Â¯KÎ˜ as the following sum of (random) rank-1 matrices: Â¯KÎ˜ = n Ã• k=1 vkvT k with vk =  Ï±  Î˜(1) k , x(i) 1  m i=1 âˆˆRm, k âˆˆ[n]. (1.48) The kernel Â¯KÎ˜ is symmetric and positive semi-deï¬nite by construction. It is positive deï¬nite if it is non-singular, i.e., if at least m of the n vectors vk, k âˆˆ[n], are linearly independent. Proposition 1.17 shows that for n = m the probability of that event is non-zero, say Î´, and is therefore at least 1 âˆ’(1 âˆ’Î´)âŒŠn/mâŒ‹for arbitrary n. In other words, the probability increases rapidly with n. It is also clear from (1.48) that E[ Â¯KÎ˜] scales linearly with n. From this intuitive derivation we conclude that, for suï¬ƒciently large n, with high probability Â¯KÎ˜ is a positive deï¬nite kernel with smallest eigenvalue Î»min( Â¯KÎ˜) scaling linearly with n. The properties of Â¯KÎ˜, in particular its positive deï¬niteness, have been studied much more rigorously, as already described in Â§1.2.1. Control of the gradient. Applying the expected behavior of the smallest eigen- value Î»min( Â¯KÎ˜) of Â¯KÎ˜ to (1.46), we conclude that with high probability âˆ¥âˆ‡Î¸r(Î˜)âˆ¥2 2 â‰¥4 m2 Î»min( Â¯KÎ˜)âˆ¥(Î¦(x(i),Î˜) âˆ’y(i))m i=1âˆ¥2 2 â‰³n mr(Î˜). (1.49) To understand what will happen when applying gradient descent, we ï¬rst need to understand how the situation changes in a neighborhood of Î˜. We ï¬x x âˆˆRd and observe that, by the mean value theorem for all Â¯Î¸ âˆˆB1(0), we have âˆ‡Î¸Î¦(x,Î˜) âˆ’âˆ‡Î¸Î¦(x,Î˜ + Â¯Î¸) 2 2 â‰² sup Ë†Î¸ âˆˆB1(0) âˆ‡2 Î¸Î¦(x,Î˜ + Ë†Î¸) 2 op, (1.50) where âˆ¥âˆ‡2 Î¸Î¦(x,Î˜+ Ë†Î¸)âˆ¥op denotes the operator norm of the Hessian of Î¦(x,Â·) at Î˜+ Ë†Î¸. 64 Berner et al. The Modern Mathematics of Deep Learning By inspecting (1.43), it is not hard to see that, for all i, j âˆˆ[n] and k,â„“âˆˆ[d + 1], E " âˆ‚2Î¦(x,Î˜) âˆ‚Î¸(2) i âˆ‚Î¸(2) j 2 # = 0, E " âˆ‚2Î¦(x,Î˜) âˆ‚Î¸(2) i âˆ‚(Î¸(1) j )k 2 # â‰²Î´i,j, and E " âˆ‚2Î¦(x,Î˜) âˆ‚(Î¸(1) i )kâˆ‚(Î¸(1) j )â„“ 2 # â‰²Î´i,j n , where Î´i,j = 0 if i , j and Î´i,i = 1 for all i, j âˆˆ[n]. For suï¬ƒciently large n, we have that âˆ‡2 Î¸Î¦(x,Î˜) is in expectation approximately a block-band matrix with bandwidth d + 1. Therefore we conclude that E  âˆ¥âˆ‡2 Î¸Î¦(x,Î˜)âˆ¥2 op  â‰²1. Hence we obtain by the concentration of Gaussian random variables that with high probability, âˆ¥âˆ‡2 Î¸Î¦(x,Î˜)âˆ¥2 op â‰²1. By the block-banded form of âˆ‡2 Î¸Î¦(x,Î˜) we have that, even after perturbation of Î˜ by a vector Ë†Î¸ with norm bounded by 1, the term âˆ¥âˆ‡2 Î¸Î¦(x,Î˜+ Ë†Î¸)âˆ¥2 op is still bounded, which yields that the right-hand side of (1.50) is bounded with high probability. Using (1.50), we can extend (1.49), which holds with high probability, to a neighborhood of Î˜ by the following argument. Let Â¯Î¸ âˆˆB1(0); then âˆ¥âˆ‡Î¸r(Î˜ + Â¯Î¸)âˆ¥2 2 â‰¥4 m2 m Ã• i=1 âˆ‡Î¸(2)Î¦(x(i),Î˜ + Â¯Î¸)(Î¦(x(i),Î˜ + Â¯Î¸) âˆ’y(i)) 2 2 = (1.50) 4 m2 m Ã• i=1 (âˆ‡Î¸(2)Î¦(x(i),Î˜) + O(1))(Î¦(x(i),Î˜ + Â¯Î¸) âˆ’y(i)) 2 2 â‰³ (âˆ—) 1 m2 (Î»min( Â¯KÎ˜) + O(1))âˆ¥(Î¦(x(i),Î˜ + Â¯Î¸) âˆ’y(i))m i=1âˆ¥2 2 â‰³n mr(Î˜ + Â¯Î¸), (1.51) where the estimate marked by (âˆ—) uses the positive deï¬niteness of Â¯KÎ˜ again and only holds for n suï¬ƒciently large, so that the O(1) term is negligible. We conclude that, with high probability over the initialization Î˜, on a ball of ï¬xed radius around Î˜ the squared Euclidean norm of the gradient of the empirical risk is lower bounded by n/m times the empirical risk. Exponential convergence of gradient descent. For suï¬ƒciently small step sizes Î·, the observation in the previous paragraph yields the following convergence rate for gradient descent as in Algorithm 1.1, speciï¬cally (1.8), with mâ€² = m and Î˜(0) = Î˜: 1.6 Tangible Eï¬€ects of Special Architectures 65 if âˆ¥Î˜(k) âˆ’Î˜âˆ¥â‰¤1 for all k âˆˆ[K + 1], then25 r(Î˜(K+1)) â‰ˆr(Î˜(K))âˆ’Î·âˆ¥âˆ‡Î¸r(Î˜(K))âˆ¥2 2 â‰¤  1âˆ’cÎ·n m  r(Î˜(K)) â‰²  1âˆ’cÎ·n m K , (1.52) for c âˆˆ(0,âˆ) so that âˆ¥âˆ‡Î¸r(Î˜(k))âˆ¥2 2 â‰¥cn m r(Î˜(k)) for all k âˆˆ[K]. Let us assume without proof that the estimate (1.51) could be extended to an equivalence. In other words, we assume that we additionally have that âˆ¥âˆ‡Î¸r(Î˜ + Â¯Î¸)âˆ¥2 2 â‰²n mr(Î˜+ Â¯Î¸). This, of course, could have been shown with tools similar to those used for the lower bound. Then we have that âˆ¥Î˜(k) âˆ’Î˜âˆ¥2 â‰¤1 for all k â‰² p m/(Î·2n). Setting t = p m/(Î·2n) and using the limit deï¬nition of the exponential function, i.e., limtâ†’âˆ(1 âˆ’x/t)t = eâˆ’x, yields, for suï¬ƒciently small Î·, that (1.52) is bounded by eâˆ’câˆš n/m. We conclude that, with high probability over the initialization, gradient descent converges at an exponential rate to an arbitrarily small empirical risk if the width n is suï¬ƒciently large. In addition, the iterates of the descent algorithm even stay in a small ï¬xed neighborhood of the initialization during training. Because the parameters only move very little, this type of training has also been coined lazy training (Chizat et al., 2019). Ideas similar to those above have led to groundbreaking convergence results of SGD for overparametrized NNs in much more complex and general settings; see, e.g., Du et al. (2018b), Li and Liang (2018), and Allen-Zhu et al. (2019). In the inï¬nite-width limit, NN training is practically equivalent to kernel regres- sion; see Â§1.2.1. If we look at Figure 1.16 we see that the most overparametrized NN interpolates the data in the same way as a kernel-based interpolator would. In a sense, which was also highlighted in Chizat et al. (2019), this shows that, while overparametrized NNs in the lazy training regime have very nice properties, they essentially act like linear methods. 1.6 Tangible Eï¬€ects of Special Architectures In this section we describe results that isolate the eï¬€ects of certain aspects of NN architectures. As discussed in Â§1.1.3, typically only either the depth or the number of parameters is used to study theoretical aspects of NNs. We have seen instances of this throughout Â§Â§1.3 and 1.4. Moreover, in Â§1.5, we saw that wider NNs enjoy certain very favorable properties from an optimization point of view. Below, we introduce certain specialized NN architectures. We start with one of the most widely used types of NNs, the convolutional neural network (CNN). In Â§1.6.2 we introduce skip connections and in Â§1.6.3 we discuss a speciï¬c class 25 Note that the step size Î· needs to be small enough to facilitate the approximation step in (1.52). Hence, we cannot simply put Î· = m/(cn) in (1.52) and have convergence after one step. 66 Berner et al. The Modern Mathematics of Deep Learning of CNNs equipped with an encoderâ€“decoder structure that is frequently used in image processing techniques. We introduce the batch normalization block in Â§1.6.4. Then, in Â§1.6.5, we discuss the sparsely connected NNs that typically result as an extraction from fully connected NNs. Finally, we brieï¬‚y comment on recurrent neural networks in Â§1.6.6. As we have noted repeatedly throughout this chapter, it is impossible to give a full account of the literature in a short introductory article. In this section this issue is especially severe since the number of special architectures studied in practice is enormous. Therefore, we have had to omit many very inï¬‚uential and widely used neural network architectures. Among those are graph neural networks, which handle data from non-Euclidean input spaces. We refer to the survey articles by Bronstein et al. (2017) and Wu et al. (2021) for a discussion. Another highly suc- cessful type of architecture comprises (variational) autoencoders (Ackley et al., 1985; Hinton and Zemel, 1994). These are neural networks with a bottleneck that enforce a more eï¬ƒcient representation of the data. Similarly, generative adversarial networks (Goodfellow et al., 2014), which are composed of two neural networks â€“ one generator and one discriminator â€“ could not be discussed here. Yet another widely used component of architectures used in practice is the so-called dropout layer. This layer functions through removing some neurons randomly during train- ing. This procedure empirically prevents overï¬tting. An in-detail discussion of the mathematical analysis behind this eï¬€ect is beyond the scope of this chapter. Instead, we refer to Wan et al. (2013), Srivastava et al. (2014), Haeï¬€ele and Vidal (2017), and Mianjy et al. (2018). Finally, the very successful attention mechanism (Bah- danau et al., 2015; Vaswani et al., 2017), which is the basis of transformer neural networks, had to be omitted. Before we start describing certain eï¬€ects of special NN architectures, a word of warning is required. The special building blocks that will be presented below have been developed on the basis of a speciï¬c need in applications and are used and combined in a very ï¬‚exible way. To describe these tools theoretically without completely inï¬‚ating the notational load, some simplifying assumptions need to be made. It is very likely that the building blocks thus simpliï¬ed do not accurately reï¬‚ect the practical applications of these tools in all use cases. 1.6.1 Convolutional Neural Networks Especially for very high-dimensional inputs where the input dimensions are spatially related, fully connected NNs seem to require unnecessarily many parameters. For example, in image classiï¬cation problems, neighboring pixels very often share information and the spatial proximity should be reï¬‚ected in the architecture. From this observation, it appears reasonable to have NNs that have local receptive ï¬elds in 1.6 Tangible Eï¬€ects of Special Architectures 67 the sense that they collect information jointly from spatially close inputs. In addition, in image processing we are not necessarily interested in a universal hypothesis set. A good classiï¬er is invariant under many operations, such as the translation or rotation of images. It seems reasonable to hard-code such invariances into the architecture. These two principles suggest that the receptive ï¬eld of a NN should be the same on diï¬€erent translated patches of the input. In this sense, the parameters of the architecture can be reused. Together, these arguments make up the three fundamental principles of convolutional NNs: local receptive ï¬elds, parameter sharing, and equivariant representations, as introduced in LeCun et al. (1989a). We will provide a mathematical formulation of convolutional NNs below and then revisit these concepts. A convolutional NN corresponds to multiple convolutional blocks, which are special types of layers. For a group G, which typically is either [d]  Z/(dZ) or [d]2  (Z/(dZ))2 for d âˆˆN, depending on whether we are performing one- dimensional or two-dimensional convolutions, the convolution of two vectors a, b âˆˆ RG is deï¬ned as (a âˆ—b)i = Ã• jâˆˆG ajbjâˆ’1i, i âˆˆG. Now we can deï¬ne a convolutional block as follows. Let e G be a subgroup of G, let p: G â†’e G be a so-called pooling operator, and let C âˆˆN denote the number of channels. Then, for a series of kernels Îºi âˆˆRG,i âˆˆ[C], the output of a convolutional block is given by RG âˆ‹x 7â†’xâ€² B (p(x âˆ—Îºi))C i=1 âˆˆ(R e G)C. (1.53) A typical example of a pooling operator is, for G = (Z/(2dZ))2 and e G = (Z/(dZ))2, the 2 Ã— 2 subsampling operator p: RG â†’R e G, x 7â†’(x2iâˆ’1,2jâˆ’1)d i,j=1. Popular alternatives are average pooling or max pooling. These operations then either compute the average or the maximum over patches of similar size. The convolutional kernels correspond to the aforementioned receptive ï¬elds. They can be thought of as local if they have small supports, i.e., few non-zero entries. As explained earlier, a convolutional NN is built by stacking multiple convolu- tional blocks one after another.26 At some point, the output can be ï¬‚attened, i.e., mapped to a vector, and is then fed into an FC NN (see Deï¬nition 1.4). We depict this set-up in Figure 1.17. 26 We assume that the deï¬nition of a convolutional block is suitably extended to input data in the Cartesian product (RG)C. For instance, one can take an aï¬ƒne linear combination of C mappings as in (1.53) acting on each coordinate. Moreover, one may also interject an activation function between the blocks. 68 Berner et al. The Modern Mathematics of Deep Learning Convolution Pooling Convolution Pooling Fully connected NN Figure 1.17 Illustration of a convolutional neural network with two-dimensional convolutional blocks and 2 Ã— 2 subsampling as the pooling operation. Owing to the fact that convolution is a linear operation, depending on the pooling operation, one may write a convolutional block (1.53) as an FC NN. For example, if G = (Z/(2dZ))2 and the 2 Ã— 2 subsampling pooling operator is used, then the convolutional block could be written as x 7â†’Wx for a block-circulant matrix W âˆˆR(Cd2)Ã—(2d)2. Since we require W to have a special structure, we can interpret a convolutional block as a special, restricted, feed-forward architecture. After these considerations, it is natural to ask how the restriction of a NN to a pure convolutional structure, i.e., one consisting only of convolutional blocks, will aï¬€ect the resulting hypothesis set. The ï¬rst natural question is whether the set of such NNs is still universal in the sense of Theorem 1.16. The answer to this question depends strongly on the type of pooling and convolution that is allowed. If the convolution is performed with padding then the answer is yes (Oono and Suzuki, 2019; Zhou, 2020b). On the other hand, for circular convolutions and without pooling, universal- ity does not hold but the set of translation-equivariant functions can be universally approximated (Yarotsky, 2018b; Petersen and Voigtlaender, 2020). Furthermore, Yarotsky (2018b) illuminates the eï¬€ect of subsample pooling by showing that if no pooling is applied then universality cannot be achieved, whereas if pooling is applied then universality is possible. The eï¬€ect of subsampling in CNNs from the viewpoint of approximation theory is further discussed in Zhou (2020a). The role of other types of pooling in enhancing invariances of the hypothesis set will be discussed in Â§1.7.1 below. 1.6.2 Residual Neural Networks Let us ï¬rst illustrate a potential obstacle when training deep NNs. Consider for L âˆˆN the product operation RL âˆ‹x 7â†’Ï€(x) = L Ã– â„“=1 xâ„“. 1.6 Tangible Eï¬€ects of Special Architectures 69 It is clear that âˆ‚ âˆ‚xk Ï€(x) = L Ã– â„“,k xâ„“, x âˆˆRL. Therefore, for suï¬ƒciently large L, we expect that âˆ‚Ï€ âˆ‚xk will be exponentially small, if |xâ„“| < Î» < 1 for all â„“âˆˆ[L]; or exponentially large, if |xâ„“| > Î» > 1 for all â„“âˆˆ[L]. The output of a general NN, considered as a directed graph, is found by repeatedly multiplying the input with parameters in every layer along the paths that lead from the input to the output neuron. Owing to the aforementioned phenomenon, it is often observed that training the NNs suï¬€ers from either an exploding-gradient or a vanishing-gradient problem, which may prevent the lower layers from training at all. The presence of an activation function is likely to exacerbate this eï¬€ect. The exploding- or vanishing-gradient problem seems to be a serious obstacle to the eï¬ƒcient training of deep NNs. In addition to the exploding- and vanishing-gradient problems, there is an em- pirically observed degradation problem (He et al., 2016). This phrase describes the fact that FC NNs seem to achieve lower accuracy on both the training and test data when increasing their depth. From an approximation-theoretic perspective, deep NNs should always be su- perior to shallow NNs. The reason for this is that NNs with two layers can either exactly represent the identity map or approximate it arbitrarily well. Concretely, for the ReLU activation function Ï±R we have that x = Ï±R(x + b) âˆ’b for x âˆˆRd with xi > âˆ’bi, where b âˆˆRd. In addition, for any activation function Ï± which is contin- uously diï¬€erentiable on a neighborhood of some point Î» âˆˆR with Ï±â€²(Î») , 0 one can approximate the identity arbitrary well; see (1.12). Because of this, extending a NN architecture by one layer can only enlarge the associated hypothesis set. Therefore, one may expect that the degradation problem is more associated with the optimization aspect of learning. This problem is addressed by a small change to the architecture of a feed-forward NN in He et al. (2016). Instead of deï¬ning an FC NN Î¦ as in (1.1), one can insert a residual block in the â„“th layer by redeï¬ning27 Â¯Î¦(â„“)(x,Î¸) = Ï±(Î¦(â„“)(x,Î¸)) + Â¯Î¦(â„“âˆ’1)(x,Î¸), (1.54) where we assume that Nâ„“= Nâ„“âˆ’1. Such a block can be viewed as the sum of a regular FC NN and the identity and is referred to as a skip connection or residual connection. A schematic diagram of a NN with residual blocks is shown in Figure 1.18. Inserting a residual block in all layers leads to a so-called residual NN. A prominent approach to analyzing residual NNs is to establish a connection with optimal control problems and dynamical systems (E, 2017; Thorpe and van 27 One can also skip multiple layers â€“ e.g., in He et al. (2016) two or three layers were skipped â€“ use a simple transformation instead of the identity (Srivastava et al., 2015), or randomly drop layers (Huang et al., 2016). 70 Berner et al. The Modern Mathematics of Deep Learning idR3 idR3 idR3 idR3 Figure 1.18 Illustration of a neural network with residual blocks. Gennip, 2018; E et al., 2019b; Li et al., 2019b; Ruthotto and Haber, 2019; Lu et al., 2020). Concretely, if each layer of a NN Î¦ is of the form (1.54) then we have that Â¯Î¦(â„“) âˆ’Â¯Î¦(â„“âˆ’1) = Ï±(Î¦(â„“)) C h(â„“,Î¦(â„“)), where for brevity we write Â¯Î¦(â„“) = Â¯Î¦(â„“)(x,Î¸) and set Â¯Î¦(0) = x. Hence, ( Â¯Î¦(â„“))Lâˆ’1 â„“=0 corresponds to an Euler discretization of the ODE Ã›Ï†(t) = h(t, Ï†(t)), Ï†(0) = x, where t âˆˆ[0, L âˆ’1] and h is an appropriate function. Using this relationship, deep residual NNs can be studied in the framework of the well-established theory of dynamical systems, where strong mathematical guarantees can be derived. 1.6.3 Framelets and U-Nets One of the most prominent application areas of deep NNs is inverse problems, particularly those in the ï¬eld of imaging science; see also Â§1.8.1. A speciï¬c ar- chitectural design of CNNs, namely so-called U-nets, introduced in Ronneberger et al. (2015), seems to perform best for this range of problems. We sketch a U-net in Figure 1.19. However, a theoretical understanding of the success of this architecture was lacking. Recently, an innovative approach called deep convolutional framelets was sug- gested in Ye et al. (2018), which we now brieï¬‚y explain. The core idea is to take a frame-theoretic viewpoint, see, e.g., Casazza et al. (2012), and regard the forward pass of a CNN as a decomposition in terms of a frame (in the sense of a generalized basis). A similar approach will be taken in Â§1.7.2 for understanding the learned ker- nels using sparse coding. However, based on the analysis and synthesis operators of the corresponding frame, the usage of deep convolutional framelets naturally leads to a theoretical understanding of encoderâ€“decoder architectures, such as U-nets. Let us describe this approach for one-dimensional convolutions on the group 1.6 Tangible Eï¬€ects of Special Architectures 71 Figure 1.19 Illustration of a simpliï¬ed U-net neural network. Down arrows stand for pooling, up arrows for deconvolution or upsampling, right-pointing arrows for convolution or fully connected steps. Lines without arrows are skip connections. G B Z/(dZ) with kernels deï¬ned on the subgroup H B Z/(nZ), where d,n âˆˆN with n < d; see also Â§1.6.1. We deï¬ne the convolution between u âˆˆRG and v âˆˆRH by zero-padding v, i.e., g âˆ—â—¦v B g âˆ—Â¯v, where Â¯v âˆˆRG is deï¬ned by Â¯vi = vi for i âˆˆH and Â¯vi = 0 else. As an important tool, we consider the Hankel matrix Hn(x) = (xi+j)iâˆˆG,jâˆˆH âˆˆRdÃ—n associated with x âˆˆRG. As one key property, matrixâ€“vector multiplications with Hankel matrices are translated to convolutions via28 âŸ¨e(i),Hn(x)vâŸ©= Ã• jâˆˆH xi+jvj = âŸ¨x,e(i) âˆ—â—¦vâŸ©, i âˆˆG, (1.55) where e(i) B 1{i} âˆˆRG and v âˆˆRH; see Yin et al. (2017). Further, we can recover the kth coordinate of x by the Frobenius inner product between Hn(x) and the Hankel matrix associated with e(k), i.e., 1 nTr Hn(e(k))THn(x) = 1 n Ã• jâˆˆH Ã• iâˆˆG e(k) i+jxi+j = 1 n |H|xk = xk. (1.56) This allows us to construct global and local bases as follows. Let p,q âˆˆN, let U =  u1 Â· Â· Â· up  âˆˆRdÃ—p, V =  v1 Â· Â· Â· vq  âˆˆRnÃ—q, e U =  Ëœu1 Â· Â· Â· Ëœup  âˆˆRdÃ—p, and 28 Here and in the following we naturally identify elements in RG and RH with the corresponding vectors in Rd and Rn. 72 Berner et al. The Modern Mathematics of Deep Learning eV =  Ëœv1 Â· Â· Â· Ëœvq  âˆˆRnÃ—q, and assume that Hn(x) = e UUTHn(x)VeVT . (1.57) For p â‰¥d and q â‰¥n, this is satisï¬ed if, for instance, U and V constitute frames whose dual frames are respectively e U and eV, i.e., e UUT = Id and VeVT = In. As a special case, one can consider orthonormal bases U = e U and V = eV with p = d and q = n. In the case p = q = r â‰¤n, where r is the rank of Hn(x), one can establish (1.57) by choosing the left and right singular vectors of Hn(x) as U = e U and V = eV, respectively. The identity in (1.57) in turn ensures the following decomposition: x = 1 n p Ã• i=1 q Ã• j=1 âŸ¨x,ui âˆ—â—¦vjâŸ©Ëœui âˆ—â—¦Ëœvj. (1.58) Observing that the vector vj âˆˆRH interacts locally with x âˆˆRG owing to the fact that H âŠ‚G, whereas ui âˆˆRG acts on the entire vector x, we refer to (vj)q j=1 as a local basis and (ui)p i=1 as a global basis. In the context of CNNs, vi can be interpreted as a local convolutional kernel and ui as a pooling operation.29 The proof of (1.58) follows directly from properties (1.55), (1.56), and (1.57): xk = 1 nTr Hn(e(k))THn(x) = 1 nTr Hn(e(k))T e UUTHn(x)VeVT  = 1 n p Ã• i=1 q Ã• j=1 âŸ¨ui,Hn(x)vjâŸ©âŸ¨Ëœui,Hn(e(k))ËœvjâŸ©. The decomposition in (1.58) can now be interpreted as the composition of an encoder and a decoder, x 7â†’C = (âŸ¨x,ui âˆ—â—¦vjâŸ©)iâˆˆ[p],jâˆˆ[q] and C 7â†’1 n p Ã• i=1 q Ã• j=1 Ci,j Ëœui âˆ—â—¦Ëœvj, (1.59) which relates it to CNNs equipped with an encoderâ€“decoder structure such as U- nets; see Figure 1.19. Generalizing this approach to multiple channels, it is possible to stack such encoders and decoders leading to a layered version of (1.58). Ye et al. (2018) show that one can make an informed decision on the number of layers on the basis of the rank of Hn(x), i.e., the complexity of the input features x. Moreover, an activation function such as the ReLU or bias vectors can also be included. The key question one can then ask is how the kernels can be chosen to obtain sparse coeï¬ƒcients C in (1.59) and a decomposition such as (1.58), i.e., perfect 29 Note that âŸ¨x, ui âˆ—â—¦vj âŸ©can also be interpreted as âŸ¨ui, vj â‹†xâŸ©, where â‹†denotes the cross-correlation between the zero-padded vj and x. This is in line with software implementations for deep learning applications, for example TensorFlow and PyTorch, where typically cross-correlations are used instead of convolutions. 1.6 Tangible Eï¬€ects of Special Architectures 73 Âµb Ïƒb Î² Î³ by = yâˆ’Âµb Ïƒb z = Î³by + Î² Figure 1.20 A batch normalization block after a fully connected neural network. The parameters Âµb, Ïƒb are the mean and the standard deviation of the output of the fully connected network computed over a batch s, i.e., a set of inputs. The parameters Î², Î³ are learnable parts of the batch normalization block. reconstruction. If U and V are chosen as the left and right singular vectors of Hn(x), one obtains a very sparse, however input-dependent, representation in (1.58) owing to the fact that Ci,j = âŸ¨x,ui âˆ—â—¦vjâŸ©= âŸ¨ui,Hn(x)vjâŸ©= 0, i , j. Finally, using the framework of deep convolutional framelets, theoretical reasons for including skip connections can be derived, since they aid in obtaining a perfect reconstruction. 1.6.4 Batch Normalization Batch normalization involves a building block of NNs that was invented in Ioï¬€e and Szegedy (2015) with the goal of reducing so-called internal covariance shift. In essence, this phrase describes the (undesirable) situation where, during training, each layer receives inputs with diï¬€erent distributions. A batch normalization block is deï¬ned as follows. For points b = (y(i))m i=1 âˆˆ(Rn)m and Î²,Î³ âˆˆR, we deï¬ne BN(Î²,Î³) b (y) B Î³ y âˆ’Âµb Ïƒb + Î², y âˆˆRn, with Âµb = 1 m m Ã• i=1 y(i) and Ïƒ2 b = 1 m m Ã• i=1 (y(i) âˆ’Âµb)2, (1.60) where all operations are to be understood componentwise; see Figure 1.20. Such a batch normalization block can be added into a NN architecture. Then b is the output of the previous layer over a batch or the whole training data.30 30 In practice, one typically uses a moving average to estimate the mean Âµ, and the standard deviation Ïƒ of the output of the previous layer, over the whole training data by using only batches. 74 Berner et al. The Modern Mathematics of Deep Learning Furthermore, the parameters Î²,Î³ are variable and can be learned during training. Note that if one sets Î² = Âµb and Î³ = Ïƒb then BN(Î²,Î³) b (y) = y for all y âˆˆRn. Therefore, a batch normalization block does not negatively aï¬€ect the expressivity of the architecture. On the other hand, batch normalization does have a tangible eï¬€ect on the optimization aspects of deep learning. Indeed, in Santurkar et al. (2018, Theorem 4.1), the following observation was made. Proposition 1.28 (Smoothening eï¬€ect of batch normalization). Let m âˆˆN with m â‰¥2, and for every Î²,Î³ âˆˆR deï¬ne B(Î²,Î³) : Rm â†’Rm by B(Î²,Î³)(b) = (BN(Î²,Î³) b (y(1)),. . .,BN(Î²,Î³) b (y(m))), b = (y(i))m i=1 âˆˆRm, (1.61) where BN(Î²,Î³) b is as given in (1.60). Let Î²,Î³ âˆˆR and let r : Rm â†’R be a diï¬€eren- tiable function. Then, for every b âˆˆRm, we have âˆ¥âˆ‡(r â—¦B(Î²,Î³))(b)âˆ¥2 2 = Î³2 Ïƒ2 b  âˆ¥âˆ‡r(b)âˆ¥2 âˆ’1 m âŸ¨1,âˆ‡r(b)âŸ©2 âˆ’1 m âŸ¨B(0,1)(b),âˆ‡r(b)âŸ©2 , where 1 = (1,. . .,1) âˆˆRm and Ïƒ2 b is as given in (1.60). For multi-dimensional y(i) âˆˆRn, i âˆˆ[m], the same statement holds for all components as, by deï¬nition, the batch normalization block acts componentwise. Proposition 1.28 follows from a convenient representation of the Jacobian of the mapping B(Î²,Î³), given by âˆ‚B(Î²,Î³)(b) âˆ‚b = Î³ Ïƒb  Im âˆ’1 m11T âˆ’1 mB(0,1)(b)(B(0,1)(b))T  , b âˆˆRm, and the fact that { 1 âˆšm, 1 âˆšmB(0,1)(b)} constitutes an orthonormal set. Choosing r to mimic the empirical risk of a learning task, Proposition 1.28 shows that, in certain situations â€“ for instance, if Î³ is smaller than Ïƒb or if m is not too large â€“ a batch normalization block can considerably reduce the magnitude of the derivative of the empirical risk with respect to the input of the batch normalization block. By the chain rule, this implies that also the derivative of the empirical risk with respect to NN parameters inï¬‚uencing the input of the batch normalization block is reduced. Interestingly, a similar result holds for second derivatives (Santurkar et al., 2018, Theorem 4.2) if r is twice diï¬€erentiable. One can conclude that adding a batch normalization block increases the smoothness of the optimization problem. Since the parameters Î² and Î³ were introduced, including a batch normalization block also increases the dimension of the optimization problem by 2. 1.6 Tangible Eï¬€ects of Special Architectures 75 Figure 1.21 A neural network with sparse connections. 1.6.5 Sparse Neural Networks and Pruning For deep FC NNs, the number of trainable parameters usually scales as the square of the number of neurons. For reasons of computational complexity and memory eï¬ƒciency, it appears sensible to seek for techniques that reduce the number of parameters or extract sparse subnetworks (see Figure 1.21) without much aï¬€ecting the output of a NN. One way to do it is by pruning (LeCun et al., 1989b; Han et al., 2016). Here, certain parameters of a NN are removed after training. This is done by, for example, setting these parameters to zero. In this context, the lottery ticket hypothesis was formulated in Frankle and Carbin (2018). It states that â€œA randomly-initialized, dense NN contains a subnetwork that is initialized such that â€“ when trained in isolation â€“ it can match the test accuracy of the original NN after training for at most the same number of iterations.â€ In Ramanujan et al. (2020) a similar hypothesis was made and empirically studied. There, it was claimed that, for a suï¬ƒciently overparametrized NN, there exists a subnetwork that matches the performance of the large NN after training without being trained itself, i.e., it already does so at initialization. Under certain simplifying assumptions, the existence of favorable subnetworks is quite easy to prove. We can use a technique that was indirectly used in Â§1.4.2 â€“ the CarathÃ©odory lemma. This result states the following. Let n âˆˆN, C âˆˆ(0,âˆ), and (H, âˆ¥Â· âˆ¥) be a Hilbert space. Let F âŠ‚H with supf âˆˆF âˆ¥f âˆ¥â‰¤C and let g âˆˆH be in the convex hull of F. Then there exist fi âˆˆF, i âˆˆ[n], and c âˆˆ[0,1]n with âˆ¥câˆ¥1 = 1, such that g âˆ’ n Ã• i=1 ci fi â‰¤C âˆšn; see, e.g., Vershynin (2018, Theorem 0.0.2). Proposition 1.29 (CarathÃ©odory pruning). Let d,n âˆˆN with n â‰¥100 and let Âµ be a probability measure on the unit ball B1(0) âŠ‚Rd. Let a = ((d,n,1), Ï±R) be the architecture of a two-layer ReLU network and let Î¸ âˆˆRP((d,n,1)) be corresponding 76 Berner et al. The Modern Mathematics of Deep Learning parameters such that Î¦a(Â·,Î¸) = n Ã• i=1 w(2) i Ï±R  âŸ¨w(1) i ,Â·âŸ©+ b(1) i , where  w(1) i , b(1) i  âˆˆRd Ã— R, i âˆˆ[n], and w(2) âˆˆRn. Assume that for every i âˆˆ[n] it holds true that âˆ¥w(1) i âˆ¥2 â‰¤1/2 and b(1) i â‰¤1/2. Then there exists a parameter ËœÎ¸ âˆˆRP((d,n,1)) with at least 99% of its entries zero such that âˆ¥Î¦a(Â·,Î¸) âˆ’Î¦a(Â·, ËœÎ¸)âˆ¥L2(Âµ) â‰¤15âˆ¥w(2)âˆ¥1 âˆšn . Speciï¬cally, there exists an index set I âŠ‚[n] with |I| â‰¤n/100 such that ËœÎ¸ satisï¬es ew(2) i = 0, if i < I, and (ew(1) i , Ëœb(1) i ) = ( (w(1) i , b(1) i ), if i âˆˆI, (0,0), if i < I. The result is clear if w(2) = 0. Otherwise, deï¬ne fi B âˆ¥w(2)âˆ¥1Ï±R(âŸ¨w(1) i ,Â·âŸ©+ b(1) i ), i âˆˆ[n], (1.62) and observe that Î¦a(Â·,Î¸) is in the convex hull of { fi}n i=1 âˆª{âˆ’fi}n i=1. Moreover, by the Cauchyâ€“Schwarz inequality, we have âˆ¥fiâˆ¥L2(Âµ) â‰¤âˆ¥w(2)âˆ¥1âˆ¥fiâˆ¥Lâˆ(B1(0)) â‰¤âˆ¥w(2)âˆ¥1. We conclude with the CarathÃ©odory lemma that there exists I âŠ‚[n] with |I| = âŒŠn/100âŒ‹â‰¥n/200 and ci âˆˆ[âˆ’1,1], i âˆˆI, such that Î¦a(Â·,Î¸) âˆ’ Ã• iâˆˆI ci fi L2(Âµ) â‰¤âˆ¥w(2)âˆ¥1 p |I| â‰¤ âˆš 200âˆ¥w(2)âˆ¥1 âˆšn , which yields the result. Proposition 1.29 shows that certain, very wide NNs can be approximated very well by sparse subnetworks in which only the output weight matrix needs to be changed. The argument of Proposition 1.29 was inspired by Barron and Klusowski (2018), where a much more reï¬ned result is shown for deep NNs. 1.6.6 Recurrent Neural Networks Recurrent NNs are NNs where the underlying graph is allowed to exhibit cycles, as in Figure 1.22; see Hopï¬eld (1982), Rumelhart et al. (1986), Elman (1990), and Jordan (1990). Above, we excluded cyclic computational graphs. For a feed-forward NN, the computation of internal states is naturally performed step by step through the layers. Since the output of a layer does not aï¬€ect the previous layers, the order 1.6 Tangible Eï¬€ects of Special Architectures 77 Figure 1.22 Sketch of a recurrent neural network. The cycles in the computational graph incor- porate the sequential structure of the input and output. in which the computations of the NN are performed corresponds to the order of the layers. For recurrent NNs the concept of layers does not exist, and the order of operations is much more delicate. Therefore, one considers time steps. In each time step, all possible computations of the graph are applied to the current state of the NN. This yields a new internal state. Given that time steps arise naturally from the deï¬nition of recurrent NNs, this NN type is typically used for sequential data. If the input to a recurrent NN is a sequence, then every input determines the internal state of the recurrent NN for the following inputs. Therefore, one can claim that these NNs exhibit a memory. This fact is extremely desirable in natural language processing, which is why recurrent NNs are widely used in this application. Recurrent NNs can be trained in a way similar to regular feed-forward NNs by an algorithm called backpropagation through time (Minsky and Papert, 1969; Werbos, 1988; Williams and Zipser, 1995). This procedure essentially unfolds the recurrent structure to yield a classical NN structure. However, the algorithm may lead to very deep structures. Owing to the vanishing- and exploding-gradient problem discussed earlier, very deep NNs are often hard to train. Because of this, special recurrent structures have been introduced that include gates that prohibit too many recurrent steps; these include the widely used long short-term memory gates, LSTMs, (Hochreiter and Schmidhuber, 1997). The application area of recurrent NNs is typically quite diï¬€erent from that of regular NNs since they are specialized on sequential data. Therefore, it is hard to quantify the eï¬€ect of a recurrent connection on a fully connected NN. However, it is certainly true that with recurrent connections certain computations can be per- formed much more eï¬ƒciently than with feed-forward NN structures. A particularly interesting construction can be found in Bohn and Feischl (2019, Theorem 4.4), where it is shown that a ï¬xed size, recurrent NN with ReLU activation function, can approximate the function x 7â†’x2 to any desired accuracy. The reason for this eï¬ƒcient representation can be seen when considering the self-referential deï¬nition of the approximant to x âˆ’x2 shown in Figure 1.9. On the other hand, with feed-forward NNs, it transpires from Theorem 1.26 that 78 Berner et al. The Modern Mathematics of Deep Learning the approximation error of ï¬xed-sized ReLU NNs for any non-aï¬ƒne function is greater than a positive lower bound. 1.7 Describing the Features that a Deep Neural Network Learns This section presents two viewpoints which help in understanding the nature of the features that can be described by NNs. Section 1.7.1 summarizes aspects of the so-called scattering transform, which constitutes a speciï¬c NN architecture that can be shown to satisfy desirable properties such as translation and deformation invariance. Section 1.7.2 relates NN features to the current paradigm of sparse coding. 1.7.1 Invariances and the Scattering Transform One of the ï¬rst theoretical contributions to the understanding of the mathematical properties of CNNs was by Mallat (2012). Their approach was to consider spe- ciï¬c CNN architectures with ï¬xed parameters that result in a stand-alone feature descriptor whose output may be fed into a subsequent classiï¬er (for example, a kernel support vector machine or a trainable FC NN). From an abstract point of view, a feature descriptor is a function Î¨ mapping from a signal space, such as L2(Rd) or the space of piecewise smooth functions, to a feature space. In an ideal world, such a classiï¬er should â€œfactorâ€ out invariances that are irrelevant to a sub- sequent classiï¬cation problem while preserving all other information of the signal. A very simple example of a classiï¬er which is invariant under translations is the Fourier modulus Î¨: L2(Rd) â†’L2(Rd), u 7â†’| Ë†u|. This follows from the fact that a translation of a signal u results in a modulation of its Fourier transform, i.e., Âœ u(Â· âˆ’Ï„)(Ï‰) = eâˆ’2Ï€iâŸ¨Ï„,Ï‰âŸ©Ë†u(Ï‰), Ï„,Ï‰ âˆˆRd. Furthermore, in most cases â€“ for example, if u is a generic compactly supported function (Grohs et al., 2020), u can be recon- structed up to a translation from its Fourier modulus (Grohs et al., 2020) and an energy conservation property of the form âˆ¥Î¨(u)âˆ¥L2 = âˆ¥uâˆ¥L2 holds true. Translation invariance is, for example, typically exhibited by image classiï¬ers, where the label of an image does not change if it is translated. In practical problems many more invariances arise. Providing an analogous rep- resentation that factors out general invariances would lead to a signiï¬cant reduction in the problem dimensionality and constitutes an extremely promising route to- wards dealing with the very high dimensionality that is commonly encountered in practical problems (Mallat, 2016). This program was carried out by Mallat (2012) for additional invariances with respect to deformations u 7â†’uÏ„ := u(Â·âˆ’Ï„(Â·)), where Ï„: Rd â†’Rd is a smooth mapping. Such transformations may occur in practice, 1.7 Describing the Features that a Deep Neural Network Learns 79 for instance, as image warpings. In particular, a feature descriptor Î¨ is designed so that, with a suitable norm âˆ¥Â· âˆ¥on the image of Î¨, it (a) is Lipschitz continuous with respect to deformations in the sense that âˆ¥Î¨(u) âˆ’Î¨(uÏ„)âˆ¥â‰²K(Ï„,âˆ‡Ï„,âˆ‡2Ï„) holds for some K that only mildly depends on Ï„ and essentially grows linearly in âˆ‡Ï„ and âˆ‡2Ï„, (b) is almost (i.e., up to a small and controllable error) invariant under translations of the input data, and (c) contains all relevant information on the input data in the sense that an energy conservation property âˆ¥Î¨(u)âˆ¥â‰ˆâˆ¥uâˆ¥L2 holds true. Observe that, while the action of translations only represents a d-parameter group, the action of deformations/warpings represents an inï¬nite-dimensional group. Thus, a deformation invariant feature descriptor represents a big potential for dimension- ality reduction. Roughly speaking, the feature descriptor Î¨ of Mallat (2012) (also coined the scattering transform) is deï¬ned by collecting features that are com- puted by iteratively applying a wavelet transform followed by a pointwise modulus nonlinearity and a subsequent low-pass ï¬ltering step, i.e., |||u âˆ—Ïˆj1| âˆ—Ïˆj2 âˆ—Â· Â· Â· | âˆ—Ïˆjâ„“| âˆ—Ï•J, where Ïˆj refers to a wavelet at scale j and Ï•J refers to a scaling function at scale J. The collection of all these so-called scattering coeï¬ƒcients can then be shown to satisfy the properties listed above in a suitable (asymptotic) sense. The proof of this result relies on a subtle interplay between the â€œdeformation covarianceâ€ property of the wavelet transform and the â€œregularizingâ€ property of the operation of convolution with the modulus of a wavelet. For a much more detailed exposition of the resulting scattering transform, we refer to Chapter 8 in this book. We remark that similar results can be shown also for diï¬€erent systems, such as Gabor frames (Wiatowski et al., 2017; Czaja and Li, 2019). 1.7.2 Hierarchical Sparse Representations The previous approach modeled the learned features by a speciï¬c dictionary, namely wavelets. It is well known that one of the striking properties of wavelets is to provide sparse representations for functions belonging to certain function classes. More generally, we speak of sparse representations with respect to a representation 80 Berner et al. The Modern Mathematics of Deep Learning system. For a vector x âˆˆRd, a sparsifying representation system D âˆˆRdÃ—p â€“ also called a dictionary â€“ is such that x = DÏ† where the coeï¬ƒcients Ï† âˆˆRp are sparse in the sense that âˆ¥Ï†âˆ¥0 B | supp(Ï†)| = |{i âˆˆ[p]: Ï†i , 0}| is small compared with p. A similar deï¬nition can be made for signals in inï¬nite-dimensional spaces. Taking sparse representations into account, the theory of sparse coding provides an approach to a theoretical understanding of the features that a deep NN learns. One common method in image processing is the utilization of not the entire image but overlapping patches of it, coined patch-based image processing. Thus of particular interest are local dictionaries which sparsify those patches but, pre- sumably, not the global image. This led to the introduction of the convolutional sparse coding (CSC) model, which links such local and global behaviors. Let us describe this model for one-dimensional convolutions on the group G := Z/(dZ) with kernels supported on the subgroup H := Z/(nZ), where d,n âˆˆN with n < d; see also Â§1.6.1. The corresponding CSC model is based on the decomposition of a global signal x âˆˆ(RG)c with c âˆˆN channels as xi = C Ã• j=1 Îºi,j âˆ—Ï†j, i âˆˆ[c], (1.63) where Ï† âˆˆ(RG)C is taken to be a sparse representation with C âˆˆN channels, and Îºi,j âˆˆRG, i âˆˆ[c], j âˆˆ[C], are local kernels with supp(Îºi,j) âŠ‚H. Let us consider a patch ((xi)g+h)iâˆˆ[c],hâˆˆH of n adjacent entries, starting at position g âˆˆG, in each channel of x. The condition on the support of the kernels Îºi,j and the representation in (1.63) imply that this patch is aï¬€ected only by a stripe of at most (2n âˆ’1) entries in each channel of Ï†. The local, patch-based sparsity of the representation Ï† can thus be appropriately measured via âˆ¥Ï†âˆ¥(n) 0,âˆB max gâˆˆG âˆ¥((Ï†j)g+k)jâˆˆ[C],kâˆˆ[2nâˆ’1]âˆ¥0; see Papyan et al. (2017b). Furthermore, note that we can naturally identify x and Ï† with vectors in Rdc and RdC and write x = DÏ†, where D âˆˆRdcÃ—dC is a matrix consisting of circulant blocks, typically referred to as a convolutional dictionary. The relation between the CSC model and deep NNs is revealed by applying the CSC model in a layer-wise fashion (Papyan et al., 2017a; Sulam et al., 2018; Papyan et al., 2018). To see this, let C0 âˆˆN and for every â„“âˆˆ[L] let Câ„“, kâ„“âˆˆN and let D(â„“) âˆˆRdCâ„“âˆ’1Ã—dCâ„“be a convolutional dictionary with kernels supported on Z/(nâ„“Z). A signal x = Ï†(0) âˆˆRdC0 is said to belong to the corresponding multi-layered CSC (ML-CSC) model if there exist coeï¬ƒcients Ï†(â„“) âˆˆRdCâ„“with Ï†(â„“âˆ’1) = D(â„“)Ï†(â„“) and âˆ¥Ï†(â„“)âˆ¥(nâ„“) 0,âˆâ‰¤kâ„“, â„“âˆˆ[L]. (1.64) We now consider the problem of reconstructing the sparse coeï¬ƒcients (Ï†(â„“))L â„“=1 1.8 Eï¬€ectiveness in Natural Sciences 81 from a noisy signal Ëœx B x + Î½, where the noise Î½ âˆˆRdC0 is assumed to have small â„“2-norm and x is assumed to follow the ML-CSC model in (1.64). In general, this problem is NP-hard. However, under suitable conditions on the ML-CSC model, it can be solved approximately, for instance by a layered thresholding algorithm. More precisely, for D âˆˆRdcÃ—dC and b âˆˆRdC, we deï¬ne a soft-thresholding operator by TD,b(x) B Ï±R(DT x âˆ’b) âˆ’Ï±R(âˆ’DT x âˆ’b), x âˆˆRdc, (1.65) where Ï±R(x) = max{0, x} is applied componentwise. If x = DÏ† as in (1.63), we obtain Ï† â‰ˆTD,b(x) roughly under the following conditions. The distance of Ï† from Ïˆ B DT x = DT DÏ† can be bounded using the local sparsity of Ï† and the mutual coherence and locality of the kernels of the convolutional dictionary D. For a suitable threshold b, the mapping Ïˆ 7â†’Ï±R(Ïˆ âˆ’b) âˆ’Ï±R(âˆ’Ïˆ âˆ’b) further recovers the support of Ï† by nullifying those entries of Ïˆ with Ïˆi â‰¤|bi|. Utilizing the soft-thresholding operator (1.65) iteratively for corresponding vectors b(â„“) âˆˆRdCâ„“, â„“âˆˆ[L], then suggests the following approximations: Ï†(â„“) â‰ˆ(TD(â„“),b(â„“) â—¦Â· Â· Â· â—¦TD(1),b(1))( Ëœx), â„“âˆˆ[L]. The resemblance to the realization of a CNN with ReLU activation function is evident. The transposed dictionary (D(â„“))T can be regarded as modeling the learned convolutional kernels, the threshold b(â„“) models the bias vector, and the soft- thresholding operator TD(â„“),b(â„“) mimics the application of a convolutional block with an ReLU nonlinearity in the â„“th layer. Using this model, a theoretical understanding of CNNs from the perspective of sparse coding is now at hand. This novel perspective gives a precise mathematical meaning of the kernels in a CNN as sparsifying dictionaries of an ML-CSC model. Moreover, the forward pass of a CNN can be understood as a layered thresholding algorithm for decomposing a noisy signal Ëœx. The results derived then have the following ï¬‚avor. Given a suitable reconstruction procedure such as thresholding or â„“1-minimization, the sparse coeï¬ƒcients (Ï†(â„“))L â„“=1 of a signal x following an ML- CSC model can be stably recovered from the noisy signal Ëœx under certain hypotheses on the ingredients of the ML-CSC model. 1.8 Eï¬€ectiveness in Natural Sciences The theoretical insights of the previous sections do not always accurately describe the performance of NNs in applications. Indeed, there often exists a considerable gap between the predictions of approximation theory and the practical performance of NNs (Adcock and Dexter, 2020). 82 Berner et al. The Modern Mathematics of Deep Learning In this section, we consider concrete applications which have been very success- fully solved with deep-learning-based methods. In Â§1.8.1 we present an overview of deep-learning-based algorithms applied to inverse problems. Section 1.8.2 then continues by describing how NNs can be used as a numerical ansatz for solv- ing PDEs, highlighting their use in the solution of the multi-electron SchrÃ¶dinger equation. 1.8.1 Deep Neural Networks Meet Inverse Problems The area of inverse problems, predominantly in imaging, was probably the ï¬rst class of mathematical methods embracing deep learning with overwhelming success. Let us consider a forward operator K : Y â†’X where X,Y are Hilbert spaces, and the associated inverse problem of ï¬nding y âˆˆY such that Ky = x for given features x âˆˆX. The classical model-based approach to regularization aims to approximate K by invertible operators, and is hence strongly based on functional analytic prin- ciples. Today, such approaches take the well-posedness of the approximation and its convergence properties, as well as the structure of regularized solutions, into ac- count. The last item allows to incorporate prior information of the original solution such as regularity, sharpness of edges, or â€“ in the case of sparse regularization (Jin et al., 2017a) â€“ a sparse coeï¬ƒcient sequence with respect to a prescribed represen- tation system. Such approaches are typically realized in a variational setting and hence aim to minimize functionals of the form âˆ¥Ky âˆ’xâˆ¥2 + Î±R(y), (1.66) where Î± âˆˆ(0,âˆ) is a regularization parameter, R: Y â†’[0,âˆ) is a regularization term, and âˆ¥Â· âˆ¥denotes the norm on Y. As already stated, the regularization term aims to model structural information about the desired solution. However, one main hurdle in this approach is the problem that, typically, solution classes such as images from computed tomography cannot be modeled accurately enough to allow, for instance, reconstruction under the constraint of a signiï¬cant amount of missing features. This has opened the door to data-driven approaches, and recently, deep NNs. Solvers of inverse problems that are based on deep learning techniques can be roughly categorized into three classes: (i) Supervised approaches. The most straightforward approach is to train a NN Î¦(Â·,Î¸): X â†’Y end-to-end, i.e., to completely learn the map from data x to the solution y. More advanced approaches in this direction incorporate information about the operator K into the NN as in Adler and Ã–ktem (2017), Gilton et al. (2019), and Monga et al. (2021). Yet another type of approach aims to combine 1.8 Eï¬€ectiveness in Natural Sciences 83 deep NNs with classical model-based approaches. The ï¬rst suggestion in this realm was that one should start by applying a standard solver and then use a deep NN, Î¦(Â·,Î¸): Y â†’Y, which serves as a denoiser for speciï¬c reconstruction arti- facts; e.g., Jin et al. (2017b). This approach was followed by more sophisticated methods such as plug-and-play frameworks for coupling inversion and denoising (Romano et al., 2017). (ii) Semi-supervised approaches. This type of approach aims to encode the regular- ization by a deep NN Î¦(Â·,Î¸): Y â†’[0,âˆ). The underlying idea often requires stronger regularization on those solutions y(i) that are more prone to artifacts or other eï¬€ects of the instability of the problem. On solutions where typically few artifacts are observed less regularization can be used. Therefore, the learning algorithm requires only a set of labels (y(i))m i=1 as well as a method for assessing how hard the inverse problem for this label would be. In this sense, the algo- rithm can be considered semi-supervised. This idea was followed, for example, in Lunz et al. (2018), and Li et al. (2020). Taking a Bayesian viewpoint, one can also learn prior distributions as deep NNs; this was done in Barbano et al. (2020). (iii) Unsupervised approaches. One highlight of what we might call unsupervised approaches in our problem setting has been the introduction of deep image priors in Dittmer et al. (2020), and Ulyanov et al. (2018). The key idea is to parametrize the solutions y as the output of a NN Î¦(Î¾,Â·): P â†’Y with parameters in a suitable space P applied to a ï¬xed input Î¾. Then, for given features x, one tries to solve minÎ¸ âˆˆP âˆ¥KÎ¦(Î¾,Î¸) âˆ’xâˆ¥2 in order to obtain parameters Ë†Î¸ âˆˆP that yield a solution candidate y = Î¦(Î¾, Ë†Î¸). Here early stopping is often applied in the training of the network parameters. As can be seen, one key conceptual question is how to â€œtake the best out of both worlds,â€ in the sense of optimally combining classical (model-based) methods â€“ in particular the forward operator K â€“ with deep learning. This is certainly sensitively linked to all characteristics of the particular application at hand, such as the availability and accuracy of training data, properties of the forward operator, and requirements for the solution. And each of the three classes of hybrid solvers follows a diï¬€erent strategy. Let us now discuss the advantages and disadvantages of methods from the three categories with a particular focus on a mathematical foundation. Supervised ap- proaches suï¬€er on the one hand from the problem that often ground-truth data is not available or only in a very distorted form, leading to the use of synthetic data as a signiï¬cant part of the training data. Thus the learned NN will mainly perform as well as the algorithm which generated the data, but will not signiï¬cantly improve on it â€“ except from an eï¬ƒciency viewpoint. On the other hand, the inversion is often 84 Berner et al. The Modern Mathematics of Deep Learning highly ill posed, i.e., the inversion map has a large Lipschitz constant, which nega- tively aï¬€ects the generalization ability of the NN. Improved approaches incorporate knowledge about the forward operator K, which helps to circumvent this issue. One signiï¬cant advantage of semi-supervised approaches is that the underlying mathematical model of the inverse problem is merely augmented by neural-network- based regularization. Assuming that the learned regularizer satisï¬es natural assump- tions, convergence proofs or stability estimates for the resulting regularized methods are still available. Finally, unsupervised approaches have the advantage that the regularization is then fully due to the speciï¬c architecture of the deep NN. This makes these methods slightly easier to understand theoretically, although, for instance, the deep prior approach in its full generality is still lacking a profound mathematical analysis. 1.8.2 PDE-Based Models Besides applications in image processing and artiï¬cial intelligence, deep learning methods have recently strongly impacted the ï¬eld of numerical analysis. In partic- ular, regarding the numerical solution of high-dimensional PDEs. These PDEs are widely used as a model for complex processes and their numerical solution presents one of the biggest challenges in scientiï¬c computing. We mention examples from three problem classes. (i) Blackâ€“Scholes model. The Nobel award-winning theory of Fischer Black, Robert Merton, and Myron Scholes proposes a linear PDE model for the determination of a fair price of a (complex) ï¬nancial derivative. The dimensionality of the model corresponds to the number of ï¬nancial assets, which is typically quite large. The classical linear model, which can be solved eï¬ƒciently via Monte Carlo methods, is quite limited. In order to take into account more realistic phenomena such as default risk, the PDE that models a fair price becomes nonlinear and much more challenging to solve. In particular (with the notable exception of multi-level Picard algorithms E et al., 2019c) no general algorithm exists that provably scales well with the dimension. (ii) SchrÃ¶dinger equation. The electronic SchrÃ¶dinger equation describes the sta- tionary non-relativistic behavior of a quantum mechanical electron system in the electric ï¬eld generated by the nuclei of a molecule. A numerical solution is required to obtain stable molecular conï¬gurations, compute vibrational spectra, or obtain forces governing molecular dynamics. If the number of electrons is large, this is again a high-dimensional problem and to date there exist no satis- factory algorithms for its solution. It is well known that diï¬€erent gold standard methods may produce completely diï¬€erent energy predictions, for example, 1.8 Eï¬€ectiveness in Natural Sciences 85 when applied to large delocalized molecules, rendering these methods useless for those problems. (iii) Hamiltonâ€“Jacobiâ€“Bellman equation. The Hamiltonâ€“Jacobiâ€“Bellman (HJB) equation models the value function of (deterministic or stochastic) optimal con- trol problems. The underlying dimensionality of the model corresponds to the dimension of the space of states to be controlled and tends to be rather high in realistic applications. This high dimensionality, together with the fact that HJB equations typically tend to be fully nonlinear with non-smooth solutions, renders the numerical solution of HJB equations extremely challenging, and no general algorithms exist for this problem. Thanks to the favorable approximation results of NNs for high-dimensional func- tions (see especially Â§Â§1.4.3), it might not come as a surprise that a NN ansatz has proven to be quite successful in solving the aforementioned PDE models. Pi- oneering work in this direction was by Han et al. (2018) who used the backwards SDE reformulation of semi-linear parabolic PDEs to reformulate the evaluation of such a PDE, at a speciï¬c point, as an optimization problem that can be solved by the deep learning paradigm. The resulting algorithm proves quite successful in the high-dimensional regime and, for instance, enables the eï¬ƒcient modeling of complex ï¬nancial derivatives including nonlinear eï¬€ects such as default risk. Another approach speciï¬cally tailored to the numerical solution of HJB equations is Nakamura-Zimmerer et al. (2021). In this work, the Pontryagin principle was used to generate samples of the PDE solution along solutions of the corresponding boundary value problem. Other numerical approaches include the deep Ritz method (E and Yu, 2018), where a Dirichlet energy is minimized over a set of NNs; or so-called physics informed neural networks (Raissi et al., 2019), where typically the PDE residual is minimized along with some natural constraints, for instance, to enforce boundary conditions. Deep-learning-based methods arguably work best if they are combined with do- main knowledge to inspire NN architecture choices. We would like to illustrate this interplay at the hand of a speciï¬c and extremely relevant example: the elec- tronic SchrÃ¶dinger equation (under the Bornâ€“Oppenheimer approximation), which amounts to ï¬nding the smallest non-zero eigenvalue of the eigenvalue problem HRÏˆ = Î»ÏˆÏˆ, (1.67) 86 Berner et al. The Modern Mathematics of Deep Learning for Ïˆ : R3Ã—n â†’R, where the Hamiltonian (HRÏˆ)(r) = âˆ’ n Ã• i=1 1 2(âˆ†riÏˆ)(r)âˆ’ n Ã• i=1 p Ã• j=1 Zj âˆ¥ri âˆ’Rjâˆ¥2 âˆ’ pâˆ’1 Ã• i=1 p Ã• j=i+1 ZiZj âˆ¥Ri âˆ’Rjâˆ¥2 âˆ’ nâˆ’1 Ã• i=1 n Ã• j=i+1 1 âˆ¥ri âˆ’rjâˆ¥2 ! Ïˆ(r) describes the kinetic energy (ï¬rst term) as well as the Coulomb attraction force between electrons and nuclei (second and third terms) and the Coulomb repul- sion force between diï¬€erent electrons (fourth term). Here, the coordinates R =  R1,. . ., Rp  âˆˆR3Ã—p refer to the positions of the nuclei, (Zi)p i=1 âˆˆNp denote the atomic numbers of the nuclei, and the coordinates r =  r1,. . .,rn  âˆˆR3Ã—n re- fer to the positions of the electrons. The associated eigenfunction Ïˆ describes the so-called wave function, which can be interpreted in the sense that |Ïˆ(r)|2/âˆ¥Ïˆâˆ¥2 L2 describes the joint probability density of the n electrons to be located at r. The smallest solution Î»Ïˆ of (1.67) describes the ground state energy associated with the nuclear coordinates R. It is of particular interest to know the ground state energy for all nuclear coordinates, the so-called potential energy surface, whose gradient determines the forces governing the dynamic motions of the nuclei. The numerical solution of (1.67) is complicated by the Pauli principle, which states that the wave function Ïˆ must be antisymmetric in all coordinates representing electrons of equal spin. We need to clarify that every electron is deï¬ned not only by its location but also by its spin, which may be positive or negative. Depending on whether two electrons have the same spin or not, their interaction changes considerably. This is reï¬‚ected in the Pauli principle mentioned above. Suppose that electrons i and j have equal spin; then the wave function must satisfy Pi,jÏˆ = âˆ’Ïˆ, (1.68) where Pi,j denotes the operator that swaps ri and rj, i.e., (Pi,jÏˆ)(r) = Ïˆ(r1,. . .,rj,. . .,ri,. . .,rn). In particular, no two electrons with the same spin can occupy the same location. The challenges associated with solving the SchrÃ¶dinger equation inspired the following famous quote of Paul Dirac (1929): â€œThe fundamental laws necessary for the mathematical treatment of a large part of physics and the whole of chemistry are thus completely known, and the diï¬ƒculty lies only in the fact that application of these laws leads to equations that are too complex to be solved.â€ We now describe how deep learning methods might help to mitigate this claim 1.8 Eï¬€ectiveness in Natural Sciences 87 to a certain extent. Let X be a random variable with density |Ïˆ(r)|2/âˆ¥Ïˆâˆ¥2 L2. Using the Rayleighâ€“Ritz principle, ï¬nding the minimal non-zero eigenvalue of (1.67) can be reformulated as minimizing the Rayleigh quotient âˆ« R3Ã—n Ïˆ(r)(HRÏˆ)(r) dr âˆ¥Ïˆâˆ¥2 L2 = E (HRÏˆ)(X) Ïˆ(X)  (1.69) over all Ïˆâ€™s satisfying the Pauli principle; see Szabo and Ostlund (2012). Since this represents a minimization problem it can in principle be solved via a NN ansatz by generating training data distributed according to X using MCMC sampling.31 Since the wave function Ïˆ will be parametrized as a NN, the minimization of (1.69) will require the computation of the gradient of (1.69) with respect to the NN parameters (the method in Pfau et al., 2020, even requires second-order derivatives), which, at ï¬rst sight, might seem to require the computation of third-order derivatives. However, due to the Hermitian structure of the Hamiltonian, one does not need to compute the derivative of the Laplacian of Ïˆ; see, for example Hermann et al. (2020, Equation (8)). Compared with the other PDE problems we have discussed, an additional com- plication arises from the need to incorporate structural properties and invariances such as the Pauli principle. Furthermore, empirical evidence shows that it is also necessary to hard-code the so-called cusp conditions which describe the asymp- totic behavior of nearby electrons and of electrons close to a nucleus into the NN architecture. A ï¬rst attempt in this direction was made by Han et al. (2019), and sig- niï¬cantly improved NN architectures have been developed in Hermann et al. (2020), Pfau et al. (2020), and Scherbela et al. (2021) opening the possibility of accurate ab initio computations for previously intractable molecules. The mathematical proper- ties of this exciting line of work remain largely unexplored. We brieï¬‚y describe the main ideas behind the NN architecture of Hermann et al. (2020); Scherbela et al. (2021). Standard numerical approaches (notably the multireference Hartreeâ€“Fock method; see Szabo and Ostlund, 2012) use a low-rank approach to minimize (1.69). Such an approach would approximate Ïˆ by sums of products of one-electron or- bitals Ãn i=1 Ï•i(ri) but clearly this would not satisfy the Pauli principle (1.68). In order to accommodate the Pauli principle, one constructs so-called Slater determi- nants from one-electron orbitals with equal spin. More precisely, suppose that the ï¬rst n+ electrons with coordinates r1,. . .,rn+ have positive spin and the last n âˆ’n+ electrons have negative spin. Then any function of the form det  Ï•i(rj)n+ i,j=1  Ã— det  Ï•i(rj)n i,j=n++1  (1.70) 31 Observe that for such sampling methods one can just use the unnormalized density |Ïˆ(r)|2 and thus avoid the computation of the normalization âˆ¥Ïˆâˆ¥2 L2. 88 Berner et al. The Modern Mathematics of Deep Learning Figure 1.23 By sharing layers across diï¬€erent nuclear geometries one can eï¬ƒciently compute diï¬€erent geometries in one single training step (Scherbela et al., 2021). Top: potential energy surface of an H10 chain computed by the deep-learning-based algorithm from Scherbela et al. (2021). The lowest energy is achieved when pairs of H atoms enter into a covalent bond to form ï¬ve H2 molecules. Bottom: the method of Scherbela et al. (2021) is capable of accurately computing forces between nuclei, which allows for molecular dynamics simulations from ï¬rst principles. satisï¬es (1.68) and is typically called a Slater determinant. While the Pauli prin- ciple establishes a (non-classical) interaction between electrons of equal spin, the so-called exchange correlation, in the representation (1.70) electrons with opposite spins are uncorrelated. In particular, (1.70) ignores interactions between electrons that arise through Coulomb forces, implying that no non-trivial wave function can be accurately represented by a single Slater determinant. To capture the physical interactions between diï¬€erent electrons, one needs to use sums of Slater determi- nants as an ansatz. However, it turns out that the number of such determinants that are needed to guarantee a given accuracy scales very badly with the system size n (to our knowledge the best currently known approximation results are contained in Yserentant (2010), where an n-independent error rate is shown; however, the implicit constant in this rate depends at least exponentially on the system size n). We would like to highlight the approach of Hermann et al. (2020), whose main 1.8 Eï¬€ectiveness in Natural Sciences 89 idea was to use NNs to incorporate interactions into Slater determinants of the form (1.70) using what is called the backï¬‚ow trick (RÃ­os et al., 2006). The basic building blocks would now consist of functions of the form det  Ï•i(rj)Î¨j(r,Î¸j)n+ i,j=1  Ã— det  Ï•i(rj)Î¨j(r,Î¸j)n i,j=n++1  , (1.71) where the Î¨k(Â·,Î¸k), k âˆˆ[n], are NNs. If these are arbitrary NNs, it is easy to see that the Pauli principle (1.68) will not be satisï¬ed. However, if we require the NNs to be symmetric, for example, in the sense that for i, j, s âˆˆ[n+] it holds true that Pi,jÎ¨k(Â·,Î¸k) = ï£±ï£´ï£´ï£´ï£² ï£´ï£´ï£´ï£³ Î¨k(Â·,Î¸k), if k < {i, j}, Î¨i(Â·,Î¸i), if k = j, Î¨j(Â·,Î¸j), if k = i, (1.72) and analogous conditions hold for i, j, k âˆˆ[n] \ [n+], the expression (1.71) does actually satisfy (1.68). The construction of such symmetric NNs can be achieved by using a modiï¬cation of the so-called SchNet architecture (SchÃ¼tt et al., 2017) which can be considered as a speciï¬c residual NN. We describe a simpliï¬ed construction inspired by Han et al. (2019) and used in a slightly more complex form in Scherbela et al. (2021). We restrict ourselves to the case of positive spin (for example, the ï¬rst n+ coordinates), the case of negative spin being handled in the same way. Let Î¥(Â·,Î¸+ emb) be a univariate NN (with possibly multivariate output) and denote Embk(r,Î¸+ emb) B n+ Ã• i=1 Î¥(âˆ¥rk âˆ’riâˆ¥2,Î¸+ emb), k âˆˆ[n+], as the kth embedding layer. For k âˆˆ[n+], we can now deï¬ne Î¨k (r,Î¸k) = Î¨k  r,(Î¸k,fc,Î¸+ emb) = Î“k   Embk(r,Î¸+ emb),(rn++1,. . .,rn) ,Î¸k,fc  , where Î“k(Â·,Î¸k,fc) denotes a standard FC NN with input dimension equal to the output dimension of Î¨+ plus the dimension of the negative-spin electrons. The networks Î¨k, k âˆˆ[n] \ [n+], are deï¬ned analogously using diï¬€erent parameters Î¸âˆ’ emb for the embeddings. It is straightforward to check that the NNs Î¨k, k âˆˆ[n], satisfy (1.72) so that the backï¬‚ow determinants (1.71) satisfy the Pauli principle (1.68). In Hermann et al. (2020) the backï¬‚ow determinants (1.71) are further augmented by a multiplicative correction term, the so-called Jastrow factor, which is also represented by a speciï¬c symmetric NN, as well as a correction term that ensures the validity of the cusp conditions. The results of Hermann et al. (2020) show that this ansatz (namely using linear combinations of backï¬‚ow determinants (1.71) instead of plain Slater determinants (1.70)) is vastly more eï¬ƒcient in terms of the number of determinants needed to obtain chemical accuracy. The full architecture provides 90 Berner et al. The Modern Mathematics of Deep Learning a general purpose NN architecture to represent complicated wave functions. A distinct advantage of this approach is that some parameters (for example, regarding the embedding layers) may be shared across diï¬€erent nuclear geometries R âˆˆR3Ã—p, which allows for the eï¬ƒcient computation of potential energy surfaces (Scherbela et al., 2021); see Figure 1.23. Finally, we would like to highlight the need for customized NN design that incorporates physical invariances, domain knowledge (for example, in the form of cusp conditions), and existing numerical methods, all of which are required for the method to reach its full potential. Acknowledgments The research of JB was supported by the Austrian Science Fund (FWF) under grant I3403-N32. GK acknowledges support from DFG-SPP 1798 Grants KU 1446/21-2 and KU 1446/27-2, DFG-SFB/TR 109 Grant C09, BMBF Grant MaGriDo, and NSF-Simons Foundation Grant SIMONS 81420. The authors would like to thank HÃ©ctor Andrade Loarca, Dennis ElbrÃ¤chter, Adalbert Fono, Pavol Harar, Lukas Liehr, Duc Anh Nguyen, Mariia Seleznova, and Frieder Simon for their helpful feedback on an early version of this chapter. In particular, Dennis ElbrÃ¤chter pro- vided help for several theoretical results. References Ackley, David H., Hinton, Geoï¬€rey E., and Sejnowski, Terrence J. 1985. A learning algorithm for Boltzmann machines. Cognitive Science, 9(1), 147â€“169. Adcock, Ben, and Dexter, Nick. 2020. The gap between theory and prac- tice in function approximation with deep neural networks. ArXiv preprint arXiv:2001.07523. Adler, Jonas, and Ã–ktem, Ozan. 2017. Solving ill-posed inverse problems using iterative deep neural networks. Inverse Problems, 33(12), 124007. Al-Hamdani, Yasmine S., Nagy, PÃ©ter R., Barton, Dennis, KÃ¡llay, MihÃ¡ly, Bran- denburg, Jan Gerit, and Tkatchenko, Alexandre. 2020. Interactions between large molecules: Puzzle for reference quantum-mechanical methods. ArXiv preprint arXiv:2009.08927. Allen-Zhu, Zeyuan, Li, Yuanzhi, and Song, Zhao. 2019. A convergence theory for deep learning via over-parameterization. Pages 242â€“252 of: Proc. Interna- tional Conference on Machine Learning. Anthony, Martin, and Bartlett, Peter L. 1999. Neural Network Learning: Theoretical Foundations. Cambridge University Press. Arora, Sanjeev, Cohen, Nadav, and Hazan, Elad. 2018a. On the optimization of References 91 deep networks: Implicit acceleration by overparameterization. Pages 372â€“389 of: Proc. International Conference on Machine Learning. Arora, Sanjeev, Ge, Rong, Neyshabur, Behnam, and Zhang, Yi. 2018b. Stronger generalization bounds for deep nets via a compression approach. Pages 254â€“ 263 of: Proc. International Conference on Machine Learning. Arora, Sanjeev, Cohen, Nadav, Golowich, Noah, and Hu, Wei. 2019a. A convergence analysis of gradient descent for deep linear neural networks. In: International Conference on Learning Representations. Arora, Sanjeev, Du, Simon S., Hu, Wei, Li, Zhiyuan, Salakhutdinov, Ruslan, and Wang, Ruosong. 2019b. On exact computation with an inï¬nitely wide neural net. Pages 8139â€“8148 of: Advances in Neural Information Processing Systems. Arridge, Simon, Maass, Peter, Ã–ktem, Ozan, and SchÃ¶nlieb, Carola-Bibiane. 2019. Solving inverse problems using data-driven models. Acta Numerica, 28, 1â€“ 174. Auer, Peter, Herbster, Mark, and Warmuth, Manfred K. 1996. Exponentially many local minima for single neurons. Page 316â€“322 of: Advances in Neural Infor- mation Processing Systems. Auï¬ƒnger, Antonio, Arous, GÃ©rard Ben, and ÄŒern`y, JiÅ™Ã­. 2013. Random matri- ces and complexity of spin glasses. Communications on Pure and Applied Mathematics, 66(2), 165â€“201. Bahdanau, Dzmitry, Cho, Kyunghyun, and Bengio, Yoshua. 2015. Neural machine translation by jointly learning to align and translate. In: Proc. International Conference on Learning Representations. Baldi, Pierre, Sadowski, Peter, and Whiteson, Daniel. 2014. Searching for exotic particles in high-energy physics with deep learning. Nature Communications, 5(1), 1â€“9. Barbano, Riccardo, Zhang, Chen, Arridge, Simon, and Jin, Bangti. 2020. Quantify- ing model uncertainty in inverse problems via Bayesian deep gradient descent. ArXiv preprint arXiv:2007.09971. Barron, Andrew R. 1992. Neural net approximation. Pages 69â€“72 of: Proc. Yale Workshop on Adaptive and Learning Systems, vol. 1. Barron, Andrew R. 1993. Universal approximation bounds for superpositions of a sigmoidal function. IEEE Transactions on Information Theory, 39(3), 930â€“ 945. Barron, Andrew R., and Klusowski, Jason M. 2018. Approximation and es- timation for high-dimensional deep learning networks. ArXiv preprint arXiv:1809.03090. Bartlett, Peter L. 1998. The sample complexity of pattern classiï¬cation with neural networks: The size of the weights is more important than the size of the network. IEEE Transactions on Information Theory, 44(2), 525â€“536. Bartlett, Peter L, Maiorov, Vitaly, and Meir, Ron. 1998. Almost linear VC-dimension bounds for piecewise polynomial networks. Neural Computation, 10(8), 2159â€“ 2173. 92 Berner et al. The Modern Mathematics of Deep Learning Bartlett, Peter L., Bousquet, Olivier, and Mendelson, Shahar. 2005. Local Rademacher complexities. Annals of Statistics, 33(4), 1497â€“1537. Bartlett, Peter L., Foster, Dylan J., and Telgarsky, Matus. 2017. Spectrally- normalized margin bounds for neural networks. Pages 6240â€“6249 of: Ad- vances in Neural Information Processing Systems. Bartlett, Peter L., Harvey, Nick, Liaw, Christopher, and Mehrabian, Abbas. 2019. Nearly-tight VC-dimension and pseudodimension bounds for piecewise linear neural networks. Journal of Machine Learning Research, 20, 63â€“1. Bartlett, Peter L., Long, Philip M., Lugosi, GÃ¡bor, and Tsigler, Alexander. 2020. Benign overï¬tting in linear regression. Proceedings of the National Academy of Sciences, 117(48), 30063â€“30070. Baum, Eric B., and Haussler, David. 1989. What size net gives valid generalization? Neural Computation, 1(1), 151â€“160. Beck, Christian, Becker, Sebastian, Grohs, Philipp, Jaafari, Nor, and Jentzen, Ar- nulf. 2021. Solving the Kolmogorov PDE by means of deep learning. Journal of Scientiï¬c Computing, 83(3), 1â€“28. Belkin, Mikhail, Ma, Siyuan, and Mandal, Soumik. 2018. To understand deep learning we need to understand kernel learning. Pages 541â€“549 of: Proc. International Conference on Machine Learning. Belkin, Mikhail, Rakhlin, Alexander, and Tsybakov, Alexandre B. 2019a. Does data interpolation contradict statistical optimality? Pages 1611â€“1619 of: Proc. International Conference on Artiï¬cial Intelligence and Statistics. Belkin, Mikhail, Hsu, Daniel, Ma, Siyuan, and Mandal, Soumik. 2019b. Rec- onciling modern machine-learning practice and the classical biasâ€“variance trade-oï¬€. Proceedings of the National Academy of Sciences, 116(32), 15849â€“ 15854. Belkin, Mikhail, Hsu, Daniel, and Xu, Ji. 2020. Two models of double descent for weak features. SIAM Journal on Mathematics of Data Science, 2(4), 1167â€“ 1180. Bellman, Richard. 1952. On the theory of dynamic programming. Proceedings of the National Academy of Sciences, 38(8), 716. Berner, Christopher, Brockman, Greg, Chan, Brooke, Cheung, Vicki, Debiak, Prze- myslaw, Dennison, Christy, Farhi, David, Fischer, Quirin, Hashme, Shariq, and Hesse, Chris. 2019a. Dota 2 with large scale deep reinforcement learning. ArXiv preprint arXiv:1912.06680. Berner, Julius, ElbrÃ¤chter, Dennis, and Grohs, Philipp. 2019b. How degenerate is the parametrization of neural networks with the ReLU activation function? Pages 7790â€“7801 of: Advances in Neural Information Processing Systems. Berner, Julius, Grohs, Philipp, and Jentzen, Arnulf. 2020a. Analysis of the general- ization error: Empirical risk minimization over deep artiï¬cial neural networks overcomes the curse of dimensionality in the numerical approximation of Blackâ€“Scholes partial diï¬€erential equations. SIAM Journal on Mathematics of Data Science, 2(3), 631â€“657. References 93 Berner, Julius, Dablander, Markus, and Grohs, Philipp. 2020b. Numerically solv- ing parametric families of high-dimensional Kolmogorov partial diï¬€erential equations via deep learning. Pages 16615â€“16627 of: Advances in Neural Information Processing Systems. Blum, Avrim, and Rivest, Ronald L. 1989. Training a 3-node neural network is NP-complete. Pages 494â€“501 of: Advances in Neural Information Processing Systems. Bohn, Jan, and Feischl, Michael. 2019. Recurrent neural networks as optimal mesh reï¬nement strategies. ArXiv preprint arXiv:1909.04275. Bourely, Alfred, Boueri, John Patrick, and Choromonski, Krzysztof. 2017. Sparse neural networks topologies. ArXiv preprint arXiv:1706.05683. Bousquet, Olivier, and Elisseeï¬€, AndrÃ©. 2002. Stability and generalization. Journal of Machine Learning Research, 2(March), 499â€“526. Bousquet, Olivier, Boucheron, StÃ©phane, and Lugosi, GÃ¡bor. 2003. Introduction to statistical learning theory. Pages 169â€“207 of: Proc. Summer School on Machine Learning. Bronstein, Michael M, Bruna, Joan, LeCun, Yann, Szlam, Arthur, and Van- dergheynst, Pierre. 2017. Geometric deep learning: Going beyond euclidean data. IEEE Signal Processing Magazine, 34(4), 18â€“42. Brown, Tom, Mann, Benjamin, Ryder, Nick, Subbiah, Melanie, Kaplan, Jared D, Dhariwal, Prafulla, Neelakantan, Arvind, Shyam, Pranav, Sastry, Girish, Askell, Amanda, Agarwal, Sandhini, Herbert-Voss, Ariel, Krueger, Gretchen, Henighan, Tom, Child, Rewon, Ramesh, Aditya, Ziegler, Daniel, Wu, Jeï¬€rey, Winter, Clemens, Hesse, Chris, Chen, Mark, Sigler, Eric, Litwin, Mateusz, Gray, Scott, Chess, Benjamin, Clark, Jack, Berner, Christopher, McCandlish, Sam, Radford, Alec, Sutskever, Ilya, and Amodei, Dario. 2020. Language models are few-shot learners. Pages 1877â€“1901 of: Advances in Neural Infor- mation Processing Systems. CandÃ¨s, Emmanuel J. 1998. Ridgelets: Theory and Applications. Ph.D. thesis, Stanford University. Caragea, Andrei, Petersen, Philipp, and Voigtlaender, Felix. 2020. Neural network approximation and estimation of classiï¬ers with classiï¬cation boundary in a Barron class. ArXiv preprint arXiv:2011.09363. Casazza, Peter G., Kutyniok, Gitta, and Philipp, Friedrich. 2012. Introduction to ï¬nite frame theory. Pages 1â€“53 of: Finite Frames: Theory and Applications. BirkhÃ¤user Boston. Chen, Lin, Min, Yifei, Belkin, Mikhail, and Karbasi, Amin. 2020. Multiple descent: Design your own generalization curve. ArXiv preprint arXiv:2008.01036. Chen, Minshuo, Jiang, Haoming, Liao, Wenjing, and Zhao, Tuo. 2019. Eï¬ƒcient approximation of deep ReLU networks for functions on low dimensional man- ifolds. Pages 8174â€“8184 of: Advances in Neural Information Processing Systems. 94 Berner et al. The Modern Mathematics of Deep Learning Chizat, Lenaic, and Bach, Francis. 2020. Implicit bias of gradient descent for wide two-layer neural networks trained with the logistic loss. Pages 1305â€“1338 of: Proc. Conference on Learning Theory. Chizat, Lenaic, Oyallon, Edouard, and Bach, Francis. 2019. On lazy training in diï¬€erentiable programming. Pages 2937â€“2947 of: Advances in Neural Information Processing Systems. Cho, Kyunghyun, van MerriÃ«nboer, Bart, Gulcehre, Caglar, Bahdanau, Dzmitry, Bougares, Fethi, Schwenk, Holger, and Bengio, Yoshua. 2014. Learning phrase representations using RNN encoderâ€“decoder for statistical machine translation. Pages 1724â€“1734 of: Proc. 2014 Conference on Empirical Meth- ods in Natural Language Processing. Choromanska, Anna, Henaï¬€, Mikael, Mathieu, Michael, Arous, GÃ©rard Ben, and LeCun, Yann. 2015a. The loss surfaces of multilayer networks. Pages 192â€“204 of: Proc. International Conference on Artiï¬cial Intelligence and Statistics. Choromanska, Anna, LeCun, Yann, and Arous, GÃ©rard Ben. 2015b. Open problem: rhe landscape of the loss surfaces of multilayer networks. Pages 1756â€“1760 of: Proc. Conference on Learning Theory. Chui, Charles K., and Mhaskar, Hrushikesh N. 2018. Deep nets for local manifold learning. Frontiers in Applied Mathematics and Statistics, 4, 12. Chui, Charles K., Li, Xin, and Mhaskar, Hrushikesh N. 1994. Neural networks for localized approximation. Mathematics of Computation, 63(208), 607â€“623. Cloninger, Alexander, and Klock, Timo. 2020. ReLU nets adapt to intrinsic dimen- sionality beyond the target domain. ArXiv preprint arXiv:2008.02545. Cucker, Felipe, and Smale, Steve. 2002. On the mathematical foundations of learning. Bulletin of the American Mathematical Society, 39(1), 1â€“49. Cucker, Felipe, and Zhou, Ding-Xuan. 2007. Learning Theory: An Approximation Theory Viewpoint. Cambridge University Press. Cybenko, George. 1989. Approximation by superpositions of a sigmoidal function. Mathematics of Control, Signals and Systems, 2(4), 303â€“314. Czaja, Wojciech, and Li, Weilin. 2019. Analysis of timeâ€“frequency scattering transforms. Applied and Computational Harmonic Analysis, 47(1), 149â€“171. Dauphin, Yann N., Pascanu, Razvan, Gulcehre, Caglar, Cho, Kyunghyun, Ganguli, Surya, and Bengio, Yoshua. 2014. Identifying and attacking the saddle point problem in high-dimensional non-convex optimization. Pages 2933â€“2941 of: Advances in Neural Information Processing Systems. Deng, Jia, Dong, Wei, Socher, Richard, Li, Li-Jia, Li, Kai, and Fei-Fei, Li. 2009. Imagenet: A large-scale hierarchical image database. Pages 248â€“255 of: Proc. IEEE Conference on Computer Vision and Pattern Recognition. DeVore, Ronald A. 1998. Nonlinear approximation. Acta Numerica, 7, 51â€“150. DeVore, Ronald, Hanin, Boris, and Petrova, Guergana. 2021. Neural network approximation. Acta Numerica, 30, 327â€“444. Devroye, Luc, GyÃ¶rï¬, LÃ¡szlÃ³, and Lugosi, GÃ¡bor. 1996. A Probabilistic Theory of Pattern Recognition. Springer. References 95 Dirac, Paul Adrien Maurice. 1929. Quantum mechanics of many-electron systems. Proceedings of the Royal Society of London. Series A, Containing Papers of a Mathematical and Physical Character, 123(792), 714â€“733. Dittmer, SÃ¶ren, Kluth, Tobias, Maass, Peter, and Baguer, Daniel Otero. 2020. Regu- larization by architecture: A deep prior approach for inverse problems. Journal of Mathematical Imaging and Vision, 62(3), 456â€“470. Donoghue, William F. 1969. Distributions and Fourier Transforms. Academic Press. Dreyfus, Stuart. 1962. The numerical solution of variational problems. Journal of Mathematical Analysis and Applications, 5(1), 30â€“45. Du, Simon S., Hu, Wei, and Lee, Jason D. 2018a. Algorithmic regularization in learning deep homogeneous models: Layers are automatically balanced. Pages 384â€“395 of: Advances in Neural Information Processing Systems. Du, Simon S., Zhai, Xiyu, Poczos, Barnabas, and Singh, Aarti. 2018b. Gradient descent provably optimizes over-parameterized neural networks. In: Proc. International Conference on Learning Representations. Du, Simon S., Lee, Jason D., Li, Haochuan, Wang, Liwei, and Zhai, Xiyu. 2019. Gradient descent ï¬nds global minima of deep neural networks. Pages 1675â€“ 1685 of: Proc. International Conference on Machine Learning. Dudley, Richard M. 1967. The sizes of compact subsets of Hilbert space and continuity of Gaussian processes. Journal of Functional Analysis, 1(3), 290â€“ 330. Dudley, Richard M. 2014. Uniform Central Limit Theorems. Cambridge University Press. Dziugaite, Gintare Karolina, and Roy, Daniel M. 2017. Computing nonvacuous generalization bounds for deep (stochastic) neural networks with many more parameters than training data. In: Proc. Conference on Uncertainty in Artiï¬cial Intelligence. E, Weinan. 2017. A proposal on machine learning via dynamical systems. Com- munications in Mathematics and Statistics, 5(1), 1â€“11. E, Weinan, and Wojtowytsch, Stephan. 2020a. On the Banach spaces associated with multi-layer ReLU networks: Function representation, approximation theory and gradient descent dynamics. ArXiv preprint arXiv:2007.15623. E, Weinan, and Wojtowytsch, Stephan. 2020b. A priori estimates for classiï¬cation problems using neural networks. ArXiv preprint arXiv:2009.13500. E, Weinan, and Wojtowytsch, Stephan. 2020c. Representation formulas and point- wise properties for Barron functions. ArXiv preprint arXiv:2006.05982. E, Weinan, and Yu, Bing. 2018. The deep Ritz method: A deep learning-based numerical algorithm for solving variational problems. Communications in Mathematics and Statistics, 6(1), 1â€“12. E, Weinan, Ma, Chao, and Wu, Lei. 2019a. Barron spaces and the compositional function spaces for neural network models. ArXiv preprint arXiv:1906.08039. 96 Berner et al. The Modern Mathematics of Deep Learning E, Weinan, Han, Jiequn, and Li, Qianxiao. 2019b. A mean-ï¬eld optimal control formulation of deep learning. Research in the Mathematical Sciences, 6(1), 1â€“41. E, Weinan, Hutzenthaler, Martin, Jentzen, Arnulf, and Kruse, Thomas. 2019c. On multilevel Picard numerical approximations for high-dimensional nonlinear parabolic partial diï¬€erential equations and high-dimensional nonlinear back- ward stochastic diï¬€erential equations. Journal of Scientiï¬c Computing, 79(3), 1534â€“1571. E, Weinan, Ma, Chao, and Wu, Lei. 2019d. A priori estimates of the population risk for two-layer neural networks. Communications in Mathematical Sciences, 17(5), 1407â€“1425. E, Weinan, Ma, Chao, Wojtowytsch, Stephan, and Wu, Lei. 2020. Towards a mathematical understanding of neural network-based machine learning: what we know and what we donâ€™t. ArXiv preprint arXiv:2009.10713. ElbrÃ¤chter, Dennis, Grohs, Philipp, Jentzen, Arnulf, and Schwab, Christoph. 2018. DNN expression rate analysis of high-dimensional PDEs:Applicationto option pricing. ArXiv preprint arXiv:1809.07669. ElbrÃ¤chter, Dennis, Perekrestenko, Dmytro, Grohs, Philipp, and BÃ¶lcskei, Hel- mut. 2019. Deep neural network approximation theory. ArXiv preprint arXiv:1901.02220. Eldan, Ronen, and Shamir, Ohad. 2016. The power of depth for feedforward neural networks. Pages 907â€“940 of: Proc. Conference on Learning Theory, vol. 49. Elman, Jeï¬€rey L. 1990. Finding structure in time. Cognitive Science, 14(2), 179â€“ 211. Faber, Felix A., Hutchison, Luke, Huang, Bing, Gilmer, Justin, Schoenholz, Samuel S., Dahl, George E., Vinyals, Oriol, Kearnes, Steven, Riley, Patrick F., and Von Lilienfeld, O. Anatole. 2017. Prediction errors of molecular machine learning models lower than hybrid DFT error. Journal of Chemical Theory and Computation, 13(11), 5255â€“5264. Frankle, Jonathan, and Carbin, Michael. 2018. The lottery ticket hypothesis: Find- ing sparse, trainable neural networks. In: proc. International Conference on Learning Representations. Freeman, Daniel C., and Bruna, Joan. 2017. Topology and geometry of half- rectiï¬ed network optimization. In: Proc. International Conference on Learning Representations. Funahashi, Ken-Ichi. 1989. On the approximate realization of continuous mappings by neural networks. Neural Networks, 2(3), 183â€“192. Ge, Rong, Huang, Furong, Jin, Chi, and Yuan, Yang. 2015. Escaping from saddle points â€“ online stochastic gradient for tensor decomposition. Pages 797â€“842 of: Proc. Conference on Learning Theory. Geiger, Mario, Jacot, Arthur, Spigler, Stefano, Gabriel, Franck, Sagun, Levent, dâ€™Ascoli, StÃ©phane, Biroli, Giulio, Hongler, ClÃ©ment, and Wyart, Matthieu. 2020. Scaling description of generalization with number of parameters in References 97 deep learning. Journal of Statistical Mechanics: Theory and Experiment, 2(2), 023401. GÃ©ron, Aurelien. 2017. Hands-On Machine Learning with Scikit-Learn and Tensor- Flow: Concepts, Tools, and Techniques to Build Intelligent Systems. Oâ€™Reilly Media. Ghadimi, Saeed, and Lan, Guanghui. 2013. Stochastic ï¬rst- and zeroth-order meth- ods for nonconvex stochastic programming. SIAM Journal on Optimization, 23(4), 2341â€“2368. Ghorbani, Behrooz, Mei, Song, Misiakiewicz, Theodor, and Montanari, Andrea. 2021. Linearized two-layers neural networks in high dimension. Annals of Statistics, 49(2), 1029â€“1054. Gilton, Davis, Ongie, Greg, and Willett, Rebecca. 2019. Neumann networks for linear inverse problems in imaging. IEEE Transactions on Computational Imaging, 6, 328â€“343. GinÃ©, Evarist, and Zinn, Joel. 1984. Some limit theorems for empirical processes. Annals of Probability, 929â€“989. Golowich, Noah, Rakhlin, Alexander, and Shamir, Ohad. 2018. Size-independent sample complexity of neural networks. Pages 297â€“299 of: Proc. Conference On Learning Theory. Gonon, Lukas, and Schwab, Christoph. 2020. Deep ReLU network expression rates for option prices in high-dimensional, exponential LÃ©vy models. ETH Zurich SAM Research Report. Goodfellow, Ian, Pouget-Abadie, Jean, Mirza, Mehdi, Xu, Bing, Warde-Farley, David, Ozair, Sherjil, Courville, Aaron, and Bengio, Yoshua. 2014. Genera- tive adversarial nets. Pages 2672â€“2680 of: Advances in Neural Information Processing Systems. Goodfellow, Ian, Bengio, Yoshua, and Courville, Aaron. 2016. Deep Learning. MIT Press. Griewank, Andreas, and Walther, Andrea. 2008. Evaluating Derivatives: Principles and Techniques of Algorithmic Diï¬€erentiation. SIAM. Grohs, Philipp, and Herrmann, Lukas. 2021. Deep neural network approximation for high-dimensional parabolic Hamiltonâ€“Jacobiâ€“Bellman equations. ArXiv preprint arXiv:2103.05744. Grohs, Philipp, and Voigtlaender, Felix. 2021. Proof of the theory-to-practice gap in deep learning via sampling complexity bounds for neural network approximation spaces. ArXiv preprint arXiv:2104.02746. Grohs, Philipp, Koppensteiner, Sarah, and Rathmair, Martin. 2020. Phase retrieval: Uniqueness and stability. SIAM Review, 62(2), 301â€“350. Grohs, Philipp, Hornung, Fabian, Jentzen, Arnulf, and von Wurstemberger, Philippe. 2021. A proof that artiï¬cial neural networks overcome the curse of dimensionality in the numerical approximation of Blackâ€“Scholes partial diï¬€erential equations. Memoirs of the American Mathematical Society, to appear. 98 Berner et al. The Modern Mathematics of Deep Learning GÃ¼hring, Ingo, Kutyniok, Gitta, and Petersen, Philipp. 2020. Error bounds for approximations with deep ReLU neural networks in Ws,p norms. Analysis and Applications, 18(05), 803â€“859. Gunasekar, Suriya, Lee, Jason D., Soudry, Daniel, and Srebro, Nathan. 2018a. Characterizing implicit bias in terms of optimization geometry. Pages 1832â€“ 1841 of: Proc. International Conference on Machine Learning. Gunasekar, Suriya, Lee, Jason D., Soudry, Daniel, and Srebro, Nathan. 2018b. Implicit bias of gradient descent on linear convolutional networks. Pages 9461â€“9471 of: Advances in Neural Information Processing Systems. Haeï¬€ele, Benjamin D., and Vidal, RenÃ©. 2017. Global optimality in neural network training. Pages 7331â€“7339 of: Proc. IEEE Conference on Computer Vision and Pattern Recognition. Hairer, Martin, Hutzenthaler, Martin, and Jentzen, Arnulf. 2015. Loss of regularity for Kolmogorov equations. Annals of Probability, 43(2), 468â€“527. Han, Song, Mao, Huizi, and Dally, William J. 2016. Deep compression: com- pressing deep neural network with pruning, trained quantization and Huï¬€man coding. In: Proc. International Conference on Learning Representations. Han, Jiequn, Jentzen, Arnulf, and E, Weinan. 2018. Solving high-dimensional partial diï¬€erential equations using deep learning. Proceedings of the National Academy of Sciences, 115(34), 8505â€“8510. Han, Jiequn, Zhang, Linfeng, and E, Weinan. 2019. Solving many-electron SchrÃ¶dinger equation using deep neural networks. Journal of Computational Physics, 399, 108929. Hanin, Boris. 2019. Universal function approximation by deep neural nets with bounded width and ReLU activations. Mathematics, 7(10), 992. Hanin, Boris, and Rolnick, David. 2019. Deep ReLU networks have surprisingly few activation patterns. Pages 359â€“368 of: Advances in Neural Information Processing Systems. Hanin, Boris, and Sellke, Mark. 2017. Approximating continuous functions by ReLU nets of minimal width. ArXiv preprint arXiv:1710.11278. Hardt, Moritz, Recht, Ben, and Singer, Yoram. 2016. Train faster, generalize better: Stability of stochastic gradient descent. Pages 1225â€“1234 of: Proc. Interna- tional Conference on Machine Learning. Hastie, Trevor, Tibshirani, Robert, and Friedman, Jerome. 2001. The Elements of Statistical Learning: Data Mining, Inference, and Prediction. Springer. Hastie, Trevor, Montanari, Andrea, Rosset, Saharon, and Tibshirani, Ryan J. 2019. Surprises in high-dimensional ridgeless least squares interpolation. ArXiv preprint arXiv:1903.08560. Haussler, David. 1995. Sphere packing numbers for subsets of the Boolean n-cube with bounded Vapnikâ€“Chervonenkis dimension. Journal of Combinatorial Theory, Series A, 2(69), 217â€“232. He, Juncai, Li, Lin, Xu, Jinchao, and Zheng, Chunyue. 2020. ReLU deep neural References 99 networks and linear ï¬nite elements. Journal of Computational Mathematics, 38(3), 502â€“527. He, Kaiming, Zhang, Xiangyu, Ren, Shaoqing, and Sun, Jian. 2015. Delving deep into rectiï¬ers: surpassing human-level performance on imagenet classiï¬ca- tion. Pages 1026â€“1034 of: Proc. IEEE International Conference on Computer Vision. He, Kaiming, Zhang, Xiangyu, Ren, Shaoqing, and Sun, Jian. 2016. Deep residual learning for image recognition. Pages 770â€“778 of: Proc. IEEE Conference on Computer Vision and Pattern Recognition. Hermann, Jan, SchÃ¤tzle, Zeno, and NoÃ©, Frank. 2020. Deep-neural-network solution of the electronic SchrÃ¶dinger equation. Nature Chemistry, 12(10), 891â€“897. Higham, Catherine F., and Higham, Desmond J. 2019. Deep learning: An intro- duction for applied mathematicians. SIAM Review, 61(4), 860â€“891. Hinton, Geoï¬€rey E., and Zemel, Richard S. 1994. Autoencoders, minimum de- scription length, and Helmholtz free energy. Advances in Neural Information Processing Systems, 6, 3â€“10. Hinz, Peter, and van de Geer, Sara. 2019. A framework for the construction of upper bounds on the number of aï¬ƒne linear regions of ReLU feed-forward neural networks. IEEE Transactions on Information Theory, 65, 7304â€“7324. Hochreiter, Sepp, and Schmidhuber, JÃ¼rgen. 1997. Long short-term memory. Neural Computation, 9(8), 1735â€“1780. Hoeï¬€ding, Wassily. 1963. Probability inequalities for sums of bounded random variables. Journal of the American Statistical Association, 58(301), 13â€“30. Hopï¬eld, John J. 1982. Neural networks and physical systems with emergent collective computational abilities. Proceedings of the National Academy of Sciences, 79(8), 2554â€“2558. Hornik, Kurt, Stinchcombe, Maxwell, and White, Halbert. 1989. Multilayer feedfor- ward networks are universal approximators. Neural Networks, 2(5), 359â€“366. Huang, Gao, Sun, Yu, Liu, Zhuang, Sedra, Daniel, and Weinberger, Kilian Q. 2016. Deep networks with stochastic depth. Pages 646â€“661 of: Proc. European Conference on Computer Vision. Hutzenthaler, Martin, Jentzen, Arnulf, Kruse, Thomas, and Nguyen, Tuan Anh. 2020. A proof that rectiï¬ed deep neural networks overcome the curse of dimensionality in the numerical approximation of semilinear heat equations. SN Partial Diï¬€erential Equations and Applications, 1(2), 1â€“34. Ioï¬€e, Sergey, and Szegedy, Christian. 2015. Batch normalization: Accelerating deep network training by reducing internal covariate shift. Pages 448â€“456 of: Proc. International Conference on Machine Learning. Jacot, Arthur, Gabriel, Franck, and Hongler, ClÃ©ment. 2018. Neural tangent kernel: Convergence and generalization in neural networks. Pages 8571â€“8580 of: Advances in Neural Information Processing Systems. Jentzen, Arnulf, Kuckuck, Benno, Neufeld, Ariel, and von Wurstemberger, Philippe. 100 Berner et al. The Modern Mathematics of Deep Learning 2020. Strong error analysis for stochastic gradient descent optimization algo- rithms. IMA Journal of Numerical Analysis, 41(1), 455â€“492. Ji, Ziwei, and Telgarsky, Matus. 2019a. Gradient descent aligns the layers of deep linear networks. In: Proc. International Conference on Learning Representa- tions. Ji, Ziwei, and Telgarsky, Matus. 2019b. A reï¬ned primalâ€“dual analysis of the implicit bias. ArXiv preprint arXiv:1906.04540. Ji, Ziwei, and Telgarsky, Matus. 2020. Directional convergence and alignment in deep learning. Pages 17176â€“17186 of: Advances in Neural Information Processing Systems. Jiang, Yiding, Krishnan, Dilip, Mobahi, Hossein, and Bengio, Samy. 2019. Pre- dicting the generalization gap in deep networks with margin distributions. In: Proc. International Conference on Learning Representations. Jiang, Yiding, Neyshabur, Behnam, Mobahi, Hossein, Krishnan, Dilip, and Bengio, Samy. 2020. Fantastic generalization measures and where to ï¬nd them. In: International Conference on Learning Representations. Jin, Bangti, MaaÃŸ, Peter, and Scherzer, Otmar. 2017a. Sparsity regularization in inverse problems. Inverse Problems, 33(6), 060301. Jin, Kyong Hwan, McCann, Michael T., Froustey, Emmanuel, and Unser, Michael. 2017b. Deep convolutional neural network for inverse problems in imaging. IEEE Transactions on Image Processing, 26(9), 4509â€“4522. Jordan, Michael I. 1990. Attractor dynamics and parallelism in a connectionist sequential machine. Pages 112â€“127 of: Artiï¬cial Neural Networks: Concept Learning. IEEE Press. Judd, Stephen J. 1990. Neural Network Design and the Complexity of Learning. MIT Press. Kakade, Sham M., and Lee, Jason D. 2018. Provably correct automatic subdif- ferentiation for qualiï¬ed programs. Pages 7125â€“7135 of: Advances in Neural Information Processing Systems. Karpinski, Marek, and Macintyre, Angus. 1997. Polynomial bounds for VC dimen- sion of sigmoidal and general Pfaï¬ƒan neural networks. Journal of Computer and System Sciences, 54(1), 169â€“176. Kelley, Henry J. 1960. Gradient theory of optimal ï¬‚ight paths. Ars Journal, 30(10), 947â€“954. Keskar, Nitish Shirish, Mudigere, Dheevatsa, Nocedal, Jorge, Smelyanskiy, Mikhail, and Tang, Ping Tak Peter. 2017. On large-batch training for deep learning: Generalization gap and sharp minima. In: Proc. International Con- ference on Learning Representations. Kidger, Patrick, and Lyons, Terry. 2020. Universal approximation with deep narrow networks. Pages 2306â€“2327 of: Proc. Conference on Learning Theory. Kiefer, Jack, and Wolfowitz, Jacob. 1952. Stochastic estimation of the maximum of a regression function. Annals of Mathematical Statistics, 23(3), 462â€“466. References 101 Krizhevsky, Alex, and Hinton, Geoï¬€rey. 2009. Learning multiple layers of features from tiny images. Technical Report. University of Toronto. Krizhevsky, Alex, Sutskever, Ilya, and Hinton, Geoï¬€rey E. 2012. ImageNet clas- siï¬cation with deep convolutional neural networks. Pages 1097â€“1105 of: Advances in Neural Information Processing Systems. Kutyniok, Gitta, Petersen, Philipp, Raslan, Mones, and Schneider, Reinhold. 2019. A theoretical analysis of deep neural networks and parametric PDEs. ArXiv preprint arXiv:1904.00377. Laakmann, Fabian, and Petersen, Philipp. 2021. Eï¬ƒcient approximation of solu- tions of parametric linear transport equations by ReLU DNNs. Advances in Computational Mathematics, 47(1), 1â€“32. Lample, Guillaume, and Charton, FranÃ§ois. 2019. Deep learning For symbolic mathematics. In: Proc. International Conference on Learning Representations. LeCun, Yann, Boser, Bernhard, Denker, John S., Henderson, Donnie, Howard, Richard E., Hubbard, Wayne, and Jackel, Lawrence D. 1989a. Backpropaga- tion applied to handwritten zip code recognition. Neural Computation, 1(4), 541â€“551. LeCun, Yann, Denker, John S., and Solla, Sara A. 1989b. Optimal brain damage. Pages 598â€“605 of: Advances in Neural Information Processing Systems. LeCun, Yann, Bottou, LÃ©on, Bengio, Yoshua, and Haï¬€ner, Patrick. 1998. Gradient- based learning applied to document recognition. Proceedings of the IEEE, 86(11), 2278â€“2324. LeCun, Yann, Bengio, Yoshua, and Hinton, Geoï¬€rey. 2015. Deep learning. Nature, 521(7553), 436â€“444. Ledoux, Michel, and Talagrand, Michel. 1991. Probability in Banach Spaces: Isoperimetry and Processes. Springer Science & Business Media. Lee, Jason D., Simchowitz, Max, Jordan, Michael I., and Recht, Benjamin. 2016. Gradient descent only converges to minimizers. Pages 1246â€“1257 of: Proc. Conference on Learning Theory. Lee, Jaehoon, Bahri, Yasaman, Novak, Roman, Schoenholz, Samuel S., Pennington, Jeï¬€rey, and Sohl-Dickstein, Jascha. 2018. Deep neural networks as Gaussian processes. In: Proc. International Conference on Learning Representations. Lee, Jaehoon, Xiao, Lechao, Schoenholz, Samuel S., Bahri, Yasaman, Novak, Ro- man, Sohl-Dickstein, Jascha, and Pennington, Jeï¬€rey. 2020. Wide neural networks of any depth evolve as linear models under gradient descent. Journal of Statistical Mechanics: Theory and Experiment, 2020(12), 124002. Leshno, Moshe, Lin, Vladimir Ya., Pinkus, Allan, and Schocken, Shimon. 1993. Multilayer feedforward networks with a nonpolynomial activation function can approximate any function. Neural Networks, 6(6), 861â€“867. Lewin, Kurt. 1943. Psychology and the process of group living. The Journal of Social Psychology, 17(1), 113â€“131. 102 Berner et al. The Modern Mathematics of Deep Learning Li, Bo, Tang, Shanshan, and Yu, Haijun. 2019a. Better approximations of high- dimensional smooth functions by deep neural networks with rectiï¬ed power units. Communications in Computational Physics, 27(2), 379â€“411. Li, Housen, Schwab, Johannes, Antholzer, Stephan, and Haltmeier, Markus. 2020. NETT: Solving inverse problems with deep neural networks. Inverse Problems, 36(6), 065005. Li, Qianxiao, Lin, Ting, and Shen, Zuowei. 2019b. Deep learning via dynamical systems: An approximation perspective. ArXiv preprint arXiv:1912.10382. Li, Weilin. 2021. Generalization error of minimum weighted norm and kernel interpolation. SIAM Journal on Mathematics of Data Science, 3(1), 414â€“438. Li, Yuanzhi, and Liang, Yingyu. 2018. Learning overparameterized neural net- works via stochastic gradient descent on structured data. Pages 8157â€“8166 of: Advances in Neural Information Processing Systems. Liang, Shiyu, and Srikant, R. 2017. Why deep neural networks for function approx- imation? In: Proc. International Conference on Learning Representations. Liang, Tengyuan, and Rakhlin, Alexander. 2020. Just interpolate: Kernel â€œridgelessâ€ regression can generalize. Annals of Statistics, 48(3), 1329â€“1347. Liang, Tengyuan, Poggio, Tomaso, Rakhlin, Alexander, and Stokes, James. 2019. Fisherâ€“Rao metric, geometry, and complexity of neural networks. Pages 888â€“ 896 of: Proc. International Conference on Artiï¬cial Intelligence and Statistics. Liang, Tengyuan, Rakhlin, Alexander, and Zhai, Xiyu. 2020. On the multiple de- scent of minimum-norm interpolants and restricted lower isometry of kernels. Pages 2683â€“2711 of: Proc. Conference on Learning Theory. Lin, Licong, and Dobriban, Edgar. 2021. What causes the test error? Going beyond bias-variance via anova. Journal of Machine Learning Research, 22(155), 1â€“82. Linnainmaa, Seppo. 1970. Alogritmin Kumulatiivinen PyÃ¶ristysvirhe YksittÃ¤isten PyÃ¶ristysvirheiden Taylor-KehitelmÃ¤nÃ¤. M.Phil. thesis, University of Helsinki. Lu, Yiping, Ma, Chao, Lu, Yulong, Lu, Jianfeng, and Ying, Lexing. 2020. A mean ï¬eld analysis of deep ResNet and beyond: Towards provable optimization via overparameterization from depth. Pages 6426â€“6436 of: Proc. International Conference on Machine Learning. Lunz, Sebastian, Ã–ktem, Ozan, and SchÃ¶nlieb, Carola-Bibiane. 2018. Adversarial regularizers in inverse problems. Pages 8507â€“8516 of: Advances in Neural Information Processing Systems. Lyu, Kaifeng, and Li, Jian. 2019. Gradient descent maximizes the margin of ho- mogeneous neural networks. In: Proc. International Conference on Learning Representations. Ma, Junshui, Sheridan, Robert P., Liaw, Andy, Dahl, George E., and Svetnik, Vladimir. 2015. Deep neural nets as a method for quantitative structureâ€“ activity relationships. Journal of Chemical Information and Modeling, 55(2), 263â€“274. References 103 Maiorov, Vitaly, and Pinkus, Allan. 1999. Lower bounds for approximation by MLP neural networks. Neurocomputing, 25(1-3), 81â€“91. Mallat, StÃ©phane. 2012. Group invariant scattering. Communications on Pure and Applied Mathematics, 65(10), 1331â€“1398. Mallat, StÃ©phane. 2016. Understanding deep convolutional networks. Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences, 374(2065), 20150203. Marcati, Carlo, Opschoor, Joost, Petersen, Philipp, and Schwab, Christoph. 2020. Exponential ReLU neural network approximation rates for point and edge singularities. ETH Zurich SAM Research Report. Matthews, Alexander G. de G., Hron, Jiri, Rowland, Mark, Turner, Richard E., and Ghahramani, Zoubin. 2018. Gaussian process behaviour in wide deep neural networks. In: Proc. International Conference on Learning Representations. McAllester, David A. 1999. PAC-Bayesian model averaging. Pages 164â€“170 of: Prc. Conference on Learning Theory. McCulloch, Warren S., and Pitts, Walter. 1943. A logical calculus of the ideas immanent in nervous activity. Bulletin of Mathematical Biophysics, 5(4), 115â€“133. McDiarmid, Colin. 1989. On the method of bounded diï¬€erences. Pages 148â€“188 of: Surveys in Combinatorics. London Mathematical Society Lecture Notes, vol. 141. Cambridge University Press. Mei, Song, and Montanari, Andrea. 2019. The generalization error of random features regression: Precise asymptotics and double descent curve. ArXiv preprint arXiv:1908.05355. Mendelson, Shahar. 2014. Learning without concentration. Pages 25â€“39 of: Proc. Conference on Learning Theory. Mendelson, Shahar, and Vershynin, Roman. 2003. Entropy and the combinatorial dimension. Inventiones Mathematicae, 152(1), 37â€“55. Mhaskar, Hrushikesh N. 1996. Neural networks for optimal approximation of smooth and analytic functions. Neural Computation, 8(1), 164â€“177. Mianjy, Poorya, Arora, Raman, and Vidal, Rene. 2018. On the implicit bias of dropout. Pages 3540â€“3548 of: Proc. International Conference on Machine Learning. Minsky, Marvin, and Papert, Seymour A. 1969. Perceptrons. MIT Press. Mnih, Volodymyr, Kavukcuoglu, Koray, Silver, David, Graves, Alex, Antonoglou, Ioannis, Wierstra, Daan, and Riedmiller, Martin. 2013. Playing Atari with deep reinforcement learning. ArXiv preprint arXiv:1312.5602. Monga, Vishal, Li, Yuelong, and Eldar, Yonina C. 2021. Algorithm unrolling: Interpretable, eï¬ƒcient deep learning for signal and image processing. IEEE Signal Processing Magazine, 38(2), 18â€“44. Montanari, Andrea, and Zhong, Yiqiao. 2020. The interpolation phase transition in neural networks: Memorization and generalization under lazy training. ArXiv preprint arXiv:2007.12826. 104 Berner et al. The Modern Mathematics of Deep Learning MontÃºfar, Guido, Pascanu, Razvan, Cho, Kyunghyun, and Bengio, Yoshua. 2014. On the number of linear regions of deep neural networks. Pages 2924â€“2932 of: Advances in Neural Information Processing Systems. Muthukumar, Vidya, Vodrahalli, Kailas, Subramanian, Vignesh, and Sahai, Anant. 2020. Harmless interpolation of noisy data in regression. IEEE Journal on Selected Areas in Information Theory, 1(1), 67â€“83. Nacson, Mor Shpigel, Lee, Jason D., Gunasekar, Suriya, Savarese, Pedro Hen- rique Pamplona, Srebro, Nathan, and Soudry, Daniel. 2019. Convergence of gradient descent on separable data. Pages 3420â€“3428 of: International Conference on Artiï¬cial Intelligence and Statistics. Nagarajan, Vaishnavh, and Kolter, J. Zico. 2019. Uniform convergence may be unable to explain generalization in deep learning. Pages 11615â€“11626 of: Advances in Neural Information Processing Systems. Nakada, Ryumei and Imaizumi, Masaaki. 2020. Adaptive approximation and gen- eralization of deep neural network with intrinsic dimensionality. Journal of Machine Learning Research, 21(174), 1â€“38. Nakamura-Zimmerer, Tenavi, Gong, Qi, and Kang, Wei. 2021. Adaptive deep learning for high-dimensional Hamiltonâ€“Jacobiâ€“Bellman equations. SIAM Journal on Scientiï¬c Computing, 43(2), A1221â€“A1247. Nakkiran, Preetum, Kaplun, Gal, Bansal, Yamini, Yang, Tristan, Barak, Boaz, and Sutskever, Ilya. 2020. Deep double descent: Where bigger models and more data hurt. In: Proc. International Conference on Learning Representations. Nemirovski, Arkadi, Juditsky, Anatoli, Lan, Guanghui, and Shapiro, Alexander. 2009. Robust stochastic approximation approach to stochastic programming. SIAM Journal on Optimization, 19(4), 1574â€“1609. Nemirovsky, Arkadi Semenovich, and Yudin, David Borisovich. 1983. Problem Complexity and Method Eï¬ƒciency in Optimization. Wiley-Interscience Series in Discrete Mathematics. Wiley. Neyshabur, Behnam, Tomioka, Ryota, and Srebro, Nathan. 2014. In search of the real inductive bias: On the role of implicit regularization in deep learning. ArXiv preprint arXiv:1412.6614. Neyshabur, Behnam, Tomioka, Ryota, and Srebro, Nathan. 2015. Norm-based capacity control in neural networks. Pages 1376â€“1401 of: Proc. Conference on Learning Theory. Neyshabur, Behnam, Bhojanapalli, Srinadh, McAllester, David, and Srebro, Nati. 2017. Exploring generalization in deep learning. Pages 5947â€“5956 of: Ad- vances in Neural Information Processing Systems. Neyshabur, Behnam, Bhojanapalli, Srinadh, and Srebro, Nathan. 2018. A PAC- Bayesian approach to spectrally-normalized margin bounds for neural net- works. In: Proc. International Conference on Learning Representations. Nguyen, Quynh, and Hein, Matthias. 2017. The loss surface of deep and wide neural networks. Pages 2603â€“2612 of: Proc. International Conference on Machine Learning. References 105 Novak, Erich, and WoÅºniakowski, Henryk. 2009. Approximation of inï¬nitely dif- ferentiable multivariate functions is intractable. Journal of Complexity, 25(4), 398â€“404. Olshausen, Bruno A., and Field, David J. 1996. Sparse coding of natural images produces localized, oriented, bandpass receptive ï¬elds. Nature, 381(60), 609. Oono, Kenta, and Suzuki, Taiji. 2019. Approximation and non-parametric esti- mation of ResNet-type convolutional neural networks. Pages 4922â€“4931 of: Proc. International Conference on Machine Learning. Opschoor, Joost, Petersen, Philipp, and Schwab, Christoph. 2020. Deep ReLU networks and high-order ï¬nite element methods. Analysis and Applications, 1â€“56. Orr, Genevieve B, and MÃ¼ller, Klaus-Robert. 1998. Neural Networks: Tricks of the Trade. Springer. Papyan, Vardan, Romano, Yaniv, and Elad, Michael. 2017a. Convolutional neu- ral networks analyzed via convolutional sparse coding. Journal of Machine Learning Research, 18(1), 2887â€“2938. Papyan, Vardan, Sulam, Jeremias, and Elad, Michael. 2017b. Working locally think- ing globally: Theoretical guarantees for convolutional sparse coding. IEEE Transactions on Signal Processing, 65(21), 5687â€“5701. Papyan, Vardan, Romano, Yaniv, Sulam, Jeremias, and Elad, Michael. 2018. The- oretical foundations of deep learning via sparse representations: A multilayer sparse model and its connection to convolutional neural networks. IEEE Signal Processing Magazine, 35(4), 72â€“89. Pardoux, Etienne, and Peng, Shige. 1992. Backward stochastic diï¬€erential equations and quasilinear parabolic partial diï¬€erential equations. Pages 200â€“217 of: Stochastic Partial Diï¬€erential Equations and Their Applications. Springer. Petersen, Philipp, and Voigtlaender, Felix. 2018. Optimal approximation of piece- wise smooth functions using deep ReLU neural networks. Neural Networks, 108, 296â€“330. Petersen, Philipp, and Voigtlaender, Felix. 2020. Equivalence of approximation by convolutional neural networks and fully-connected networks. Proceedings of the American Mathematical Society, 148(4), 1567â€“1581. Petersen, Philipp, Raslan, Mones, and Voigtlaender, Felix. 2020. Topological prop- erties of the set of functions generated by neural networks of ï¬xed size. Foundations of Computational Mathematics, 21, 375â€“444. Pfau, David, Spencer, James S., Matthews, Alexander G. D. G., and Foulkes, W. M. C. 2020. Ab initio solution of the many-electron SchrÃ¶dinger equation with deep neural networks. Physical Review Research, 2(3), 033429. Pham, Hieu, Guan, Melody, Zoph, Barret, Le, Quoc, and Dean, Jeï¬€. 2018. Eï¬ƒcient neural architecture search via parameters sharing. Pages 4095â€“4104 of: Proc. International Conference on Machine Learning. Poggio, Tomaso, Rifkin, Ryan, Mukherjee, Sayan, and Niyogi, Partha. 2004. Gen- eral conditions for predictivity in learning theory. Nature, 428(6981), 419â€“422. 106 Berner et al. The Modern Mathematics of Deep Learning Poggio, Tomaso, Kawaguchi, Kenji, Liao, Qianli, Miranda, Brando, Rosasco, Lorenzo, Boix, Xavier, Hidary, Jack, and Mhaskar, Hrushikesh N. 2017a. Theory of deep learning III: explaining the non-overï¬tting puzzle. ArXiv preprint arXiv:1801.00173. Poggio, Tomaso, Mhaskar, Hrushikesh N., Rosasco, Lorenzo, Miranda, Brando, and Liao, Qianli. 2017b. Why and when can deep â€“ but not shallow â€“ net- works avoid the curse of dimensionality: a review. International Journal of Automation and Computing, 14(5), 503â€“519. Poole, Ben, Lahiri, Subhaneil, Raghu, Maithra, Sohl-Dickstein, Jascha, and Gan- guli, Surya. 2016. Exponential expressivity in deep neural networks through transient chaos. Pages 3368â€“3376 of: Advances in Neural Information Pro- cessing Systems. Raghu, Maithra, Poole, Ben, Kleinberg, Jon, Ganguli, Surya, and Sohl-Dickstein, Jascha. 2017. On the expressive power of deep neural networks. Pages 2847â€“ 2854 of: Proc. International Conference on Machine Learning. Rahimi, Ali, Recht, Benjamin, et al. 2007. Random features for large-scale kernel machines. Pages 1177â€“1184 of: Advances in Neural Information Processing Systems. Raissi, Maziar, Perdikaris, Paris, and Karniadakis, George E. 2019. Physics- informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial diï¬€erential equations. Jour- nal of Computational Physics, 378, 686â€“707. Ramanujan, Vivek, Wortsman, Mitchell, Kembhavi, Aniruddha, Farhadi, Ali, and Rastegari, Mohammad. 2020. Whatâ€™s hidden in a randomly weighted neural network? Pages 11893â€“11902 of: Proc. IEEE Conference on Computer Vision and Pattern Recognition. RÃ­os, P. LÃ³pez, Ma, Ao, Drummond, Neil D., Towler, Michael D., and Needs, Richard J. 2006. Inhomogeneous backï¬‚ow transformations in quantum Monte Carlo calculations. Physical Review E, 74(6), 066701. Robbins, Herbert, and Monro, Sutton. 1951. A stochastic approximation method. Annals of Mathematical Statistics, 400â€“407. Romano, Yaniv, Elad, Michael, and Milanfar, Peyman. 2017. The little engine that could: Regularization by denoising (RED). SIAM Journal on Imaging Sciences, 10(4), 1804â€“1844. Ronneberger, Olaf, Fischer, Philipp, and Brox, Thomas. 2015. U-net: convolutional networks for biomedical image segmentation. Pages 234â€“241 of: Proc. In- ternational Conference on Medical image Computing and Computer-Assisted Intervention. Rosenblatt, Frank. 1958. The perceptron: a probabilistic model for information storage and organization in the brain. Psychological Review, 65(6), 386. Rudin, Walter. 2006. Real and Complex Analysis. McGraw-Hill. Rumelhart, David E., Hinton, Geoï¬€rey E., and Williams, Ronald J. 1986. Learning representations by back-propagating errors. Nature, 323(6088), 533â€“536. References 107 Ruthotto, Lars, and Haber, Eldad. 2019. Deep neural networks motivated by partial diï¬€erential equations. Journal of Mathematical Imaging and Vision, 1â€“13. Safran, Itay, and Shamir, Ohad. 2016. On the quality of the initial basin in over- speciï¬ed neural networks. Pages 774â€“782 of: Proc. International Conference on Machine Learning. Safran, Itay, and Shamir, Ohad. 2017. Depthâ€“width tradeoï¬€s in approximating nat- ural functions with neural networks. Pages 2979â€“2987 of: Proc. International Conference on Machine Learning. Safran, Itay, and Shamir, Ohad. 2018. Spurious local minima are common in two-layer ReLU neural networks. Pages 4433â€“4441 of: Proc. International Conference on Machine Learning. Sakurai, Akito. 1999. Tight bounds for the VC-dimension of piecewise polynomial networks. Pages 323â€“329 of: Advances in Neural Information Processing Systems. Santurkar, Shibani, Tsipras, Dimitris, Ilyas, Andrew, and Madry, Aleksander. 2018. How does batch normalization help optimization? Pages 2488â€“2498 of: Ad- vances in Neural Information Processing Systems. Saxton, David, Grefenstette, Edward, Hill, Felix, and Kohli, Pushmeet. 2018. Analysing mathematical reasoning abilities of neural models. In: Proc. In- ternational Conference on Learning Representations. Scherbela, Michael, Reisenhofer, Rafael, Gerard, Leon, Marquetand Philipp, and Grohs, Philipp. 2021. Solving the electronic SchrÃ¶dinger equation for multiple nuclear geometries with weight-sharing deep neural network. ArXiv preprint arXiv:2105.08351. Schmidhuber, JÃ¼rgen. 2015. Deep learning in neural networks: An overview. Neural Networks, 61, 85â€“117. Schmidt-Hieber, Johannes. 2019. Deep ReLU network approximation of functions on a manifold. ArXiv preprint arXiv:1908.00695. SchÃ¼tt, Kristof T., Kindermans, Pieter-Jan, Sauceda, Huziel E., Chmiela, Ste- fan, Tkatchenko, Alexandre, and MÃ¼ller, Klaus-Robert. 2017. Schnet: A continuous-ï¬lter convolutional neural network for modeling quantum inter- actions. Pages 992â€“1002 of: Advances in Neural Information Processing Systems. Schwab, Christoph, and Zech, Jakob. 2019. Deep learning in high dimension: Neural network expression rates for generalized polynomial chaos expansions in UQ. Analysis and Applications, 17(01), 19â€“55. Senior, Andrew W., Evans, Richard, Jumper, John, Kirkpatrick, James, Sifre, Lau- rent, Green, Tim, Qin, Chongli, Å½Ã­dek, Augustin, Nelson, Alexander W. R., and Bridgland, Alex. 2020. Improved protein structure prediction using potentials from deep learning. Nature, 577(7792), 706â€“710. Shaham, Uri, Cloninger, Alexander, and Coifman, Ronald R. 2018. Provable ap- proximation properties for deep neural networks. Applied and Computational Harmonic Analysis, 44(3), 537â€“557. 108 Berner et al. The Modern Mathematics of Deep Learning Shalev-Shwartz, Shai, and Ben-David, Shai. 2014. Understanding Machine Learn- ing: From Theory to Algorithms. Cambridge University Press. Shalev-Shwartz, Shai, Shamir, Ohad, Srebro, Nathan, and Sridharan, Karthik. 2009. Stochastic convex optimization. In: Proc. Conference on Learning Theory. Shapiro, Alexander, Dentcheva, Darinka, and RuszczyÅ„ski, Andrzej. 2014. Lectures on Stochastic Programming: Modeling and Theory. SIAM. Shen, Zuowei. 2020. Deep network approximation characterized by number of neurons. Communications in Computational Physics, 28(5), 1768â€“1811. Silver, David, Huang, Aja, Maddison, Chris J., Guez, Arthur, Sifre, Laurent, Van Den Driessche, George, Schrittwieser, Julian, Antonoglou, Ioannis, Panneer- shelvam, Veda, and Lanctot, Marc. 2016. Mastering the game of Go with deep neural networks and tree search. Nature, 529(7587), 484â€“489. Silver, David, Schrittwieser, Julian, Simonyan, Karen, Antonoglou, Ioannis, Huang, Aja, Guez, Arthur, Hubert, Thomas, Baker, Lucas, Lai, Matthew, and Bolton, Adrian. 2017. Mastering the game of Go without human knowledge. Nature, 550(7676), 354â€“359. Å Ã­ma, JiÅ™Ã­. 2002. Training a single sigmoidal neuron is hard. Neural Computation, 14(11), 2709â€“2728. Soudry, Daniel, Hoï¬€er, Elad, Nacson, Mor Shpigel, Gunasekar, Suriya, and Srebro, Nathan. 2018. The implicit bias of gradient descent on separable data. Journal of Machine Learning Research, 19, 1â€“57. Srivastava, Nitish, Hinton, Geoï¬€rey, Krizhevsky, Alex, Sutskever, Ilya, and Salakhutdinov, Ruslan. 2014. Dropout: a simple way to prevent neural net- works from overï¬tting. Journal of Machine Learning Research, 15(1), 1929â€“ 1958. Srivastava, Rupesh Kumar, Greï¬€, Klaus, and Schmidhuber, JÃ¼rgen. 2015. Training very deep networks. Pages 2377â€“2385 of: Advances in Neural Information Processing Systems. Sulam, Jeremias, Papyan, Vardan, Romano, Yaniv, and Elad, Michael. 2018. Mul- tilayer convolutional sparse modeling: Pursuit and dictionary learning. IEEE Transactions on Signal Processing, 66(15), 4090â€“4104. Szabo, Attila, and Ostlund, Neil S. 2012. Modern Quantum Chemistry: Introduction to Advanced Electronic Structure Theory. Courier Corporation. Szegedy, Christian, Liu, Wei, Jia, Yangqing, Sermanet, Pierre, Reed, Scott, Anguelov, Dragomir, Erhan, Dumitru, Vanhoucke, Vincent, and Rabinovich, Andrew. 2015. Going deeper with convolutions. Pages 1â€“9 of: Proc. IEEE Conference on Computer Vision and Pattern Recognition. Talagrand, Michel. 1994. Sharper bounds for Gaussian and empirical processes. Annals of Probability, 28â€“76. Telgarsky, Matus. 2015. Representation beneï¬ts of deep feedforward networks. ArXiv preprint arXiv:1509.08101. Thorpe, Matthew, and van Gennip, Yves. 2018. Deep limits of residual neural networks. ArXiv preprint arXiv:1810.11741. References 109 Ulyanov, Dmitry, Vedaldi, Andrea, and Lempitsky, Victor. 2018. Deep image prior. Pages 9446â€“9454 of: Proc. IEEE Conference on Computer Vision and Pattern Recognition. van der Vaart, Aad W., and Wellner, Jon A. 1997. Weak convergence and empirical processes with applications to statistics. Journal of the Royal Statistical Society Series A: Statistics in Society, 160(3), 596â€“608. Vapnik, Vladimir. 1999. An overview of statistical learning theory. IEEE Transac- tions on Neural Networks, 10(5), 988â€“999. Vapnik, Vladimir. 2013. The Nature of Statistical Learning Theory. Springer Science & Business Media. Vapnik, Vladimir, and Chervonenkis, Alexey. 1971. On the uniform convergence of relative frequencies of events to their probabilities. Theory of Probability & its Applications, 16(2), 264â€“280. Vaswani, Ashish, Shazeer, Noam, Parmar, Niki, Uszkoreit, Jakob, Jones, Llion, Gomez, Aidan N., Kaiser, Åukasz, and Polosukhin, Illia. 2017. Attention is all you need. Pages 5998â€“6008 of: Advances in Neural Information Processing Systems. Venturi, Luca, Bandeira, Alfonso S., and Bruna, Joan. 2019. Spurious valleys in one-hidden-layer neural network optimization landscapes. Journal of Machine Learning Research, 20(133), 1â€“34. Vershynin, Roman. 2018. High-Dimensional Probability: An Introduction with Applications in Data Science. Cambridge University Press. Vinyals, Oriol, Babuschkin, Igor, Czarnecki, Wojciech M., Mathieu, MichaÃ«l, Dudzik, Andrew, Chung, Junyoung, Choi, David H., Powell, Richard, Ewalds, Timo, and Georgiev, Petko. 2019. Grandmaster level in StarCraft II using multi-agent reinforcement learning. Nature, 575(7782), 350â€“354. Wan, Li, Zeiler, Matthew, Zhang, Sixin, Le Cun, Yann, and Fergus, Rob. 2013. Regularization of neural networks using dropconnect. Pages 1058â€“1066 of: Proc. International Conference on Machine Learning. Werbos, Paul J. 1988. Generalization of backpropagation with application to a recurrent gas market model. Neural Networks, 1(4), 339â€“356. Whitney, Hassler. 1934. Analytic extensions of diï¬€erentiable functions deï¬ned in closed sets. Transactions of the American Mathematical Society, 36(1), 63â€“89. Wiatowski, Thomas, Grohs, Philipp, and BÃ¶lcskei, Helmut. 2017. Energy propaga- tion in deep convolutional neural networks. IEEE Transactions on Information Theory, 64(7), 4819â€“4842. Williams, Ronald J., and Zipser, David. 1995. Gradient-based learning algorithms for recurrent networks and their computational complexity. Pages 433â€“486 of: Backpropagation: Theory, Architectures, and Applications, Psychology Press. Wu, Zonghan, Pan, Shirui, Chen, Fengwen, Long, Guodong, Zhang, Chengqi, and Philip, S. Yu. 2021. A comprehensive survey on graph neural networks. IEEE Transactions on Neural Networks and Learning Systems, 32(1), 4â€“24. 110 Berner et al. The Modern Mathematics of Deep Learning Xu, Huan, and Mannor, Shie. 2012. Robustness and generalization. Machine learning, 86(3), 391â€“423. Yang, Greg. 2019. Scaling limits of wide neural networks with weight sharing: Gaussian process behavior, gradient independence, and neural tangent kernel derivation. ArXiv preprint arXiv:1902.04760. Yarotsky, Dmitry. 2017. Error bounds for approximations with deep ReLU net- works. Neural Networks, 94, 103â€“114. Yarotsky, Dmitry. 2018a. Optimal approximation of continuous functions by very deep ReLU networks. Pages 639â€“649 of: Proc. Conference on Learning Theory. Yarotsky, Dmitry. 2018b. Universal approximations of invariant maps by neural networks. ArXiv preprint arXiv:1804.10306. Yarotsky, Dmitry. 2021. Elementary superexpressive activations. ArXiv preprint arXiv:2102.10911. Yarotsky, Dmitry, and Zhevnerchuk, Anton. 2020. The phase diagram of approx- imation rates for deep neural networks. In: Advances in Neural Information Processing Systems, vol. 33. Ye, Jong Chul, Han, Yoseob, and Cha, Eunju. 2018. Deep convolutional framelets: A general deep learning framework for inverse problems. SIAM Journal on Imaging Sciences, 11(2), 991â€“1048. Yin, Rujie, Gao, Tingran, Lu, Yue M., and Daubechies, Ingrid. 2017. A tale of two bases: Localâ€“nonlocal regularization on image patches with convolution framelets. SIAM Journal on Imaging Sciences, 10(2), 711â€“750. Young, Tom, Hazarika, Devamanyu, Poria, Soujanya, and Cambria, Erik. 2018. Recent trends in deep learning based natural language processing. IEEE Computational Intelligence Magazine, 13(3), 55â€“75. Yserentant, Harry. 2010. Regularity and Approximability of Electronic Wave Func- tions. Springer. Zaslavsky, Thomas. 1975. Facing up to Arrangements: Face-Count Formulas for Partitions of Space by Hyperplanes. Memoirs of the American Mathematical Society. American Mathematical Society. Zbontar, Jure, Knoll, Florian, Sriram, Anuroop, Murrell, Tullie, Huang, Zheng- nan, Muckley, Matthew J., Defazio, Aaron, Stern, Ruben, Johnson, Patricia, Bruno, Mary, Parente, Marc, Geras, Krzysztof J., Katsnelson, Joe, Chan- darana, Hersh, Zhang, Zizhao, Drozdzal, Michal, Romero, Adriana, Rabbat, Michael, Vincent, Pascal, Yakubova, Naï¬ssa, Pinkerton, James, Wang, Duo, Owens, Erich, Zitnick, C. Lawrence, Recht, Michael P., Sodickson, Daniel K., and Lui, Yvonne W. 2018. fastMRI: An open dataset and benchmarks for accelerated MRI. ArXiv preprint arXiv:1811.08839. Zhang, Chiyuan, Bengio, Samy, Hardt, Moritz, Recht, Benjamin, and Vinyals, Oriol. 2017. Understanding deep learning requires rethinking generalization. In: Proc. International Conference on Learning Representations. References 111 Zhang, Chiyuan, Bengio, Samy, and Singer, Yoram. 2019. Are all layers created equal? ArXiv preprint arXiv:1902.01996. Zhang, Chiyuan, Bengio, Samy, Hardt, Moritz, Mozer, Michael C., and Singer, Yoram. 2020. Identity crisis: Memorization and generalization under extreme overparameterization. In: Proc. International Conference on Learning Repre- sentations. Zhou, Ding-Xuan. 2020a. Theory of deep convolutional neural networks: Down- sampling. Neural Networks, 124, 319â€“327. Zhou, Ding-Xuan. 2020b. Universality of deep convolutional neural networks. Applied and Computational Harmonic Analysis, 48(2), 787â€“794. Zhou, Hao, Alvarez, Jose M., and Porikli, Fatih. 2016. Less is more: Towards compact CNNs. Pages 662â€“677 of: Proc. European Conference on Computer Vision. Zoph, Barret, and Le, Quoc V. 2017. Neural architecture search with reinforcement learning. In: Proc.Dobriban International Conference on Learning Represen- tations. Zou, Difan, Cao, Yuan, Zhou, Dongruo, and Gu, Quanquan. 2020. Gradient de- scent optimizes over-parameterized deep ReLU networks. Machine Learning, 109(3), 467â€“492. 2 Generalization in Deep Learning K. Kawaguchi, Y. Bengio, and L. Kaelbling Abstract: This chapter provides theoretical insights into why and how deep learn- ing can generalize well, despite its large capacity, complexity, possible algorithmic instability, non-robustness, and sharp minima. This chapter forms a response to an open question in the literature. We also discuss approaches to provide non-vacuous generalization guarantees for deep learning. On the basis of theoretical observations, we propose new open problems. 2.1 Introduction Deep learning has seen signiï¬cant practical success and has had a profound impact on the conceptual bases of machine learning and artiï¬cial intelligence. Along with its practical success, the theoretical properties of deep learning have been a subject of active investigation. For the expressivity of neural networks, there are classical results regarding their universality (Leshno et al., 1993) and their exponential ad- vantages over hand-crafted features (Barron, 1993). Another series of theoretical studies has considered how trainable (or optimizable) deep hypothesis spaces are, revealing structural properties that may enable non-convex optimization (Choro- manska et al., 2015; Kawaguchi, 2016b). However, merely having an expressive and trainable hypothesis space does not guarantee good performance in predicting the values of future inputs, because of possible over-ï¬tting to training data. This leads to the study of generalization, which is the focus of this chapter. Some classical theory work attributes generalization ability to the use of a low- capacity class of hypotheses (Vapnik, 1998; Mohri et al., 2012). From the viewpoint of compact representation, which is related to small capacity, it has been shown that deep hypothesis spaces have an exponential advantage over shallow hypothesis spaces for representing some classes of natural target functions (Pascanu et al., 2014; Montufar et al., 2014; Livni et al., 2014; Telgarsky, 2016; Poggio et al., 2017). In other words, when some assumptions implicit in the hypothesis space (e.g., the 112 2.2 Background 113 deep composition of piecewise linear transformations) are approximately satisï¬ed by the target function, one can achieve very good generalization, compared with methods that do not rely on that assumption. However, a recent paper (Zhang et al., 2017) showed empirically that successful deep hypothesis spaces have suï¬ƒcient capacity to memorize random labels. This observation has been called an â€œapparent paradoxâ€ and has led to active discussion by many researchers (Arpit et al., 2017; Krueger et al., 2017; Hoï¬€er et al., 2017; Wu et al., 2017; Dziugaite and Roy, 2017; Dinh et al., 2017). Zhang et al. (2017) concluded with an open problem stating that understanding such observations requires the rethinking of generalization, while Dinh et al. (2017) stated that explaining why deep learning models can generalize well, despite their overwhelming capacity, is an open area of research. In Â§2.3 we illustrate that, even in the case of linear models, hypothesis spaces with overwhelming capacity can result in arbitrarily small test errors and expected risks. Here, the test error is the error of a learned hypothesis on data on which it was not trained, but which is often drawn from the same distribution. Test error is a measure of how well the hypothesis generalizes to new data. We will examine this phenomenon closely, extending the original open problem from previous pa- pers (Zhang et al., 2017; Dinh et al., 2017) into a new open problem that strictly includes the original. We reconcile an apparent paradox by checking theoretical consistency and identifying a diï¬€erence in the underlying assumptions. Consider- ing the diï¬€erences in focus of theory and practice, we outline possible practical roles that generalization theory can play. Towards addressing these issues, Â§2.4 presents generalization bounds based on validation datasets, which can provide non-vacuous and numerically-tight general- ization guarantees for deep learning in general. Section 2.5 analyzes generalization errors based on training datasets, focusing on a speciï¬c case of feed-forward neural networks with ReLU units and max pooling. Under these conditions, the devel- oped theory provides quantitatively tight theoretical insights into the generalization behavior of neural networks. 2.2 Background Let R[ f ] be the expected risk of a function f , R[ f ] = EX,Yâˆ¼P(X,Y)[L( f (X),Y)], where X and Y are a input and a target, L is a loss function, and P(X,Y) is the true distribution. Let Ë†fA,S : X â†’Y be a model learned by a learning algorithm A (including random seeds for simplicity) using a training dataset S = ((X(i),Y(i)))m i=1 of size m. Let RS[ f ] be the empirical risk of f as RS[ f ] = 1 m Ãm i=1 L( f (X(i)),Y(i)) with {(X(i),Y(i))}m i=1 = S. Let F be a set of functions endowed with some structure, i.e., a hypothesis space. All vectors are column vectors in this chapter. For any given variable v, let dv be its dimensionality. 114 Kawaguchi et al: Generalization in Deep Learning A goal in machine learning is typically framed as the minimization of the expected risk R[ Ë†fA,S]. We typically aim to minimize the non-computable expected risk R[ Ë†fA,S] by minimizing the computable empirical risk RS[ Ë†fA,S] (i.e., empirical risk minimization). One goal of generalization theory is to explain and justify when and how minimizing RS[ Ë†fA,S] is a sensible approach to minimizing R[ Ë†fA,S] by analyzing the generalization gap := R[ Ë†fA,S] âˆ’RS[ Ë†fA,S]. In this section only, we use the typical assumption that S is generated by independent and identically distributed (i.i.d.) draws according to the true distribution P(X,Y); the following sections of this chapter do not utilize this assumption. Under this assumption, a primary challenge of analyzing the generalization gap stems from the dependence of Ë†fA,S on the same dataset S as that used in the deï¬nition of RS. Several approaches in statistical learning theory have been developed to handle this dependence. The hypothesis-space complexity approach handles this dependence by decou- pling Ë†fA,S from the particular dataset S by considering the worst-case gap for functions in the hypothesis space as R[ Ë†fA,S] âˆ’RS[ Ë†fA,S] â‰¤sup f âˆˆF R[ f ] âˆ’RS[ f ], and by carefully analyzing the right-hand side. Because the cardinality of F is typically (uncountably) inï¬nite, a direct use of the union bound over all elements in F yields a vacuous bound, leading to the need to consider diï¬€erent quantities for characterizing F , e.g., Rademacher complexity and the Vapnikâ€“Chervonenkis (VC) dimension. For example, if the codomain of L is in [0,1], we have (Mohri et al., 2012, Theorem 3.1) that for any Î´ > 0, with probability at least 1 âˆ’Î´, sup f âˆˆF R[ f ] âˆ’RS[ f ] â‰¤2Rm(F ) + s ln 1 Î´ 2m , where Rm(F ) is the Rademacher complexity of the set {(X,Y) 7â†’L( f (X),Y): f âˆˆ F } and is deï¬ned by Rm(F ) = ES,Î¾ " sup f âˆˆF 1 m m Ã• i=1 Î¾iL( f (X(i)),Y(i)) # , where Î¾ = (Î¾1,. . .,Î¾n) and Î¾1,. . .,Î¾n are independent uniform random variables taking values in {âˆ’1,+1} (i.e., Rademacher variables). For the deep-learning hy- pothesis spaces F , there are several well-known bounds on Rm(F ), including those with explicit exponential dependence on depth (Sun et al., 2016; Neyshabur et al., 2.2 Background 115 2015a; Xie et al., 2015) and explicit linear dependence on the number of train- able parameters (Shalev-Shwartz and Ben-David, 2014). There has been signiï¬cant work on improving the bounds in this approach, but all existing solutions with this approach still depend on the complexity of a hypothesis space or a sequence of hypothesis spaces, resulting in vacuous, and numerically too loose, generalization bounds. The stability approach deals with the dependence of Ë†fA,S on the dataset S by considering the stability of algorithm A with respect to diï¬€erent datasets. The considered stability is a measure of how much changing a data point in S can change Ë†fA,S. For example, an algorithm A is said to have uniform stability Î² (w.r.t. L) if we have that for all S âˆˆ(X Ã— Y)m, all i âˆˆ{1,. . .,m}, and all (X,Y) âˆˆX Ã— Y, |L( Ë†fA,S(X),Y) âˆ’L( Ë†fA,S\i(X),Y)| â‰¤Î², where S\i = ((X(1),Y(1)),. . .,(X(iâˆ’1),Y(iâˆ’1)),(X(i+1),Y(i+1)),. . .,(X(m),Y(m))) (S\i is S with the ith sample being removed). If the algorithm A has uniform stability Î² (w.r.t. L) and if the codomain of L is in [0, M], we have (Bousquet and Elisseeï¬€, 2002) that for any Î´ > 0, with probability at least 1 âˆ’Î´, R[ Ë†fA,S] âˆ’RS[ Ë†fA,S] â‰¤2Î² + (4mÎ² + M) s ln 1 Î´ 2m . On the basis of previous work on stability (e.g., Hardt et al., 2016; Kuzborskij and Lampert, 2017; Gonen and Shalev-Shwartz, 2017), one may conjecture some reason for generalization in deep learning. The robustness approach avoids dealing with certain details of the dependence of Ë†fA,S on S by considering the robustness of algorithm A for all possible datasets. In contrast with stability, robustness is the measure of how much the loss value can vary with respect to the space of the values of (X,Y). More precisely, an algorithm A is said to be (|â„¦|, Î¶(Â·))-robust if X Ã— Y can be partitioned into |â„¦| disjoint sets â„¦1,. . .,â„¦|â„¦|, such that, for all S âˆˆ(X Ã— Y)m, all (X,Y) âˆˆS, all (Xâ€²,Y â€²) âˆˆX Ã— Y, and all i âˆˆ{1,. . ., |â„¦|}, if (X,Y),(Xâ€²,Y â€²) âˆˆâ„¦i then |L( Ë†fA,S(X),Y) âˆ’L( Ë†fA,S(Xâ€²),Y â€²)| â‰¤Î¶(S). If algorithm A is (â„¦, Î¶(Â·))-robust and the codomain of L is upper-bounded by M, given a dataset S we have (Xu and Mannor, 2012) that for any Î´ > 0, with probability at least 1 âˆ’Î´, |R[ Ë†fA,S] âˆ’RS[ Ë†fA,S]| â‰¤Î¶(S) + M s 2|â„¦| ln 2 + 2 ln 1 Î´ m . The robustness approach requires an a priori known and ï¬xed partition of the input 116 Kawaguchi et al: Generalization in Deep Learning space such that the number of sets in the partition is |â„¦| and the change of loss values in each set of the partition is bounded by Î¶(S) for all S (Deï¬nition 2 and the proof of Theorem 1 in Xu and Mannor, 2012). In classiï¬cation, if the margin is ensured to be large, we can ï¬x the partition with balls of radius corresponding to this large margin, ï¬lling the input space. Recently, this idea was applied to deep learning (Sokolic et al., 2017a,b), producing insightful and eï¬€ective generalization bounds while still suï¬€ering from the curse of the dimensionality of the a priori known and ï¬xed input manifold. With regard to the above approaches, ï¬‚at minima can be viewed as the concept of low variation in the parameter space; i.e., a small perturbation in the parameter space around a solution results in a small change in the loss surface. Several studies have provided arguments for generalization in deep learning based on ï¬‚at minima (Keskar et al., 2017). However, Dinh et al. (2017) showed that ï¬‚at minima in practical deep-learning hypothesis spaces can be turned into sharp minima via re-parameterization without aï¬€ecting the generalization gap, indicating that this requires further investigation. There have been investigations into the connection between generalization and stochastic gradient descent (SGD) based on the Rademacher complexity, stability, and ï¬‚at minima. For the Rademacher complexity, we can deï¬ne the hypothesis space F explored by SGD, and could argue that the Rademacher complexity of it is somehow small; e.g., SGD with an appropriate initialization ï¬nds a minimal-norm solution (Poggio et al., 2018). The stability of SGD has been also analyzed (e.g., Hardt et al., 2016), but it is known that the existing bounds quickly become too loose and vacuous as the training time increases, even in the practical regime of the training time where a neural network can still generalize well. One can also argue that SGD prefers ï¬‚at minima and degenerate minima, resulting in better generalization (Keskar et al., 2017; Banburski et al., 2019). Stochastic gradient descent with added noise has been also studied, but its convergence rate grows exponentially as the number of parameters increases (Raginsky et al., 2017). For all these approaches, there is yet no theoretical proof with a non-vacuous and numerically tight generalization bound on the practical regime of deep learning. 2.3 Rethinking Generalization Zhang et al. (2017) demonstrated empirically that several deep hypothesis spaces can memorize random labels, while having the ability to produce zero training error and small test errors for particular natural datasets (e.g., CIFAR-10). They also observed empirically that regularization on the norm of weights seemed to be unnecessary to obtain small test errors, in contradiction to conventional wisdom. These observations suggest the following open problem. 2.3 Rethinking Generalization 117 Open Problem 1. How to tightly characterize the expected risk R[ f ] or the gen- eralization gap R[ f ] âˆ’RS[ f ] with a suï¬ƒciently complex deep-learning hypothesis space F âˆ‹f , to produce theoretical insights and distinguish the case of â€œnaturalâ€ problem instances (P(X,Y),S) (e.g., images with natural labels) from the case of other problem instances (Pâ€² (X,Y),Sâ€²) (e.g., images with random labels). In support of and extending the empirical observations by Zhang et al. (2017), we provide a theorem (Theorem 2.1) stating that the hypothesis space of over- parameterized linear models can memorize any training data and decrease the training and test errors arbitrarily close to zero (including zero itself) with arbi- trarily large parameters norms, even when the parameters are arbitrarily far from the ground-truth parameters. Furthermore, Corollary 2.2 shows that conventional wisdom regarding the norms of the parameters w can fail to explain generalization, even in linear models that might seem not to be over-parameterized. All proofs for this chapter are presented in the appendix. Theorem 2.1. Consider a linear model with the training prediction Ë†Y(w) = Î¦w âˆˆ RmÃ—s, where Î¦ âˆˆRmÃ—n is a ï¬xed feature matrix of the training inputs. Let Ë†Ytest(w) = Î¦testw âˆˆRmtestÃ—s be the test prediction, where Î¦test âˆˆRmtestÃ—n is a ï¬xed feature matrix of the test inputs. Let M = [Î¦âŠ¤,Î¦âŠ¤ test]âŠ¤. Then, if n > m and if rank(Î¦) = m and rank(M) < n, (i) for any Y âˆˆRmÃ—s, there exists a parameter wâ€² such that Ë†Y(wâ€²) = Y, and (ii) if there exists a ground truth wâˆ—satisfying Y = Î¦wâˆ—and Ytest = Î¦testwâˆ—then, for any Ïµ,Î´ â‰¥0, there exists a parameter w such that (a) Ë†Y(w) = Y + Ïµ A for some matrix A with âˆ¥Aâˆ¥F â‰¤1, and (b) Ë†Ytest(w) = Ytest + ÏµB for some matrix B with âˆ¥Bâˆ¥F â‰¤1, and (c) âˆ¥wâˆ¥F â‰¥Î´ and âˆ¥w âˆ’wâˆ—âˆ¥F â‰¥Î´. Corollary 2.2. If n â‰¤m and if rank(M) < n, then statement (ii) in Theorem 2.1 holds. Whereas Theorem 2.1 and Corollary 2.2 concern test errors rather than the expected risk (in order to be consistent with empirical studies), Proposition 2.3 below shows the same phenomena for the expected risk for general machine learning models not limited to deep learning and linear hypothesis spaces; i.e., Proposition 2.3 shows that, regarding small capacity, low complexity, stability, robustness, and ï¬‚at minima, none of these is necessary for generalization in machine learning for any given problem instance (P(X,Y),S), although one of them can be suï¬ƒcient for generalization. This statement does not contradict the necessary conditions and the no-free-lunch theorem from previous learning theory, as will be explained later in the chapter. 118 Kawaguchi et al: Generalization in Deep Learning Proposition 2.3. Given a pair (P(X,Y),S) and a desired Ïµ > inf f âˆˆYX R[ f ]âˆ’RS[ f ], let f âˆ— Ïµ be a function such that Ïµ â‰¥R[ f âˆ— Ïµ ] âˆ’RS[ f âˆ— Ïµ ]. Then, (i) for any hypothesis space F whose hypothesis-space complexity is large enough to memorize any dataset and which includes f âˆ— Ïµ possibly at an arbitrarily sharp minimum, there exist learning algorithms A such that the generalization gap of Ë†fA,S is at most Ïµ, and (ii) there exist arbitrarily unstable and arbitrarily non-robust algorithms A such that the generalization gap of Ë†fA,S is at most Ïµ. Proposition 2.3 is a direct consequence of the following remark which captures the essence of all the above observations (see Appendix A5 for the proof of Propo- sition 2.3). Remark 2.4. The expected risk R[ f ] and the generalization gap R[ f ] âˆ’RS[ f ] of a hypothesis f with a true distribution P(X,Y) and a dataset S are completely determined by the tuple (P(X,Y),S, f ), independently of other factors, such as the hypothesis space F (and hence its properties such as capacity, Rademacher com- plexity, and ï¬‚at-minima) or the properties of random datasets diï¬€erent from the given S (e.g., the stability and robustness of the learning algorithm A). In contrast, conventional wisdom states that these other factors are what matter. This has created the â€œapparent paradoxâ€ in the literature. From these observations, we propose the following open problem. Open Problem 2. To tightly characterize the expected risk R[ f ] or the generaliza- tion gap R[ f ]âˆ’RS[ f ] of a hypothesis f with a pair (P(X,Y),S) of a true distribution and a dataset, so as to produce theoretical insights based only on properties of the hypothesis f and the pair (P(X,Y),S). Solving Open Problem 2 for deep learning implies solving Open Problem 1, but not vice versa. Open Problem 2 encapsulates the essence of Open Problem 1 and all the issues from our Theorem 2.1, Corollary 2.2, and Proposition 2.3. 2.3.1 Consistency of Theory The empirical observations in Zhang et al. (2017) and our results above may seem to contradict the results of statistical learning theory. However, there is no contradiction, and the apparent inconsistency arises from the misunderstanding and misuse of the precise meanings of the theoretical statements. Statistical learning theory can be considered to provide two types of statements relevant to the scope of this chapter. The ï¬rst type (which comes from upper 2.3 Rethinking Generalization 119 bounds) is logically in the form of â€œp implies q,â€ where p := â€œthe hypothesis- space complexity is smallâ€ (or another statement about stability, robustness, or ï¬‚at minima), and q := â€œthe generalization gap is small.â€ Notice that â€œp implies qâ€ does not imply â€œq implies p.â€ Thus, based on statements of this type, it is entirely possible that the generalization gap is small even when the hypothesis-space complexity is large or the learning mechanism is unstable, non-robust, or subject to sharp minima. The second type of statement (which comes from lower bounds) is, logically, in the following form. In a set Uall of all possible problem conï¬gurations, there exists a subset U âŠ†Uall such that â€œq implies pâ€ in U (with the same deï¬nitions of p and q as in the previous paragraph). For example, Mohri et al. (2012, Section 3.4) derived lower bounds on the generalization gap by showing the existence of a â€œbadâ€ distribution that characterizes U. Similarly, the classical no-free-lunch theorems are the results that give a worst-case distribution for each algorithm. However, if the problem instance at hand (e.g., object classiï¬cation with MNIST or CIFAR-10) is not in such a subset U in the proofs (e.g., if the data distribution is not among the â€œbadâ€ ones considered in the proofs), q does not necessarily imply p. Thus, it is still naturally possible that the generalization gap is small with large hypothesis-space complexity, instability, non-robustness, and sharp minima. Therefore, there is no contradiction or paradox. 2.3.2 Diï¬€erences in Assumptions and Problem Settings Under certain assumptions, many results in statistical learning theory have been shown to be tight and insightful (e.g., Mukherjee et al., 2006; Mohri et al., 2012). Hence, the need to rethink generalization comes partly from diï¬€erences in the assumptions and problem settings. Figure 2.1 illustrates the diï¬€erences between the assumptions in statistical learn- ing theory and in some empirical studies. On the one hand, in statistical learning theory a distribution P(X,Y) and a dataset S are usually unspeciï¬ed except that P(X,Y) is in some set P and the dataset S âˆˆD is drawn randomly according to P(X,Y) (typ- ically with the i.i.d. assumption). On the other hand, in most empirical studies and in our theoretical results (Theorem 2.1 and Proposition 2.3), the distribution P(X,Y) is still unknown, yet speciï¬ed (e.g., via a real-world process), and the dataset S is speciï¬ed and usually known (e.g., CIFAR-10 or ImageNet). Intuitively, whereas statistical learning theory needs to consider a set P Ã— D because of weak assump- tions, some empirical studies can focus on a speciï¬ed point (P(X,Y),S) in a set P Ã—D because of stronger assumptions. Therefore, by using the same terminology such as â€œexpected riskâ€ and â€œgeneralizationâ€ in both cases, we are susceptible to confusion and apparent contradiction. Lower bounds, necessary conditions, and tightness in statistical learning theory 120 Kawaguchi et al: Generalization in Deep Learning ğ· ğ‘†âˆ¼ ğ‘† Figure 2.1 An illustration of diï¬€erences in assumptions. Statistical learning theory analyzes the generalization behaviors of Ë†fA,S over randomly drawn unspeciï¬ed datasets S âˆˆD according to some unspeciï¬ed distribution P(X ,Y) âˆˆP. Intuitively, statistical learning theory is concerned more with questions regarding the set P Ã— D because of the unspeciï¬ed nature of (P(X ,Y), S), whereas certain empirical studies (e.g., Zhang et al., 2017) can focus on questions regarding each speciï¬ed point (P(X,Y), S) âˆˆP Ã— D. are typically deï¬ned via a worst-case distribution Pworst (X,Y) âˆˆP. For instance, classical â€œno-free-lunchâ€ theorems and certain lower bounds on the generalization gap (e.g., Mohri et al., 2012, Section 3.4) have actually been proven for the worst-case distri- bution Pworst (X,Y) âˆˆP. Therefore, â€œtightâ€ and â€œnecessaryâ€ typically mean â€œtightâ€ and â€œnecessaryâ€ for the set P Ã— D (e.g., through the worst or average case), but not for each particular point (P(X,Y),S) âˆˆP Ã— D. From this viewpoint, we can understand that even if the quality of the set P Ã— D is â€œbadâ€ overall, there may exist a â€œgoodâ€ point (P(X,Y),S) âˆˆP Ã— D. Several approaches to statistical learning theory, such as the data-dependent and Bayesian ones (Herbrich and Williamson, 2002; Dziugaite and Roy, 2017), use further assumptions on the set P Ã— D to take advantage of more prior and posterior information; these have the ability to tackle Open Problem 1. However, these approaches do not apply to Open Problem 2 as they still depend on factors other than the given (P(X,Y),S, f ). For example, data-dependent bounds with the luckiness framework (Shawe-Taylor et al., 1998; Herbrich and Williamson, 2002) and empirical Rademacher complexity (Koltchinskii and Panchenko, 2000; Bartlett et al., 2002) still depend on the concept of hypothesis spaces (or the sequence of hypothesis spaces), and the robustness approach (Xu and Mannor, 2012) depends on datasets diï¬€erent from a given S via the deï¬nition of robustness (i.e., in Â§2.2, Î¶(S) is a data-dependent term, but the deï¬nition of Î¶ itself and â„¦depend on datasets other than S). We note that analyzing a set P Ã— D is of signiï¬cant interest for its own merits and is a natural task in the ï¬eld of computational complexity (e.g., categorizing a set of problem instances into subsets with or without polynomial solvability). Indeed, a situation where theory theory focuses on a set whereas many practical studies focus on each element in the set is prevalent in computer science (see the discussion in Appendix A2 for more detail). 2.4 Generalization Bounds via Validation 121 2.3.3 Practical Role of Generalization Theory From the discussions above, we can see that there is a logically expected diï¬€erence between the scope of theory and the focus in practice; it is logically expected that there are problem instances where theoretical bounds are pessimistic. In order for generalization theory to have maximal impact in practice, we must be clear on the set of diï¬€erent roles it can play regarding practice, and then work to extend and strengthen it in each of these roles. We have identiï¬ed the following practical roles for a theory: Role 1 To provide guarantees on expected risk. Role 2 To guarantee that the generalization gap: Role 2.1 is small for a given ï¬xed S, and/or Role 2.2 approaches zero for a ï¬xed model class as m increases. Role 3 To provide theoretical insights to guide the search over model classes. 2.4 Generalization Bounds via Validation In practical deep learning, we typically adopt the trainingâ€“validation paradigm, usually with a held-out validation set. We then search over hypothesis spaces by changing architectures (and other hyper-parameters) to obtain a low validation error. In this view we can conjecture the reason why deep learning can sometimes generalize well to be as follows: it is partially because we can obtain a good model via search using a validation dataset. Indeed, Proposition 2.5 states that if the validation error of a hypothesis is small, it is guaranteed to generalize well regardless of its capacity, Rademacher complexity, stability, robustness, or ï¬‚at minima. Let S(val) be a held-out validation dataset, of size mval, which is independent of the training dataset S. Proposition 2.5. (Generalization guarantee via validation error) Assume that S(val) is generated by i.i.d. draws according to a true distribution P(X,Y). Let Îºf ,i = R[ f ] âˆ’L( f (X(i)),Y(i)) for (X(i),Y(i)) âˆˆS(val). Suppose that E[Îº2 f ,i] â‰¤Î³2 and |Îºf ,i| â‰¤C almost surely, for all ( f,i) âˆˆFval Ã— {1,. . .,mval}. Then, for any Î´ > 0, with probability at least 1 âˆ’Î´, the following holds for all f âˆˆFval: R[ f ] â‰¤RS(val)[ f ] + 2C ln(|Fval|/Î´) 3mval + s 2Î³2 ln(|Fval|/Î´) mval . Here, Fval is deï¬ned as a set of models f that is independent of a held-out validation dataset S(val). Importantly, Fval can, however, depend on the training dataset S, because a dependence on S does not imply a dependence on S(val). For example, Fval can contain a set of models f such that each f is a model, obtained 122 Kawaguchi et al: Generalization in Deep Learning at the end of training, with at least 99.5% training accuracy. In this example, |Fval| equals at most the number of end epochs times the cardinality of the set of possible hyper-parameter settings, and it is likely to be much smaller than that because of the 99.5% training accuracy criterion and the fact that a space of many hyper- parameters is narrowed down by using the training dataset as well as other datasets from diï¬€erent tasks. If a hyper-parameter search depends on the validation dataset, Fval will be the possible space of the search instead of the space actually visited by the search. We can also use a sequence (F (j) val )j to obtain validation-data-dependent bounds (see Appendix A6). The bound in Proposition 2.5 is non-vacuous and tight enough to be practically meaningful. For example, consider a classiï¬cation task with 0â€“1 loss. Set mval = 10,000 (e.g., as in MNIST and CIFAR-10) and Î´ = 0.1. Then, even in the worst case, with C = 1 and Î³2 = 1, and even with |Fval| = 1,000,000,000, we have, with probability at least 0.9, that R[ f ] â‰¤RS(val)[ f ] + 6.94% for all f âˆˆFval. In a non-worst-case scenario, for example, with C = 1 and Î³2 = (0.05)2, we can replace 6.94% by 0.49%. With a larger validation set (e.g., as in ImageNet) and/or more optimistic C and Î³2 values, we can obtain much better bounds. Although Proposition 2.5 poses the concern of increasing that the generalization bound will be increased when one is using a single validation dataset with too large a value of |Fval|, the rate of increase goes as only ln |Fval| and p ln |Fval|. We can also avoid dependence on the cardinality of Fval by using Remark 2.6. Remark 2.6. Assume that S(val) is generated by i.i.d. draws according to P(X,Y). By applying Mohri et al. (2012, Theorem 3.1) to Fval, if the codomain of L is in [0,1], with probability at least 1 âˆ’Î´, then, for all f âˆˆFval, R[ f ] â‰¤RS(val)[ f ] + 2Rm(Fval) + p (ln 1/Î´)/mval. Unlike the standard use of Rademacher complexity with a training dataset, the set Fval cannot depend on the validation set S but can depend on the training dataset S in any manner, and hence Fval diï¬€ers signiï¬cantly from the typical hypothesis space deï¬ned by the parameterization of models. We can thus end up with a very diï¬€erent eï¬€ective capacity and hypothesis complexity (as selected by model search using the validation set) depending on whether the training data are random or have interesting structure which the neural network can capture. 2.5 Direct Analyses of Neural Networks Unlike the previous section, this section analyzes the generalization gap with a training dataset S. In Â§2.3, we extended Open Problem 1 to Open Problem 2, and identiï¬ed the diï¬€erent assumptions in theoretical and empirical studies. Accord- 2.5 Direct Analyses of Neural Networks 123 Table 2.1 Additional notation for DAG in Section 2.5 Description Notation pre-activation output of the lth hidden layer given (X,w) z[l](X,w) component of an input X(i) used for the jth path Â¯X(i) j path activation for the jth path given X(i) and (X(i),w) Â¯Ïƒj(X(i),w) vector of trainable parameters w the jth path weight for the kth output unit Â¯wk,j vector of parameters trained with (A,S) wS vector of parameters frozen in two-phase training wÏƒ weight matrix connecting the (l âˆ’1)th layer to the lth layer W[l] weight matrix connecting the lâ€²th layer to the lth layer W(l,lâ€²) ingly, this section aims to address these problems, to some extent, both in the case of particular speciï¬ed datasets and the case of random unspeciï¬ed datasets. To achieve this goal, this section presents a direct analysis of neural networks, rather than deriving results about neural networks from more generic theories based on capacity, Rademacher complexity, stability, or robustness. Sections 2.5.2 and 2.5.3 deal with the squared loss, while Â§2.5.4 considers 0â€“1 loss with multi-labels. Table 2.1 summarizes the notation used in this section for a directed acyclic graph (DAG). 2.5.1 Model Description via Deep Paths We consider general neural networks of any depth that have the structure of a DAG with ReLU nonlinearity and/or max pooling. This includes a feedforward network of any structure and with convolutional and/or fully connected layers, potentially with skip connections. For pedagogical purposes, we ï¬rst discuss our model description for layered networks without skip connections, and then describe it for DAGs. Layered nets without skip connections. Let z[l](X,w) âˆˆRnl be the pre-activation vector of the lth hidden layer, where nl is the width of the lth hidden layer and w represents the trainable parameters. Let L âˆ’1 be the number of hidden layers. For layered networks without skip connections, the pre-activation (or pre-nonlinearity) vector of the lth layer can be written as z[l](X,w) = W[l]Ïƒ(lâˆ’1)  z[lâˆ’1](X,w)  , with a boundary deï¬nition Ïƒ(0)  z[0](X,w) â‰¡X, where Ïƒ(lâˆ’1) represents nonlinear- ity via ReLU and/or max pooling at the (l âˆ’1)th hidden layer, and W[l] âˆˆRnlÃ—nlâˆ’1 124 Kawaguchi et al: Generalization in Deep Learning is a matrix of weight parameters connecting the (l âˆ’1)th layer to the lth layer. Here, W[l] can have any structure (e.g., shared and sparse weights to represent a convolutional layer). Let Ã›Ïƒ[l](X,w) be a vector, each of whose elements is 0 or 1, such that Ïƒ[l]  z[l](X,w) = Ã›Ïƒ[l](X,w)â—¦z[l](X,w), which is an elementwise product of the vectors Ã›Ïƒ[l](X,w) and z[l](X,w). Then we can write the pre-activation of the kth output unit at the last layer l = L as z[L] k (X,w) = nLâˆ’1 Ã• jLâˆ’1=1 W[L] k jLâˆ’1 Ã›Ïƒ(Lâˆ’1) jLâˆ’1 (X,w)z[Lâˆ’1] jLâˆ’1 (X,w). By expanding z[l](X,w) repeatedly and exchanging the sum and product via the distributive law of multiplication, we obtain z[L] k (X,w) = nLâˆ’1 Ã• jLâˆ’1=1 nLâˆ’2 Ã• jLâˆ’2=1 Â· Â· Â· n0 Ã• j0=1 Wk jLâˆ’1 jLâˆ’2...j0 Ã›ÏƒjLâˆ’1 jLâˆ’2...j1(X,w)Xj0, where Wk jLâˆ’1 jLâˆ’2Â·Â·Â·j0 = W[L] k jLâˆ’1 Lâˆ’1 Ã– l=1 W[l] jl jlâˆ’1 and Ã›ÏƒjLâˆ’1 jLâˆ’2Â·Â·Â·j1(X,w) = Lâˆ’1 Ã– l=1 Ã›Ïƒ[l] jl (X,w). By merging the indices j0,. . ., jLâˆ’1 into j with some bijection between {1,. . .,n0} Ã—Â· Â· Â·Ã— {1,. . .,nLâˆ’1} âˆ‹(j0,. . ., jLâˆ’1) and {1,. . .,n0n1 Â· Â· Â· nLâˆ’1} âˆ‹j, we have z[L] k (X,w) = Ã j Â¯wk,j Â¯Ïƒj(X,w) Â¯Xj, where Â¯wk,j, Â¯Ïƒj(X,w) and Â¯Xj represent Wk jLâˆ’1 jLâˆ’2Â·Â·Â·j0, Ã›ÏƒjLâˆ’1 jLâˆ’2Â·Â·Â·j1(X,w) and Xj0, respectively with the change of indices (i.e., Ïƒj(X,w) and Â¯Xj, respectively, contain the n0 numbers and n1 Â· Â· Â· nLâˆ’1 numbers of the same copy of each Ã›ÏƒjLâˆ’1 jLâˆ’2Â·Â·Â·j1(X,w) and Xj0). Note that Ã j represents summation over all the paths from the input X to the kth output unit. DAGs. Remember that every DAG has at least one topological ordering, which can be used to to create a layered structure with possible skip connections (e.g., see Healy and Nikolov, 2001; Neyshabur et al., 2015a). In other words, we consider 2.5 Direct Analyses of Neural Networks 125 DAGs such that the pre-activation vector of the lth layer can be written as z[l](X,w) = lâˆ’1 Ã• lâ€²=0 W(l,lâ€²)Ïƒ[lâ€²]  z[lâ€²](X,w)  with a boundary deï¬nition Ïƒ(0)  z[0](X,w) â‰¡X, where W(l,lâ€²) âˆˆRnlÃ—nlâ€² is a matrix of weight parameters connecting the lâ€²th layer to the lth layer. Again, W(l,lâ€²) can have any structure. Thus, in the same way as with layered networks without skip connections, for all k âˆˆ{1,. . ., dy}, z[L] k (X,w) = Ã j Â¯wk,j Â¯Ïƒj(X,w) Â¯Xj, where Ã j represents summation over all paths from the input X to the kth output unit; i.e., Â¯wk,j Â¯Ïƒj(X,w) Â¯Xj is the contribution from the jth path to the kth output unit. Each of Â¯wk,j, Â¯Ïƒj(X,w), and Â¯Xj is deï¬ned in the same manner as in the case of layered networks without skip connections. In other words, the jth path weight Â¯wk,j is the product of the weight parameters in the jth path, and Â¯Ïƒj(X,w) is the product of the 0â€“1 activations in the jth path, corresponding to ReLU nonlinearity and max pooling; Â¯Ïƒj(X,w) = 1 if all units in the jth path are active, and Â¯Ïƒj(X,w) = 0 otherwise. Also, Â¯Xj is the input used in the jth path. Therefore, for DAGs, including layered networks without skip connections, z[L] k (X,w) = [ Â¯X â—¦Â¯Ïƒ(X,w)]âŠ¤Â¯wk, (2.1) where [ Â¯X â—¦Â¯Ïƒ(X,w)]j = Â¯Xj Â¯Ïƒj(X,w) and ( Â¯wk)j = Â¯wk,j are vectors whose size is the number of paths. 2.5.2 Theoretical Insights via Tight Theory for Every Pair (P,S) Theorem 2.7 below solves Open Problem 2 (and hence Open Problem 1) for neural networks with squared loss by stating that the generalization gap of a trainable parameter vector w with respect to a problem (P(X,Y),S) is tightly analyzable with theoretical insights, based only on the quality of w and the pair (P(X,Y),S). We do not assume that S is generated randomly on the basis of some relationship with P(X,Y); the theorem holds for any dataset, regardless of how it was generated. Let wS and Â¯wS k be the parameter vectors w and Â¯wk learned with a dataset S and A. Let R[wS] and RS[wS] be the expected risk and empirical risk of the model with the learned parameter wS. Let zi = [ Â¯X(i) â—¦Â¯Ïƒ(X(i),wS)]. Let G = EX,Yâˆ¼P(X,Y)[zzâŠ¤]âˆ’1 m Ãm i=1 zizâŠ¤ i and v = 1 m Ãm i=1 Y(i) k zi âˆ’EX,Yâˆ¼P(X,Y)[Ykz]. Given a matrix M, let Î»max(M) be the largest eigenvalue of M. Theorem 2.7. Let {Î»j}j and be a set of eigenvalues and {uj}j the corresponding 126 Kawaguchi et al: Generalization in Deep Learning orthonormal set of eigenvectors of G. Let Î¸(1) Â¯wk ,j be the angle between uj and Â¯wk. Let Î¸(2) Â¯wk be the angle between v and Â¯wk. Then (deterministically), R[wS] âˆ’RS[wS] âˆ’cy = s Ã• k=1 2âˆ¥vâˆ¥2âˆ¥Â¯w S kâˆ¥2 cos Î¸(2) Â¯wS k + âˆ¥Â¯w S kâˆ¥2 2 Ã• j Î»j cos2 Î¸(1) Â¯wS k ,j ! â‰¤ s Ã• k=1  2âˆ¥vâˆ¥2âˆ¥Â¯w S kâˆ¥2 + Î»max(G)âˆ¥Â¯w S kâˆ¥2 2  , where cy = EY[âˆ¥Y âˆ¥2 2] âˆ’1 m Ãm i=1 âˆ¥Y(i)âˆ¥2 2. Proof Idea From Equation (2.1) with squared loss, we can decompose the gener- alization gap into three terms: R[wS] âˆ’RS[wS] = s Ã• k=1 " ( Â¯w S k)âŠ¤ E[zzâŠ¤] âˆ’1 m m Ã• i=1 zizâŠ¤ i ! Â¯w S k # + 2 s Ã• k=1 " 1 m m Ã• i=1 Y(i) k zâŠ¤ i âˆ’E[YkzâŠ¤] ! Â¯w S k # + E[YâŠ¤Y] âˆ’1 m m Ã• i=1 (Y(i))âŠ¤Y(i). (2.2) By manipulating each term, we obtain the desired statement. See Appendix C3 for a complete proof. â–¡ In Theorem 2.7, there is no issue of a vacuous or too loose a bound. Instead, it indicates that if the norm of the weights âˆ¥Â¯wS kâˆ¥2 is small then the generalization gap is small, with the tightest bound (i.e., equality) having no dependence on the hypothesis space. Importantly, in Theorem 2.7, there are two other signiï¬cant factors in addition to the norm of the weights âˆ¥Â¯wS kâˆ¥2. First, the eigenvalues of G and v measure the concentration of the given dataset S with respect to the (unknown) P(X,Y) in the space of the learned representation zi = [ Â¯X(i) â—¦Â¯Ïƒ(X(i),wS)]. Here, we can see the beneï¬t of deep learning from the viewpoint of â€œdeep-pathâ€ feature learning: even if a given S is not concentrated in the original space, optimizing w can result in concentrating it in the space of z. Similarly, cy measures the concentration of âˆ¥Y âˆ¥2 2, but cy is independent of w and remains unchanged after a pair (P(X,Y),S) is given. Second, the cos Î¸ terms measure the similarity between Â¯wS k and these concentration terms. Because the norm of the weights âˆ¥Â¯wS kâˆ¥2 is multiplied by these other factors, the generalization gap can remain small, even if âˆ¥Â¯wS kâˆ¥2 is large, as long as some of the other factors are small. On the basis of a generic bound-based theory, Neyshabur et al. (2015b) and 2.5 Direct Analyses of Neural Networks 127 Neyshabur et al. (2015a) proposed controlling the norm of the path weights âˆ¥Â¯wkâˆ¥2, which is consistent with our direct bound-less result (and which is as computation- ally tractable as a standard forwardâ€“backward pass1). Unlike the previous results, we do not require a predeï¬ned bound on âˆ¥Â¯wkâˆ¥2 over diï¬€erent datasets, but require only its ï¬nal value for each S in question, in addition to tighter insights (besides the norm) via equality as discussed above. In addition to the predeï¬ned norm bound, these previous results have an explicit exponential dependence on the depth of the network, which does not appear in our Theorem 2.7. Similarly, some previous results speciï¬c to layered networks without skip connections (Sun et al., 2016; Xie et al., 2015) contain the 2Lâˆ’1 factor and a bound on the product of the norms of weight matrices, ÃL l=1 âˆ¥W(l)âˆ¥, rather than on Ã k âˆ¥Â¯wS kâˆ¥2. Here, Ã k âˆ¥Â¯wkâˆ¥2 2 â‰¤ÃL l=1 âˆ¥W(l)âˆ¥2 F because the latter contains all the same terms as the former as well as additional non-negative additive terms after expanding the sums in the deï¬nition of the norms. Therefore, unlike previous bounds, Theorem 2.7 generates these new theoretical insights from a the tight equality (in the ï¬rst line of the equation in Theorem 2.7). Notice that, without manipulating the generalization gap, we can always obtain equality. However, the question answered here is whether or not we can obtain competitive theoretical insights (the path norm bound) via equality instead of in- equality. From a practical view point, if the insights obtained are the same (e.g., they regularize the norm), then equality-based theory has the obvious advantage of being more precise. 2.5.3 Probabilistic Bounds over Random Datasets While the previous subsection tightly analyzed each given point (P(X,Y),S), this subsection considers the set P Ã— D âˆ‹(P(X,Y),S), where D is the set of possible datasets S endowed with an i.i.d. product measure Pm (X,Y) where P(X,Y) âˆˆP (see Â§2.3.2). In Equation (2.2), the generalization gap is decomposed into three terms, each containing the diï¬€erence between a sum of dependent random variables and its expectation. The dependence comes from the fact that the zi = [ Â¯X(i) â—¦Â¯Ïƒ(X(i),wS)] are dependent on the sample index i, because of the dependence of wS on the entire dataset S. We then observe the following: in z[L] k (X,w) = [ Â¯X â—¦Â¯Ïƒ(X,w)]âŠ¤Â¯w, the derivative of z = [ Â¯X â—¦Â¯Ïƒ(X,w)] with respect to w is zero everywhere (except for the measure-zero set, where the derivative does not exist). Therefore, each step of the (stochastic) gradient descent greedily chooses the best direction in terms of Â¯w (with the current z = [ Â¯X â—¦Â¯Ïƒ(X,w)]), but not in terms of the w in z = [ Â¯X â—¦Â¯Ïƒ(X,w)] 1 From the derivation of Equation (2.1), one can compute âˆ¥Â¯wS k âˆ¥2 2 with a single forward pass using element-wise squared weights, an identity input, and no nonlinearity. One can also follow (Neyshabur et al., 2015b) for this computation. 128 Kawaguchi et al: Generalization in Deep Learning 0.9 0.92 0.94 0.96 0.98 1 1.02 0 0.2 0.4 0.6 0.8 1 MNIST (ND) MNIST CIFAR10 Test accuracy ratio Î± Figure 2.2 Test accuracy ratio (two-phase/base). Notice that the y-axis starts with high initial accuracy, even with a very small dataset size, Î±m, for learning wÏƒ. (see Appendix A3 for more detail). This observation leads to a conjecture that the dependence of zi = [ Â¯X(i) â—¦Â¯Ïƒ(X(i),wS)], via the training process on the whole dataset S, is not entirely â€œbadâ€ in terms of the concentration of the sum of the terms involving zi. Empirical Observations As a ï¬rst step in investigating the dependences of zi, we evaluated the following novel two-phase training procedure which explicitly breaks the dependence of zi on the sample index i. We ï¬rst train a network in a standard way, but only using a partial training dataset SÎ±m = {(X(1),Y(1)),. . .,(X(Î±m),Y(Î±m))} of size Î±m, where Î± âˆˆ(0,1) (this is the standard phase of the procedure). We then assign the value of wSÎ±m to a new placeholder wÏƒ := wSÎ±m and freeze wÏƒ, meaning that, as w changes, wÏƒ does not change. At this point, we have that z[L] k (X,wSÎ±m) = [ Â¯X â—¦Â¯Ïƒ(X,wÏƒ)]âŠ¤Â¯wSÎ±m k . We then keep training only the Â¯wSÎ±m k part with the entire training dataset of size m (the freeze phase), yielding the ï¬nal model via this two-phase training procedure as Ëœz[L] k (X,wS) = [ Â¯X â—¦Â¯Ïƒ(X,wÏƒ)]âŠ¤Â¯wS k . (2.3) Note that the vectors wÏƒ = wSÎ±m and Â¯wS k contain the untied parameters in Ëœz[L] k (X,wS). See Appendix A4 for a simple implementation of this two-phase training procedure that requires at most (approximately) twice as much computational cost as the normal training procedure. We implemented the two-phase training procedure with the MNIST and CIFAR- 10 datasets. The test accuracies of the standard training procedure (the base case) were 99.47% for MNIST (ND), 99.72% for MNIST, and 92.89% for CIFAR-10. Here MNIST (ND) indicates MNIST with no data augmentation. The experimental details are given in Appendix B. Our source code is available at: http://lis.csail.mit.edu/code/gdl.html Figure 2.2 presents the test accuracy ratios for varying Î±: the test accuracy 2.5 Direct Analyses of Neural Networks 129 of the two-phase training procedure divided by the test accuracy of the standard (base) training procedure. The plot in Figure 2.2 begins with Î± = 0.05, for which Î±m = 3000 in MNIST and Î±m = 2500 in CIFAR-10. Somewhat surprisingly, using a much smaller dataset for learning wÏƒ still resulted in a competitive performance. A dataset from which we could more easily obtain a better generalization (i.e., MNIST) allowed us to use a smaller value of Î±m to achieve a competitive performance, which is consistent with our discussion above. Theoretical Results We now prove a probabilistic bound for the hypotheses resulting from the two-phase training algorithm. Let Ëœzi = [ Â¯X(i) â—¦Â¯Ïƒ(X(i),wÏƒ)] where wÏƒ := wSÎ±m, as deï¬ned in the two-phase training procedure above. Our two-phase training procedure forces ËœzÎ±m+1,. . ., Ëœzm over samples to be independent random variables (each Ëœzi is depen- dent over coordinates, which is taken care of in our proof), while maintaining the competitive practical performance of the output model Ëœz[L] k (Â·,wS). As a result, we obtain the following bound on the generalization gap for the practical deep models Ëœz[L] k (Â·,wS). Let mÏƒ = (1 âˆ’Î±)m. Given a matrix M, let âˆ¥Mâˆ¥2 be the spectral norm of M. Assumption 2.8. Let G(i) = EX[ËœzËœzâŠ¤] âˆ’Ëœzi ËœzâŠ¤ i , V(i) kkâ€² = Y(i) k Ëœzi,kâ€² âˆ’EX,Y[Yk Ëœzkâ€²], and c(i) y = EY[âˆ¥Y âˆ¥2 2] âˆ’âˆ¥Y(i)âˆ¥2 2. Assume that, for all i âˆˆ{Î±m + 1,. . .,m}: â€¢ Czz â‰¥Î»max(G(i)) and Î³2 zz â‰¥âˆ¥EX[(G(i))2]âˆ¥2; â€¢ Cyz â‰¥maxk,kâ€² |V(i) kkâ€²| and Î³2 yz â‰¥maxk,kâ€² EX[(V(i) kkâ€²)2]); â€¢ Cy â‰¥|c(i) y | and Î³2 y â‰¥EX[(c(i) y )2]. Theorem 2.9. Suppose that Assumption 2.8 holds. Assume that S\SÎ±m is generated by i.i.d. draws according to the true distribution P(X,Y). Assume further that S \SÎ±m is independent of SÎ±m. Let Ë†fA,S be the model learned by the two-phase training procedure with S. Then, for each wÏƒ := wSÎ±m, for any Î´ > 0, with probability at least 1 âˆ’Î´, R[ Ë†fA,S] âˆ’RS\SÎ±m[ Ë†fA,S] â‰¤Î²1 s Ã• k=1 Â¯wS k 1 + 2Î²2 s Ã• k=1 Â¯wS k 2 2 + Î²3, where Î²1 = 2Czz 3mÏƒ ln 3dz Î´ + s 2Î³2zz mÏƒ ln 3dz Î´ , Î²2 = 2Cyz 3mÏƒ ln 6dydz Î´ + s Î³2yz mÏƒ ln 6dydz Î´ , 130 Kawaguchi et al: Generalization in Deep Learning and Î²2 = 2Cy 3mÏƒ ln 3 Î´ + s 2Î³2y mÏƒ ln 3 Î´ . Our proof does not require independence of the coordinates of Ëœzi from the entries of the random matrices Ëœzi ËœzâŠ¤ i (see the proof of Theorem 2.9). The bound in Theorem 2.9 is data dependent because the norms of the weights Â¯wS k depend on each particular S. As with Theorem 2.7, the bound in Theorem 2.9 does not contain a predetermined bound on the norms of weights and can be independent of the choice of hypothesis space, if desired; i.e., Assumption 2.8 can be also satisï¬ed without referencing a hypothesis space of w, because Ëœz = [ Â¯X(i) â—¦Â¯Ïƒ(X(i),wÏƒ)] with Â¯Ïƒj(X(i),wÏƒ) âˆˆ{0,1}. However, unlike Theorem 2.7, Theorem 2.9 implicitly contains the properties of datasets diï¬€erent from a given S, via the predeï¬ned bounds in Assumption 2.8. This is expected since Theorem 2.9 makes claims about the set of random datasets S rather than about each instantiated S. Therefore, while Theorem 2.9 presents a strongly data-dependent bound (over random datasets), Theorem 2.7 is tighter for each given S; indeed, the main equality of Theorem 2.7 is as tight as possible. Theorems 2.7 and 2.9 provide generalization bounds for practical deep learning models that do not necessarily have an explicit dependence on the number of weights or an exponential dependence on depth or the eï¬€ective input dimensionality. Although the size of the vector Â¯wS k can be exponentially large in the depth of the network, the norms of the vector need not be. Because Ëœz[L] k (X,wS) = âˆ¥Â¯X â—¦ Â¯Ïƒ(X,wÏƒ)âˆ¥2âˆ¥Â¯wS kâˆ¥2 cos Î¸, we have that âˆ¥Â¯wS kâˆ¥2 = z[L] k (X,w)/(âˆ¥Â¯X â—¦Â¯Ïƒ(X,wÏƒ)âˆ¥2 cos Î¸) (unless the denominator is zero), where Î¸ is the angle between Â¯X â—¦Â¯Ïƒ(X,wÏƒ) and Â¯wS k. Additionally, as discussed in Â§2.5.2, Ã k âˆ¥Â¯wkâˆ¥2 2 â‰¤ÃL l=1 âˆ¥W(l)âˆ¥2 F. 2.5.4 Probabilistic Bound for 0â€“1 Loss with Multi-Labels For a 0â€“1 loss with multi-labels, the two-phase training procedure in Â§2.5.3 yields the generalization bound in Theorem 2.10. Similarly to the bounds in Theorems 2.7 and 2.9, the generalization bound in Theorem 2.10 does not necessarily have a dependence on the number of weights, or an exponential dependence on depth, or eï¬€ective input dimensionality. Theorem 2.10. Assume that S \ SÎ±m is generated by i.i.d. draws according to a true distribution P(X,Y). Assume also that S \ SÎ±m is independent of SÎ±m. Fix Ï > 0 and wÏƒ. Let F be the set of models with the two-phase training procedure. Suppose that EX[âˆ¥Â¯X â—¦Â¯Ïƒ(X,wÏƒ)âˆ¥2 2] â‰¤C2 Ïƒ and maxk âˆ¥Â¯wkâˆ¥2 â‰¤Cw for all f âˆˆF . Then, for 2.6 Discussions and Open Problems 131 any Î´ > 0, with probability at least 1 âˆ’Î´, the following holds for all f âˆˆF : R[ f ] â‰¤R(Ï) S\SÎ±m[ f ] + 2d2 y(1 âˆ’Î±)âˆ’1/2CÏƒCw ÏâˆšmÏƒ + s ln 1 Î´ 2mÏƒ . Here, the empirical margin loss R(Ï) S [ f ] is deï¬ned as R(Ï) S [ f ] = 1 m m Ã• i=1 Lmargin,Ï( f (X(i)),Y(i)), where Lmargin,Ï is deï¬ned as follows: Lmargin,Ï( f (X),Y) = L(2) margin,Ï(L(1) margin,Ï( f (X),Y)), with L(1) margin,Ï( f (X),Y) = z[L] y (X) âˆ’max y,yâ€² z[L] yâ€² (X) âˆˆR and L(2) margin,Ï(t) = ï£±ï£´ï£´ï£´ï£² ï£´ï£´ï£´ï£³ 0 if Ï â‰¤t, 1 âˆ’t/Ï if 0 â‰¤t â‰¤Ï, 1 if t â‰¤0. 2.6 Discussions and Open Problems It is very diï¬ƒcult to make a detailed characterization of how well a speciï¬c hy- pothesis generated by a certain learning algorithm will generalize in the absence of detailed information about the given problem instance. Traditional learning theory addresses this very diï¬ƒcult question and has developed bounds that are as tight as possible given the generic information available. In this chapter, we have worked toward drawing stronger conclusions by developing theoretical analyses tailored for the situations with more detailed information, including actual neural network structures, and actual performance on a validation set. Optimization and generalization in deep learning are closely related via the following observation: if we make optimization easier by changing the model architecture, the generalization performance can be degraded, and vice versa. Hence, the non-pessimistic generalization theory discussed in this chapter might allow more architectural choices and assumptions in optimization theory. We now note an additional important open problem designed to address the gap between learning theory and practice: for example, theoretically motivated algorithms can degrade actual performances when compared with heuristics. Deï¬ne 132 Kawaguchi et al: Generalization in Deep Learning the partial order of problem instances (P,S, f ) as (P,S, f ) â‰¤(Pâ€²,Sâ€², f â€²) â‡” RP[ f ] âˆ’RS[ f ] â‰¤RPâ€²[ f â€²] âˆ’RSâ€²[ f â€²], where RP[ f ] is the expected risk with probability measure P. Then, any theoretical insights without partial order preservation can be misleading as they can change the preference ranking of (P,S, f ). For example, theoretically motivated algorithms can be worse than heuristics, if the theory does not preserve the partial order of (P,S, f ). This observation suggests the following open problem. Open Problem 3. Tightly characterize the expected risk R[ f ] or the generalization gap R[ f ]âˆ’RS[ f ] of a hypothesis f together with a pair (P,S), producing theoretical insights while partially yet provably preserving the partial order of (P,S, f ). Theorem 2.7 addresses Open Problem 3 by preserving the exact ordering via equality without bounds, while producing the same and more tight practical in- sights (e.g., regularizing the norm) when compared with existing bound-based theory. However, it would be beneï¬cial also to consider a weaker notion of order preservation in order to gain analyzability with more useful insights, as stated in Open Problem 3. Our discussion of Proposition 2.5 and Remark 2.6 suggests another open prob- lem: analyzing the role and inï¬‚uence of human intelligence on generalization. For example, human intelligence often seems to be able to ï¬nd good architectures (and other hyper-parameters) that get low validation errors (without non-exponentially large |Fval| as in Proposition 2.5, or a low complexity of Fval as in Remark 2.6). A close look at the deep learning literature seems to suggest that this question is fundamentally related to the progress of science and engineering, because many successful architectures have been designed based on the physical properties and engineering priors of the problems at hand (e.g., their hierarchical nature, convo- lution, and architecture for motion, such as that considered by Finn et al., 2016, memory networks, and so on). While this further open problem is a hard ques- tion, understanding it would be beneï¬cial for further automating the role of human intelligence towards the goal of artiï¬cial intelligence. Acknowledgements We gratefully acknowledge support from NSF grants 1420316, 1523767, and 1723381, from AFOSR FA9550-17-1-0165, from ONR grant N00014-14-1-0486, and from ARO grant W911 NF1410433, as well as support from the NSERC, CIFAR, and Canada Research Chairs. Any opinions, ï¬ndings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reï¬‚ect the views of our sponsors. Appendix A Additional Discussions 133 Appendix A Additional Discussions This appendix contains additional results and discussions. A1 Simple Regularization Algorithm In general, theoretical bounds from statistical learning theory can be too loose to be directly used in practice. In addition, many theoretical results in statistical learning theory end up simply suggesting the regularization of some notion of smoothness of a hypothesis class. Indeed, by upper-bounding a distance between two functions (e.g., a hypothesis and the ground-truth function corresponding to the expected true labels), one can immediately see without statistical learning theory that regularizing the smoothness of the hypothesis class helps guarantees on generalization. Then, by an Occamâ€™s razor argument, one might prefer a simpler (yet still rigorous) theory and a corresponding simpler algorithm. Accordingly, this subsection examines another simple regularization algorithm that directly regularizes the smoothness of the learned hypothesis. In this subsection we focus on multi-class classiï¬cation problems with dy classes, such as object classiï¬cation with images. Accordingly, we analyze the expected risk with 0â€“1 loss as R[ f ] = EX[1{ f (X) = Y}], where f (X) = argmaxkâˆˆ{1,...,dy }(z[L] k (X)) is the model prediction and Y âˆˆ{1,. . ., dy} is the true label of X. This subsection proposes the following family of simple regularization algo- rithms: given any architecture and method, add a new regularization term for each training mini-batch as loss = original loss + Î» Â¯m max k Â¯m Ã• i=1 Î¾iz[L] k (X(i)) , where X(i) is drawn from some distribution approximating the true distribution of X, Î¾1,. . .,Î¾ Â¯m are independently and uniformly drawn from {âˆ’1,1}, Â¯m is the mini- batch size, and Î» is a hyper-parameter. Importantly, the distribution approximating the true distribution of X is used only for regularization purposes and hence need not be precisely accurate (as long as it plays its role in regularization). For example, the true distribution can be approximated by populations generated by a generative neural network and/or an extra data augmentation process. For simplicity, we refer to this family of methods as directly approximately regularizing complexity (DARC). In this chapter, as a ï¬rst step, we evaluated only a very simple version of the proposed family of methods. That is, our experiments employed the following 134 Kawaguchi et al: Generalization in Deep Learning Table A.2 Test error (%) Method MNIST CIFAR-10 Baseline 0.26 3.52 DARC1 0.20 3.43 Table A.3 Test error ratio (DARC1/Base) MNIST (ND) MNIST CIFAR-10 mean stdv mean stdv mean stdv Ratio 0.89 0.61 0.95 0.67 0.97 0.79 Table A.4 Values of 1 m  maxk Ãm i=1 |z[L] k (X(i))|  Method MNIST (ND) MNIST CIFAR-10 mean stdv mean stdv mean stdv Base 17.2 2.40 8.85 0.60 12.2 0.32 DARC1 1.30 0.07 1.35 0.02 0.96 0.01 simple and easy-to-implement method, called DARC1: loss = original loss + Î» Â¯m max k Â¯m Ã• i=1 |z[L] k (X(i))| ! , (A.1) where X(i) is the ith sample in the training mini-batch. The additional computational cost and programming eï¬€ort due to this new regularization is almost negligible because z[L] k (X(i)) is already used in computing the original loss. This simplest version was derived by approximating the true distribution of X by the empirical distribution of the training data. We evaluated the proposed method (DARC1) by simply adding the new regu- larization term in (A.1) to the existing standard codes for MNIST and CIFAR-10. Standard variants of LeNet (LeCun et al., 1998) and ResNeXt-29(16 Ã— 64d) (Xie et al., 2017) were used for MNIST and CIFAR-10, and compared with the addition of the studied regularizer. For all the experiments, we ï¬xed (Î»/ Â¯m) = 0.001 with Â¯m = 64. We used a single model without ensemble methods. The experimental details are given in Appendix B. The source code is available at http://lis.csail.mit.edu/code/gdl.html Table A.2 shows the error rates in comparison with previous results. To the best of our knowledge, the previous state-of-the-art classiï¬cation error is 0.23% for MNIST with a single model (Sato et al., 2015) (and 0.21% with an ensemble, Appendix A Additional Discussions 135 by Wan et al., 2013). To further investigate the improvement, we ran 10 random trials with computationally less expensive settings, to gather the mean and standard deviation (stdv). For MNIST, we used fewer epochs with the same model. For CIFAR-10, we used a smaller model class (pre-activation ResNet with only 18 layers). Table A.3 summarizes the improvement ratio, i.e., the new modelâ€™s error divided by the base modelâ€™s error. We observed improvements in all cases. The test errors (standard deviations) of the base models were 0.53 (0.029) for MNIST (ND), 0.28 (0.024) for MNIST, and 7.11 (0.17) for CIFAR-10 (all in %). Table A.4 contains the values of the regularization term 1 m(maxk Ãm i=1 |z[L] k (X(i))|) for each obtained model. The models learned with the proposed method were signiï¬cantly diï¬€erent from the base models in terms of this value. Interestingly, a comparison of the base cases for MNIST (ND) and MNIST shows that data augmentation by itself implicitly regularized what we explicitly regularized in the proposed method. A2 Relationship to Other Fields The situation where theoretical studies focus on a set of problems and practical applications are concerned with each element in a set is prevalent in the machine learning and computer science literature, and is not limited to the ï¬eld of learning theory. For example, for each practical problem instance q âˆˆQ, the size of the set Q that had been analyzed in theory for optimal exploration in Markov decision processes (MDPs) was demonstrated to be frequently too pessimistic, and a method- ology to partially mitigate the issue was proposed (Kawaguchi, 2016a). Bayesian optimization would suï¬€er from a pessimistic set Q regarding each problem instance q âˆˆQ, the issue of which had been partially mitigated (Kawaguchi et al., 2015). Moreover, characterizing a set of problems Q only via a worst-case instance qâ€² âˆˆ Q (i.e., worst-case analysis) is known to have several issues in theoretical computer science, and so-called beyond worst-case analysis (e.g., smoothed analysis) is an active area of research to mitigate these issues. A3 SGD Chooses Direction in Terms of Â¯w Recall that z[L] k (X,w) = zâŠ¤Â¯w = [ Â¯X â—¦Â¯Ïƒ(X,w)]âŠ¤Â¯w. Note that Ïƒ(X,w) is 0 or 1 for max pooling and/or ReLU nonlinearity. Thus, the derivative of z = [ Â¯X â—¦Â¯Ïƒ(X,w)] with respect to w is zero everywhere (except at the measure-zero set, where the derivative does not exists). Thus, by the chain rule (and power rule), the gradient of the loss with respect to w contains only a contribution 136 Kawaguchi et al: Generalization in Deep Learning from the derivative of z[L] k with respect to Â¯w, but not from that with respect to w in z. A4 Simple Implementation of Two-Phase Training Procedure Directly implementing Equation (2.3) requires a summation over all paths, which can be computationally expensive. To avoid thi, we implemented it by creating two deep neural networks, one of which deï¬nes Â¯w paths hierarchically, the other deï¬ning wÏƒ paths hierarchically, resulting in a computational cost at most (approximately) twice as much as the original cost of training standard deep learning models. We tied wÏƒ and Â¯w in the two networks during the standard phase, and untied them during the freeze phase. Our source code is available at http://lis.csail.mit.edu/code/gdl.html The computation of the standard network without skip connection can be re- written as z[l](X,w) = Ïƒ[l](W[l]z[lâˆ’1](X,w)) = Ã›Ïƒ[l](W[l]z[lâˆ’1](X,w)) â—¦W[l]z[lâˆ’1](X,w) = Ã›Ïƒ[l](W[l] Ïƒ z[lâˆ’1] Ïƒ (X,w)) â—¦W[l]z[lâˆ’1](X,w), where W[l] Ïƒ := W[l], z[lâˆ’1] Ïƒ := Ïƒ(W[l] Ïƒ z[lâˆ’1] Ïƒ (X,w)) and Ã›Ïƒ[l] j (W[l]z[lâˆ’1](X,w)) =  1 if the jth unit at the lth layer is active, 0 otherwise. Note that because W[l] Ïƒ = W[l], we have that z[lâˆ’1] Ïƒ = z[l] in the standard phase and standard models. In the two-phase training procedure, we created two networks for W[l] Ïƒ z[lâˆ’1] Ïƒ (X,w) and W[l]z[lâˆ’1](X,w) separately. We then set W[l] Ïƒ = W[l] during the standard phase, and froze W[l] Ïƒ and just trained W[l] during the freeze phase. By following the same derivation of Equation (2.1), we can see that this deï¬nes the desired computation without explicitly computing the summation over all paths. By the same token, this applies to DAGs. A5 On Proposition 2.3 Proposition 2.3 is a direct consequence of Remark 2.4. Consider statement (i). Given such an F , consider any tuples (A, F,S) such that A takes F and S as input Appendix B Experimental Details 137 and outputs f âˆ— Ïµ . Clearly, there are many such tuples (A, F,S) because of Remark 2.4. This establishes statement (i). Consider statement (ii). Given any dataset Sâ€², consider any algorithm Aâ€² that happens to output f âˆ— Ïµ if S = Sâ€² and outputs any f â€² otherwise, such that f â€² is arbitrar- ily non-robust and |L( f âˆ— Ïµ (X),Y) âˆ’L( f â€²(X),Y)| is arbitrarily large (i.e., arbitrarily non-stable). This proves statement (ii). Note that although this particular Aâ€² suï¬ƒces to prove statement (ii), there are clearly many other tuples (A, F,P(X,Y),S) that could be used to prove statement (ii) because of Remark 2.4. A6 On Extensions Theorem 2.7 addresses Open Problem 2 with limited applicability, i.e., to certain neural networks with squared loss. In contrast, a parallel study (Kawaguchi et al., 2018) presented a novel generic learning theory to address Open Problem 2 for general cases in machine learning. It would be beneï¬cial to explore both a generic analysis (Kawaguchi et al., 2018) and a concrete analysis in deep learning to get theoretical insights that are tailored for each particular case. For previous bounds with a hypothesis space F , if we try diï¬€erent such spaces then F , depending on S, the basic proof breaks down. An easy recovery at the cost of an extra quantity in a bound is achieved by taking a union bound over all possible Fj for j = 1,2,. . ., where we pre-decide (Fj)j without dependence on S (because simply considering the â€œlargestâ€ F âŠ‡Fj can result in a very loose bound for each Fj). Then, after training with a dataset S, if we have Ë†fA,S âˆˆFj, we can use the complexity of Fj without using the complexity of other Fi with i , j. Because the choice of Fj (out of (Fj)j) depends on S, it is called a data-dependent bound and indeed this is the idea behind data-dependent bounds in statistical learning theory. Similarly, if we need to try many wÏƒ := wSÎ±m depending on the whole of S in Theorem 2.9, we can take a union bound over w(j) Ïƒ for j = 1,2,. . ., where we pre-determine {w(j) Ïƒ }j without dependence on S\SÎ±m but with dependence on SÎ±m. We can do the same with Proposition 2.5 and Remark 2.6 to use many diï¬€erent Fval depending on the validation dataset S(val) with a predeï¬ned sequence. Appendix B Experimental Details For MNIST: We used the following ï¬xed architecture: (i) Convolutional layer with 32 ï¬lters with ï¬lter size 5 by 5, followed by max pooling of size 2 by 2 and ReLU. (ii) Convolution layer with 32 ï¬lters with ï¬lter size 5 by 5, followed by max pooling of size 2 by 2 and ReLU. 138 Kawaguchi et al: Generalization in Deep Learning (iii) Fully connected layer with output 1024 units, followed by ReLU and Dropout with its probability set to 0.5. (iv) Fully connected layer with output 10 units. Layer (iv) outputs z[L] in our notation. For training purpose, we used the softmax of z[L]. Also, f (X) = argmax(z[L](X)) was taken as the label prediction. We ï¬xed the learning rate to be 0.01, the momentum coeï¬ƒcient to be 0.5, and the optimization algorithm to be the standard stochastic gradient descent (SGD). We ï¬xed the data augmentation process as: a random crop with size 24, a random rotation up to Â±15 degrees, and scaling of 15%. We used 3000 epochs for Table A.2, and 1000 epochs for Tables A.3 and A.4. For CIFAR-10: For data augmentation, we used a random horizontal ï¬‚ip with probability 0.5 and a random crop of size 32 with padding of size 4. For Table A.2, we used ResNeXt-29(16Ã—64d) (Xie et al., 2017). We set the initial learning rate to be 0.05, decreasing it to 0.005 at 150 epochs and to 0.0005 at 250 epochs. We ï¬xed the momentum coeï¬ƒcient to be 0.9, the weight decay coeï¬ƒcient to be 5 Ã— 10âˆ’4, and the optimization algorithm to be stochastic gradient descent (SGD) with Nesterov momentum. We stopped the training at 300 epochs. For Tables A.3 and A.4, we used pre-activation ResNet with only 18 layers (pre-activation ResNet-18) (He et al., 2016). We ï¬xed learning rate to be 0.001 and momentum coeï¬ƒcient to be 0.9, and optimization algorithm to be (standard) stochastic gradient descent (SGD). We used 1000 epochs. Appendix C Proofs We will use the following lemma in the proof of Theorem 2.7. Lemma 2.11. (Matrix Bernstein inequality: corollary to Theorem 1.4 in Tropp, 2012) Consider a ï¬nite sequence {Mi} of independent, random, self-adjoint ma- trices with dimension d. Assume that each random matrix satisï¬es that E[Mi] = 0 and Î»max(Mi) â‰¤R almost surely. Let Î³2 = âˆ¥Ã i E[M2 i ]âˆ¥2. Then, for any Î´ > 0, with probability at least 1 âˆ’Î´, Î»max Ã• i Mi ! â‰¤2R 3 ln d Î´ + r 2Î³2 ln d Î´ . Proof Theorem 1.4 of Tropp (2012) states that, for all t â‰¥0, P " Î»max Ã• i Mi ! â‰¥t # â‰¤d Â· exp  âˆ’t2/2 Î³2 + Rt/3  . Appendix C Proofs 139 Setting Î´ = d exp  âˆ’ t2/2 Î³2+Rt/3  implies âˆ’t2 + 2 3 R(ln d/Î´)t + 2Î³2 ln d/Î´ = 0. Solving for t with the quadratic formula and bounding the solution using the sub- additivity of square roots of non-negative terms (i.e., âˆš a + b â‰¤âˆša + âˆš b for all a, b â‰¥0), gives t â‰¤2 3 R(ln d/Î´) + 2Î³2 ln d/Î´. â–¡ C1 Proof of Theorem 2.1 Proof For any matrix M, let Col(M) and Null(M) be the column space and null space of M. Since rank(Î¦) â‰¥m and Î¦ âˆˆRmÃ—n, the set of its columns spans Rm, which proves statement (i). Let wâˆ—= wâˆ— 1 + wâˆ— 2 where Col(wâˆ— 1) âŠ†Col(MT) and Col(wâˆ— 2) âŠ†Null(M). For statement (ii), set the parameter as w := wâˆ— 1 + ÏµC1 + Î±C2 where Col(C1) âŠ†Col(MT), Col(C2) âŠ†Null(M), Î± â‰¥0, and C2 = 1 Î±wâˆ— 2 + Â¯C2. Since rank(M) < n, Null(M) , {0} and there exist non-zero Â¯C2. Then Ë†Y(w) = Y + ÏµÎ¦C1 and Ë†Ytest(w) = Ytest + ÏµÎ¦testC1. Setting A = Î¦C1 and B = Î¦testC1 with a proper normalization of C1 yields (ii)(a) and (ii)(b) in statement (ii) (note that C1 has an arbitrary freedom in the bound on its scale because the only condition on it is Col(C1) âŠ†Col(MT)). At the same time, with the same parameter, since Col(wâˆ— 1 + ÏµC1) âŠ¥Col(C2) we have âˆ¥wâˆ¥2 F = âˆ¥wâˆ— 1 + ÏµC1âˆ¥2 F + Î±2âˆ¥C2âˆ¥2 F and âˆ¥w âˆ’wâˆ—âˆ¥2 F = âˆ¥ÏµC1âˆ¥2 F + Î±2âˆ¥Â¯C2âˆ¥2 F, which grows unboundedly as Î± â†’âˆwithout changing A and B, proving (ii)(c) in statement (ii). â–¡ C2 Proof of Corollary 2.2 Proof This follows the fact that the proof in Theorem 2.1 uses the assumption of n > m and rank(Î¦) â‰¥m only for statement (i). â–¡ 140 Kawaguchi et al: Generalization in Deep Learning C3 Proof of Theorem 2.7 Proof From Equation (2.1), the squared loss of the deep models for each point (X,Y) can be rewritten as s Ã• k=1 (zâŠ¤Â¯wk âˆ’Yk)2 = s Ã• k=1 Â¯wâŠ¤ k (zzâŠ¤) Â¯wk âˆ’2YkzâŠ¤Â¯wk + Y2 k . Thus, from Equation (2.1) for a squared loss, we can decompose the generalization gap into three terms as R[wS] âˆ’RS[wS] = s Ã• k=1 " ( Â¯w S k)âŠ¤ E[zzâŠ¤] âˆ’1 m m Ã• i=1 zizâŠ¤ i ! Â¯w S k # + 2 s Ã• k=1 " 1 m m Ã• i=1 Y(i) k zâŠ¤ i âˆ’E[YkzâŠ¤] ! Â¯w S k # + E[YâŠ¤Y] âˆ’1 m m Ã• i=1 (Y(i))âŠ¤Y(i) ! . As G, deï¬ned before Theorem 2.7, is a real symmetric matrix, we can write an eigendecomposition of G as G = UÎ›UâŠ¤where the diagonal matrix Î› contains eigenvalues Î›j j = Î»j with corresponding orthogonal eigenvector matrix U; uj is the jth column of U. Then ( Â¯w S k)âŠ¤G Â¯w S k = Ã• j Î»j(uâŠ¤ j Â¯w S k)2 = âˆ¥Â¯w S kâˆ¥2 2 Ã• j Î»j cos2 Î¸(1) Â¯wS k ,j and Ã• j Î»j(uâŠ¤ j Â¯w S k)2 â‰¤Î»max(G)âˆ¥UâŠ¤Â¯w S kâˆ¥2 2 = Î»max(G)âˆ¥Â¯w S kâˆ¥2 2. Also, vâŠ¤Â¯w S k = âˆ¥vâˆ¥2âˆ¥Â¯w S kâˆ¥2 cos Î¸(2) Â¯wS k â‰¤âˆ¥vâˆ¥2âˆ¥Â¯w S kâˆ¥2. Using these expressions we obtain R[wS] âˆ’RS[wS] âˆ’cy = s Ã• k=1 2âˆ¥vâˆ¥2âˆ¥Â¯w S kâˆ¥2 cos Î¸(2) Â¯wS k + âˆ¥Â¯w S kâˆ¥2 2 Ã• j Î»j cos2 Î¸(1) Â¯wS k ,j ! â‰¤ s Ã• k=1  2âˆ¥vâˆ¥2âˆ¥Â¯w S kâˆ¥2 + Î»max(G)âˆ¥Â¯w S kâˆ¥2 2  â–¡ as required. Appendix C Proofs 141 C4 Proof of Theorem 2.9 Proof We do not require independence of the coordinates of Ëœzi and the entries of the random matrices Ëœzi ËœzâŠ¤ i because of the deï¬nition of independence required for the matrix Bernstein inequality (for 1 mÏƒ ÃmÏƒ i=1 Ëœzi ËœzâŠ¤ i ); see, e.g., Section 2.2.3 of Tropp et al., 2015) and because of the union bound over the coordinates (for 1 mÏƒ ÃmÏƒ i=1 Y(i) k Ëœzi). We use the fact that ËœzÎ±m+1,. . ., Ëœzm are independent random vari- ables over the sample index (although dependent on the coordinates), because each wÏƒ := wSÎ±m is ï¬xed and independent of X(Î±m+1),. . ., X(m). From Equation (2.2), with the deï¬nition of induced matrix norm and the Cauchyâ€“ Schwarz inequality, R[ Ë†fA,S] âˆ’RS\SÎ±m[ Ë†fA,S] â‰¤ s Ã• k=1 Â¯wS k 2 2 Î»max E[ËœzËœzâŠ¤] âˆ’1 mÏƒ m Ã• i=Î±m+1 Ëœzi ËœzâŠ¤ i ! + 2 s Ã• k=1 âˆ¥Â¯wS kâˆ¥1 1 mÏƒ m Ã• i=Î±m+1 Y(i) k Ëœzi âˆ’E[Yk Ëœz] âˆ + E[YâŠ¤Y] âˆ’1 mÏƒ m Ã• i=Î±m+1 (Y(i))âŠ¤Y(i) ! . (C.1) In what follows, we bound each term on the right-hand side with concentration inequalities. For the ï¬rst term: The matrix Bernstein inequality (Lemma 2.11) states that for any Î´ > 0, with probability at least 1 âˆ’Î´/3, Î»max E[ËœzËœzâŠ¤] âˆ’1 mÏƒ m Ã• i=Î±m+1 Ëœzi ËœzâŠ¤ i ! â‰¤2Czz 3mÏƒ ln 3dz Î´ + s 2Î³2zz mÏƒ ln 3dz Î´ . Here, the matrix Bernstein inequality was applied as follows. Let Mi = ( 1 mÏƒ G(i)). Then Ãm i=Î±m+1 Mi = E[ËœzËœzâŠ¤] âˆ’ 1 mÏƒ Ãm i=Î±m+1 Ëœzi ËœzâŠ¤ i . We have that E[Mi] = 0 for all i. Also, Î»max(Mi) â‰¤ 1 mÏƒ Czz and âˆ¥Ã i E[M2 i ]âˆ¥2 â‰¤ 1 mÏƒ Î³2 zz. For the second term: To each (k, kâ€²) âˆˆ{1,. . ., s} Ã— {1,. . ., dz} we apply the matrix Bernstein inequality and take the union bound over dydz events, obtaining that for any Î´ > 0, with probability at least 1 âˆ’Î´/3, for all k âˆˆ{1,2,. . ., s}, 1 mÏƒ m Ã• i=Î±m+1 Y(i) k Ëœzi âˆ’E[Yk Ëœz] âˆ â‰¤2Cyz 3mÏƒ ln 6dydz Î´ + s Î³2yz mÏƒ ln 6dydz Î´ . For the third term: From the matrix Bernstein inequality, with probability at least 142 Kawaguchi et al: Generalization in Deep Learning 1 âˆ’Î´/3, E[YâŠ¤Y] âˆ’1 mÏƒ m Ã• i=Î±m+1 (Y(i))âŠ¤Y(i) â‰¤2Cy 3m ln 3 Î´ + s 2Î³2y m ln 3 Î´ . Putting it all together: Pulling all this together, for a ï¬xed (or frozen) wÏƒ, with probability (over S \ SÎ±m = {(X(Î±m+1),Y(Î±m+1)),. . .,(X(m),Y(m))}) at least 1 âˆ’Î´, we have that Î»max E[ËœzËœzâŠ¤] âˆ’1 mÏƒ m Ã• i=Î±m+1 Ëœzi ËœzâŠ¤ i ! â‰¤Î²1, 1 mÏƒ m Ã• i=Î±m+1 Y(i) k Ëœzi âˆ’E[Yk Ëœz] âˆ â‰¤Î²2 (for all k), and E[YâŠ¤Y] âˆ’1 mÏƒ m Ã• i=Î±m+1 (Y(i))âŠ¤Y(i) â‰¤Î²3. Since Equation (C.1) always hold deterministically (with or without such a dataset), the desired statement of this theorem follows. â–¡ C5 Proof of Theorem 2.10 Proof Deï¬ne SmÏƒ as SmÏƒ = S \ SÎ±m = {(X(Î±m+1),Y(Î±m+1)),. . .,(X(m),Y(m))}. Recall the following fact: using the result of Koltchinskii and Panchenko (2002), we have that for any Î´ > 0, with probability at least 1 âˆ’Î´, the following holds for all f âˆˆF : R[ f ] â‰¤RSmÏƒ ,Ï[ f ] + 2d2 y ÏmÏƒ Râ€² mÏƒ(F ) + s ln 1 Î´ 2mÏƒ , where Râ€² mÏƒ(F ) is the Rademacher complexity, deï¬ned as Râ€² mÏƒ(F ) = ESmÏƒ ,Î¾ " sup k,w mÏƒ Ã• i=1 Î¾iz[L] k (X(i),w) # . Here, Î¾i is the Rademacher variable, and the supremum is taken over all k âˆˆ {1,. . ., s} and all w allowed in F . Then, for our parameterized hypothesis spaces, Appendix C Proofs 143 with any frozen wÏƒ, Râ€² mÏƒ(F ) = ESmÏƒ ,Î¾ " sup k, Â¯wk mÏƒ Ã• i=1 Î¾i[ Â¯X(i) â—¦Â¯Ïƒ(X(i),wÏƒ)]âŠ¤Â¯wk # â‰¤ESmÏƒ ,Î¾ " sup k, Â¯wk mÏƒ Ã• i=1 Î¾i[ Â¯X(i) â—¦Â¯Ïƒ(X(i),wÏƒ)] 2 âˆ¥Â¯wkâˆ¥2 # â‰¤CwESmÏƒ ,Î¾ " mÏƒ Ã• i=1 Î¾i[ Â¯X(i) â—¦Â¯Ïƒ(X(i),wÏƒ)] 2 # . Because the square root is concave in its domain, by using Jensenâ€™s inequality and linearity of expectation, we obtain ESmÏƒ ,Î¾ " mÏƒ Ã• i=1 Î¾i[ Â¯X(i) â—¦Â¯Ïƒ(X(i),wÏƒ)] 2 # â‰¤Â©Â­ Â« ESmÏƒ mÏƒ Ã• i=1 mÏƒ Ã• j=1 EÎ¾[Î¾iÎ¾j][ Â¯X(i) â—¦Â¯Ïƒ(X(i),wÏƒ)]âŠ¤[ Â¯Xj â—¦Â¯Ïƒ(Xj,wÏƒ)]ÂªÂ® Â¬ 1/2 = mÏƒ Ã• i=1 ESmÏƒ  [ Â¯X(i) â—¦Â¯Ïƒ(X(i),wÏƒ)] 2 2 !1/2 â‰¤CÏƒ âˆšmÏƒ. Putting it all together, we ï¬nd Râ€² m(F ) â‰¤CÏƒCwâˆšmÏƒ. â–¡ C6 Proof of Proposition 2.5 Proof Consider a ï¬xed f âˆˆFval. Because Fval is independent of the validation dataset S(val), it follows that Îºf ,1,. . ., Îºf ,mval are independent zero-mean random variables, given, as above, a ï¬xed f âˆˆFval (note that we can make Fval dependent on S because we are considering S(val) here, resulting in the requirement that Fval is independent of S(val), instead of S). Thus, we can apply the matrix Bernstein inequality, yielding P 1 mval mval Ã• i=1 Îºf ,i > Ïµ ! â‰¤exp  âˆ’Ïµ2mval/2 Î³2 + ÏµC/3  . By taking the union bound over all elements in Fval, we ï¬nd P Â©Â­ Â« Ã˜ f âˆˆFval ( 1 mval mval Ã• i=1 Îºf ,i > Ïµ ) ÂªÂ® Â¬ â‰¤|Fval| exp  âˆ’Ïµ2mval/2 Î³2 + ÏµC/3  . 144 Kawaguchi et al: Generalization in Deep Learning Setting Î´ = |Fval| exp  âˆ’Ïµ2mval/2 Î³2+ÏµC/3  and solving for Ïµ gives (via the quadratic formula), Ïµ = 2C ln( |Fval | Î´ ) 6mval Â± 1 2 v u t 2C ln( |Fval | Î´ ) 3mval !2 + 8Î³2 ln( |Fval | Î´ ) mval . Noticing that the solution of Ïµ with the minus sign results in Ïµ < 0, which is invalid for the matrix Bernstein inequality, we obtain the valid solution, the one with with the plus sign. Then we have Ïµ â‰¤ 2C ln( |Fval | Î´ ) 3mval + s 2Î³2 ln( |Fval | Î´ ) mval , where we have used âˆš a + b â‰¤âˆša + âˆš b. By taking the negation of the statement, we obtain that, for any Î´ > 0, with probability at least 1 âˆ’Î´, for all f âˆˆFval, 1 mval mval Ã• i=1 Îºf ,i â‰¤ 2C ln( |Fval | Î´ ) 3mval + s 2Î³2 ln( |Fval | Î´ ) m , where 1 mval mval Ã• i=1 Îºf ,i = R[ f ] âˆ’RS(val)[ f ]. â–¡ References Arpit, Devansh, Jastrzebski, Stanislaw, Ballas, Nicolas, Krueger, David, Bengio, Emmanuel, Kanwal, Maxinder S., Maharaj, Tegan, Fischer, Asja, Courville, Aaron, Bengio, Yoshua, et al. 2017. A Closer Look at Memorization in Deep Networks. In: Proc. International Conference on Machine Learning. Banburski, Andrzej, Liao, Qianli, Miranda, Brando, Rosasco, Lorenzo, Liang, Bob, Hidary, Jack, and Poggio, Tomaso. 2019. Theory III: Dynamics and Gener- alization in Deep Networks. Massachusetts Institute of Technology CBMM Memo No. 90. Barron, Andrew R. 1993. Universal Approximation Bounds for Superpositions of a Sigmoidal Function. IEEE Transactions on Information theory, 39(3), 930â€“945. Bartlett, Peter L., Boucheron, StÃ©phane, and Lugosi, GÃ¡bor. 2002. Model Selection and Error Estimation. Machine Learning, 48(1), 85â€“113. Bousquet, Olivier, and Elisseeï¬€, AndrÃ©. 2002. Stability and Generalization. Journal of Machine Learning Research, 2(Mar), 499â€“526. References 145 Choromanska, Anna, Henaï¬€, Mikael, Mathieu, Michael, Ben Arous, Gerard, and LeCun, Yann. 2015. The Loss Surfaces of Multilayer Networks. Pages 192â€“ 204 of: Proc. 18th International Conference on Artiï¬cial Intelligence and Statistics. Dinh, Laurent, Pascanu, Razvan, Bengio, Samy, and Bengio, Yoshua. 2017. Sharp Minima Can Generalize for Deep Nets. In: International Conference on Ma- chine Learning. Dziugaite, Gintare Karolina, and Roy, Daniel M. 2017. Computing Nonvacuous Generalization Bounds for Deep (Stochastic) Neural Networks with Many More Parameters than Training Data. In: Proc. 33rd Conference on Uncertainty in Artiï¬cial Intelligence. Finn, Chelsea, Goodfellow, Ian, and Levine, Sergey. 2016. Unsupervised Learning for Physical Interaction through Video Prediction. Pages 64â€“72 of: Advances in Neural Information Processing Systems. Gonen, Alon, and Shalev-Shwartz, Shai. 2017. Fast Rates for Empirical Risk Min- imization of Strict Saddle Problems. Pages 1043â€“1063 of: Proc. Conference on Learning Theory. Hardt, Moritz, Recht, Ben, and Singer, Yoram. 2016. Train Faster, Generalize Better: Stability of Stochastic Gradient Descent. Pages 1225â€“1234 of: proc. International Conference on Machine Learning. He, Kaiming, Zhang, Xiangyu, Ren, Shaoqing, and Sun, Jian. 2016. Identity Mappings in Deep Residual Networks. Pages 630â€“645 of: Proc. European Conference on Computer Vision. Springer. Healy, Patrick, and Nikolov, Nikola S. 2001. How to Layer a Directed Acyclic Graph. Pages 16â€“30 of: proc. International Symposium on Graph Drawing. Springer. Herbrich, Ralf, and Williamson, Robert C. 2002. Algorithmic Luckiness. Journal of Machine Learning Research, 3, 175â€“212. Hoï¬€er, Elad, Hubara, Itay, and Soudry, Daniel. 2017. Train Longer, Generalize Better: Closing the Generalization Gap in Large Batch Training of Neural Networks. Pages 1731â€“1741 of: Advances in Neural Information Processing Systems. Kawaguchi, Kenji. 2016a. Bounded Optimal Exploration in MDP. In: Proc. 30th AAAI Conference on Artiï¬cial Intelligence. Kawaguchi, Kenji. 2016b. Deep Learning without Poor Local Minima. In: Advances in Neural Information Processing Systems. Kawaguchi, Kenji, Bengio, Yoshua, Verma, Vikas, and Kaelbling, Leslie Pack. 2018. Generalization in Machine Learning via Analytical Learning Theory. Massachusetts Institute of Technology, Report MIT-CSAIL-TR-2018-019. Kawaguchi, Kenji, Kaelbling, Leslie Pack, and Lozano-PÃ©rez, TomÃ¡s. 2015. Bayesian Optimization with Exponential Convergence. In: Advances in Neural Information Processing. 146 Kawaguchi et al: Generalization in Deep Learning Keskar, Nitish Shirish, Mudigere, Dheevatsa, Nocedal, Jorge, Smelyanskiy, Mikhail, and Tang, Ping Tak Peter. 2017. On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima. In: Proc. International Conference on Learning Representations. Koltchinskii, Vladimir, and Panchenko, Dmitriy. 2000. Rademacher Processes and Bounding the Risk of Function Learning. Pages 443â€“457 of: High Dimensional Probability II. Springer. Koltchinskii, Vladimir, and Panchenko, Dmitry. 2002. Empirical Margin Distribu- tions and Bounding the Generalization Error of Combined Classiï¬ers. Annals of Statistics, 1â€“50. Krueger, David, Ballas, Nicolas, Jastrzebski, Stanislaw, Arpit, Devansh, Kan- wal, Maxinder S., Maharaj, Tegan, Bengio, Emmanuel, Fischer, Asja, and Courville, Aaron. 2017. Deep Nets Donâ€™t Learn via Memorization. In: proc. Workshop Track of International Conference on Learning Representations. Kuzborskij, Ilja, and Lampert, Christoph. 2017. Data-Dependent Stability of Stochastic Gradient Descent. ArXiv preprint arXiv:1703.01678. LeCun, Yann, Bottou, LÃ©on, Bengio, Yoshua, and Haï¬€ner, Patrick. 1998. Gradient- Based Learning Applied to Document Recognition. Proc. IEEE, 86(11), 2278â€“2324. Leshno, Moshe, Lin, Vladimir Ya., Pinkus, Allan, and Schocken, Shimon. 1993. Multilayer Feedforward Networks with a Nonpolynomial Activation Function can Approximate Any Function. Neural Networks, 6(6), 861â€“867. Livni, Roi, Shalev-Shwartz, Shai, and Shamir, Ohad. 2014. On the Computational Eï¬ƒciency of Training Neural Networks. Pages 855â€“863 of: Advances in Neural Information Processing Systems. Mohri, Mehryar, Rostamizadeh, Afshin, and Talwalkar, Ameet. 2012. Foundations of Machine Learning. MIT Press. Montufar, Guido F., Pascanu, Razvan, Cho, Kyunghyun, and Bengio, Yoshua. 2014. On the Number of Linear Regions of Deep Neural Networks. Pages 2924â€“2932 of: Advances in Neural Information Processing Systems. Mukherjee, Sayan, Niyogi, Partha, Poggio, Tomaso, and Rifkin, Ryan. 2006. Learn- ing Theory: Stability is Suï¬ƒcient for Generalization and Necessary and Suf- ï¬cient for Consistency of Empirical Risk Minimization. Advances in Compu- tational Mathematics, 25(1), 161â€“193. Neyshabur, Behnam, Tomioka, Ryota, and Srebro, Nathan. 2015a. Norm-Based Capacity Control in Neural Networks. Pages 1376â€“1401 of: Proc. 28th Con- ference on Learning Theory. Neyshabur, Behnam, Salakhutdinov, Ruslan R., and Srebro, Nati. 2015b. Path-SGD: Path-Normalized Optimization in Deep Neural Networks. Pages 2422â€“2430 of: Advances in Neural Information Processing Systems. Pascanu, Razvan, Montufar, Guido, and Bengio, Yoshua. 2014. On the Number of Response Regions of Deep Feed Forward Networks with Piece-Wise Linear Activations. In: Proc. International Conference on Learning Representations. References 147 Poggio, Tomaso, Kawaguchi, Kenji, Liao, Qianli, Miranda, Brando, Rosasco, Lorenzo, Boix, Xavier, Hidary, Jack, and Mhaskar, Hrushikesh. 2018. Theory of Deep Learning III: Explaining the Non-overï¬tting Puzzle. Massachusetts Institute of Technology CBMM Memo No. 73. Poggio, Tomaso, Mhaskar, Hrushikesh, Rosasco, Lorenzo, Miranda, Brando, and Liao, Qianli. 2017. Why and When Can Deep â€“ But not Shallow â€“ Net- works Avoid the Curse of Dimensionality: A Review. International Journal of Automation and Computing, 14, 1â€“17. Raginsky, Maxim, Rakhlin, Alexander, and Telgarsky, Matus. 2017. Non-Convex Learning via Stochastic Gradient Langevin Dynamics: A Nonasymptotic Anal- ysis. Pages 1674â€“1703 of: Proc. Conference on Learning Theory. Sato, Ikuro, Nishimura, Hiroki, and Yokoi, Kensuke. 2015. Apac: Augmented Pat- tern Classiï¬cation with Neural Networks. ArXiv preprint arXiv:1505.03229. Shalev-Shwartz, Shai, and Ben-David, Shai. 2014. Understanding Machine Learn- ing: From Theory to Algorithms. Cambridge University Press. Shawe-Taylor, John, Bartlett, Peter L., Williamson, Robert C., and Anthony, Martin. 1998. Structural Risk Minimization over Data-Dependent Hierarchies. IEEE Transactions on Information Theory, 44(5), 1926â€“1940. Sokolic, Jure, Giryes, Raja, Sapiro, Guillermo, and Rodrigues, Miguel. 2017a. Generalization Error of Invariant Classiï¬ers. Pages 1094â€“1103 of: Artiï¬cial Intelligence and Statistics. Sokolic, Jure, Giryes, Raja, Sapiro, Guillermo, and Rodrigues, Miguel R. D. 2017b. Robust Large Margin Deep Neural Networks. IEEE Transactions on Signal Processing, 65(16), 4265â€“4280. Sun, Shizhao, Chen, Wei, Wang, Liwei, Liu, Xiaoguang, and Liu, Tie-Yan. 2016. On the Depth of Deep Neural Networks: A Theoretical View. Pages 2066â€“2072 of: Proc. 30th AAAI Conference on Artiï¬cial Intelligence. AAAI Press. Telgarsky, Matus. 2016. Beneï¬ts of Depth in Neural Networks. Pages 1517â€“1539 of: Proc. 29th Annual Conference on Learning Theory. Tropp, Joel A. 2012. User-Friendly Tail Bounds for Sums of Random Matrices. Foundations of Computational Mathematics, 12(4), 389â€“434. Tropp, Joel A., et al. 2015. An Introduction to Matrix Concentration Inequalities. Foundations and TrendsÂ® in Machine Learning, 8(1-2), 1â€“230. Vapnik, Vladimir. 1998. Statistical Learning Theory. Vol. 1. Wiley. Wan, Li, Zeiler, Matthew, Zhang, Sixin, Cun, Yann L., and Fergus, Rob. 2013. Regularization of Neural Networks using Dropconnect. Pages 1058â€“1066 of: Proc. 30th International Conference on Machine Learning. Wu, Lei, Zhu, Zhanxing, et al. 2017. Towards Understanding Generalization of Deep Learning: Perspective of Loss Landscapes. ArXiv preprint arXiv:1706.10239. Xie, Pengtao, Deng, Yuntian, and Xing, Eric. 2015. On the Generalization Er- ror Bounds of Neural Networks under Diversity-Inducing Mutual Angular Regularization. ArXiv preprint arXiv:1511.07110. 148 Kawaguchi et al: Generalization in Deep Learning Xie, Saining, Girshick, Ross, DollÃ¡r, Piotr, Tu, Zhuowen, and He, Kaiming. 2017. Aggregated Residual Transformations for Deep Neural Networks. Pages 1492â€“ 1500 of: Proc. IEEE Conference on Computer Vision and Pattern Recognition. Xu, Huan, and Mannor, Shie. 2012. Robustness and Generalization. Machine Learning, 86(3), 391â€“423. Zhang, Chiyuan, Bengio, Samy, Hardt, Moritz, Recht, Benjamin, and Vinyals, Oriol. 2017. Understanding Deep Learning Requires Rethinking Generalization. In: Proc. International Conference on Learning Representations. 3 Expressivity of Deep Neural Networks Ingo GÃ¼hring, Mones Raslan, and Gitta Kutyniok Abstract: In this chapter, we give a comprehensive overview of the large variety of approximation results for neural networks. The approximation rates for classical function spaces as well as the beneï¬ts of deep neural networks over shallow ones for speciï¬cally structured function classes are discussed. While the main body of existing results applies to general feedforward architectures, we also review approximation results for convolutional, residual and recurrent neural networks. 3.1 Introduction While many aspects of the success of deep learning still lack a comprehensive mathematical explanation, the approximation properties1 of neural networks have been studied since around 1960 and are relatively well understood. Statistical learn- ing theory formalizes the problem of approximating â€“ in this context also called learning â€“ a function from a ï¬nite set of samples. Next to statistical and algorithmic considerations, approximation theory plays a major role in the analysis of statisti- cal learning problems. We will clarify this in the following by introducing some fundamental notions.2 Assume that X is an input space, Y is a target space, L : Y Ã— Y â†’[0,âˆ] is a loss function, and P(X,Y) a (usually unknown) probability distribution on some Ïƒ-algebra of X Ã— Y. We then aim at ï¬nding a minimizer of the risk functional3 R : YX â†’[0,âˆ], f 7â†’ âˆ« XÃ—Y L ( f (x), y) dP(X,Y)(x, y), induced by L and P(X,Y) (where YX denotes the set of all functions from X to Y). 1 Throughout the chapter, we will interchangeably use the term approximation theory and expressivity theory. 2 Cucker and Zhou (2007) provides a concise introduction to statistical learning theory from the point of view of approximation theory. 3 With the convention that R(f ) = âˆif the integral is not well deï¬ned. 149 150 GÃ¼hring et al. Expressivity of Deep Neural Networks That means we are looking for a function Ë†f with Ë†f = argmin  R( f ): f âˆˆYX . In the overwhelming majority of practical applications, however, this optimization problem turns out to be infeasible for three reasons: (i) The set YX is simply too large, so one usually ï¬xes a priori some hypothesis class H âŠ‚YX and instead searches for Ë†fH = argmin {R( f ): f âˆˆH} . In the context of deep learning, the set H consists of deep neural networks, which we will introduce later. (ii) Since P(X,Y) is unknown, one cannot compute the risk of a given function f . Instead, we are given a training set S = ((xi, yi))m i=1, which consists of m âˆˆN i.i.d. samples drawn from X Ã— Y with respect to P(X,Y). Thus, we can only hope to ï¬nd the minimizer of the empirical risk RS( f ) = 1 m Ãm i=1 L( f (xi), yi)), given by Ë†fH,S = argmin {RS( f ): f âˆˆH} . (iii) In the case of deep learning one needs to solve a complicated non-convex optimization problem to ï¬nd Ë†fH,S; this is called training and can only be done approximately. Denoting the approximate solution by Ë†f âˆ— H,S âˆˆH, the overall error is (see Fig- ure 3.1) R  Ë†f  âˆ’R  Ë†f âˆ— H,S  â‰¤ R  Ë†f âˆ— H,S  âˆ’R  Ë†fH,S  | {z } training error + R  Ë†fH,S  âˆ’R  Ë†fH  | {z } estimation error + R  Ë†fH  âˆ’R  Ë†f  | {z } approximation error . The results discussed in this chapter deal with estimating the approximation error if the set H consists of deep neural networks. However, the observant reader will notice that practically all the results presented below ignore the dependence on the unknown probability distribution P(X,Y). This can be justiï¬ed by various strategies (see also Cucker and Zhou, 2007) one of which we will describe here. Under suitable conditions it is possible to bound the approximation error by R  Ë†fH  âˆ’R  Ë†f  â‰¤error  Ë†fH âˆ’Ë†f , 3.1 Introduction 151 YX H Ë†f âˆ— H,S Ë†fH,S Ë†fH Training Error (Optimization) Estimation Error (Generalization) Ë†f Approximation Error (Expressivity) Figure 3.1 Decomposition of the overall error into training error, estimation error and approxi- mation error. where error(Â·) is an expression (e.g. the âˆ¥Â· âˆ¥âˆnorm) that is independent of P(X,Y). As an example, assume that Y âŠ‚R and that the loss function L(Â·, y) is Lipschitz continuous for all y âˆˆY with uniform Lipschitz constant Lip(L). We then get R  Ë†fH  âˆ’R  Ë†f  â‰¤Lip(L) Â· Ë†fH âˆ’Ë†f âˆ, (3.1) and hence the upper bound of âˆ¥Ë†fH âˆ’Ë†f âˆ¥âˆcan be used to upper-bound the approxi- mation error. The universal approximation theorem (see Funahashi, 1989; Cybenko, 1989; Hornik et al., 1989; Hornik, 1991), which is the starting point of approximation theory of neural networks, states: For every Ë†f âˆˆC(K) with K âŠ‚Rd compact and every Ïµ > 0 there exists a neural network Ë†fH,Îµ such that âˆ¥Ë†fH âˆ’Ë†f âˆ¥âˆâ‰¤Ïµ. Utilizing the fact that neural networks are universal approximators, we can now see from Equation (3.1) that for H = C(K) the approximation error can be made arbitrarily small. In practice, we are faced with a ï¬nite memory and computation budget, which shows the importance of results similar to the theorem above that additionally quantify the complexity of Ë†fH. We now proceed by introducing the notion of neural networks, considered throughout this chapter, in mathematical terms. 3.1.1 Neural Networks We now give a mathematical deï¬nition of feedforward neural networks, which were ï¬rst introduced in McCulloch and Pitts (1943). More reï¬ned architectures, such as convolutional, residual and recurrent neural networks, are deï¬ned in later sections (see Â§3.8). 152 GÃ¼hring et al. Expressivity of Deep Neural Networks In most cases it makes the exposition simpler to diï¬€erentiate between a neural network considered as a collection of weights and biases, and the corresponding function, referred to as its realization.4 The following notion was introduced in Petersen and Voigtlaender (2018). Deï¬nition 3.1. Let d, s, L âˆˆN. A neural network Î¦ with input dimension d, output dimension s and L layers (see Figure 3.2) is a sequence of matrixâ€“vector tuples Î¦ =  (W[1],b[1]),(W[2],b[2]),. . .,(W[L],b[L]), where n0 = d, nL = s and n1,. . .,nLâˆ’1 âˆˆN, and where each W[â„“] is an nâ„“Ã— nâ„“âˆ’1 matrix and b[â„“] âˆˆRnâ„“. If Î¦ is a neural network as above, K âŠ‚Rd, and if Ïƒ: R â†’R is arbitrary then we deï¬ne the associated realization of Î¦ with activation function Ïƒ over K (in short, the Ïƒ-realization of Î¦ over K) as the map RÏƒ(Î¦): K â†’Rs such that RÏƒ(Î¦)(x) = x[L], where x[L] results from the following scheme: x[0] B x, x[â„“] B Ïƒ(W[â„“] x[â„“âˆ’1] + b[â„“]), for â„“= 1,. . ., L âˆ’1, x[L] B W[L] x[Lâˆ’1] + b[L], and where Ïƒ acts componentwise; that is, Ïƒ(v) = (Ïƒ(v1),. . .,Ïƒ(vm)) for every v = (v1,. . .,vm) âˆˆRm. We call N(Î¦) B d + ÃL j=1 nj the number of neurons of the neural network Î¦ and L = L(Î¦) the number of layers. For â„“â‰¤L we call Mâ„“(Î¦) B âˆ¥W[â„“]âˆ¥0 + âˆ¥b[â„“]âˆ¥0 the number of weights in the â„“th layer and we deï¬ne M(Î¦) B ÃL â„“=1 Mâ„“(Î¦), which we call the number of weights of Î¦. Finally, we denote by max{N1,. . ., NLâˆ’1} the width of Î¦. Although the activation can be chosen arbitrarily, a variety of particularly useful activation functions has been used in the context of deep learning. We refer to Table 3.1, which is an adapted version of Petersen et al. (2021, Table 1), for an overview of frequently used activation functions. Many research results give a relation between the approximation accuracy and the complexity of a neural network Î¦, which is measured in terms of the number of neurons N(Î¦), the number of non-zero weights and biases M(Î¦) and the number of layers L(Î¦). Before we proceed, let us ï¬x the following notions concerning the set of all (realizations of) neural networks. 4 However, if it is clear from the context, in the following we denote by the term â€œneural networksâ€ both the parameter collections as well as their corresponding realizations. 3.1 Introduction 153 Figure 3.2 Visualization of (left) a shallow and (right) a deep (feedforward) neural network with input dimension d = 4 and output dimension s = 3. Name Given by rectiï¬ed linear unit (ReLU) max{0, x} a-leaky ReLU max{ax, x} for some a â‰¥0, a , 1 exponential linear unit x Â· Ï‡xâ‰¥0(x) + (exp(x) âˆ’1) Â· Ï‡x<0(x) softsign x 1 + |x| a-inverse square root linear unit x Â· Ï‡xâ‰¥0(x) + x âˆš 1 + ax2 Â· Ï‡x<0(x) for a > 0 a-inverse square root unit x âˆš 1 + ax2 for a > 0 sigmoidal (type) limxâ†’âˆÏƒ(x) = 1 and limxâ†’âˆ’âˆÏƒ(x) = 0. sigmoid / logistic 1 1 + exp(âˆ’x) tanh exp(x) âˆ’exp(âˆ’x) exp(x) + exp(âˆ’x) arctan arctan(x) softplus ln(1 + exp(x)) Table 3.1 Commonly used activation functions. Deï¬nition 3.2. Let d = n0,n1,. . .,nLâˆ’1, s = nL âˆˆN for some L âˆˆN and Ïƒ: R â†’ R. Then we set N(d,n1,...,nLâˆ’1,s) B {Î¦ neural network with L layers, nâ„“neurons in layer â„“} as well as RN(d,n1,...,nLâˆ’1,s),Ïƒ B  RÏƒ(Î¦): Î¦ âˆˆN(d,n1,...,nLâˆ’1,s) . 154 GÃ¼hring et al. Expressivity of Deep Neural Networks In the following, if not stated otherwise, we ï¬x an input dimension d âˆˆN as well as an output dimension s âˆˆN. 3.1.2 Goal and Outline of this Chapter The aim of this chapter is to provide a comprehensive overview of the area of approximation theory for neural networks. In Â§3.2, we start with the universal approximation theorem for shallow neural networks. These were the main types of neural networks studied until the 1990s. We then relate the approximation accuracy of neural networks to their complexity. It turns out that for many well-known classical function spaces one can derive upper and lower complexity bounds in terms of the number of weights, neurons and layers. Since 2012, deep neural networks have been remarkably successful in various ï¬elds of applications. In Â§3.3, we transfer universality results for shallow neural networks to their deep counterparts. We then proceed with ï¬nding approximation rates of deep neural networks for classes of smooth functions in Â§3.4 and piece- wise smooth functions in Â§3.5. Function classes for which the so-called curse of dimensionality can be overcome will be discussed in Â§3.6. Another important line of research aims to explain the beneï¬t of deep neural networks over shallow ones. This will be the focus of Â§3.7. Finally, there exists a large variety of diï¬€erent network architectures, each adapted to speciï¬c applications. It remains an interesting question how architectural design choices inï¬‚uence expressivity. In Â§3.8, we will present results that cover this ques- tion for convolutional neural networks, which have shown tremendous successes in computer vision, residual neural networks, which have allowed the use of much deeper models and, lastly, recurrent neural networks, which can be viewed as dy- namical systems. 3.1.3 Notation We denote by N = {1,2,. . . } the set of all natural numbers and deï¬ne N0 B Nâˆª{0}. For a âˆˆR we set âŒŠaâŒ‹B max{b âˆˆZ: b â‰¤a} and âŒˆaâŒ‰B min{b âˆˆZ: b â‰¥a}. For two real-valued functions f,g we say that f â‰²g if there exists some constant C > 0 such that f â‰¤Cg. Conversely, we write f â‰³g if g â‰²f and f âˆ¼g if f â‰²gb and f â‰³g. For two sets A, B such that A âŠ‚B we denote by 1A the indicator function of A in B. Moreover |A| denotes the cardinality of A. If (B,T) is a topological space, we denote by âˆ‚A the boundary of A and by A its closure. For x âˆˆRn we denote by |x| its Euclidean norm and by âˆ¥xâˆ¥p its p-norm, p âˆˆ[1,âˆ]. If (V, âˆ¥Â· âˆ¥V) is a normed vector space, we denote by (Vâˆ—, âˆ¥Â· âˆ¥V âˆ—) the 3.2 Shallow Neural Networks 155 topological dual space of V, i.e. the set of all scalar-valued, linear, continuous functions equipped with the operator norm. Let d, s âˆˆN. For a measurable set K âŠ‚Rd we denote by Cn(K,Rs), for n âˆˆ N0 âˆª{âˆ}, the spaces of n times continuously diï¬€erentiable functions with values in Rs. Equipped with âˆ¥f âˆ¥Cn = maxâˆ¥Î±âˆ¥1 â‰¤n âˆ¥DÎ± f âˆ¥âˆthese spaces are Banach spaces if K is compact. We denote by Câˆ 0 (K) the set of inï¬nitely many times diï¬€erentiable functions with compact support in K. In the case s = 1, we simply write C(K) B C(K,R). Let Î² = (n, Î¶) for some n âˆˆN0, Î¶ âˆˆ(0,1], and let K âŠ‚Rd be compact. Then, for f âˆˆCn(K), we write âˆ¥f âˆ¥CÎ² B max  max âˆ¥Î±âˆ¥1 â‰¤n âˆ¥DÎ± f âˆ¥âˆ, max âˆ¥Î±âˆ¥1=n LipÎ¶(DÎ± f )  âˆˆ[0,âˆ], where LipÎ¶( f ) = sup x,yâˆˆK, x,y | f (x) âˆ’f (y)| |x âˆ’y|Î¶ . We denote by CÎ²(K) B { f âˆˆCn(K): âˆ¥f âˆ¥CÎ² < âˆ} the space of all Î²-HÃ¶lder continuous functions. For an n-times diï¬€erentiable function f : K âŠ‚R â†’R we denote by f (n) its nth derivative. For a measure space (K, G, Âµ) we denote by Lp(K; Âµ), for p âˆˆ[1,âˆ], the spaces of equivalence classes of G-measurable real-valued functions f : K â†’R which coincide Âµ-almost everywhere and for which âˆ¥f âˆ¥p < âˆ, where âˆ¥f âˆ¥p B ï£±ï£´ï£´ï£² ï£´ï£´ï£³ âˆ« K | f (x)|pdÂµ(x) 1/p , if p < âˆ, ess supxâˆˆK | f (x)|, if p = âˆ. If Î» is the Lebesgue measure on the Lebesgue Ïƒ-algebra of K âŠ‚Rd then we will simply write Lp(K) = Lp(K; Î») as well as dx = dÎ»(x). Let Wn,p(K) be the Sobolev space of order n consisting of f âˆˆLp(K) satisfying DÎ± f âˆˆLp(K) for all multi-indices âˆ¥Î±âˆ¥1 â‰¤n, where DÎ± f âˆˆLp(K) denotes the weak derivative. Finally, we denote by Fp n,d the unit ball in Wn,p([0,1]d). 3.2 Shallow Neural Networks In this section we examine expressivity results for shallow neural networks, which form the groundwork for a variety of results connected with deep neural networks. After reviewing their universality properties in Â§3.2.1, we examine lower complexity bounds in Â§3.2.2 and upper complexity bounds in Â§3.2.3. 156 GÃ¼hring et al. Expressivity of Deep Neural Networks 3.2.1 Universality of Shallow Neural Networks The most famous type of expressivity result for neural networks states that shallow neural networks are universal approximators. This means that, for a wide variety of relevant function classes C, every function f âˆˆC can be arbitrarily well approx- imated by a shallow neural network. In mathematical terms the statement reads as follows. For every f âˆˆC and every Ïµ > 0 as well as diï¬€erent types of activation functions Ïƒ: R â†’R there exists some N âˆˆN and some neural network Î¦f ,Ïµ âˆˆN(d,N,s) such that f âˆ’RÏƒ  Î¦f ,Ïµ  C â‰¤Ïµ. For all commonly used activation functions Ïƒ and many relevant function classes C,the widths N of the approximating neural networks Î¦f ,Ïµ do not remain uniformly bounded over C and Ïµ but grow with increasing approximation accuracy. Within a very short period of time three papers containing results in this direction appeared. The ï¬rst one, Funahashi (1989), established the universality of shallow neural networks with non-constant, bounded and monotonically increasing contin- uous activation functions for the space C(K), with K âŠ‚Rd compact. The idea of the proof is based on Fourier theory, Paleyâ€“Wiener theory and an integral formula from Irie and Miyake (1988). A slightly diï¬€erent set of activation functions (monotonically increasing, sig- moidal) was considered in Hornik et al. (1989), where universality was established for C = C(K) and C = Lp(Rd; Âµ),. There, Âµ is a probability measure deï¬ned on the Borel Ïƒ-algebra of Rd. The main idea behind the proof is based on the Stoneâ€“Weierstrass theorem. Shortly afterwards, the universality of continuous sigmoidal functions was proved in Cybenko (1989) for the function space C = C(K), where K âŠ‚Rd is compact. The proof, whose main ideas we will sketch in Theorem 3.4, is based on an ele- gant application of the Hahnâ€“Banach theorem combined with a measure-theoretic version of the Riesz representation theorem. An extension of this result for discrete choices of scaling weights and biases wasgiven in Chui and Li (1992). We note that the statements given in Funahashi (1989), Hornik et al. (1989), Cybenko (1989) and Chui and Li (1992) are all applicable to sigmoidal activation functions, which were commonly used in practice at that time. The result of Cybenko considers the more general activation function class of discriminatory functions (see Deï¬nition 3.3) with which he was able to establish the universality of shallow neural networks for C = L1(K). Moreover, the universality of shallow neural networks with a sigmoidal activation function for 3.2 Shallow Neural Networks 157 C = L2(K) based on so-called Fourier networks was demonstrated in Gallant and White (1988) and, closely related to this result, in Hecht-Nielsen (1989). Another universal result for the space C = L2(K) for continuous sigmoidal activation functions employing the Radon transform was given in Carroll and Dickinson (1989). In Hornik et al. (1990) and Hornik (1991) universality for functions with high-order derivatives was examined. In this case C is given by the Sobolev space Wn,p(K) or the space Cn(K), and Ïƒ is a suï¬ƒciently smooth function. Further advances under milder conditions on the activation function were made in Leshno et al. (1993). Again, their result is based on an application of the Stoneâ€“ Weierstrass theorem. The precise statement along with the main proof idea are depicted in Theorem 3.5. In the following we present a selection of elegant proof strategies for universal approximation theorems. We start by outlining a proof strategy utilizing the Riesz representation theorem for measures (see Theorem 6.19 of Rudin, 1987). Deï¬nition 3.3 (Cybenko, 1989). Let K âŠ‚Rd be compact. A measurable function f : R â†’R is discriminatory with respect to K if, for every ï¬nite, signed, regular Borel measure Âµ on K we have that  âˆ« K f (Wx + b)dÂµ(x) = 0, for all W âˆˆR1Ã—d and b âˆˆR  =â‡’ Âµ = 0 . In fact, in Cybenko (1989, Lemma 1) it was demonstrated that every sigmoidal function is indeed discriminatory with respect to closed cubes in Rd. The universal approximation result for discriminatory functions now reads as follows. Theorem 3.4 (Cybenko, 1989). Let Ïƒ âˆˆC(R) be discriminatory with respect to a compact set K âŠ‚Rd. Then RN(d,âˆ,1),Ïƒ B âˆªN âˆˆNRN(d,N,1),Ïƒ is dense in C(K). Proof We restrict ourselves to the case s = 1. Towards a contradiction, assume that the linear subspace RN(d,âˆ,1),Ïƒ is not dense in C(K). Set RN B RN(d,âˆ,1),Ïƒ. Then there exists some f âˆˆC(K)\RN. By the Hahnâ€“Banach theorem, there exists some Îº âˆˆC(K)âˆ—with Îº( f ) , 0 and Îº|RN = 0. Invoking the Riesz representation theorem (see Theorem 6.19 of Rudin, 1987), there exists some ï¬nite, non-zero, signed Borel measure Âµ such that Îº( f ) = âˆ« K f (x)dÂµ(x), for all f âˆˆC(K). Notice that, for all W âˆˆR1Ã—d and all b âˆˆR, we have that Ïƒ(W(Â·) + b) âˆˆ RN(d,âˆ,1),Ïƒ. This implies that 0 = Îº (Ïƒ(W(Â·) + b)) = âˆ« K Ïƒ(Wx + b)dÂµ(x), for all W âˆˆR1Ã—d,b âˆˆR. But since Ïƒ is a discriminatory function, Âµ = 0, which is a contradiction. â–¡ 158 GÃ¼hring et al. Expressivity of Deep Neural Networks Now we proceed by describing the universality result given in Leshno et al. (1993), which is based on an application of the Stoneâ€“Weierstrass theorem. Theorem 3.5 (Leshno et al., 1993). Let K âŠ‚Rd be a compact set and Ïƒ: R â†’R be continuous and not a polynomial. Then RN(d,âˆ,s),Ïƒ is dense in C(K). Sketch of Proof We consider only the case s = 1. Moreover, by Leshno et al. (1993, Proof of Theorem 1, Step 2), we can restrict ourselves to the case d = 1. This follows from the fact that if RN(1,âˆ,1),Ïƒ is dense in C( ËœK) for all compact ËœK âŠ‚R then RN(d,âˆ,1),Ïƒ is dense in C(K) for all compact K âŠ‚Rd. In the following, we will write that f âˆˆM C(R) for some M âŠ‚C(R) if for every compact set K âŠ‚R and every Ïµ > 0 there exists some g âˆˆM such that âˆ¥f |K âˆ’g|K âˆ¥âˆâ‰¤Ïµ. Hence, the claim follows if we can show that RN(1,âˆ,1),Ïƒ C(R) = C(R). Step 1 (Activation Ïƒ âˆˆCâˆ(R)): Assume that Ïƒ âˆˆCâˆ(R). Since for every W,b âˆˆ R, h âˆˆR \ {0}, we know Ïƒ((W + h) Â· +b) âˆ’Ïƒ(W Â· +b) h âˆˆRN(1,âˆ,1), we obtain that d dWÏƒ(WÂ·+b) âˆˆRN(1,âˆ,1) C(R). By an inductive argument, we obtain dk dWk Ïƒ(W Â· +b) âˆˆRN(1,âˆ,1) C(R) for every k âˆˆN0. Moreover, dk dWk Ïƒ(W Â· +b) = (Â·)kÏƒ(k)(W Â· +b). Since Ïƒ is not a polynomial, for every k âˆˆN0 there exists some bk âˆˆR such that Ïƒ(k)(bk) , 0. Hence, (Â·)k Â· Ïƒ(k)(bk) âˆˆRN(1,âˆ,1) C(R) \ {0} for all k âˆˆN0 and RN(1,âˆ,1),Ïƒ C(R) contains all monomials and hence also all polynomials. Since polynomials are dense in C(R) by the Weierstrass theorem, RN(1,âˆ,1),Ïƒ C(R) = C(R). Step 2 (Activation Ïƒ âˆˆC(R)): Now assume that Ïƒ âˆˆC(R) and Ïƒ is not a polynomial. Step 1 is used via a molliï¬cation argument by showing that, for z âˆˆ Câˆ 0 (R), Ïƒ âˆ—z = âˆ« R Ïƒ(Â· âˆ’y)z(y) dy âˆˆRN(1,âˆ,1) C(R), holds by an approximation of the integral by a Riemann series. If Ïƒ âˆ—z is not a polynomial then by invoking Step 1 we conclude that RN(1,âˆ,1),Ïƒ C(R) = C(R). 3.2 Shallow Neural Networks 159 Finally, using standard arguments from functional analysis, it can be shown that, for every Ïƒ âˆˆC(R) for which Ïƒ is not a polynomial, there exists some z âˆˆCâˆ 0 (R) such that Ïƒ âˆ—z is not a polynomial. This yields the claim. â–¡ As mentioned before, these universality results do not yield an estimate of the width of a neural network necessary to achieve a certain approximation accuracy. However, due to hardware-induced constraints on the network size, such an analysis is imperative. We will see in the following, that many of the subsequent results suï¬€er from the infamous curse of dimensionality (Bellman, 1952), i.e. the number of parameters of the approximating networks grows exponentially in the input dimension. To be more precise, for a variety of function classes C âŠ‚{ f : Rd â†’Rs}, in order to obtain f âˆ’RÏƒ  Î¦f ,Ïµ  C â‰¤Ïµ, for an unspeciï¬ed f âˆˆC, the width of Î¦f ,Ïµ needs to scale asymptotically as Ïµâˆ’d/C for a constant C = C(C) > 0 as Ïµ â†˜0. In other words, the complexity of the networks in question grows exponentially in the input dimension with increasing approximation accuracy. 3.2.2 Lower Complexity Bounds We now look at answering the following question: Given a function space C and an approximation accuracy Îµ, how many (unspeciï¬ed) weights and neurons are necessary for a neural network such that its realizations are potentially able to achieve approximation accuracy Ïµ for an arbitrary function f âˆˆC? We start by presenting results for classical function spaces C where the curse of dimensionality can in general not be avoided. The ï¬rst lower bounds have been deduced by a combination of two arguments in Maiorov et al. (1999) and Maiorov and Pinkus (1999) for the case where no restrictions on the parameter selection process are imposed. It was also shown in Maiorov et al. (1999) that the set of functions such that this lower bound is attained is of large measure. Theorem 3.6 (Maiorov et al., 1999; Maiorov and Pinkus, 1999). Let Ïƒ âˆˆC(R), d â‰¥2, Îµ > 0 and N âˆˆN such that, for each f âˆˆF2 n,d, there exists a neural network Î¦f ,Ïµ âˆˆN(d,N,s) satisfying âˆ¥RÏƒ  Î¦f ,Îµ  âˆ’f âˆ¥2 â‰¤Îµ. Then N â‰³Îµâˆ’(dâˆ’1/n). 160 GÃ¼hring et al. Expressivity of Deep Neural Networks The next theorem can be used to derive lower bounds if the parameter selection is required to depend continuously on the function to be approximated. We will state the theorem in full generality and draw out the connection to neural networks afterwards. Theorem 3.7 (DeVore et al., 1989). Let Îµ > 0 and 1 â‰¤p â‰¤âˆ. For M âˆˆN, let Ï†: RM â†’Lp([0,1]d) be an arbitrary function. Suppose there is a continuous function P : Fp n,d â†’RM such that âˆ¥f âˆ’Ï†(P( f ))âˆ¥p â‰¤Ïµ for all f âˆˆFp n,d. Then M â‰³Ïµâˆ’d/n. In Yarotsky (2017) it was observed that, when taking M as the number of weights and Ï† as a function mapping from the weight space to functions realized by neural networks, one can directl obtain a lower complexity bound. We note that the increased regularity (expressed in terms of n) of the function to be approximated implies a potentially better approximation rate, something which is also apparent in many results to follow. 3.2.3 Upper Complexity Bounds Now we turn our attention to results examining the sharpness of the deduced lower bounds by deriving upper bounds. In principle, it was proved in Maiorov and Pinkus (1999) that there do indeed exist sigmoidal, strictly increasing, activation functions Ïƒ âˆˆCâˆ(R) for which the bound N â‰²Ïµâˆ’(dâˆ’1)/n of Theorem 3.6 is attained. However, the construction of such an activation function is based on the separability of the space C([âˆ’1,1]) and hence is not useful in practice. A more relevant upper bound is given in Mhaskar (1996). Theorem 3.8 (Mhaskar, 1996). Let n âˆˆN and p âˆˆ[1,âˆ]. Moreover, let Ïƒ: R â†’R be a function such that Ïƒ|I âˆˆCâˆ(I) for some open interval I âŠ‚R and Ïƒ(k)(x0) , 0 for some x0 âˆˆI and all k âˆˆN0. Then, for every f âˆˆFp n,d and every Ïµ > 0, there exists a shallow neural network Î¦f ,Ïµ âˆˆN(d,N,1) such that f âˆ’RÏƒ  Î¦f ,Ïµ  p â‰¤Ïµ, and N â‰²Ïµâˆ’d/n. We note that this rate is optimal if one assumes a continuous dependence of the parameters on the approximating function (see Theorem 3.7). This is fulï¬lled in the proof of the upper bounds of Theorem 3.8, since it is based on the approximation of Taylor polynomials by realizations of shallow neural networks. It was mentioned in Mhaskar (1996, Section 4) that this rate can be improved if the function f is analytic. 3.3 Universality of Deep Neural Networks 161 The aforementioned lower and upper bounds suï¬€er from the curse of dimension- ality. This can for example be avoided if the function class under consideration is assumed to have strong regularity. As an example we state a result given in Barron (1993, 1994) and Makovoz (1996), where ï¬nite Fourier moment conditions are used. Theorem 3.9 (Barron, 1993, 1994; Makovoz, 1996). Let Ïƒ be a bounded, measur- able, and sigmoidal function. Then, for every f âˆˆ  g: Rd â†’R, âˆ« Rd |Î¾| Â· |F g(Î¾)| dÎ¾ < âˆ  , where F g denotes the Fourier transform of g, and for every Ïµ > 0, there exists a shallow neural network Î¦f ,Ïµ âˆˆN(d,N,1) with f âˆ’RÏƒ  Î¦f ,Ïµ  2 â‰¤Ïµ, and N â‰²Ïµâˆ’2d/(d+1). Although the dimension appears in the underlying rate, the curse of dimension- ality is absent. Lastly, we present a result where a complexity bound is derived for approxima- tions of a ï¬nite set of test points. It was shown in Sontag (1992a) that if Ïƒ: R â†’R is sigmoidal and diï¬€erentiable at one point x âˆˆR with non-zero derivative, and if (x1, y1),. . ., (x2N+1, y2N+1) âˆˆR Ã— R for some N âˆˆN is a set of test points, then, for every Ïµ > 0, there exists a neural network Î¦Ïµ âˆˆN(1,N+1,1) such that sup i=1,...,2N+1 |RÏƒ (Î¦Ïµ) (xi) âˆ’yi| â‰¤Ïµ. This concludes our examination of shallow neural networks. Because of the multitude of existing results we could only discuss a representative selection. For a more comprehensive overview focusing solely on shallow neural networks we refer to Pinkus (1999). 3.3 Universality of Deep Neural Networks So far, our focus has been entirely on shallow neural networks. Recently, however, the use of deep neural networks, i.e. networks with L > 2 layers, has become signiï¬cant. Early attempts to study the expressivity of deep neural networks were based on the Kolmogorov superposition theorem (see, for instance, Kolmogorov, 1957, 1961). A variant of this theorem (see Sprecher, 1965) states that every continuous 162 GÃ¼hring et al. Expressivity of Deep Neural Networks function f : [0,1]d â†’R can be exactly represented as f (x1,. . ., xd) = 2d+1 Ã• i=1 g Â©Â­ Â« d Ã• j=1 kjÏ†i(xj)ÂªÂ® Â¬ . (3.2) Here, kj > 0, for j = 1,. . ., d, such that Ãd j=1 kj â‰¤1; Ï†i : [0,1] â†’[0,1], i = 1,. . .,2d + 1, is strictly increasing, and g âˆˆC([0,1]) are functions depending on f . In Hecht-Nielsen (1987), Equation (3.2) was interpreted in the context of (a general version of) a neural network. It yields that every continuous function can be exactly represented by a 3-layer neural network with width 2d2 + d and diï¬€erent activation functions in each neuron depending on the function to be approximated. However, quoting Hecht-Nielsen (1987), the â€œdirect usefulness of this result is doubtfulâ€, since it is not known how to construct the functions g, Ï†1,. . ., Ï†2d+1, which play the roles of the activation functions. Furthermore, Girosi and Poggio (1989) pointed out that the dependence of the activation functions on f makes this approach hardly usable in practice. We refer to their paper for a more detailed discussion about the suitability of the Kolmogorov superposition theorem in this context. Subsequent contributions focused on more practical neural network architectures, where the activation functions were ï¬xed a priori and only the parameters of the aï¬ƒne maps were adjusted. Under these assumptions, however, the capability of representing every function in C([0,1]d) in an exact way is lost. Consequently, the expressivity of neural networks with a ï¬xed activation function has been studied in terms of their approximative power for speciï¬c function classes C. We note that the universality results for shallow neural networks from the last section in general also hold for deep neural networks with a ï¬xed number of layers (see Funahashi, 1989; Hornik et al., 1989). In Hornik et al. (1989, Corollary 2.7) universality for shallow networks in C(K) was transferred to the multi-layer case via Lemma A.6 of the same paper. It states that if F,G âŠ‚C(R) are such that F|K,G|K are dense sets for every compact subset K âŠ‚R then also { f â—¦g: f âˆˆF,g âˆˆG} is dense in C(K). In contrast with the lower bounds in Theorem 3.6, for the case of three layer neural networks it is possible to show the existence of a pathological (i.e. in practice unusable) activation function Ïƒ such that the set RN(d,2d+1,4d+3,1),Ïƒ is dense in C([âˆ’1,1]d) (see Maiorov and Pinkus, 1999). As in the case of Equation (3.2), the remarkable independence of the complexity from the approximation error and its mere linear dependence on d (implying that the curse of dimensionality can be circumvented) is due to the choice of the activation function. However, for practically used activation functions such as the ReLU, parametric ReLU, exponential linear unit, softsign and tanh, universality in C(K) does not hold (Remark 2.3 in Petersen et al., 2021). The dual problem of considering neural networks with ï¬xed depth and unbounded 3.4 Approximation of Classes of Smooth Functions 163 width, is to explore expressivity of neural networks with ï¬xed width and unrestricted depth. In Hanin and Sellke (2017) and Hanin (2019) it is shown that the set of ReLU neural networks with width â‰¥d + n and unrestricted depth is an universal approximator for the function class C = C([0,1]d,Rn). In the case n = 1, the lower bound on the width is sharp. For C = L1(Rd), Lu et al. (2017) established universality of deep ReLU neural networks with width â‰¥d + 4. The necessary width for ReLU neural networks to yield universal approximators is bounded from below by d. 3.4 Approximation of Classes of Smooth Functions In this chapter, we examine approximation rates of deep neural networks for func- tions characterized by smoothness properties. This chapter can be seen as a coun- terpart of Â§Â§3.2.2 and 3.2.3 with three major diï¬€erences. We now focus on deep neural networks (instead of shallow ones); most of these results were shown after the rise of deep learning in 2012; currently used activation functions (such as, e.g., the ReLU5) are analyzed. A ground-laying result was given in Yarotsky (2017). There it was shown that, for each Ïµ > 0 and for each function f in Fâˆ n,d, there exists a ReLU neural network with L â‰²log2(1/Îµ) layers, as well as M, N â‰²Îµâˆ’d/n log2(1/Îµ) weights and neurons capable of approximating f with Lâˆ-approximation error Ïµ. A generalization of this result for functions from Fp n,d with error measured in the Wr,p-norm6 for 0 â‰¤r â‰¤1 was obtained in GÃ¼hring et al. (2019). Arbitrary Sobolev norms and general activation functions were examined in GÃ¼hring and Raslan (2021). These results show that the error can also be measured in norms that include the distance of the derivative and that there is a trade-oï¬€between the regularity r used in the approximating norm and the complexity of the network. The following theorem summarizes the ï¬ndings of Yarotsky (2017)7 and GÃ¼hring et al. (2019). Theorem 3.10 (Yarotsky, 2017; GÃ¼hring et al., 2019). For every f âˆˆFp n,d there exists a neural network Î¦f ,Îµ whose realization is capable of approximating f with error Îµ in the Wr,p-norm (0 â‰¤r â‰¤1), with 5 The ï¬rst activation function that was used was the threshold function Ïƒ = 1[0,âˆ) (see McCulloch and Pitts, 1943). This was biologically motivated and constitutes a mathematical interpretation of the fact that â€œa neuron ï¬res if the incoming signal is strong enoughâ€. Since this function is not diï¬€erentiable everywhere and its derivative is zero almost everywhere, smoothed versions (sigmoidal functions) have been employed to allow for the usage of the backpropagation algorithm. These functions, however, are subject to the vanishing gradient problem, which is not as common for the ReLU (see Glorot et al., 2011). Another advantage of the ReLU is that it is easy to compute and promotes sparsity in data representation (see Bengio et al., 2013). 6 Here, W r , p for r âˆˆ(0, 1) denotes the Sobolevâ€“Slobodeckij spaces as considered in GÃ¼hring et al. (2019, Deï¬nition 3.3). 7 The results are presented in a slightly modiï¬ed version. The neural networks considered in Yarotsky (2017) are allowed to have skip connections possibly linking a layer to all its successors. The rates obtained are equal, except that the square power of the logarithm needs to be removed. 164 GÃ¼hring et al. Expressivity of Deep Neural Networks (i) M(Î¦f ,Îµ) â‰²Ïµâˆ’d/(nâˆ’r) Â· log2 2  Îµâˆ’n/(nâˆ’r) , and (ii) L(Î¦f ,Îµ) â‰²log2  Îµâˆ’n/(nâˆ’r). Remark 3.11. The depth scales logarithmically in the approximation accuracy. This is due to the ReLU, which renders the approximation of the map x 7â†’x2 diï¬ƒcult. For activation functions Ïƒ with Ïƒ(x0), Ïƒ â€²(x0), Ïƒ â€²â€²(x0) , 0, for some x0 âˆˆR it is possible to approximate x 7â†’x2 by a neural network with a ï¬xed number of weights and L = 2 (see Rolnick and Tegmark, 2018). A common tool for deducing upper complexity bounds is based on the approx- imation of suitable representation systems. Smooth functions, for instance, can be locally described by (Taylor) polynomials, i.e. f â‰ˆ n Ã• âˆ¥Î±âˆ¥1 â‰¤k ck(Â· âˆ’x0)Î± locally. Approximating monomials by neural networks then yields an approximation of f .8 The proof strategy employed by Yarotsky is also based on this idea. It has been picked up in several follow-up works (GÃ¼hring et al., 2019; Petersen and Voigtlaender, 2018) and we describe it here in more detail. Proof sketch The core of the proof is an approximation of the square function x 7â†’x2 by a piecewise linear interpolation that can be expressed by ReLU neural networks (see Figure 3.3 for a visualization). First, we deï¬ne g: [0,1] â†’[0,1], by g(x) B min{2x,2 âˆ’2x}. Notice that the hat function g is representable by a ReLU neural network. Multiple compositions of g with itself result in saw-tooth functions (see Figure 3.3). We set, for m âˆˆN, g1 B g and gm+1 B g â—¦gm. It was demonstrated in Yarotsky (2017) that x2 = lim nâ†’âˆfn(x) B lim nâ†’âˆx âˆ’ n Ã• m=1 gm(x) 22m , for all x âˆˆ[0,1]. Hence, there exist neural networks Î¦x2,Ïµ the ReLU realizations of which approxi- mate x 7â†’x2 uniformly on [0,1] up to an error Ïµ. It can be shown that M(Î¦x2,Ïµ), L(Î¦x2,Ïµ), N(Î¦x2,Ïµ) â‰²log2(1/Ïµ). From this a neural network Î¦mult is constructed via the polarization identity xy = 1 2  (x + y)2 âˆ’x2 âˆ’y2 for x, y âˆˆR, the ReLU realizations of which locally approximatesthemultiplicationmap(x, y) 7â†’ xy. It is now straight-forward to approximate arbitrary polynomials by realizations 8 For shallow neural networks this ansatz was used, for example, in Mhaskar (1996). 3.4 Approximation of Classes of Smooth Functions 165 g g2 g3 x 7â†’x2 f0 f1 f2 Figure 3.3 Visualization of the approximation of the square function x 7â†’x2 by ReLU realiza- tions of neural networks as in Yarotsky (2017). of neural networks with L â‰²log2(1/Ïµ) layers and M â‰²polylog(1/Ïµ) weights.9 In order to approximate f globally, a partition of unity is constructed with ReLU neural networks and combined with approximate polynomials. Since the domain needs to be partitioned into roughly Îµâˆ’d/n patches, the curse of dimensionality occurs in these bounds. â–¡ From Theorem 3.7 we can deduce that, under the hypothesis of a continuous dependence of weights and biases on the function f to be approximated, the upper bound shown in Theorem 3.10 for the case r = 0 is tight (up to a log factor). Dropping the assumption of continuous weight dependency, Yarotsky derived a bound for the case r = 0, p = âˆand in GÃ¼hring et al. (2019) the case r = 1, p = âˆ was covered. Both cases are combined in the next theorem. For its exposition we need to introduce additional notation. Analogously to Yarotsky (2017), we denote by A a neural network with unspeciï¬ed non-zero weights and call it a neural network architecture. We say that the architecture A is capable of approximating a function f with error Îµ and activation function Ïƒ, if this can be achieved by the Ïƒ-realization of some weight assignment. Theorem 3.12 (Yarotsky, 2017; GÃ¼hring et al., 2019). Let Ïµ > 0 and r âˆˆ{0,1}. If AÎµ is a neural network architecture that is capable of approximating every function from Fâˆ n,d with error Îµ in Wr,âˆ-norm and with activation function ReLU, then M(AÎµ) â‰³Îµâˆ’d/2(nâˆ’r). Remark 3.13. The gap between the upper bound MÎµ â‰²Ïµâˆ’d/n of Theorem 3.10 and the lower bound MÎµ â‰³Ïµâˆ’d/2n for r = 0 is discussed in Yarotsky (2018) and Yarotsky and Zhevnerchuk (2019). It is an instance of the beneï¬t of deep (in this case very) over shallow neural networks. It was shown that for every Î¶ âˆˆ[d/(2n), d/n) 9 In Telgarsky (2017), approximation rates for polynomials are extended to rational functions. It is shown that one can locally uniformly approximate rational functions f : [0, 1]d â†’R up to an error Ïµ > 0 by ReLU realizations of neural networks Î¦f ,Ïµ of size M  Î¦f ,Ïµ  â‰²poly(d) Â· polylog(1/Ïµ). 166 GÃ¼hring et al. Expressivity of Deep Neural Networks there exist neural networks with MÎµ â‰²Ïµâˆ’Î¶ non-zero weights and LÎµ â‰²Ïµâˆ’d/(r(Î¶âˆ’1)) layers that uniformly Îµ-approximate functions in Fâˆ n,d. Proof sketch We prove the statement only for r = 0. The case r = 1 is proven similarly. The proof provides a general way of showing lower complexity bounds based on the Vapnikâ€“Chervonenkis dimension (VCdim) Vapnik and Chervonenkis (2015). The quantity VCdim measures the expressiveness of a set of binary-valued functions H deï¬ned on some set A, and is itself deï¬ned by VCdim(H) B sup ï£±ï£´ï£´ï£´ï£² ï£´ï£´ï£´ï£³ m âˆˆN: there exist x1,. . ., xm âˆˆA such that for every y âˆˆ{0,1}m there is a function h âˆˆH with h(xi) = yi for i = 1,. . .,m ï£¼ï£´ï£´ï£´ï£½ ï£´ï£´ï£´ï£¾ . We deï¬ne the set of thresholded realizations of neural networks H B n 1(âˆ’âˆ,a] â—¦RÏƒ (Î¦Î¸) : Î¸ âˆˆRM(Î¦)o , for some (carefully chosen) a âˆˆR, and derive the chain of inequalities c Â· Îµâˆ’d/n â‰¤VCdim(H) â‰¤C Â· M(Î¦Îµ)2. (3.3) The upper bound on VCdim(H) in Equation (3.3) was given in Anthony and Bartlett (2009, Theorem 8.7). To establish the lower bound, set N B  Îµâˆ’1/n and let x1,. . ., xN d âˆˆ[0,1]d such that |xm âˆ’xn| â‰¥1/N for all m,n = 1,. . ., Nd with m , n. For arbitrary y = (y1,. . ., yN d) âˆˆ{0,1}N d, Yarotsky constructed a function fy âˆˆFâˆ n,d with fy(xm) = ym Â· Nâˆ’n for m = 1,. . ., Nd. Now let Î¦fy be a neural network such that RÏƒ  Î¦fy  Îµ-approximates fy; then we have for a thresholded neural network realization 1(âˆ’âˆ,a] â—¦RÏƒ  Î¦fy  that RÏƒ  Î¦fy  (xm) = ym (see Figure 3.4). Figure 3.4 The function fy in d = 2 dimensions. Using the deï¬nition of VCdim it is easy to see that c Â· Îµâˆ’d/n â‰¤Nd â‰¤VCdim(H), 3.5 Approximation of Piecewise Smooth Functions 167 which is the lower bound in the inequality (3.3). The theorem now follows easily from (3.3). â–¡ Approximations in the Lp-norm for Î²-HÃ¶lder-continuous functions were consid- ered in Schmidt-Hieber (2017) and Petersen and Voigtlaender (2018). In contrast with Yarotsky (2017) and GÃ¼hring et al. (2019), the depth of the networks involved remains ï¬xed and does not depend on the approximation accuracy. Additionally in Petersen and Voigtlaender (2018), the weights are required to be encodable.10 We summarize their ï¬ndings in the following theorem. Theorem 3.14 (Petersen and Voigtlaender, 2018). Let Î² = (n, Î¶) for n âˆˆN0, Î¶ âˆˆ (0,1] and p âˆˆ(0,âˆ). Then, for every Ïµ > 0 and every f âˆˆCÎ²([âˆ’1/2,1/2]d) with âˆ¥f âˆ¥CÎ² â‰¤1, there exist ReLU neural networks Î¦f ,Ïµ with encodable weights, L(Î¦f ,Ïµ) â‰²log2((n+Î¶))Â·(n+Î¶)/d layers and M(Î¦f ,Ïµ) â‰²Ïµâˆ’d/(n+Î¶) non-zero weights such that f âˆ’RÏƒ  Î¦f ,Ïµ  p â‰¤Ïµ. There also exist results based on the approximation of B-splines (Mhaskar, 1993) or ï¬nite elements (He et al., 2018; Opschoor et al., 2020). It is shown in those works that neural networks perform as well as the underlying approximation procedure. Finally, instead of examining the approximation rates of deep neural networks for speciï¬c function classes, one could also ask the following question. Which properties does the set of all functions that can be approximated by deep neural networks at a given rate fulï¬l? This question was discussed extensively in Gribonval et al. (2019). Among other results, it was shown that, under certain assumptions on the neural network architecture, these sets of functions can be embedded into classical function spaces, such as Besov spaces, of a certain degree of smoothness. 3.5 Approximation of Piecewise Smooth Functions When modelling real-world phenomena one often assumes only piecewise smooth- ness. A prominent example is given by cartoon-like functions (see Donoho, 2001), En [0,1]d B  f1 + 1B f2 : f1, f2 âˆˆCn  [0,1]d , B âŠ‚(0,1)d, âˆ‚B âˆˆCn and âˆ¥gâˆ¥Cn â‰¤1 for g = f1, f2,âˆ‚B  , which are commonly used as a mathematical model for images. Figure 3.5 provides an illustration for the case d = 2. The ï¬rst expressivity results in this direction were deduced in BÃ¶lcskei et al. (2019) for neural networks with weights of restricted complexity.11 To present this result, we ï¬rst need to introduce some notions from information theory. 10 Tha is, the weights are representable by no more than âˆ¼log2(1/Ïµ) bits. 11 Note that computers can also only store weights of restricted complexity. 168 GÃ¼hring et al. Expressivity of Deep Neural Networks Figure 3.5 Illustration of a cartoon-like function on [0, 1]2. The minimax code length describes the necessary length of bitstrings of an encoded representation of functions from a function class C âŠ‚L2(K) such that it can be decoded with an error smaller then Îµ > 0. The precise deï¬nition is given as follows. Deï¬nition 3.15 (see BÃ¶lcskei et al. (2019) and the references therein). Let K âŠ‚Rd be measurable, and let C âŠ‚L2(K) be compact. For each â„“âˆˆN, we denote by Eâ„“B  E : C â†’{0,1}â„“ the set of binary encoders mapping elements of C to bit strings of length â„“, and we let Dâ„“B  D: {0,1}â„“â†’L2(K) , be the set of binary decoders mapping bit strings of length â„“to elements of L2(K). An encoderâ€“decoder pair (Eâ„“, Dâ„“) âˆˆEâ„“Ã— Dâ„“is said to achieve distortion Ïµ > 0 over the function class C if sup f âˆˆC Dâ„“(Eâ„“( f )) âˆ’f 2 â‰¤Ïµ. Finally, for Ïµ > 0 the minimax code length L(Ïµ, C) is L(Ïµ, C) B min  â„“âˆˆN: âˆƒ Eâ„“, Dâ„“ âˆˆEâ„“Ã— Dâ„“: supf âˆˆC Dâ„“(Eâ„“( f )) âˆ’f 2 â‰¤Ïµ  , with the interpretation L(Ïµ, C) = âˆif supf âˆˆC âˆ¥Dâ„“(Eâ„“( f )) âˆ’f âˆ¥2 > Ïµ for all (Eâ„“, Dâ„“) âˆˆEâ„“Ã— Dâ„“and arbitrary â„“âˆˆN. We are particularly interested in the asymptotic behavior of L(Ïµ, C), which can be quantiï¬ed by the optimal exponent. Deï¬nition 3.16. Let K âŠ‚Rd and C âŠ‚L2(K). Then, the optimal exponent Î³âˆ—(C) is deï¬ned by Î³âˆ—(C) := inf {Î³ âˆˆR: L(Ïµ, C) â‰²Ïµâˆ’Î³, as Ïµ â†˜0} . 3.5 Approximation of Piecewise Smooth Functions 169 The optimal exponent Î³âˆ—(C) describes how fast L(Ïµ, C) tends to inï¬nity as Ïµ decreases. For function classes C1 and C2, the relation Î³âˆ—(C1) < Î³âˆ—(C2) indicates that asymptotically, i.e., for Ïµ â†˜0, the necessary length of the encoding bit string for C2 is larger than that for C1. In other words, a smaller exponent indicates a smaller description complexity. Example 3.17. For many function classes the optimal exponent is well known (see BÃ¶lcskei et al. (2019) and the references therein). Let n âˆˆN, 1 â‰¤p,q â‰¤âˆ; then (i) Î³âˆ—  f âˆˆCn([0,1]d): âˆ¥f âˆ¥Cn â‰¤1  = d/n, (ii) if n âˆˆ{1,2} then Î³âˆ—(En([0,1]d)) = 2(d âˆ’1)/n. The next theorem connects the description complexity of a function class with the necessary complexity of neural network approximations with encodable weights. It shows that, at best, one can hope for an asymptotic growth governed by the optimal exponent. Theorem 3.18 (BÃ¶lcskei et al., 2019). Let K âŠ‚Rd, Ïƒ: R â†’R, c > 0, and C âŠ‚ L2(K). Let Ïµ âˆˆ(0, 1 2) and MÎµ âˆˆN. If for every f âˆˆC there exists a neural network Î¦Îµ,f with weights encodable with âŒˆc log2( 1 Îµ)âŒ‰bits and if âˆ¥f âˆ’RÏƒ  Î¦Îµ,f âˆ¥L2 â‰¤Îµ and M(Î¦Îµ,f ) â‰¤MÎµ then MÎµ â‰³Îµâˆ’Î³ for all Î³ < Î³âˆ—(C). We will now consider the deduction of optimal upper bounds. We have seen already, in many instances, that one of the main ideas behind establishing approxi- mation rates for neural networks is to demonstrate how other function systems, often polynomials, can be emulated by them. In Shaham et al. (2018), a similar approach was followed by demonstrating that neural networks can reproduce wavelet-like functions (instead of polynomials) and thereby also sums of wavelets. This ob- servation lets us transfer M-term approximation rates with wavelets to M-weight approximation rates with neural networks. In BÃ¶lcskei et al. (2019) this route is taken for general aï¬ƒne systems. An aï¬ƒne system is constructed by applying aï¬ƒne linear transformations to a generating function. We will not give the precise deï¬- nition of an aï¬ƒne system here (see e.g. BÃ¶lcskei et al., 2019) but intend to build some intuition by considering shearlets in R2 as an example. Shearlet systems (Kutyniok and Labate, 2012) are representation systems used mainly in signal and image processing. As with the Fourier transform, which ex- pands a function in its frequencies, a shearlet decomposition allows an expansion associated with diï¬€erent location, direction and resolution levels. To construct a 170 GÃ¼hring et al. Expressivity of Deep Neural Networks shearlet system SH, one needs a parabolic scaling operation, deï¬ned by the matrix Aj B ï£®ï£¯ï£¯ï£¯ï£¯ï£° 2j 0 0 2j/2 ï£¹ï£ºï£ºï£ºï£ºï£» , and a shearing operation, deï¬ned by Sk B ï£®ï£¯ï£¯ï£¯ï£¯ï£° 1 k 0 1 ï£¹ï£ºï£ºï£ºï£ºï£» , together with the translation operation. These operations are applied to a generating function Ïˆ âˆˆL2(R2) (satisfying some technical conditions) to obtain a shearlet system SH B  23j/4Ïˆ(Sk Aj(Â·) âˆ’n): j âˆˆZ, k âˆˆZ,n âˆˆZ2 . Shearlet systems are particularly well suited for the class of cartoon-like functions. To make this statement rigorous, we ï¬rst need the following deï¬nition. Deï¬nition 3.19. For a normed space V and a system (Ï†i)iâˆˆI âŠ‚V we deï¬ne the error of best M-term approximation of a function f âˆˆV as Î£M( f ) := inf IM âŠ‚I,|IM |=M, (ci)iâˆˆIM Ã• iâˆˆIM ciÏ†i âˆ’f V . For C âŠ‚V, the system (Ï†i)iâˆˆI yields an M-term approximation rate of Mâˆ’r for r âˆˆR+ if sup f âˆˆC Î£M( f ) =â‰²Mâˆ’r for M â†’âˆ. It is possible to show that certain shearlet systems yield almost optimal M-term approximation rates12 for the class of cartoon-like functions En([0,1]2) (see, for instance, Kutyniok and Lim, 2010). In BÃ¶lcskei et al. (2019, Theorem. 6.8), (optimal) M-term approximation rates of shearlets are transferred to M-weight approximations with neural networks. It is shown that with certain assumptions on the activation function Ïƒ one can emulate a generating function Ïˆ with a ï¬xed-size neural network Î¦Ïˆ such that Ïˆ â‰ˆRÏƒ  Î¦Ïˆ . As a consequence, for every element Ï†i of the system SH there exists a corresponding ï¬xed-size neural network Î¦i with Ï†i â‰ˆRÏƒ (Î¦i). An M- term approximation Ã iâˆˆIM ci( f )Ï†i of a function f can then be approximated by a parallelization of networks Î¦i with â‰²M weights. This line of argument was ï¬rst 12 The optimal M-term approximation rate is the best rate that can be achieved under some restrictions on the representation system and the selection procedure of the coeï¬ƒcients. See Donoho (2001) for optimal M-term approximation rates for cartoon-like functions. 3.5 Approximation of Piecewise Smooth Functions 171 used in Shaham et al. (2018) and also works for general aï¬ƒne systems. This is made precise in the next theorem. Theorem 3.20 (BÃ¶lcskei et al., 2019). Let K âŠ‚Rd be bounded and D = (Ï•i)iâˆˆN âŠ‚ L2(K) be an aï¬ƒne system with generating function Ïˆ âˆˆL2(K). Suppose that for an activation function Ïƒ: R â†’R there exists a constant C âˆˆN such that for all Ïµ > 0 and all D > 0 there is a neural network Î¦D,Ïµ, with at most C non-zero weights, satisfying âˆ¥Ïˆ âˆ’RÏƒ  Î¦D,Ïµ  âˆ¥L2([âˆ’D,D]d) â‰¤Ïµ. Then, if Ïµ > 0, M âˆˆN, g âˆˆL2(K) such that there exist (di)M i=1 satisfying g âˆ’ M Ã• i=1 diÏ•i 2 â‰¤Ïµ, there exists a neural network Î¦ with â‰²M non-zero weights such that âˆ¥g âˆ’RÏƒ (Î¦) âˆ¥2 â‰¤2Ïµ. Consequently, if shearlet systems yield a certain M-term approximation rate for a function class C then neural networks produce at least that error rate in terms of weights. We can conclude from Theorem 3.20 that neural networks yield an optimal M-weight approximation rate of â‰²Mâˆ’n/2. On the other hand, we saw in Example 3.17 that Î³âˆ—(En([0,1]2)) = 2/n, so Theorem 3.18 demonstrates that â‰²Mâˆ’n/2 is also the optimal approximation rate. Similar results were deduced in Grohs et al. (2019). An extension to functions f âˆˆLp([âˆ’1/2,1/2]d), p âˆˆ(0,âˆ), that are CÎ²-smooth apart from CÎ²-singularity hypersurfaces is derived in Petersen and Voigtlaender (2020). It is shown that the Heaviside function can be approximated by a shallow ReLU neural networks with ï¬ve weights (Lemma A.2. in Petersen and Voigtlaender, 2018). Combining this with Theorem 3.14 then yields the next theorem. Theorem 3.21 (Petersen and Voigtlaender, 2018). Let Î² = (n, Î¶), n âˆˆN0, Î¶ âˆˆ(0,1] and p âˆˆ(0,âˆ). Let f = 1K Â· g, where we assume that g âˆˆCÎ²â€²([âˆ’1/2,1/2]d) for Î²â€² = (dÎ²)/(p(d âˆ’1)) with âˆ¥gâˆ¥CÎ²â€² â‰¤1 and that K âŠ‚[âˆ’1/2,1/2]d with âˆ‚K âˆˆCÎ². Moreover, let Ïƒ = ReLU. Then, for every Ïµ > 0, there exists a neural network Î¦f ,Ïµ with encodable weights, L(Î¦f ,Îµ) â‰²log2((n + Î¶)) Â· (n + Î¶)/d layers as well as M(Î¦f ,Ïµ) â‰²Ïµâˆ’p(dâˆ’1)/(n+Î¶) non-zero weights such that f âˆ’RÏƒ  Î¦f ,Ïµ  p â‰¤Îµ. This rate can also be shown to be optimal. We note that approximation rates for 172 GÃ¼hring et al. Expressivity of Deep Neural Networks piecewise-HÃ¶lder functions in L2 were proven in Imaizumi and Fukumizu (2019) and more general spaces, such as Besov spaces, wereconsidered in Suzuki (2019). Some remarks on the role of depth for the results presented above can be found in Â§3.7. 3.6 Assuming More Structure We have seen in the last sections that even approximations by deep neural networks face the curse of dimensionality for classical function spaces. How to avoid this by assuming more structured function spaces is the topic of this section. We start in Â§3.6.1 by assuming a hierarchical structure for which deep neural networks overcome the curse of dimensionality but shallow ones do not. Afterwards, we review approximations of high-dimensional functions lying on a low-dimensional set in Â§3.6.2. Of a similar ï¬‚avor are the results of Â§3.6.3, where we examine speciï¬cally structured solutions of (parametric) partial diï¬€erential equations. 3.6.1 Hierachical Structure Tight approximation rates for smooth functions f : [0,1]d â†’R, x 7â†’g1 â—¦g2 â—¦Â· Â· Â· â—¦gk â—¦l(x) with a hierarchical structure, where l is a multivariate polynomial and g1,. . .,gk are suï¬ƒciently smooth univariate functions, were derived in Liang and Srikant (2016). Achieving a uniform approximation error Ïµ > 0 with L â‰²1 layers requires N â‰³poly(1/Ïµ) neurons, whereas neural networks with L âˆ¼1/Ïµ layers only require N â‰²polylog(1/Ïµ) neurons. The proof idea is again based on approximation by Taylor polynomials. Similarly to Liang and Srikant (2016), superior deep neural network approxi- mation rates for high-dimensional functions with a compositional structure were deduced by Mhaskar et al. (2016, 2017), Mhaskar and Poggio (2016), and Pog- gio et al. (2017). They argued that computations on, for example, images should reï¬‚ect properties of image statistics such as locality and shift invariance, which naturally leads to a compositional structure. As an example, consider a function f : [âˆ’1,1]d â†’R with input dimension d = 8 such that f (x1,. . ., x8) = f3   f21( f11(x1, x2), f12(x3, x4)), f22( f13(x5, x6), f14(x7, x8)), where each function f3, f21, f22, f11, f12, f13, f14 is bivariate and in Wn,âˆ([âˆ’1,1]2) (see Figure 3.6 for a visualization). Eï¬ƒcient approximations can be constructed in two steps. First, each bivariate function is approximated by a shallow neural network with smooth, non-polynomial activation function and size M â‰²Ïµâˆ’2/n. Then, the 3.6 Assuming More Structure 173 x1 x2 x3 x4 x5 x6 x7 x8 f3 f21 f11 f12 f22 f13 f14 Figure 3.6 A visualization of the hierarchically structured function f . neural networks are concatenated in a suitable way by allowing depth L â‰²log2(d). The resulting neural network has M â‰²(d âˆ’1)Ïµâˆ’2/n weights (see Theorem 2 in Poggio et al., 2017). In contrast, shallow neural networks with the same activation function require M â‰³Ïµâˆ’d/n parameters (see Theorem 1 in Poggio et al., 2017). Moreover, if the components fij of f are simply assumed to be Lipschitz continuous, then shallow ReLU neural networks in general require M â‰³Ïµâˆ’d parameters. On the other hand, deep ReLU neural networks with L â‰²log2(d) layers require only M â‰²(d âˆ’1)Ïµâˆ’2 non-zero weights (see Theorem 4 in Poggio et al., 2017). In Montanelli and Du (2019), neural network approximations for the Korobov spaces K2,p([0,1]d) =  f âˆˆLp([0,1]d): f |âˆ‚[0,1]d = 0, DÎ± f âˆˆLp([0,1]d), âˆ¥Î±âˆ¥âˆâ‰¤2 , for p âˆˆ[2,âˆ], are considered. We note that K2,p([0,1]d) âŠ‚W2,p([0,1]d) holds trivially. These functions admit a hierarchical representation (similar to a wavelet decomposition) with respect to basis functions obtained from sparse grids (see Bungartz and Griebel, 2004). By emulating these basis functions, the authors were able to show that, for every f âˆˆK2,p([0,1]d) and every Ïµ > 0, there exists a neural network Î¦f ,Ïµ with L(Î¦f ,Ïµ) â‰²log2(1/Ïµ) log2(d) layers as well as N(Î¦f ,Ïµ) â‰²Ïµâˆ’1/2 Â· log2(1/Ïµ) 3 2(dâˆ’1)+1 log2(d) 174 GÃ¼hring et al. Expressivity of Deep Neural Networks Figure 3.7 Visualization of the Swiss roll data manifold. neurons such that f âˆ’RÏƒ  Î¦f ,Ïµ  âˆâ‰¤Ïµ, where Ïƒ = ReLU. The curse of dimensionality is signiï¬cantly lessened since it appears only in the log factor. 3.6.2 Assumptions on the Data Manifold A typical assumption is that high-dimensional data actually reside on a much lower dimensional manifold. A standard example is provided in Figure 3.7. One may consider the set of images with 128 Ã— 128 pixels as being in R128Ã—128: certainly, most elements in this high-dimensional space are not perceived as an image by a human being, so images are a proper subset of R128Ã—128. Moreover, images are governed by edges and faces and, thus, form a highly structured set. This motivates the idea that the set of images in R128Ã—128 can be described by a lower-dimensional manifold. In Shaham et al. (2018) the approximation rates of ReLU neural networks for functions f âˆˆL2(K) residing on a d-dimensional manifold K âŠ‚RD, possibly with d â‰ªD, were shown to depend only weakly on D. In particular, the following theorem was proved. Theorem 3.22 (Shaham et al., 2018). Let K âŠ‚RD be a smooth d-dimensional manifold, and f âˆˆL2(K). Then there exists a depth-4 neural network Î¦f ,N with M(Î¦f ,N) â‰²DCK + dCKN whose ReLU realization computes a wavelet approxi- 3.6 Assuming More Structure 175 mation of f with N wavelet terms. The constant CK depends only on the curvature of K. Note that the underlying dimension D scales with CK, whereas the number of wavelet terms that inï¬‚uences the approximation accuracy scales only with d. Additional assumptions on how well f can be approximated by wavelets can, with Theorem 3.22, be transferred directly to approximation rates by neural networks. The following corollaries provide two examples for diï¬€erent assumptions on the wavelet representation. Corollary 3.23. If f has wavelet coeï¬ƒcients in â„“1 then, for every Îµ > 0, there exists a depth-4 network Î¦f ,Îµ with M(Î¦f ,Îµ) â‰²DCK + dC2 K Mf Îµâˆ’1 such that âˆ¥f âˆ’RÏƒ  Î¦f ,Îµ âˆ¥L2(K) â‰¤Îµ, where Mf is some constant depending on f and Ïƒ = ReLU. Corollary 3.24. If f âˆˆC2(K) has bounded Hessian matrix then, for every Îµ > 0, there exists a depth-4 neural network Î¦f ,Îµ with M(Î¦f ,Îµ) â‰²DCK + dCKÎµâˆ’d/2 satisfying âˆ¥f âˆ’RÏƒ  Î¦f ,Îµ âˆ¥Lâˆ(K) â‰¤Îµ, where Ïƒ = ReLU. Lastly, we mention a result shown in Petersen and Voigtlaender (2018, Theorem 5.4). There, functions of the type f = gâ—¦h, where h: [âˆ’1/2,1/2]D â†’[âˆ’1/2,1/2]d is a smooth dimension-reduction map and g: [âˆ’1/2,1/2]d â†’R is piecewise smooth, are examined. They showed that the approximation rate is primarily gov- erned by the reduced dimension d. 3.6.3 Expressivity of Deep Neural Networks for Solutions of PDEs Recently, neural-network-based algorithms have shown promising results for the numerical solution of partial diï¬€erential equations (PDEs): see for instance Lagaris et al. (1998), E et al. (2017), E and Yu (2018), Sirignano and Spiliopoulos (2018), Han et al. (2018), Beck et al. (2018), Han and E (2016), Beck et al. (2019), Khoo et al. (2017), Hesthaven and Ubbiali (2018), Lee and Carlberg (2020), Yang and Perdikaris (2018), Raissi (2018), and Lu et al. (2019). There also exist several results showing that speciï¬cally structured solutions of PDEs admit an approximation by neural networks that does not suï¬€er from the curse of dimensionality: see Grohs et al. (2018), ElbrÃ¤chter et al. (2018), Berner et al. (2018), Jentzen and Welti (2018),Reisinger and Zhang (2019), and Hutzenthaler et al. (2019, 2020). 176 GÃ¼hring et al. Expressivity of Deep Neural Networks The key idea behind these contributions is a stochastic interpretation of a deter- ministic PDE.13 As an example, we consider the Blackâ€“Scholes equation, which models the price of a ï¬nancial derivative. For T > 0, a < b, the Blackâ€“Scholes equation (a special case of the linear Kolmogorov equation) is given by ( âˆ‚tu(t, x) = 1 2trace Îº(x)Îº(x)âˆ—(Hessianxu)(t, x) + âŸ¨ËœÎº(x),(âˆ‡xu)(t, x)âŸ©, u(0, x) = Ï•(x), (3.4) where (i) Ï• âˆˆC(Rd) is the initial value,14 (ii) Îº âˆˆC(Rd,RdÃ—d), ËœÎº âˆˆC(Rd,Rd) are assumed to be aï¬ƒne, and (iii) u âˆˆC([0,T] Ã— Rd) is the solution. In Berner et al. (2018) the goal is to ï¬nd a neural network (of moderate complexity) that L2-approximates the end value of the solution [a, b]d âˆ‹x 7â†’u(T,Â·). Using the Feynmanâ€“Kac formula (Grohs et al., 2018, Section 2), the deterministic PDE (3.4) can be associated with a stochastic PDE of the form dSx t = Îº(Sx t )dBt + ËœÎº(Sx t )dt, Sx 0 = x, (3.5) on some probability space (â„¦, G,P), where x âˆˆ[a, b]d, (Bt)t âˆˆ[0,T] is a d-dimensional Brownian motion, and (Sx t )t âˆˆ[0,T] is the stochastic process that solves (3.5). Deï¬ne Y B Ï•(Sx t ). Then the terminal state u(T, x) has an integral representation with respect to the solution of (3.5) given by u(T, x) = E(Y). Using this relation, standard estimates on Monte Carlo sampling and the special structure of Îº, ËœÎº (Berner et al., 2018) obtained the following statement. For each i âˆˆN, there exist aï¬ƒne maps Wi(Â·) + bi : Rd â†’R such that 1 (b âˆ’a)d u(T,Â·) âˆ’1 n n Ã• i=1 Ï•(Wi(Â·) + bi) 2 2 â‰²d1/2 n . This implies that u(T,Â·) can be approximated in L2 by ReLU neural networks Î¦u,Ïµ with M(Î¦u,Ïµ) â‰²poly(d,1/Ïµ). Thus, the curse of dimensionality is avoided. In the setting of parametric PDEs one is interested in computing solutions uy of a family of PDEs parametrized by y âˆˆRp. Such PDEs are typically modelled as operator equations in their variational form: by(uy,v) = fy(v), for all v âˆˆH, y âˆˆP (the parameter set), where, for every y âˆˆP âŠ‚Rp, with p âˆˆN âˆª{âˆ}, 13 This stochastic point of view can also be utilized to estimate the generalization error in such a setting (Berner et al., 2018). 14 The initial value is typically either exactly representable or well-approximable by a small ReLU neural network. 3.7 Deep Versus Shallow Neural Networks 177 (i) the maps by : H Ã— H â†’R are parameter-dependent bilinear forms (derived from the PDE) deï¬ned on some Hilbert space H âŠ‚Lâˆ(K) for K âŠ‚Rn, (ii) fy âˆˆHâˆ—is the parameter-dependent right-hand side, and (iii) uy âˆˆH is the parameter-dependent solution. The parameter y âˆˆRp models uncertainties in real-world phenomena such as geo- metric properties of the domain and physical quantities such as elasticity coeï¬ƒcients or the distribution of sources. Parametric PDEs occur for instance in the context of multi-query applications and in the framework of uncertainty quantiï¬cation. In the context of deep learning, the goal is, for a given Ïµ > 0, to substitute the solution map y 7â†’uy by a neural network Î¦Ïµ such that sup yâˆˆP RÏƒ (Î¦Ïµ(y,Â·)) âˆ’uy H â‰¤Ïµ. (3.6) A common observation is that the solution manifold {uy : y âˆˆP} is low dimensional for many parametric problems. More precisely, there exist (Ï•i)d i=1 âŠ‚H, where d is comparatively small compared with the ambient dimension, such that for every y âˆˆY there exists some coeï¬ƒcient vector (ci(y))d i=1 with uy âˆ’ d Ã• i=1 ci(y)Ï•i H â‰¤Ïµ. For analytic solution maps y 7â†’uy Schwab and Zech (2019) constructed neural networks with smooth or ReLU activation functions that fulï¬ll (3.6). Exploiting a sparse Taylor decomposition of the solution with respect to the parameters y, they were able to avoid the curse of dimensionality in the complexity of the approximat- ing networks. In Kutyniok et al. (2019) the following oberservation was made. If the forward maps y 7â†’by(u,v), y 7â†’fy(v), are well-approximable by neural networks for all u,v âˆˆH, then the map y 7â†’(ci(y))d i=1 is also approximable by ReLU neural networks Î¦c,Ïµ with M(Î¦c,Ïµ) â‰²poly(d) Â· polylog(1/Ïµ). Since in many cases d â‰² log2(1/Ïµ)p and for some cases one can even completely avoid the dependence of d on p, the curse of dimensionality is either signiï¬cantly lessened or completely overcome. The main idea behind the computation of the coeï¬ƒcients by neural networks lies in the eï¬ƒcient approximation of the map y 7â†’((by(Ï•j, Ï•i))d i=1)âˆ’1. This is done via a Neumann series representation of the matrix inverse, which possesses a hierarchical structure. 3.7 Deep Versus Shallow Neural Networks Deep neural networks have advanced the state of the art in many ï¬elds of application. We have already seen a number of results highlighting diï¬€erences between the 178 GÃ¼hring et al. Expressivity of Deep Neural Networks expressivity of shallow and deep neural networks throughout the chapter. In this section, we present advances made in approximation theory explicitly aiming at revealing the role of depth. First of all, we note that basically all the upper bounds given in Â§Â§3.4 and 3.5 use L > 2 layers. Furthermore, in Yarotsky (2017, Section 4.4) andPetersen and Voigtlaender (2018, Section 4.2), it is shown that the upper bound of non-zero weights in Theorems 3.10, 3.14 and 3.21 cannot be achieved by shallow networks. For a ï¬xed number of layers, Petersen and Voigtlaender (2018) showed that the number of layers in Theorems 3.14 and 3.21 is optimal up to the involved log factors. A common strategy in many works that compare the expressive power of very deep over moderately deep neural networks is to construct functions that are eï¬ƒ- ciently approximable by deep neural networks, whereas far more complex shallow neural networks are needed to obtain the same approximation accuracy. The ï¬rst re- sults in this direction dealt with the approximation of Boolean functions by Boolean circuits: see, e.g., Sipser (1983) or HÃ¥stad (1986). In the latter, the existence of func- tion classes that can be computed with Boolean circuits of polynomial complexity and depth k is proven. In contrast, if the depth is restricted to k âˆ’1, an exponential complexity is required. Later works such as HÃ¥stad and Goldmann (1991), Hajnal et al. (1993), Martens and Medabalimi (2014) and the references therein, focused on the representation and approximation of Boolean functions by deep neural networks where the activation is the threshold function Ïƒ = 1[0,âˆ).15 In Delalleau and Bengio (2011), networks built from sum and product neu- rons, called sumâ€“product networks, were investigated. A sum neuron computes a weighted sum16 of its inputs, and a product neuron computes the product of its inputs. The identity function is used as activation function and layers of product neurons are alternated with layers of sum neurons. The authors then considered functions implemented by deep sumâ€“product networks with input dimension d, depth L â‰²log2(d) and N â‰²d neurons that can be represented only by shallow sumâ€“product networks with at least N â‰³2 âˆš d neurons. Thus, the complexity of the shallow representations grows exponentially in âˆš d. Similar statements were shown for slightly diï¬€erent settings. Rolnick and Tegmark (2018) examined the approximation rates of neural net- works with smooth activation functions for polynomials. It was shown that in order to approximate a polynomial by a shallow neural network, one needs exponentially more parameters than with a corresponding deep counterpart. In Telgarsky (2016) neural networks with L layers are compared with networks 15 These are called deep threshold circuits. 16 as in the standard setting 3.7 Deep Versus Shallow Neural Networks 179 with L3 layers. Telgarsky showed that there exist functions f : Rd â†’R which can be represented by neural networks with âˆ¼L3 layers and with âˆ¼L3 neurons but which, in L1, cannot be arbitrarily well approximated by neural networks with â‰²L layers and â‰²2L neurons. This result is valid for a large variety of activation functions such as piecewise polynomials. The main argument of Telgarsky (2016) is based on the representation and approximation of functions with oscillations. First, it is shown that functions with a small number of oscillations cannot approximate functions with a large number of oscillations arbitrarily well. Afterwards, it is shown that functions computed by neural networks with a small number of layers can have only a small number of oscillations, whereas functions computed by deeper neural networks can have many oscillations. A combination of both arguments leads to the result. Eldan and Shamir (2016) and Safran and Shamir (2017) showed similar results for a larger variety of activation functions17 in which the L2-approximation of certain radial basis functions18 requires M â‰²2d weights, whereas three-layered networks can represent these functions exactly with M â‰²poly(d) weights. Extensions and improvements of the results of Telgarsky (2016) were derived in Chatziafratis et al. (2020a,b). In Pascanu et al. (2013); MontÃºfar et al. (2014) it is shown that deep ReLU neural networks have the capability of dividing the input domain into an exponentially larger number of linear regions than shallow ones.19 A ReLU neural network with input dimension d, width n â‰¥d, and L layers is capable of computing functions with number of linear regions less than (n/d)(Lâˆ’1)dnd, whereas ReLU neural networks with input dimension d, two layers and width Ln are only able to divide the input domain into Ldnd linear regions. Similarly, Raghu et al. (2017) gave corresponding upper bounds on the number of linear regions representable by deep ReLU neural networks. It was shown that ReLU neural networks with input dimension d, width n, and L layers are only able to represent â‰²nLd linear regions. Improved estimates on the number of linear regions werederived in Serra et al. (2018) and Serra and Ramalingam (2020). Results comparable with those of the aforementioned papers can be found in Arora et al. (2018). In particular, Theorem 3.1of that paper establishes that for every L âˆˆN, K â‰¥2, there exists a function f : R â†’R which is representable by ReLU neural networks with L + 1 layers, and K Â· L neurons. Moreover, if f is also representable by the ReLU realization of a neural network ËœÎ¦ with ËœL + 1 â‰¤L + 1 layers, then the number of neurons of ËœÎ¦ is bounded from below by â‰³ËœLK L/ ËœL. Bianchini and Scarselli (2014) connected the approximative power of deep neural networks to a topological complexity measure based on Betti numbers: for shallow 17 including the ReLU and sigmoidal functions 18 i.e. functions of the form f : Rd â†’R, where f (x) = g(âˆ¥x âˆ¥1) for some univariate function g 19 A linear region of a function f : Rd â†’Rs is a maximally connected subset A of Rd such that f |A is linear. 180 GÃ¼hring et al. Expressivity of Deep Neural Networks neural networks, the representation power grows polynomially in the numbers of parameters whereas it grows exponentially for deep neural networks. They also drew connections to complexity measures based on VC-dimensions. 3.8 Special Neural Network Architectures and Activation Functions So far we have focused entirely on general feedforward neural network architec- tures. But since neural networks are used for many diï¬€erent types of data in various problem settings, there exists a plethora of architectures, each adapted to a speciï¬c task. In the following we cover expressivity results for three prominent architectures: convolutional neural networks, residual neural networks, and recurrent neural net- works. Since explaining the architectures and their applications in detail is beyond the scope of this chapter we will just review the basics and give references for the interested reader.20 3.8.1 Convolutional Neural Networks Convolutional neural networks were ï¬rst introduced in Lecun (1989) and since then have led to tremendous success, particularly in computer vision tasks. As an example, consider the popular ILSVR21 challenge (see Russakovsky et al., 2015), an image recognition task on the ImageNet database (see Deng et al., 2009) con- taining variable-resolution images that are to be classiï¬ed into categories. In 2012 a convolutional neural network called AlexNet (see Krizhevsky et al., 2012) achieved a top-ï¬ve error of 16.4%, realizing a 10% error rate drop compared with the winner of the previous year. The winners of all annual ILSVR challenges since then have been convolutional neural networks; see Zeiler and Fergus (2014), Simonyan and Zisserman (2015), Szegedy et al. (2015), He et al. (2016),. . . . We now start with a brief explanation of the basic building blocks of a convolu- tional neural network and recommend Goodfellow et al. (2016, Chapter 9) for an extensive and in-depth introduction. Instead of vectors in Rd, the input of a convolutional neural network potentially consists of tensors x[0] âˆˆRd1Ã—Â·Â·Â·Ã—dn where n = 2 or n = 3 for the case of im- ages described above. The main characteristic of convolutional neural networks is the use of convolutional layers. The input x[iâˆ’1] of layer i is subject to an aï¬ƒne 20 Note that in this section we sometimes deviate from the notation used so far in this chapter. For instance, in the case of convolutional neural networks, we no longer diï¬€erentiate between the network as a collection of weights and biases and the function realized by it. Such a distinction would make the exposition of the statements in these cases unnecessarily technical. Moreover, we stick closely to the conventional notation for recurrent neural networks, where the indices of the layers are expressed in terms of discrete time steps t. 21 The abbreviation ILSRC stands for ImageNet Large Scale Visual Recognition Challenge. 3.8 Special Neural Network Architectures and Activation Functions 181 transformation of the form z[i] = w[i] âˆ—x[iâˆ’1] + b[i], where w[i] denotes the convolution ï¬lter and b[i] the bias. In fact, the applica- tion of a convolution can be rewritten as an aï¬ƒne transformation of the form z[i] = W[i]x[iâˆ’1] + b[i], where W[i] is a matrix with a speciï¬c structure and poten- tially many zero entries (depending on the size of the ï¬lter). Convolutional layers enforce locality, in the sense that neurons in a speciï¬c layer are connected only to neighboring neurons in the preceeding layer, as well as weight sharing, i.e. diï¬€erent neurons in a given layer share weights with neurons in other parts of the layer. The application of convolutions results in translation equivariant outputs in the sense that w[i] âˆ—(Tx[iâˆ’1]) = T(w[i] âˆ—x[iâˆ’1]), where T is a translation (or shift) operator. As in the case of feedforward neural networks, the convolution is followed by a nonlinear activation function Ïƒ: R â†’R, which is applied coordinate-wise to z[i]. Hence, a convolutional layer corresponds to a standard layer in a feedforward neural network with a particular structure of the corresponding aï¬ƒne map. Often the nonlinearity is followed by a pooling layer, which can be seen as a dimension-reduction step. Popular choices for pooling operations include max pooling, where only the maxima of certain regions in Ïƒ(z[i]) are kept, or average pooling, where the average over certain regions is taken. In contrast with con- volutions, pooling often induces translation invariance in the sense that a small translation of the input x[iâˆ’1] does not change the output of the pooling stage. Stacking several convolutional and pooling layers results in a deep convolu- tional neural network. After an architecture has been ï¬xed, the goal is to learn the convolutional ï¬lters w[i] and the biases b[i]. We now turn our attention to the expressivity results of convolutional neural networks. Motivated by classiï¬cation tasks, Cohen et al. (2016) compared the expressivity of deep and shallow convolutional arithmetic circuits for the repre- sentation of score functions. A convolutional arithmetic circuit can be interpreted as a convolutional neural network with linear activation function, product pooling, and convolutional layers consisting of 1 Ã— 1 convolutions. We consider a classi- ï¬cation task of the following form. For an observation X = (x1,. . .,xm) âˆˆRdm, where X could be an image represented by a collection of vectors, ï¬nd a suitable classiï¬cation n âˆˆY = {1,. . ., N}, where Y is a set of possible labels. This can be modeled by per-label score functions {hn}nâˆˆ{1,...,N }, with hn : Rdm â†’R. The label corresponding to X is then denoted by argmaxnâˆˆY hn(X). It is assumed that these score functions have a tensor decomposition of the form hn(x1,. . .,xm) = rÃ• i1,...,im=1 cn i1,...,im m Ã– j=1 fij(xi), 182 GÃ¼hring et al. Expressivity of Deep Neural Networks for some representation functions f1,. . ., fr : Rd â†’R and a coeï¬ƒcient tensor cn. Hence every hn is representable by a shallow convolutional arithmetic circuit with, however, a potentially large number of parameters. Using hierarchichal tensor de- compositions the main result states, roughly speaking, that it is with high probabil- ity not possible to signiï¬cantly reduce the complexity of the shallow convolutional arithmetic circuit in order to approximate or represent the underlying score func- tion. However, deep convolutional arithmetic circuits are able to represent score functions exactly with exponentially fewer parameters. This is yet another instance of the beneï¬t of deep over shallow neural networks, this time for a larger function class rather than just for special instances of functions such as those considered in Â§3.7. The main proof ideas rely on tools from matrix algebra, tensor analysis, and measure theory. In Yarotsky (2019) the classiï¬cation task is approached by directly approximat- ing the classiï¬cation function f : L2(R2) â†’R, which maps an image to a real number. Here, images are modelled as elements of the function space L2(R2) (see also Â§3.5, where images are modelled by cartoon-like functions). In order to deal with a non-discrete input (in this case elements of L2), the neural networks con- sidered in that paper consist of a discretization step L2(R2) â†’V (where V is isometrically isomorphic to RDÎµ for some DÎµ < âˆ) followed by convolutional lay- ers and downsampling operations, which replace pooling. The following theorem (Yarotsky, 2019, Theorem 3.2) was proved. Theorem 3.25. Let f : L2(R2) â†’R. Then the following conditions are equivalent. (i) The function f is continuous (in the norm topology). (ii) For every Îµ > 0 and every compact set K âŠ‚L2(R2) there exists a convo- lutional neural network (in the above sense, with downsampling) Î¦K,Îµ that approximates f uniformly on K, i.e. sup Î¾ âˆˆK | f (Î¾) âˆ’Î¦K,Îµ(Î¾)| â‰¤Îµ. A second setting considered in Yarotsky (2019) deals with approximating trans- lation equivariant image-to-image mappings. Think for example of a segmentation task where an image (e.g. of a cell) is mapped to another image (e.g. the binary segmentation mask). Translating the input image should result in a translation of the predicted segmentation mask, which means in mathematical terms that the mapping is translation equivariant. To make the convolutional neural networks also translation equivariant, no downsampling is applied. The next theorem (Yarotsky, 2019, Theorem 3.1) addresses the second setting. Theorem 3.26 (simpliï¬ed). Let f : L2(R2) â†’L2(R2). Then the following condi- tions are equivalent. 3.8 Special Neural Network Architectures and Activation Functions 183 (i) The function f is continuous (in the norm topology) and translation equivariant, i.e. f (Î¾(Â· âˆ’Ï„)) = f (Î¾)(Â· âˆ’Ï„) for all Ï„ âˆˆR2. (ii) For every Îµ > 0 and every compact set K âŠ‚L2(R2),there exists a convolutional neural network (in the above sense) Î¦K,Îµ, which approximates f uniformly on K, i.e. sup Î¾ âˆˆK âˆ¥f (Î¾) âˆ’Î¦K,Îµ(Î¾)âˆ¥L2 â‰¤Îµ. We note that Yarotsky (2019, Theorem 3.1) considers equivariances with respect to more general transformations, including rotations of the function domain. A result of similar ï¬‚avor to Theorem 3.26 for functions f : Rd â†’Rs that are equivariant with respect to ï¬nite groups was given in Petersen and Voigtlaender (2020). In that paper, it was established that every fully connected neural net- work can be expressed by a convolutional neural network without pooling and with periodic padding (in order to preserve equivariance) with a comparable num- ber of parameters, and vice versa. This result can then be used to transfer ap- proximation rates of fully connected neural networks for a function class C to convolutional neural networks approximating a subclass Cequi containing equiv- ariant functions. As an example, we consider translation equivariant HÃ¶lder func- tions f âˆˆCequi = CÎ²([âˆ’1/2,1/2]d) with Î² = (n, Î¶). By Petersen and Voigtlaen- der (2020, Proposition 4.3), there exist convolutional neural networks Î¦f ,Ïµ that Ïµ-approximate f in Lp([âˆ’1/2,1/2]d) and have M â‰²Ïµâˆ’d/(n+Î¶) parameters and L â‰²log2((n + Î¶)) Â· (n + Î¶)/d layers. Note that this rate coincides with that provided by Theorem 3.14. Zhou (2020) mainly derives two types of results. The ï¬rst, Theorem A therein, establishes the universality of deep purely convolutional networks (i.e. no pooling is applied) mapping from Rd â†’R for functions in C(K), with K âŠ‚Rd compact. To be more precise, it is shown that, for every f âˆˆC(K) and every Ïµ > 0, there exist an L âˆˆN and a convolutional neural network Î¦f ,Ïµ equipped with L convolutional layers and the ReLU activation function, such that f âˆ’Î¦f ,Îµ âˆâ‰¤Îµ. Note that, in contrast with most classical universality results, the depth does not remain uniformly bounded over the whole function class C(K). The second result, Theorem B, establishes approximation rates for Sobolev func- tions by convolutional neural networks, which we present in a simpliï¬ed form. Theorem 3.27 (Zhou, 2020). Let r > d/2 + 2. Then, for every Îµ > 0 and every f âˆˆF2 r,d, there exists a convolutional neural network Î¦f ,Îµ (without pooling layers) with depth L â‰²Îµâˆ’1/d such that âˆ¥f âˆ’Î¦f ,Îµâˆ¥âˆâ‰¤Îµ. 184 GÃ¼hring et al. Expressivity of Deep Neural Networks In contrast with most expressivity results considered before, Nguyen and Hein (2018) examined the ability of deep convolutional neural networks to ï¬t a ï¬nite set of n samples (X,Y) âˆˆRdÃ—n Ã— RsÃ—n for some n âˆˆN. It was shown that for such a dataset there exist over-parametrized convolutional neural networks (in this case the number of neurons in one of the layers is larger than the size of the data set n) Î¦(X,Y) equipped with max pooling and rather generic activation functions such that n Ã• i=1 Yi âˆ’Î¦(X,Y)(Xi) 2 = 0. Further expressivity results for convolutional neural networks can be found in Oono and Suzuki (2018). 3.8.2 Residual Neural Networks Driven by the observation that the performance of a neural network very much beneï¬ts from its depth, architectures with an increasing number of layers have been tested. However, in Srivastava et al. (2015), He and Sun (2015), and He et al. (2016) the observation was made that after adding a certain number of layers the test and training accuracy decreases again. He et al. (2016) argued that that the drop in performance caused by adding more and more layers is not related to overï¬tting, for otherwise the training accuracy should still increase while only the test accuracy degrades. Knowing that deeper models should perform at least as well as shallower ones, since the additional stacked layers could learn an identity mapping, the authors concluded that the performance drop is caused by the optimization algorithm not ï¬nding a suitable weight conï¬guration. Motivated by those ï¬ndings, a new network architecture called residual neural network (ResNet) was proposed in He et al. (2016). It incorporates identity shortcuts such that the network only needs to learn a residual. These networks were shown to yield excellent performance (winning several challenges) by employing a large number of layers. One of the popular models, called ResNet-152, employs 152 layers, but ResNets with even more layers have been used. To be precise, ResNets are composed of blocks of stacked layers (see Figure 3.8(a)) having a shortcut identity connection from the input of the block to the output (Figure 3.8(b)). In detail, the function implemented by such a block can be parametrized by weight matrices V and W âˆˆRkÃ—k with bias vector b âˆˆRk and is given by z: Rk â†’Rk with z(x) = VÏƒ(Wx + b) + x. Assume that those two layers are intended to learn a function F : Rk â†’Rk; then the block is forced to learn the residual H(x) B F(x) âˆ’x. The function implemented 3.8 Special Neural Network Architectures and Activation Functions 185 by a ResNet now results from composing the residual blocks z[i], i = 1,. . ., L: ResNet: Rk â†’Rk with ResNet(x) = z[L] â—¦Ïƒ(z[Lâˆ’1])â—¦Â· Â· Â· Â· Â· Â·â—¦Ïƒ(z[1]x). (3.7) Even if the idea behind the residual learning framework is to make the optimization process easier, in this chapter we will focus only on the approximation properties of the proposed architecture. In Hardt and Ma (2016), the ability of ResNets to ï¬t a ï¬nite set of training samples perfectly is shown. The authors considered the case where the input data consists of pairs (xi, yi) with xi âˆˆRd and yi âˆˆ{e1,. . .,es} for i = 1,. . .,n. Here, ej denotes the jth unit vector in Rs such that s can be interpreted as the number of classes in a classiï¬cation task. The ResNet architecture considered in Hardt and Ma (2016) slightly diï¬€ers from the one in Equation (3.7), in that no activation function is used after the residual blocks and some of the matrices are of diï¬€erent shapes to allow for inputs and outputs of diï¬€erent dimensions. Under mild additional assumptions on the training set the following theorem was shown. Theorem 3.28 (Hardt and Ma, 2016). Let n, s âˆˆN and (x1, y1),. . ., (xn, yn) be a set of training samples as above. Then there exists a ResNet Î¦ with M â‰²n log n + s2 weights such that Î¦(xi) = yi, for all i = 1,. . .,n. In Lin and Jegelka (2018) it was shown that ResNets are universal approximators for functions from L1(Rd). This is of particular interest, since the width of the ResNets considered in that paper is bounded by the input dimension d, and it is shown in Hanin (2019) and Lu et al. (2017) that standard feedforward neural networks with width bounded by d are not universal approximators (see also Â§3.3). The idea of the proof relies on a re-approximation of step functions (which are dense in L1). Instead of forcing skip connections to encode an identity function, one can also consider more general skip connections linking earlier layers to deeper layers with connections that are learned in the training process. Such networks have been considered in, for instance, Yarotsky (2017) and GÃ¼hring et al. (2019). Slightly better approximation rates for those architectures in contrast with the standard feedforward case are shown for functions from Fp n,d. In more detail, when allowing skip connections, the upper approximation bounds shown in Theorem 3.10 can be improved by dropping the square of the log terms. 3.8.3 Recurrent Neural Networks Until now we have considered only feedforward network architectures that were able to deal with a ï¬xed input and output dimensions. However, in many applications it is 186 GÃ¼hring et al. Expressivity of Deep Neural Networks Input 7 Ã— 7 conv, 64, /2 pool, /2 3 Ã— 3 conv, 64 3 Ã— 3 conv, 64 3 Ã— 3 conv, 64 3 Ã— 3 conv, 64 3 Ã— 3 conv, 64 3 Ã— 3 conv, 64 3 Ã— 3 conv, 128, /2 3 Ã— 3 conv, 128 3 Ã— 3 conv, 128 3 Ã— 3 conv, 128 Output weight layer weight layer + g(x) + x Output x ReLU g(x) x Id. ReLU + (a) (b) Figure 3.8 Visualizations of CNNs and ResNet. (a) Stacked block architecture of a convolutional neural network of ResNet type. (b) Building block of ResNet with identity shortcut connection. desirable to use a neural network that can handle sequential data with varying length as input/output. This is for example often the case for natural language processing tasks where one might be interested in predicting the topic of a sentence or text (varying input dimension, ï¬xed output) or in producing an image caption for a given image (ï¬xed input dimension, varying output dimension). Recurrent neural networks are specialized for those tasks. In its vanilla form, a recurrent neural network computes a hidden state ht = fÎ¸(htâˆ’1, xt) from the current input xt and, using a recurrent connection, the previous hidden state htâˆ’1. The hidden state ht can then be used in a second step to compute an output yt = gÎ¸(ht) enabling the network to memorize features from previous inputs until time t, such that the output yt depends on x0,. . ., xt (see Figure 3.9). It it important to note 3.8 Special Neural Network Architectures and Activation Functions 187 that the same functions fÎ¸ and gÎ¸, parametrized by the weight vector Î¸, are used in each time step. For an in-depth treatment of recurrent neural networks we refer to Goodfellow et al. (2016, Chapter 10). xt fÎ¸ gÎ¸ yt Figure 3.9 Visualization of the architecture of a recurrent neural network. Recurrent neural networks can be viewed as dynamical systems (Sontag, 1992b). Hence, it is natural to ask whether the universal approximation capacity of feedfor- ward neural networks can be transferred to recurrent neural networks regarded as dynamical systems. Discrete-time dynamical systems of the form ht+1 = f (ht, xt), yt = g(ht), where f : Rn Ã— Rm â†’Rn and g: Rn â†’Rp, and xt âˆˆRn, ht âˆˆRm, yt âˆˆRp, were considered in Sontag (1992b). We call the tuple Î£ = (n,m, p, f,g) a time-discrete dynamical system and say that n is the dimension of the hidden state of Î£, m its input dimension and p its output dimension. Moreover, we call x0,. . ., xT the inputs of the system until time T âˆˆN and h0 the initial state. With these tools at hand we can now formally deï¬ne a shallow sequence-to-sequence recurrent neural network. Deï¬nition 3.29. Let Î£ = (n,m, p, f,g) be a dynamical system and Ïƒ: R â†’R. Then Î£ is a shallow sequence-to-sequence recurrent neural network with activation function Ïƒ if, for matrices A âˆˆRnÃ—n, B âˆˆRmÃ—n, and C âˆˆRnÃ—p, the functions f and g are of the form f (h, x) = Ïƒ(Ah + Bx) and g(x) = Cx. It was then shown in Sontag (1992b) that recurrent neural networks of the form described above are able to approximate time-discrete dynamical systems (with only weak assumptions on f and g) arbitrarily well. To formalize this claim, we ï¬rst describe how the closeness of two dynamical systems is measured by the authors. For this, we introduce the following deï¬nition from Sontag (1992b). 188 GÃ¼hring et al. Expressivity of Deep Neural Networks Deï¬nition 3.30. Let Î£ = (n,m, p, f,g) and ËœÎ£ = (Ëœn,m, p, Ëœf, Ëœg) be two discrete-time dynamical systems as introduced in Deï¬nition 3.29. Furthermore, let K1 âŠ‚Rn and K2 âŠ‚Rm be compact, T âˆˆN, and Îµ > 0. Then we say ËœÎ£ approximates Î£ with accuracy Îµ on K1 and K2 in time T, if the following holds. There exist continuous functions Î±: R Ëœn â†’Rn and Î²: Rn â†’R Ëœn such that, for all initial states h0 âˆˆK1 and Î²(h0) for Î£ and ËœÎ£, respectively, and all inputs x0,. . ., xT âˆˆK2 for Î£ and ËœÎ£, we have âˆ¥ht âˆ’Î±(Ëœht)âˆ¥2 < Îµ for t = 0,. . .,T and âˆ¥yt âˆ’Ëœyt âˆ¥2 < Îµ for t = 1,. . .,T. Note that while input and output dimensions of Î£ and ËœÎ£ coincide, the dimension of the hidden states might diï¬€er. Using the universal approximation theorem for shallow feedforward neural networks (see Â§3.2.1), a universality result for recurrent neural networks with respect to dynamical systems can be derived. Theorem 3.31 (Sontag, 1992b). Let Ïƒ: R â†’R be an activation function for which the universal approximation theorem holds. Let Î£ = (n,m, p, f,g) be a discrete-time dynamical system with f , g continuously diï¬€erentiable. Furthermore, let K1 âŠ‚Rn, K2 âŠ‚Rm be compact and T âˆˆN. Then, for each Îµ > 0, there exist Ëœn âˆˆN and matrices A âˆˆR ËœnÃ— Ëœn, B âˆˆRmÃ— Ëœn, C âˆˆR ËœnÃ—p such that the shallow recurrent neural network ËœÎ£ = (Ëœn,m, p, Ëœf, Ëœg) with activation function Ïƒ, Ëœf (h, x) = Ïƒ(Ah + Bx) and Ëœg(x) = Cx, approximates Î£ with accuracy Îµ on K1 and K2 in time T. Similar results also based on the universality of shallow feedforward neural networks were shown in SchÃ¤fer and Zimmermann (2006) and in Doya (1993). In Doya (1993) and Polycarpou and Ioannou (1991), the case of continuous dynamical systems was additionally included. Discrete-time fading-memory systems are dynamical systems that asymptotically â€œforgetâ€ inputs that were fed into the system earlier in time. Matthews (1993) showed the universality of recurrent neural networks for these types of systems. The above results cover the case of recurrentneuralnetworksthatrealizesequence- to-sequence mappings. As mentioned in the introduction to this section, recurrent neural networks are also used in practice to map sequences of varying length to scalar outputs, thus (for Rm-valued sequences) realizing functions mapping from Ã iâ‰¥0(Rm)i â†’R. In Hammer (2000b), the capacity of such recurrent neural net- works to approximate arbitrary measurable functions f : Ã iâ‰¥0(Rm)i â†’R with high probability was shown. Note that in contrast with approximation results for 3.8 Special Neural Network Architectures and Activation Functions 189 time-discrete dynamical systems, where the system to be approximated naturally displays a recursive structure (as does the recurrent neural network), no such as- sumption was used. It was furthermore shown that recurrent neural networks are not capable of universally approximating such functions with respect to the sup norm. For the rest of this section we denote by (Rm)âˆ—B Ã iâ‰¥0(Rm)i the set of ï¬nite sequences with elements in Rm. To present the aforementioned results in more detail, we start with a formal introduction to the type of recurrent neural network considered in Hammer (2000b, Deï¬nition 1). Deï¬nition 3.32. Any function f : Rn Ã— Rm â†’Rn and initial state h0 âˆˆRn recur- sively induce a mapping Ëœfh0 : (Rm)âˆ—â†’Rn as follows: Ëœfh0([x1,. . ., xi]) = ( h0, if i = 0, f ( Ëœfh0([x1,. . ., xiâˆ’1]), xi), otherwise. A shallow sequence-to-vector recurrent neural network with initial state h0 âˆˆRn and activation function Ïƒ: R â†’R is a mapping from (Rm)âˆ—to Rp of the form x 7â†’C Â· Ëœfh0(x). Here, C âˆˆRnÃ—p is a matrix and f is the realization of a shallow feedforward neural network with no hidden layer and nonlinearity Ïƒ (applied coordinatewise) in the output layer, i.e. f : Rn Ã— Rm â†’Rn is of the form f (h, x) = Ïƒ(Ah + Bx) for h âˆˆRn, x âˆˆRm, where A âˆˆRnÃ—n and B âˆˆRmÃ—n. In simpler terms, a sequence-to-vector recurrent neural network is a sequence- to-sequence recurrent neural network where only the last output is used. Next, we need the deï¬nition of approximation in probability from Hammer (2000b). Deï¬nition 3.33. Let P be a probability measure on (Rm)âˆ—and f1, f2 : (Rm)âˆ—â†’Rp be two measurable functions. Then f1 approximates f2 with accuracy Îµ > 0 and probability Î´ > 0 if P(| f1 âˆ’f2| > Îµ) < Î´. The next theorem establishes the universal approximation capacity in probability of recurrent neural networks for real-valued functions with real-valued sequences of arbitrary length as input. Theorem 3.34 (Hammer, 2000b). Let Ïƒ: R â†’R be an activation function for 190 GÃ¼hring et al. Expressivity of Deep Neural Networks which the universal approximation theorem holds and Îµ,Î´ > 0. Then every measur- able function f : (Rm)âˆ—â†’Rn can be approximated with accuracy Îµ and probability Î´ by a shallow recurrent neural network. Interestingly, even though Theorem 3.31 deals only with approximations of dy- namical systems in ï¬nite time, it can still be utilized for a proof of Theorem 3.34. Hammer (2000b) outlined a proof based on this idea,22 which we will present here to underline the close connections between the theorems. The main ingredient is the observation that P(Ã i>i0(Rm)i) converges to zero for i0 â†’âˆ. As a consequence, one needs to approximate a function f : (Rm)âˆ—â†’Rn only on sequences up to a certain length in order to achieve approximation in probability. The ï¬rst step of the proof now consists of approximating f by the output of a dynamical system for input sequences up to a certain length. In the second step one can make use of Theorem 3.31 to approximate the dynamical system by a recurrent neural network. In Khrulkov et al. (2017) the expressive power of certain recurrent neural net- works, where a connection to a tensor train decomposition can be made is analyzed. Network architectures dealing with more structured inputs, such as trees or graphs, that also make use of recurrent connections and can thus be seen as a generalization of recurrent neural networks, were considered in Hammer (1999), Hammer (2000a, Ch. 3), Bianchini et al. (2005) and Scarselli et al. (2008). Acknowledgements. The authors would like to thank Philipp Petersen for valuable comments that improve this manuscript. Moreover, they would like to thank Mark Cheng for creating most of the ï¬gures and Johannes von Lindheim for providing Figure 3.7. I.G. acknowledges support from the Research Training Group â€œDiï¬€erential Equation- and Data-Driven Models in Life Sciences and Fluid Dynamics: An Interdisciplinary Research Training Group (DAEDALUS)â€ (GRK 2433) funded by the German Research Foundation (DFG). G.K. acknowledges partial support by the Bundesministerium fÃ¼r Bildung und Forschung (BMBF) through the Berliner Zentrum for Machine Learning (BZML), Project AP4, by the Deutsche Forschungs- gemeinschaft (DFG), through grants CRC 1114 â€œScaling Cascades in Complex Systemsâ€, Project B07, CRC/TR 109 â€œDiscretization in Geometry and Dynamicsâ€, Projects C02 and C03, RTG DAEDALUS (RTG 2433), Projects P1 and P3, RTG BIOQIC (RTG 2260), Projects P4 and P9, and SPP 1798 â€œCompressed Sensing in Information Processingâ€, Coordination Project and Project Massive MIMO-I/II, by 22 The actual proof given in Hammer (2000b) is not based on Theorem 3.31, so the authors are able to draw some conclusions about the complexity necessary for the recurrent neural network in special situations. References 191 the Berlin Mathematics Research Center MATH+, and Projects EF1-1, and EF1-4, by the Einstein Foundation Berlin. References Anthony, M., and Bartlett, P. 2009. Neural Network Learning: Theoretical Foun- dations. Cambridge University Press. Arora, R., Basu, A., Mianjy, P., and Mukherjee, A. 2018. Understanding deep neural networks with rectiï¬ed linear units. In: Proc. 2018 International Conference on Learning Representations (ICLR). Barron, A. R. 1993. Universal approximation bounds for superpositions of a sig- moidal function. IEEE Trans. Inf. Theory, 39(3), 930â€“945. Barron, A. R. 1994. Approximation and estimation bounds for artiï¬cial neural networks. Mach. Learn., 14(1), 115â€“133. Beck, C., Becker, S., Grohs, P., Jaafari, N., and Jentzen, A. 2018. Solving stochastic diï¬€erential equations and Kolmogorov equations by means of deep learning. ArXiv preprint arXiv:1806.00421. Beck, C., Becker, S., Cheridito, P., Jentzen, A., and Neufeld, A. 2019. Deep splitting method for parabolic PDEs. ArXiv preprint arXiv:1907.03452. Bellman, R. 1952. On the theory of dynamic programming. Proc. Natl. Acad. SciA., 38(8), 716. Bengio, Y., Courville, A., and Vincent, P. 2013. Representation learning: A review and new perspectives. IEEE Trans. Pattern Anal. Mach. Intell., 35(8), 1798â€“ 1828. Berner, J., Grohs, P., and Jentzen, A. 2018. Analysis of the generalization error: Empirical risk minimization over deep artiï¬cial neural networks overcomes the curse of dimensionality in the numerical approximation of Blackâ€“Scholes partial diï¬€erential equations. ArXiv preprint arXiv:1809.03062. Bianchini, M., and Scarselli, F. 2014. On the complexity of neural network clas- siï¬ers: A comparison between shallow and deep architectures. IEEE Trans. Neural Netw. Learn. Syst., 25(8), 1553â€“1565. Bianchini, M., Maggini, M., Sarti, L., and Scarselli, F. 2005. Recursive neural networks for processing graphs with labelled edges: Theory and applications. Neural Netw., 18(8), 1040â€“1050. BÃ¶lcskei, H., Grohs, P., Kutyniok, G., and Petersen, P. C. 2019. Optimal approx- imation with sparsely connected deep neural networks. SIAM J. Math. Data Sci., 1, 8â€“45. Bungartz, H., and Griebel, M. 2004. Sparse grids. Acta Numer., 13, 147â€“269. Carroll, S., and Dickinson, B. 1989. Construction of neural nets using the Radon transform. Pages 607â€“611 of: Proc. International 1989 Joint Conference on Neural Networks, vol. 1. 192 GÃ¼hring et al. Expressivity of Deep Neural Networks Chatziafratis, Vaggos, Nagarajan, Sai Ganesh, and Panageas, Ioannis. 2020a. Better depthâ€“width trade-oï¬€s for neural networks through the lens of dynamical systems. ArXiv preprint arxiv:2003.00777. Chatziafratis, Vaggos, Nagarajan, Sai Ganesh, Panageas, Ioannis, and Wang, Xiao. 2020b. Depthâ€“width trade-oï¬€s for ReLU networks via Sharkovskyâ€™s theorem. In: Proc. International Conference on Learning Representations. Chui, C. K., and Li, X. 1992. Approximation by ridge functions and neural networks with one hidden layer. J. Approx. Theory, 70(2), 131 â€“ 141. Cohen, N., Sharir, O., and Shashua, A. 2016. On the expressive power of deep learning: A tensor analysis. Pages 698â€“728 of: Proc. 29th Annual Conference on Learning Theory. Cucker, F., and Zhou, D. 2007. Learning Theory: An Approximation Theory View- point. Cambridge Monographs on Applied and Computational Mathematics, vol. 24. Cambridge University Press. Cybenko, G. 1989. Approximation by superpositions of a sigmoidal function. Math. Control Signals Systems, 2(4), 303â€“314. Delalleau, O., and Bengio, Y. 2011. Shallow vs. deep sumâ€“product networks. Pages 666â€“674 of: Advances in Neural Information Processing Systems vol. 24. Deng, J., Dong, W., Socher, R., Li, L., Li, K., and Fei-Fei, L. 2009. ImageNet: A large-scale hierarchical image database. Pages 248â€“255 of: Proc. 2009 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). DeVore, R., Howard, R., and Micchelli, C. 1989. Optimal nonlinear approximation. Manuscripta Math., 63(4), 469â€“478. Donoho, D. L. 2001. Sparse components of images and optimal atomic decompo- sitions. Constr. Approx., 17(3), 353â€“382. Doya, K. 1993. Universality of fully connected recurrent neural networks. technical Report, Deptment of Biology, UCSD. E, W., and Yu, B. 2018. The Deep Ritz method: A deep learning-based numerical algorithm for solving variational problems. Commun. Math. Statistics, 6(1), 1â€“12. E, W., Han, J., and Jentzen, A. 2017. Deep learning-based numerical methods for high-dimensional parabolic partial diï¬€erential equations and backward stochastic diï¬€erential equations. Commun. Math. Statistics, 5(4), 349â€“380. ElbrÃ¤chter, D., Grohs, P., Jentzen, A., and Schwab, C. 2018. DNN Expression rate analysis of high-dimensional PDEs: Application to option pricing. ArXiv preprint arXiv:1809.07669. Eldan, R., and Shamir, O. 2016. The power of depth for feedforward neural networks. Pages 907â€“940 of: Proc. 29th Annual Conference on Learning Theory. Funahashi, K. 1989. On the approximate realization of continuous mappings by neural networks. Neural Netw., 2(3), 183 â€“ 192. Gallant, A. R., and White, H. 1988. There exists a neural network that does not make avoidable mistakes. Pages 657â€“664 of: Proc. IEEE 1988 International Conference on Neural Networks, vol. 1. References 193 Girosi, F., and Poggio, T. 1989. Representation properties of networks: Kol- mogorovâ€™s theorem is irrelevant. Neural Comp., 1(4), 465â€“469. Glorot, X., Bordes, A., and Bengio, Y. 2011. Deep sparse rectiï¬er neural net- works. Pages 315â€“323 of: Proc. 14th International Conference on Artiï¬cial Intelligence and Statistics. Goodfellow, I., Bengio, Y., and Courville, A. 2016. Deep Learning. MIT Press. Gribonval, R., Kutyniok, G., Nielsen, M., and Voigtlaender, F. 2019. Approximation spaces of deep neural networks. ArXiv preprint arXiv:1905.01208. Grohs, P., Hornung, F., Jentzen, A., and von Wurstemberger, P. 2018. A proof that artiï¬cial neural networks overcome the curse of dimensionality in the nu- merical approximation of Blackâ€“Scholes partial diï¬€erential equations. ArXiv preprint arXiv:1809.02362. Grohs, P., Perekrestenko, D., ElbrÃ¤chter, D., and BÃ¶lcskei, H. 2019. Deep neural network approximation theory. ArXiv preprint arXiv:1901.02220. GÃ¼hring, I., and Raslan, M. 2021. Approximation rates for neural networks with encodable weights in smoothness spaces. Neural Netw., 134, 107â€“130. GÃ¼hring, I., Kutyniok, G., and Petersen, P. 2019. Error bounds for approximations with deep ReLU neural networks in Ws,p norms. Anal. Appl. (Singap.), 1â€“57. Hajnal, A., Maass, W., PudlÃ¡k, P., Szegedy, M., and TurÃ¡n, G. 1993. Threshold circuits of bounded depth. J. Comput. Syst. Sci., 46(2), 129â€“154. Hammer, B. 1999. Approximation capabilities of folding networks. Pages 33â€“38 of: Proc. 7th European Symposium on Artiï¬cial Neural Networks. Hammer, B. 2000a. Learning with Recurrent Neural Networks. Lecture Notes in Control and Information Sciences, vol. 254. Springer. Hammer, B. 2000b. On the approximation capability of recurrent neural networks. Neurocomputing, 31(1-4), 107â€“123. Han, J., and E, W. 2016. Deep learning approximation for stochastic control problems. ArXiv preprint arXiv:1611.07422. Han, J., Jentzen, A., and E, W. 2018. Solving high-dimensional partial diï¬€erential equations using deep learning. Proc. Natl. Acad. Sci., 115(34), 8505â€“8510. Hanin, B. 2019. Universal function approximation by deep neural nets with bounded width and ReLU activations. Mathematics, 7(10), 992. Hanin, B., and Sellke, M. 2017. Approximating continuous functions by ReLU nets of minimal width. ArXiv preprint arXiv:1710.11278. Hardt, M., and Ma, T. 2016. Identity matters in deep learning. ArXiv preprint arXiv:1611.04231. HÃ¥stad, J. 1986. Almost optimal lower bounds for small depth circuits. Page 6â€“20 of: Proc. 18th Annual ACM Symposium on Theory of Computing. HÃ¥stad, J., and Goldmann, M. 1991. On the power of small-depth threshold circuits. Comput. Compl., 1(2), 113â€“129. He, J., Li, L., Xu, J., and Zheng, C. 2018. ReLU deep neural networks and linear ï¬nite elements. ArXiv preprint arXiv:1807.03973. 194 GÃ¼hring et al. Expressivity of Deep Neural Networks He, K., and Sun, J. 2015. Convolutional neural networks at constrained time cost. Pages 5353â€“5360 of: Proc. IEEE Conference on Computer Vision and Pattern Recognition. He, K., Zhang, X., Ren, S., and Sun, J. 2016. Deep residual learning for image recognition. Pages 770â€“778 of: Proc. IEEE Conference on Computer Vision and Pattern Recognition. Hecht-Nielsen, R. 1987. Kolmogorovâ€™s mapping neural network existence theo- rem. Pages 11â€“13 of: Proc. IEEE First International Conference on Neural Networks, vol. III. Hecht-Nielsen, R. 1989. Theory of the backpropagation neural network. Pages 593â€“605 of: Proc. International 1989 Joint Conference on Neural Networks, vol. 1. Hesthaven, J. S., and Ubbiali, S. 2018. Non-intrusive reduced order modeling of nonlinear problems using neural networks. J. Comput. Phys., 363, 55â€“78. Hornik, K. 1991. Approximation capabilities of multilayer feedforward networks. Neural Netw., 4(2), 251â€“257. Hornik, K., Stinchcombe, M., and White, H. 1989. Multilayer feedforward networks are universal approximators. Neural Netw., 2(5), 359â€“366. Hornik, K., Stinchcombe, M., and White, H. 1990. Universal approximation of an unknown mapping and its derivatives using multilayer feedforward networks. Neural Netw., 3(5), 551â€“560. Hutzenthaler, M., Jentzen, A., and von Wurstemberger, P. 2019. Overcoming the curse of dimensionality in the approximative pricing of ï¬nancial derivatives with default risks. ArXiv preprint arXiv:1903.05985. Hutzenthaler, M., Jentzen, A., Kruse, T., and Nguyen, T.A. 2020. A proof that rectiï¬ed deep neural networks overcome the curse of dimensionality in the numerical approximation of semilinear heat equations. SN Partial Diï¬€er. Equ. Appl., 1(10). Imaizumi, M., and Fukumizu, K. 2019. Deep neural networks learn non-smooth functions eï¬€ectively. Pages 869â€“878 of: Proc. Machine Learning Research, vol. 89. Irie, B., and Miyake, S. 1988. Capabilities of three-layered perceptrons. Pages 641â€“648 of: IProc. EEE 1988 International Conference on Neural Networks, vol. 1. Jentzen, A., Salimova D., and Welti, T. 2018. A proof that deep artiï¬cial neural networks overcome the curse of dimensionality in the numerical approxima- tion of Kolmogorov partial diï¬€erential equations with constant diï¬€usion and nonlinear drift coeï¬ƒcients. ArXiv preprint arXiv:1809.07321. Khoo, Y., Lu, J., and Ying, L. 2017. Solving parametric PDE problems with artiï¬cial neural networks. ArXiv preprint arXiv:1707.03351. Khrulkov, V., Novikov, A., and Oseledets, I. 2017. Expressive power of recurrent neural networks. ArXiv preprint arXiv:1711.00811. References 195 Kolmogorov, A. N. 1957. On the representation of continuous functions of many variables by superposition of continuous functions of one variable and addi- tion. Doklady Akademii Nauk, 114, 953â€“956. Kolmogorov, A. N. 1961. On the representation of continuous functions of several variables by superpositions of continuous functions of a smaller number of variables. Amer. Math. Soc. Transl. (2), 17, 369â€“373. Krizhevsky, A., Sutskever, I., and Hinton, G. 2012. ImageNet classiï¬cation with deep convolutional neural networks. Pages 1097â€“1105 of: Advances in Neural Information Processing Systems, vol. 25. Kutyniok, G., and Labate, D. 2012. Shearlets: Multiscale Analysis for Multivariate Data. BirkhÃ¤user. Kutyniok, G., and Lim, W.-Q. 2010. Compactly supported shearlets are optimally sparse. J. Approx. Theory, 163, 1564â€“1589. Kutyniok, G., Petersen, P. C., Raslan, M., and Schneider, R. 2019. A theoreti- cal analysis of deep neural networks and parametric PDEs. ArXiv preprint, arXiv:1904.00377. Lagaris, I.E., Likas, A., and Fotiadis, D.I. 1998. Artiï¬cial neural networks for solving ordinary and partial diï¬€erential equations. IEEE Trans. Neural Netw., 9(5), 987â€“1000. Lecun, Y. 1989. Generalization and network design strategies. In: Connectionism in Perspective, Pfeifer, R., Schreter, Z., Fogelman, F., and Steels, L. (eds). Elsevier. Lee, K., and Carlberg, K. 2020. Model reduction of dynamical systems on nonlinear manifolds using deep convolutional autoencoders. J. Comput. Phys., 404, 108973. Leshno, M., Lin, V. Ya., Pinkus, A., and Schocken, S. 1993. Multilayer feedfor- ward networks with a nonpolynomial activation function can approximate any function. Neural Netw., 6(6), 861â€“867. Liang, S., and Srikant, R. 2016. Why deep neural networks for function approxi- mation? ArXiv preprint arXiv:1610.04161. Lin, H., and Jegelka, S. 2018. ResNet with one-neuron hidden layers is a Uni- versal Approximator. Pages 6172â€“6181 of: Advances in Neural Information Processing Systems, vol. 32. Lu, L., Meng, X., Mao, T., and Karniadakis, G. 2019. DeepXDE: A deep learning library for solving diï¬€erential equations. ArXiv preprint arXiv:1907.04502. Lu, Z., Pu, H., Wang, F., Hu, Z., and Wang, L. 2017. The expressive power of neural networks: A view from the width. Pages 6231â€“6239 of: Advances in Neural Information Processing Systems, vol. 30. Maiorov, V., and Pinkus, A. 1999. Lower bounds for approximation by MLP neural networks. Neurocomputing, 25(1â€“3), 81â€“91. Maiorov, V., Meir, R., and Ratsaby, J. 1999. On the approximation of functional classes equipped with a uniform measure using ridge functions. J. Approx. Theory, 99(1), 95 â€“ 111. 196 GÃ¼hring et al. Expressivity of Deep Neural Networks Makovoz, Y. 1996. Random approximants and neural networks. J. Approx. Theory, 85(1), 98â€“109. Martens, J., and Medabalimi, V. 2014. On the expressive eï¬ƒciency of sum product networks. ArXiv preprint arXiv:1411.7717. Matthews, M. 1993. Approximating nonlinear fading-memory operators using neural network models. Circuits, Systems, and Signal Processing, 12(2), 279â€“ 307. McCulloch, W., and Pitts, W. 1943. A logical calculus of ideas immanent in nervous activity. Bull. Math. Biophys., 5, 115â€“133. Mhaskar, H. 1993. Approximation properties of a multilayered feedforward artiï¬cial neural network. Adv. Comput. Math., 1(1), 61â€“80. Mhaskar, H. 1996. Neural networks for optimal approximation of smooth and analytic functions. Neural Comp., 8(1), 164â€“177. Mhaskar, H., and Poggio, T. 2016. Deep vs. shallow networks: An approximation theory perspective. Anal. Appl., 14(06), 829â€“848. Mhaskar, H., Liao, Q., and Poggio, T. 2016. Learning functions: When is deep better than shallow? ArXiv preprint arXiv:1603.00988. Mhaskar, H., Liao, Q., and Poggio, T. 2017. When and why are deep networks better than shallow ones? Pages 2343â€“2349 of: Proc. 31st AAAI Conference on Artiï¬cial Intelligence. Montanelli, H., and Du, Q. 2019. New error bounds for deep ReLU networks using sparse grids. SIAM J. Math. Data Sci., 1(1), 78â€“92. MontÃºfar, G.F., Pascanu, R., Cho, K., and Bengio, Y. 2014. On the number of linear regions of deep neural networks. Pages 2924â€“2932 of: Advances in Neural Information Processing Systems, vol. 27. Nguyen, Q., and Hein, M. 2018. The loss surface and expressivity of deep convo- lutional neural networks. In: Proc. 6th International Conference on Learning Representations. Oono, K., and Suzuki, T. 2018. Approximation and non-parametric estimation of ResNet-type convolutional neural networks via block-sparse fully-connected neural networks. ArXiv preprint arXiv:1903.10047. Opschoor, J. A. A., Petersen, P. C., and Schwab, C. 2020. Deep ReLU networks and high-order ï¬nite element methods. Anal. Appl., 18(05), 715â€“770. Pascanu, R., MontÃºfar, G., and Bengio, Y. 2013. On the number of response regions of deep feed forward networks with piece-wise linear activations. ArXiv preprint arXiv:1312.6098. Petersen, P. C., and Voigtlaender, F. 2018. Optimal approximation of piecewise smooth functions using deep ReLU neural networks. Neural Netw., 180, 296â€“ 330. Petersen, P. C., and Voigtlaender, F. 2020. Equivalence of approximation by con- volutional neural networks and fully-connected networks. Proc. Amer. Math. Soc., 148, 1567â€“1581. References 197 Petersen, P., Raslan, M., and Voigtlaender, F. 2021. Topological properties of the set of functions generated by neural networks of ï¬xed size. Found. Comp. Math., 21, 275â€“444. Pinkus, A. 1999. Approximation theory of the MLP model in neural networks. Acta Numer., 8, 143â€“195. Poggio, T., Mhaskar, H., Rosasco, L., Miranda, B., and Liao, Q. 2017. Why and when can deep â€“ but not shallow â€“ networks avoid the curse of dimensionality: A review. Int. J. Autom. Comput., 14(5), 503â€“519. Polycarpou, M., and Ioannou, P. 1991. Identiï¬cation and Control of Nonlinear Sys- tems using Neural Network Models: Design and Stability Analysis. University of Southern California. Raghu, M., Poole, B., Kleinberg, J., Ganguli, S., and Sohl-Dickstein, J. 2017. On the expressive power of deep neural networks. Pages 2847â€“2854 of: Proc. 34th International Conference on Machine Learning. Raissi, M. 2018. Deep hidden physics models: Deep learning of nonlinear partial diï¬€erential equations. J. Mach. Learn. Res., 19(1), 932â€“955. Reisinger, C., and Zhang, Y. 2019. Rectiï¬ed deep neural networks overcome the curse of dimensionality for nonsmooth value functions in zero-sum games of nonlinear stiï¬€systems. ArXiv preprint arXiv:1903.06652. Rolnick, D., and Tegmark, M. 2018. The power of deeper networks for expressing natural functions. In: Proc. International Conference on Learning Represen- tations. Rudin, W. 1987. Real and Complex Analysis. Third edition. McGrawâ€“Hill. Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein, M., Berg, A., and Fei-Fei, L. 2015. ImageNet large scale visual recognition challenge. Int. J. Comput. Vis., 115(3), 211â€“252. Safran, I., and Shamir, O. 2017. Depth-Width Tradeoï¬€s in Approximating Natural Functions with Neural Networks. Pages 2979â€“2987 of: Proceedings of the 34th International Conference on Machine Learning. Scarselli, F., Gori, M., Tsoi, A., Hagenbuchner, M., and Monfardini, G. 2008. Computational capabilities of graph neural networks. IEEE Trans. Neural Netw., 20(1), 81â€“102. SchÃ¤fer, A., and Zimmermann, H. 2006. Recurrent neural networks are universal approximators. Pages 632â€“640 of: Proc. Conference on Artiï¬cial Neural Networks. Schmidt-Hieber, J. 2017. Nonparametric regression using deep neural networks with ReLU activation function. ArXiv preprint arXiv:1708.06633. Schwab, C., and Zech, J. 2019. Deep learning in high dimension: Neural network expression rates for generalized polynomial chaos expansions in UQ. Anal. Appl. (Singapore), 17(1), 19â€“55. Serra, Thiago, and Ramalingam, Srikumar. 2020. Empirical bounds on linear 198 GÃ¼hring et al. Expressivity of Deep Neural Networks regions of deep rectiï¬er networks. Pages 5628â€“5635 of: Proc. 34th AAAI Conference on Artiï¬cial Intelligence. Serra, Thiago, Tjandraatmadja, Christian, and Ramalingam, Srikumar. 2018. Bounding and counting linear regions of deep neural networks. Pages 4558â€“ 4566 of: Proc. 35th International Conference on Machine Learning. Shaham, U., Cloninger, A., and Coifman, R. R. 2018. Provable approximation properties for deep neural networks. Appl. Comput. Harmon. Anal., 44(3), 537â€“557. Simonyan, K., and Zisserman, A. 2015. Very deep convolutional networks for large- scale image recognition. In: Proc. 3rd International Conference on Learning Representations. Sipser, M. 1983. A complexity-theoretic approach to randomness. Pages 330â€“335 of: Proc. 15th Annual ACM Symposium on Theory of Computing. Sirignano, J., and Spiliopoulos, K. 2018. DGM: A deep learning algorithm for solving partial diï¬€erential equations. J. Comput. Syst. Sci., 375, 1339â€“1364. Sontag, E. 1992a. Feedforward nets for interpolation and classiï¬cation. J. Comput. Syst. Sci., 45(1), 20â€“48. Sontag, E. 1992b. Neural nets as systems models and controllers. Pages 73â€“79 of: Proc. 7th Yale Workshop on Adaptive and Learning Systems. Sprecher, D. 1965. On the structure of continuous functions of several variables. Trans. Amer. Math. Soc., 115, 340â€“355. Srivastava, R., Greï¬€, K., and Schmidhuber, J. 2015. Highway networks. ArXiv preprint arXiv:1505.00387. Suzuki, T. 2019. Adaptivity of deep ReLU network for learning in Besov and mixed smooth Besov spaces: Optimal rate and curse of dimensionality. In: Proc. 7th International Conference on Learning Representations. Szegedy, C., Liu, W., Jia, Y., Pierre, S., Reed, S., Anguelov, D., Erhan, D., Van- houcke, V., and Rabinovich, A. 2015. Going deeper with convolutions. Pages 1â€“9 of: Proc. IEEE Conference on Computer Vision and Pattern Recognition. Telgarsky, M. 2016. Beneï¬ts of depth in neural networks. Pages 1517â€“1539 of: Proc. 29th Annual Conference on Learning Theory. Telgarsky, M. 2017. Neural networks and rational functions. Pages 3387â€“3393 of: Proc. 34th International Conference on Machine Learning. Vapnik, V.N., and Chervonenkis, A. Y. 2015. On the uniform convergence of relative frequencies of events to their probabilities. Pages 11â€“30 of Measures of Com- plexities, Vladimir Vovk, Harris Papadopoulos and Alexander Gammerman (eds). Springer. Yang, Y., and Perdikaris, P. 2018. Physics-informed deep generative models. ArXiv preprint arXiv:1812.03511. Yarotsky, D. 2017. Error bounds for approximations with deep ReLU networks. Neural Netw., 94, 103â€“114. References 199 Yarotsky, D. 2018. Optimal approximation of continuous functions by very deep ReLU networks. Pages 639â€“649 of: Proc. 31st Conference On Learning Theory. Yarotsky, D. 2019. Universal approximations of invariant maps by neural networks. ArXiv preprint arXiv:1804.10306. Yarotsky, D., and Zhevnerchuk, A. 2019. The phase diagram of approximation rates for deep neural networks. ArXiv preprint arXiv:1906.09477. Zeiler, M. D., and Fergus, R. 2014. Visualizing and understanding convolutional networks. Pages 818â€“833 of: Proc. ECCV. Zhou, D.-X. 2020. Universality of deep convolutional neural networks. Appl. Comput. Harmon. Anal., 48(2), 787â€“794. 4 Optimization Landscape of Neural Networks RenÃ© Vidal, Zhihui Zhu, and Benjamin D. Haeï¬€ele Abstract: Many tasks in machine learning involve solving a convex optimization problem which signiï¬cantly facilitates the analysis of properties of the resulting algorithms, such as their optimality, robustness, and generalization. An important challenge in training neural networks occurs when the associated optimization problem is non-convex; this complicates the analysis because global optima can be diï¬ƒcult to characterize and the optimization landscape can also include spurious local minima and saddle points. As a consequence, diï¬€erent algorithms might attract diï¬€erent weights depending on initialization, parameter tuning, etc. Despite this challenge, in practice existing algorithms routinely converge to good solutions, which suggests that the landscape might be simpler than expected, at least for certain classes of networks. This chapter summarizes recent advances in the analysis of optimization land- scapes in neural network training. We ï¬rst review classical results for linear net- works trained with a squared loss and without regularization. Such results show that, under certain conditions on the inputâ€“output data, spurious local minima are guaranteed not to exist, i.e. critical points are either saddle points or global minima. Moreover, globally optimal weights can be found by factorizing certain matrices obtained from the inputâ€“output covariance matrices. We then review recent results for deep networks with a parallel structure, positively homogeneous network map- ping and regularization, and trained with a convex loss. Such results show that a non-convex objective on the weights can be lower-bounded by a convex objective on the network mapping. Moreover, when the network is suï¬ƒciently wide, local minima of the non-convex objective that satisfy a certain condition yield global minima of both the non-convex and convex objectives, and that there is always a non-increasing path to a global minimizer from any initialization. 200 4.1 Introduction 201 4.1 Introduction Many machine learning tasks involve solving an optimization problem of the form min W L(Î¦(X,W),Y) + Î»Î˜(W). (4.1) For example, in the case of classiï¬cation, L(Î¦(X,W),Y) is a loss function that mea- sures the agreement between the true matrix of labels, Y, and the predicted matrix of labels, Î¦(X,W), where X is the input data matrix, W represents the classiï¬er parameters, Î˜(W) is a regularization function designed to prevent overï¬tting,1 and Î» > 0 is a parameter that controls the trade-oï¬€between the loss function and the regularization function. Another example is regression, where the setting is essen- tially the same, except that Y is typically continuous-valued, while in classiï¬cation Y is categorical. Some machine learning problems, such as linear regression, support vector ma- chines, â„“1 minimization, and nuclear norm minimization, involve solving a convex optimization problem, where both the loss and the regularization functions are as- sumed to be convex functions of W. For example, in linear regression with a squared loss and Tikhonov regularization we have2 L(Î¦(X,W),Y) = âˆ¥Y âˆ’WâŠ¤Xâˆ¥2 F and Î˜(W) = âˆ¥Wâˆ¥2 F. (4.2) When the optimization problem is convex, non-global local minima and saddle points are guaranteed not to exist, which signiï¬cantly facilitates the analysis of optimization algorithms, especially the study of their convergence to a global min- imizer. In addition, convexity allows one to analyze properties of the resulting machine learning algorithm, such as robustness and generalization, without having to worry about the particulars of the optimization method, such as initialization, step size (learning rate), etc., as the global optima are easily characterized and many optimization schemes exist which provide guaranteed convergence to a global min- imizer.3 Unfortunately, many other machine learning problems, particularly those that seek to learn an appropriate representation of features directly from the data â€“ with principal component analysis (PCA), non-negative matrix factorization, sparse dic- tionary learning, tensor factorization, and deep learning being well-known examples 1 Note that Î˜ could also depend on X, but we will omit this for notational simplicity. 2 The squared loss between vectors y and z is L(y, z) = âˆ¥y âˆ’zâˆ¥2 2. Here we consider a dataset (X, Y) with m training examples arranged as columns of a matrix (X, Y) =  [x(1), . . . , x(m)], [y(1), . . . , y(m)]  . The sum of the squared losses over the training examples Ãm i=1 âˆ¥y(i) âˆ’z(i) âˆ¥2 2 becomes the Frobenius norm âˆ¥Y âˆ’Zâˆ¥2 F . 3 For convex learning problems, the convergence of the optimization method to a global minimum does depend on initialization and parameter tuning, but the analysis of generalization does not. 202 Vidal et al: Optimization Landscape of Neural Networks â€“ involve solving a non-convex optimization problem of the form: min {W[l]}L l=1 L(Î¦(X,W[1],. . .,W[L]),Y) + Î»Î˜(W[1],. . .,W[L]), (4.3) where Î¦ is an arbitrary convexity-destroying mapping. In PCA, for example, the goal is to factorize a given data matrix Y as the product of two matrices W[1] and W[2], subject to the constraint that the columns of W[1] are orthonormal. In that case, Î¦(X,W[1],W[2]) = W[1]W[2]âŠ¤and Î˜ enforces the orthogonality con- straints W[1]âŠ¤W[1] = I, both of which make the optimization problem non-convex. Similarly, in deep neural network training, the output of the network is typically generated by applying an alternating series of linear and non-linear functions to the input data: Î¦(X,W[1],. . .,W[L])=ÏˆL(W[L]ÏˆLâˆ’1(W[Lâˆ’1] Â· Â· Â· Ïˆ2(W[2]Ïˆ1(W[1]X)) Â· Â· Â· )), (4.4) where each W[l] is an appropriately sized matrix that contains the connection weights between layers l âˆ’1 and l of the network, and the Ïˆl(Â·) functions apply some form of non-linearity after each matrix multiplication, e.g., a sigmoid function, a rectiï¬ed linear unit (ReLU), or max-pooling.4 For a very small number of non-convex problems, e.g., PCA, one is fortunate, and a global minimizer can be found in closed form. For other problems, e.g., â„“0 mini- mization, rank minimization, and low-rank matrix completion, one can replace the non-convex objective by a convex surrogate and show that under certain conditions the solutions to both problems are the same.5 In most cases, however, the optimal solutions cannot be computed in closed form, and a good convex surrogate may not be easy to ï¬nd. This presents signiï¬cant challenges to existing optimization algo- rithms â€“ including (but certainly not limited to) alternating minimization, gradient descent, stochastic gradient descent, block coordinate descent, back propagation, and quasi-Newton methods â€“ which are typically guaranteed only to converge to a critical point of the objective function (Mairal et al., 2010; Rumelhart et al., 1986; Wright and Nocedal, 1999; Xu and Yin, 2013). As the set of critical points for non- convex problems includes not only global minima but also spurious (non-global) local minima, local maxima, saddle points, and saddle plateaus, as illustrated in Figure 4.1, the non-convexity of such a problem leaves the model somewhat ill- posed in the sense that it is not just the model formulation that is important but also the implementation details, such as how the model is initialized and particulars of the optimization algorithm, which can have a signiï¬cant impact on the performance of the model. Despite these challenges, optimization methods that combine backpropagation 4 Here we have shown the linear operation as simple matrix multiplication to simplify notation, but this easily generalizes to other linear operators (e.g., convolution) and aï¬ƒne operators (i.e., those using bias terms). 5 See e.g. Donoho (2006) and CandÃ¨s and Tao (2010) for the relationships between â„“0 and â„“1 minimization. 4.1 Introduction 203 Critical Points of Non-Convex Function Guarantees of Our Framework (a) (i) (b) (c) (d) (e) (f) (g) (h) Figure 4.1 Example critical points of a non-convex function (shown in red). (a,c) Plateaus. (b,d) Global minima. (e,g) Local maxima. (f,h) Local minima. Â© 2017 IEEE. Reprinted, with permission, from Haeï¬€ele and Vidal (2017). (Werbos, 1974) with variants of stochastic gradient descent (Robbins and Monro, 1951), such as Nesterov accelerated gradient (Nesterov, 1983), Adam (Kingma and Ba, 2014), and Adagrad (Duchi et al., 2017), appear to routinely yield good solutions for training deep networks. Recent work attempting to understand this phenomenon can be broadly classiï¬ed into three main themes. (i) Benign optimization landscape: While the optimization problem in (4.3) is not convex for deep network training, there are certain classes of networks for which there are no spurious local minima (Baldi and Hornik, 1989; Kawaguchi, 2016; Haeï¬€ele and Vidal, 2017), local minima concentrate near the global optimum (Choromanska et al., 2015), or critical points are more likely to be saddle points rather than spurious local minima Dauphin et al. (2014). A similar benign landscape has also been observed for non-convex problems arising in phase retrieval (Sun et al., 2018), dictionary learning (Sun et al., 2017), and blind deconvolution (Zhang et al., 2018). (ii) Optimization dynamics lead to global optima: In addition to study of the land- scape of the learning objective, there has also been work focused on how speciï¬c algorithms (largely gradient-descent-based) perform when optimizing neural networks. For example, Gori and Tesi (1991, 1992) showed that gradient descent generally ï¬nds a global minimizer for linearly separable data. More generally, work has also shown that if the optimization landscape satisï¬es the strict saddle property (where the Hessian evaluated at every saddle point has a suï¬ƒciently negative eigenvalue) then gradient descent and many other ï¬rst-order descent techniques are guaranteed to converge to a local minimum and not get stuck in saddle points (Ge et al., 2015; Lee et al., 2019). Using these results, it has been shown that gradient descent converges to a global minimum (Kawaguchi, 2016; Nouiehed and Razaviyayn, 2018; Zhu et al., 2020) for linear neural net- works that satisfy the strict saddle conditions. Unfortunately, however, the strict saddle property does not typically hold for non-linear neural networks. Never- theless, several recent studies have shown that if the network is suï¬ƒciently large then, under certain conditions, gradient descent will converge at a linear rate to global minimizers. However, the necessary conditions are potentially quite strict. Moreover, it is unclear whether such results can be generalized to other 204 Vidal et al: Optimization Landscape of Neural Networks formulations that include regularization on the network parameters (Du et al., 2019; Allen-Zhu et al., 2019). (iii) Implicit bias of the optimization algorithm: Another possible explanation for the observed success of deep learning is that the optimization algorithm either explores only a subset of the landscape (depending on properties of the data or initialization of the algorithm) or automatically induces a regularizer that avoids spurious local minima. For example, Gunasekar et al. (2017, 2018a,b) showed that gradient descent applied to certain classes of linear networks automatically induces a bias towards solutions that minimize a certain norm. Further, Arora et al. (2019) extended this idea to deep linear models, arguing that depth in linear networks trained with gradient descent induces a low-rank regularization through the dynamics of gradient descent. Further, other optimization techniques such as dropout (Srivastava et al., 2014), which adds stochastic noise by randomly setting the output of neurons to zero during training, have been shown to induce low-rank structures in the solution (Cavazza et al., 2018; Mianjy et al., 2018; Pal et al., 2020). This chapter concentrates on the ï¬rst theme by presenting an overview of the optimization landscape of neural network training. In Â§4.3 we study the landscape of linear networks. Speciï¬cally, in Â§4.3.1 we review classical results from Baldi and Hornik (1989) for single-hidden-layer linear networks trained using a squared loss, which show that under certain conditions on the network width and the inputâ€“output data, every critical point is either a global minimum or a saddle point. We also review recent results from Nouiehed and Razaviyayn (2018) and Zhu et al. (2020) which show that all saddle points are strict (i.e., at least one eigenvalue of the Hessian is negative). Moreover, Baldi and Hornik (1989), Nouiehed and Razaviyayn (2018), and Zhu et al. (2020) have also shown that globally optimal weights can be found by factorizing a certain matrix obtained from the inputâ€“output covariance matrices. Then, in Â§4.3.2 we review the work of Kawaguchi (2016), which extends these results to networks of any depth and width by showing that critical points are also either global minima or saddle points. In the same paper Kawaguchi also shows that saddle points of networks with one hidden layer are strict, but networks with two or more layers can have â€œbadâ€ (non-strict) saddle points. In Â§4.4 we study the landscape of nonlinear networks. Speciï¬cally, we review recent results from Haeï¬€ele and Vidal (2017, 2019) that study the conditions under which the optimization landscape for the non-convex optimization problem in (4.3) is such that all critical points are either global minimizers or saddle points or plateaus, as shown in Figure 4.2. Their results show that if the network size is large enough and the functions Î¦ and Î˜ are sums of positively homogeneous functions of 4.2 Basics of Statistical Learning 205 (i) Figure 4.2 Guaranteed properties of the proposed framework. Starting from any initialization, a non-increasing path exists to a global minimizer. Starting from points on a plateau, a simple â€œslidingâ€ method exists to ï¬nd the edge of the plateau (green points). Â© 2017 IEEE. Reprinted, with permission, from Haeï¬€ele and Vidal (2017). the same degree then a monotonically decreasing path to a global minimizer exists from every point. 4.2 Basics of Statistical Learning Let (x,y) âˆˆXÃ—Y be a pair of random variables drawn from an unknown distribution P(x,y), where X is the input space and Y the output space. Assume we wish to predict y from an observation about x by ï¬nding a hypothesis Ë†f âˆˆYX, i.e. Ë†f : X â†’Y, that minimizes the expected loss or risk, R( f ); i.e. we want to ï¬nd min f âˆˆF h R( f )  Ex,y[L( f (x),y)] i . (4.5) Here F âŠ‚YX is the space of hypotheses (e.g., the space of linear functions or the space of measurable functions from X to Y) and L : Y Ã— Y â†’[0,âˆ] is a loss function, where L( f (x),y) gives the cost of predicting y as f (x) (e.g., the zero-one loss 1y,f (x) for classiï¬cation or the squared loss âˆ¥y âˆ’f (x)âˆ¥2 2 for regression). The smallest expected risk R( Ë†f ) is called the Bayes error. Since P(x,y) is unknown, Ë†f and R( Ë†f ) cannot be computed. Instead, we assume we are given a training set S = {(x(i),y(i))}m i=1 of i.i.d. samples from P(x,y) and seek to ï¬nd a hypothesis Ë†fF,S that minimizes the empirical risk min f âˆˆF h RS( f )  1 m m Ã• i=1 L( f (x(i)),y(i)) i . (4.6) Since the objective functions in (4.5) and (4.6) are diï¬€erent, a priori there is no guarantee that Ë†fF,S or its risk, R( Ë†fF,S), will be close to Ë†f or R( Ë†f ), respectively. This leads to the question of generalization, which seeks to understand the performance of Ë†fF,S not just on the training set S but on the entire population. In principle, we could use the error R( Ë†fF,S) âˆ’R( Ë†f ) to assess the quality of Ë†fF,S. However, Ë†fF,S depends on the data S, so Ë†fF,S is a random function and R( Ë†fF,S) is a random variable. While we could use the expectation of R( Ë†fF,S) with respect to the data, 206 Vidal et al: Optimization Landscape of Neural Networks ES[R( Ë†fF,S) âˆ’R( Ë†f )], or verify that Ë†fF,S is universally consistent, i.e. check that lim mâ†’âˆR( Ë†fF,S) = R( Ë†f ) almost surely, (4.7) both approaches are diï¬ƒcult to implement because P(x,y) is unknown. To address this issue, a common practice is to decompose the error as R( Ë†fF,S) âˆ’R( Ë†f ) = R( Ë†fF,S) âˆ’RS( Ë†fF,S) +  RS( Ë†fF,S) âˆ’RS( Ë†f ) +  RS( Ë†f ) âˆ’R( Ë†f ) (4.8) and use the fact that the second group of terms is nonpositive and the third group of terms has zero expectation to arrive at an upper bound on the expected error ES[R( Ë†fF,S) âˆ’R( Ë†f )] â‰¤ES[R( Ë†fF,S) âˆ’RS( Ë†fF,S)] â‰¤ES[supf âˆˆF R( f ) âˆ’RS( f )]. (4.9) As the bound on the right-hand side may not be easily computable, a typical approach is to derive an easier-to-compute upper bound, say Î˜( f ), and then solve the regularized empirical risk minimization problem min f âˆˆF[RS( f ) + Î˜( f )]. (4.10) In other words, rather than minimizing the empirical risk, RS( f ), we usually min- imize the regularized empirical risk, RS( f ) + Î˜( f ), in the hope of controlling the error ES[R( Ë†fF,S) âˆ’R( Ë†f )]. Therefore, this chapter will focus on understanding the landscape of the op- timization problem in (4.10), although we will also make connections with the optimization problem in (4.5) whenever possible (e.g., for single-hidden-layer lin- ear networks trained with a squared loss). We refer the reader to other chapters in this book for a study of the generalization properties. 4.3 Optimization Landscape of Linear Networks In this section we study the landscape of linear networks trained using a squared loss. In Â§4.3.1 we show that under certain conditions every critical point of a single- hidden-layer linear network is either a global minimum or a strict saddle point and that globally optimal weights can be obtained using linear-algebraic methods. In Â§4.3.2 we show that the critical points of linear networks with more than two layers are either global minima or saddle points, but such saddle points may not be strict. 4.3 Optimization Landscape of Linear Networks 207 4.3.1 Single-Hidden-Layer Linear Networks with Squared Loss and Fixed Size Regularization Let us ï¬rst consider the case of linear networks with n0 inputs, n2 outputs, and a single hidden layer with n1 neurons. In this case, the hypothesis space F can be parametrized by the network weights (W[1],W[2]) = (U,V) as6 F = { f âˆˆYX : f (x) = UVâŠ¤x, where U âˆˆRn2Ã—n1 and V âˆˆ (4.11) Rn0Ã—n1}. (4.12) In this section, we study the optimization landscape for single-hidden-layer linear networks trained using a squared loss, L(z,y) = âˆ¥y âˆ’zâˆ¥2 2. No regularization on the network weights is assumed, except that the network size n1 is assumed to be known and suï¬ƒciently small relative to the inputâ€“output dimensions, i.e., n1 â‰¤min{n0,n2}. Under these assumptions, the problem of minimizing the expected risk reduces to7 min U,V  R(U,V)  Ex,y[âˆ¥y âˆ’UVâŠ¤xâˆ¥2 2]  . (4.13) Letting Î£xx = E[xxâŠ¤] âˆˆRn0Ã—n0, Î£xy = E[xyâŠ¤] âˆˆRn0Ã—n2, Î£yx = E[yxâŠ¤] = Î£âŠ¤ xy âˆˆ Rn2Ã—n0, and Î£yy = E[yyâŠ¤] âˆˆRn2Ã—n2, the expected risk can be rewritten as: R(U,V) = trace(Î£yy âˆ’2Î£yxVUâŠ¤+ UVâŠ¤Î£xxVUâŠ¤). (4.14) Consider now the empirical-risk-minimization problem min U,V h RS(U,V) = 1 m m Ã• i=1 âˆ¥y(i) âˆ’UVâŠ¤x(i)âˆ¥2 2 = 1 m âˆ¥Y âˆ’UVâŠ¤Xâˆ¥2 F i , (4.15) where S = {(x(i),y(i))}m i=1 is the training set and X = [x(1),. . .,x(m)] and Y = [y(1),. . .,y(m)] are the input and output data matrices. It is easy to see that the empirical risk RS(U,V) is equal to R(U,V) if the covariance matrices Î£xx, Î£xy, and Î£yy are substituted by their empirical estimates 1 mXXâŠ¤, 1 mXYâŠ¤, and 1 mYYâŠ¤, respectively. Therefore, in this case, analysis of the optimization landscape for both the expected and empirical risk can be done by analyzing the landscape of R(U,V). To motivate the analysis of the landscape of R(U,V), let us ï¬rst analyze the landscape of the risk as a function of the product of the weights, i.e., Z = UVâŠ¤, which is given by R(Z) = trace(Î£yy âˆ’2Î£yxZâŠ¤+ ZÎ£xxZâŠ¤). When there is no constraint on Z (e.g. when U and V are full column rank), the risk is a convex function of Z and the ï¬rst-order condition for optimality is given by ZÎ£xx = Î£yx. Thus, if Î£xx is invertible, the global minimum is unique and is given by Zâˆ—= Uâˆ—Vâˆ—âŠ¤= Î£yxÎ£âˆ’1 xx. Of course, this provides a characterization of the optimal Z, but 6 For simplicity of notation, if we only have two groups of parameters we will use (U, V) rather than (W[1], W[2]). 7 With an abuse of notation, we will write the risk as a function of the network weights, i.e., R(U, V), rather than as a function of the inputâ€“output map, i.e., R(f ). 208 Vidal et al: Optimization Landscape of Neural Networks not of the optimal U and V. The challenge in characterizing the landscape of R(U,V) is hence to understand the eï¬€ect of the low-rank constraint n1 â‰¤min{n0,n2}, i.e. to consider the possibility that critical points for U or V might not be low-rank. The following lemma characterizes properties of the critical points of R(U,V). The original statements and proofs for these results can be found in Baldi and Hornik (1989). Here we provide a uniï¬ed treatment for both the expected and empirical risk, as well as alternative derivations. Lemma 4.1. If (U,V) is a critical point of R then UVâŠ¤Î£xxV = Î£yxV and Î£xxVUâŠ¤U = Î£xyU. (4.16) Moreover, if Î£xx is invertible then the following three properties hold. (i) If V is full column rank then U = Î£yxV(VâŠ¤Î£xxV)âˆ’1. (ii) If U is full column rank then V = Î£âˆ’1 xx Î£xyU(UâŠ¤U)âˆ’1. (iii) Let Î£ = Î£yxÎ£âˆ’1 xx Î£xy. If U is full column rank and PU = U(UâŠ¤U)âˆ’1UâŠ¤then Î£PU = (Î£PU)âŠ¤= PUÎ£. Proof The gradient of R with respect to U is given by âˆ‚R âˆ‚U = âˆ’2(Î£yx âˆ’UVâŠ¤Î£xx)V = 0 =â‡’UVâŠ¤Î£xxV = Î£yxV. (4.17) Therefore, when Î£xx is invertible and V is full column rank, we have U = Î£yxV(VâŠ¤Î£xxV)âˆ’1, (4.18) as claimed in 4.1(i). On the other hand, the gradient of R with respect to V is given by âˆ‚R âˆ‚V = âˆ’2(Î£xy âˆ’Î£xxVUâŠ¤)U = 0 =â‡’Î£xxVUâŠ¤U = Î£xyU. (4.19) Therefore, when Î£xx is invertible and U is full column rank, we have V = Î£âˆ’1 xx Î£xyU(UâŠ¤U)âˆ’1, (4.20) as claimed in 4.1(ii). Moreover, notice that UVâŠ¤= U(UâŠ¤U)âˆ’1UâŠ¤Î£yxÎ£âˆ’1 xx = PUÎ£yxÎ£âˆ’1 xx . (4.21) Combining this with the ï¬rst equation in (4.16) we obtain UVâŠ¤Î£xxVUâŠ¤= Î£yxVUâŠ¤, (4.22) PUÎ£yxÎ£âˆ’1 xx Î£xxÎ£âˆ’1 xx Î£xyPU = Î£yxÎ£âˆ’1 xx Î£xyPU, (4.23) PUÎ£PU = Î£PU. (4.24) As a consequence, Î£PU = (Î£PU)âŠ¤= PUÎ£, as claimed in 4.1(iii). â–¡ 4.3 Optimization Landscape of Linear Networks 209 Baldi and Hornik (1989) used these properties to show that, under certain con- ditions, the expected loss has a unique global minimum (up to an equivalence) and that all other critical points are saddle points. Recently, Nouiehed and Razaviyayn (2018) and Zhu et al. (2020) extended such results to show that all saddle points are strict. Recall that a critical point is a strict saddle if the Hessian evaluated at this point has a strictly negative eigenvalue, indicating that not only it is not a local minimum, but also the objective function has a negative curvature at this point. The following theorem characterizes the landscape of the risk functional for single-hidden-layer linear networks. Theorem 4.2. Assume Î£xx is invertible and Î£ = Î£yxÎ£âˆ’1 xx Î£xy is full rank with n2 dis- tinct eigenvalues Î»1 > Î»2 > Â· Â· Â· > Î»n2. Let Î£ = QÎ›QâŠ¤be the eigendecomposition of Î£, where the columns of Q âˆˆRn2Ã—n2 contain the corresponding eigenvectors. Let QJ denote the submatrix of Q whose columns are indexed by J. Then the following holds. â€¢ If U is full column rank, the set of critical points of R(U,V) is given by U = QJC and V = Î£âˆ’1 xx Î£xyQJCâˆ’âŠ¤, (4.25) where J is an ordered subset of [n2] of cardinality n1, i.e. J âŠ‚[n2] and |J | = n1, and C âˆˆRn1Ã—n1 is an arbitrary invertible matrix. â€¢ If U is full column rank, then critical points with J , [n1] are strict saddles, i.e., the Hessian evaluated at these points has a strictly negative eigenvalue, while critical points with J = [n1] are global minima. Speciï¬cally, the set of global minima (U,V) of the risk R is given by U = Q1: n1C, V = Î£âˆ’1 xx Î£xyQ1: n1Câˆ’âŠ¤, UVâŠ¤= Q1: n1QâŠ¤ 1: n1Î£yxÎ£âˆ’1 xx, (4.26) where C is an arbitrary invertible matrix. â€¢ If U is rank deï¬cient then any critical point is a strict saddle. Proof The proof of part (i) is based on Baldi and Hornik (1989), while the proofs of parts (ii) and (iii) are based on Nouiehed and Razaviyayn (2018) and Zhu et al. (2020). For part (i), note that PQâŠ¤U = QâŠ¤U(UâŠ¤QQâŠ¤U)âˆ’1UâŠ¤Q = QâŠ¤U(UâŠ¤U)âˆ’1UâŠ¤Q = QâŠ¤PUQ, which together with Lemma 4.1(iii) gives PQâŠ¤UÎ› = QâŠ¤PUQÎ›QâŠ¤Q = QâŠ¤PUÎ£Q = QâŠ¤Î£PUQ = Î›PQâŠ¤U. Since Î› is a diagonal matrix with diagonal entries Î»1 > Î»2 > Â· Â· Â· > Î»n2 > 0, 210 Vidal et al: Optimization Landscape of Neural Networks it follows that PQâŠ¤U is also a diagonal matrix. Notice that PQâŠ¤U is an orthogonal projector of rank n1, i.e., it has n1 eigenvalues equal to 1 and n2 âˆ’n1 eigenvalues equal to 0. Therefore PQâŠ¤U = IJIâŠ¤ J, where J âŠ‚[n2] is an ordered subset of [n2] with cardinality |J | = n1. Here we denote by IJ the submatrix of the identity matrix I obtained by keeping only the columns indexed by J. It follows that PU = QPQâŠ¤UQâŠ¤= QIJIâŠ¤ JQâŠ¤= QJQâŠ¤ J, which implies that U and QJ have the same column spaces. Thus, there exists an invertible n1 Ã— n1 matrix C such that U = QJC. Now according to Lemma 4.1(ii), we have V = Î£âˆ’1 xx Î£xyQJCâˆ’âŠ¤. We now prove the ï¬rst statement in part (ii), i.e., for any J , [n1], the corre- sponding critical point has strictly negative curvature. Towards that goal, standard computations give the Hessian quadrature form8 âˆ‡2R(U,V)[âˆ†,âˆ†] = âˆ¥(Uâˆ†âŠ¤ V + âˆ†UVâŠ¤)Î£1/2 xx âˆ¥2 F + 2âŸ¨âˆ†Uâˆ†âŠ¤ V,UVâŠ¤Î£xx âˆ’Î£yxâŸ©. (4.27) for any âˆ†= (âˆ†U,âˆ†V) âˆˆRn2Ã—n1 Ã— Rn0Ã—n1. Since J , [n1], there exists k â‰¤n1 such that k < J. Let J be the largest element of J, and choose âˆ†U = qkeâŠ¤ n1C and âˆ†V = Î£âˆ’1 xx Î£xyqkeâŠ¤ n1Câˆ’âŠ¤, where qk is the kth column of Q and en1 is the n1th standard basis vector of appropriate dimension, i.e. all entries of en1 âˆˆRn1 are zero except for the last entry, which is equal to 1. The ï¬rst term in (4.27) reduces to âˆ¥(Uâˆ†âŠ¤ V + âˆ†UVâŠ¤)Î£1/2 xx âˆ¥2 F = âˆ¥QJen1qâŠ¤ k Î£yxÎ£âˆ’1/2 xx + qkeâŠ¤ n1QâŠ¤ JÎ£yxÎ£âˆ’1/2 xx âˆ¥2 F = âˆ¥qJqâŠ¤ k Î£yxÎ£âˆ’1/2 xx âˆ¥2 F + âˆ¥qkqâŠ¤ J Î£yxÎ£âˆ’1/2 xx âˆ¥2 F â‰¤âˆ¥qâŠ¤ k Î£yxÎ£âˆ’1/2 xx âˆ¥2 2 + âˆ¥qâŠ¤ J Î£yxÎ£âˆ’1/2 xx âˆ¥2 2 = Î»k + Î»J. Similarly, the second term in (4.27) can be computed as âŸ¨âˆ†Uâˆ†âŠ¤ V,UVâŠ¤Î£xx âˆ’Î£yxâŸ©= âŸ¨qkqâŠ¤ k Î£yxÎ£âˆ’1 xx,QJQâŠ¤ JÎ£yx âˆ’Î£yxâŸ© = âˆ’âŸ¨qkqâŠ¤ k Î£yxÎ£âˆ’1 xx,Î£yxâŸ©= âˆ’Î»k. 8 For a scalar function f (W): RmÃ—n â†’R, its Hessian âˆ‡2 f (W) is a 4D tensor, or an (mn) Ã— (mn) matrix if we vectorize the variable W. Alternatively, we can represent the Hessian by a bilinear form deï¬ned via [âˆ‡2 f (W)](A, B) = Ã i, j,k,l âˆ‚2 f (W) âˆ‚Wi j âˆ‚Wkl Ai j Bkl for any A, B âˆˆRmÃ—n. When n = 1, i.e., âˆ‡2 f (W) âˆˆ RmÃ—m and A and B are vectors, this bilinear form reduces to the standard matrix-vector multiplication [âˆ‡2 f (W)](A, B) = AâŠ¤âˆ‡2 f (W)B. Thus, using this bilinear form lets us represent the Hessian in a simple way even when the variable is a matrix. Also, without explicitly computing the eigenvalues of âˆ‡2 f (W), we know that it has a strictly negative eigenvalue if we can ï¬nd a direction âˆ†âˆˆRmÃ—n such that [âˆ‡2 f (W)](âˆ†, âˆ†) < 0. Note that this quadratic form appears naturally in the Taylor expansion f (W + âˆ†) = f (W) + âŸ¨âˆ‡f (W), âˆ†âŸ©+ 1 2 [âˆ‡2 f (W)](âˆ†, âˆ†)+ Â· Â· Â· , which indeed provides a simple but very useful trick for computing [âˆ‡2 f (W)](âˆ†, âˆ†), as long as f (W + âˆ†) can be easily expanded. For example, when f (W) = 1 2 âˆ¥Y âˆ’Wâˆ¥2 F , we have f (W + âˆ†) = 1 2 âˆ¥Y âˆ’W âˆ’âˆ†âˆ¥2 F = 1 2 âˆ¥Y âˆ’Wâˆ¥2 F + âŸ¨W âˆ’Y, âˆ†âŸ©+ 1 2 âˆ¥âˆ†âˆ¥2 F , which implies that [âˆ‡2 f (W)](âˆ†, âˆ†) = âˆ¥âˆ†âˆ¥2 F . 4.3 Optimization Landscape of Linear Networks 211 Therefore, since k â‰¤n1 and J > n1, we have âˆ‡2R(U,V)[âˆ†,âˆ†] = Î»k + Î»J âˆ’2Î»k = Î»J âˆ’Î»k < 0. As a consequence, all critical points with J , [n1] are strict saddles, and all critical points with J = [n1] are global minima. To show part (iii), notice that when rank(U) < n1, there exists a non-zero vector a âˆˆRn1 such that Ua = 0. Since Î£xx and Î£ = Î£yxÎ£âˆ’1 xx Î£xy are assumed to be invertible, Î£xy must have full column rank n2, hence n2 â‰¤n0. Since the rank of UVâŠ¤Î£xx is at most the rank of U and Î£yx has rank n2, we know that UVâŠ¤Î£xxâˆ’Î£yx , 0. Without loss of generality, we assume its (i, j)th entry is non-zero and choose âˆ†U = eiaâŠ¤and âˆ†V = Î±ËœejaâŠ¤, where ei âˆˆRn2 and Ëœej âˆˆRn0 are the standard basis vectors, whose entries are equal to zero except for their ith and jth elements, respectively. With this, we compute the ï¬rst term in (4.27) as âˆ¥(Uâˆ†âŠ¤ V + âˆ†UVâŠ¤)Î£1/2 xx âˆ¥2 F = âˆ¥(Î±UaËœeâŠ¤ j + eiaâŠ¤VâŠ¤)Î£1/2 xx âˆ¥2 F = âˆ¥eiaâŠ¤VâŠ¤Î£1/2 xx âˆ¥2 F and the second term in (4.27) by writing âŸ¨âˆ†Uâˆ†âŠ¤ V,UVâŠ¤Î£xx âˆ’Î£yxâŸ©= âŸ¨Î±âˆ¥aâˆ¥2eiËœeâŠ¤ j ,UVâŠ¤Î£xx âˆ’Î£yxâŸ© = Î±âˆ¥aâˆ¥2(UVâŠ¤Î£xx âˆ’Î£yx)ij, from which it follows that âˆ‡2R(U,V)[âˆ†,âˆ†] = âˆ¥eiaâŠ¤VâŠ¤Î£1/2 xx âˆ¥2 F + 2Î±âˆ¥aâˆ¥2(UVâŠ¤Î£xx âˆ’Î£yx)ij, where the right-hand side can always be made negative by choosing an appropri- ate Î±. Thus, R has negative curvature when U is rank deï¬cient. â–¡ Theorem 4.2 implies that, under certain conditions on the input-output covariance matrices, Î£xx, Î£yx and Î£yy, both the expected and empirical risk of a single-hidden layer linear neural network with the squared loss have no spurious local minima and the saddle points are strict. This benign geometry ensures that a number of local search algorithms (such as gradient descent) converge to a global minimum when training a single-hidden layer linear neural network (Ge et al., 2015; Lee et al., 2019). But what if the conditions in Theorem 4.2 are violated? When Î£xx is invertible but Î£ is rank deï¬cient, a more sophisticated analysis shows that one can still characterize the set of critical points with full column rank U as in (4.25) but with a slightly diï¬€erent form (Zhu et al., 2020). Then, following the sequence of arguments in the proof of Theorem 4.2, one can show that a critical point (U,V) that is not a global minimum has strictly negative curvature, by ï¬nding a direction (which depends on whether U is full column rank) such that the corresponding Hessian quadrature form is strictly negative. 212 Vidal et al: Optimization Landscape of Neural Networks When Î£xx is not invertible, it seems diï¬ƒcult to characterize all the critical points as in (4.25). Nevertheless, by exploiting the local openness property of the risk R(U,V), Nouiehed and Razaviyayn (2018) showed that any local minimum is a global minimum for all possible inputâ€“output covariance matrices, Î£xx, Î£yx, and Î£yy. We summarize these results in the following theorem, but we refer to Nouiehed and Razaviyayn (2018) and Zhu et al. (2020) for the full proof. Theorem 4.3 (Nouiehed and Razaviyayn, 2018; Zhu et al., 2020). Any local mini- mum of R is a global minimum. Moreover, if Î£xx is invertible then any critical point of R that is not a global minimum is a strict saddle point. 4.3.2 Deep Linear Networks with Squared Loss We now extend the analysis of the optimization landscape of single-hidden-layer linear networks, trained using an unregularized squared loss, to deep linear net- works. We consider a network with dimensions n0,n1,. . .,nL, where n0 is the input dimension, nL is the output dimension, n1,. . .,nLâˆ’1 are the hidden-layer dimen- sions, W[l] âˆˆRnlÃ—nlâˆ’1 is the matrix of weights between layers l âˆ’1 and l, and L is the number of weight layers. The hypothesis space F can be parametrized in terms of the network weights W = {W[l]}L l=1 as F = { f âˆˆYX : f (x) = W[L]W[Lâˆ’1] Â· Â· Â· W[1]x, where W[l] âˆˆRnlÃ—nlâˆ’1}. (4.28) Therefore, the problem of minimizing the expected risk becomes min {W[l]}L l=1  R(W)  Ex,y[âˆ¥y âˆ’W[L]W[Lâˆ’1] Â· Â· Â· W[1]xâˆ¥2 2]  . (4.29) Similarly to the single-hidden-layer case in (4.14), the expected risk can be rewritten as R(W) = trace(Î£yy âˆ’2Î£yxWâŠ¤ 1: L + W1: LÎ£xxWâŠ¤ 1: L), (4.30) where WL : 1 = W[L]W[Lâˆ’1] Â· Â· Â· W[1]. As with to the single-hidden-layer case in (4.15), the problem of minimizing the empirical risk min {W[l]}L l=1 h RS(W)  1 m âˆ¥Y âˆ’WL : 1Xâˆ¥2 F i , (4.31) where X and Y are the input and output data matrices, is equivalent to minimiz- ing R(W), except that Î£xx, Î£xy and Î£yy need to be substituted by their empirical estimates 1 mXXâŠ¤, 1 mXYâŠ¤, and 1 mYYâŠ¤, respectively. Thus, the analysis of the op- timization landscape of both the expected risk and the empirical risk can be done by analyzing the landscape of R(W). As discussed in Â§4.3.1, when there is no constraint on WL : 1 (e.g., when the dimensions of the hidden layers is suï¬ƒciently 4.3 Optimization Landscape of Linear Networks 213 large) and Î£xx is invertible, the optimal inputâ€“output weight matrix is given by Wâˆ— L : 1 = Î£yxÎ£âˆ’1 xx. However, this result does not provide a characterization of the optimal weight matrices W[l] for each layer. Thus, the challenge in characteriz- ing the landscape of R(W) is to understand the eï¬€ect of the low-rank constraint nl â‰¤min{n0,nL} or to consider the possibility that critical points for W[l] might not be low-rank. Recent work by Kawaguchi (2016) provided a formal analysis of the optimization landscape of R(W). In particular, by using a purely deterministic approach that exploits both ï¬rst-order and second-order information at critical points (as also used in the proof of Theorem 4.2), Kawaguchi characterized the following properties of critical points of R(W). Theorem 4.4 (Kawaguchi, 2016). Assume that Î£xx and Î£xy are of full rank with nL â‰¤n0 and that Î£ = Î£yxÎ£âˆ’1 xx Î£xy is of full rank with nL distinct eigenvalues. Then R(W) has the following properties: â€¢ Any local minimum is a global minimum. â€¢ Every critical point that is not a global minimum is a saddle point. â€¢ A saddle point W such that rank(W[Lâˆ’1] Â· Â· Â· W[2]) = min1â‰¤lâ‰¤Lâˆ’1 nl is strict, i.e., the Hessian of R at W has a strictly negative eigenvalue. â€¢ A saddle point W such that rank(W[Lâˆ’1] Â· Â· Â· W[2]) < min1â‰¤lâ‰¤Lâˆ’1 nl may not be strict, i.e., the Hessian at W may not have any negative eigenvalues. On the one hand, similarly to Theorem 4.2 for one-hidden-layer linear networks, Theorem 4.4 guarantees that, under similar conditions on Î£xx and Î£xy, any local minimum of the risk is a global minimum. On the other hand, unlike the results for single-hidden-layer linear networks where every saddle point is strict, Theorem 4.4 shows that networks with two or more hidden layers can have â€œbadâ€ (non-strict) saddle points, which are also referred to as degenerate saddle points or higher-order saddle points, since the ï¬rst- and second-order derivatives cannot distinguish them from local optima. To illustrate why depth introduces degenerate saddle points, consider the simplest case where L = 3 and n0 = n1 = n2 = n3 = 1. The risk then becomes R(w[1],w[2],w[3]) = Ïƒyy âˆ’2Ïƒyxw[1]w[2]w[3] + Ïƒxx(w[1]w[2]w[3])2. (4.32) By computing the gradient (ï¬rst-order derivatives) and Hessian (second-order derivatives), it is easy to see that (0,0,0) is a critical point but the Hessian is the zero matrix, which has no negative eigenvalues. This also holds true for general deep linear networks. Intuitively, when the network has more layers, the objective function tends to be ï¬‚atter at the origin, making the origin a higher-order saddle. This is similar to the fact that 0 is a critical point of both the functions (1 âˆ’u2)2 and 214 Vidal et al: Optimization Landscape of Neural Networks (1 âˆ’u3)2: the former has a negative second-order derivative at 0, while the latter has a zero second-order derivative at 0. We end our discussion of deep linear networks by noting that there is recent work that improves upon Theorem 4.4, mostly with weaker conditions to guarantee the absence of spurious local minima. For example, to show that any local minimum is a global minimum, Lu and Kawaguchi (2017) require Î£xx and Î£xy to be of full rank, while Laurent and Brecht (2018) require only that the size of the hidden layers be larger than or equal to the input or output dimensions, i.e., n1,. . .,nLâˆ’1 â‰¥ min{n0,nL}, which could potentially help guide the design of network architectures. 4.4 Optimization Landscape of Nonlinear Networks In this section, we review work by Haeï¬€ele and Vidal (2015, 2017) on the analysis of the landscape of a class of nonlinear networks with positively homogeneous activation functions, such as rectiï¬ed linear units (ReLU), max-pooling, etc. Critical to the analysis tools that are employed is to consider networks regularized by a function that is also positively homogeneous and of the same degree as the network mapping. These results apply to a class of deep networks whose output is formed as the sum of the outputs of multiple positively homogeneous subnetworks connected in parallel (see the right-hand panel in Figure 4.3), where the architecture of a subnetwork can be arbitrary provided that the overall mapping of the subnetwork is a positively homogeneous function of the network parameters. Speciï¬cally, we show that, when the network is suï¬ƒciently wide, a path to a global minimizer always exists from any initialization (i.e., local minima which require one to increase the objective in order to escape are guaranteed not to exist). As a motivating example, before considering the case of deep positively homoge- neous networks, in Â§4.4.1 we revisit the case of shallow linear networks discussed in Â§4.3.1, as this simple case conveys the key insights behind the more general cases discussed in Â§4.4.2. The primary diï¬€erence from the case of shallow linear networks discussed in Â§4.3.1 is that, rather than ï¬xing the number of columns in (U,V) a priori, we constrain the hypothesis space using Tykhonov regularization on (U,V) while allowing the number of columns in (U,V) to be variable. As we will see, the Tykhonov regularization results in the promotion of low-rank solutions even though we do not place an explicit constraint on the number of columns in (U,V). The extension of these results to deep positively homogeneous networks will highlight the importance of using similar explicit regularization to constrain the overall size of the network. 4.4 Optimization Landscape of Nonlinear Networks 215 0 ReLU Network with One Hidden Layer Multilayer Parallel Network Figure 4.3 Example networks. (Left panel) ReLU network with a single hidden layer with the mapping Î¦n1 described by the equation in (4.45) with (n1 = 4). Each color corresponds to one element of the elemental mapping Ï†(X, W[1] i , W[2] i ). The colored hidden units have rectifying nonlinearities, while the black units are linear. (Right panel) Multilayer ReLU network with three fully connected parallel subnetworks (r = 3), where each color corresponds to the subnetwork described by the elemental mapping Ï†(X, W[1] i , W[2] i , W[3] i , W[4] i ). Â© 2017 IEEE. Reprinted, with permission, from Haeï¬€ele and Vidal (2017). 4.4.1 Motivating Example Consider the empirical risk minimization problem in (4.15) for a single-hidden- layer linear network with n0 inputs, n1 hidden neurons, and n2 outputs, which is equivalent to min U,V 1 2 âˆ¥Y âˆ’UVâŠ¤Xâˆ¥2 F, (4.33) where U âˆˆRn2Ã—n1 and V âˆˆRn0Ã—n1 are the network weights. We showed in Â§4.3.1 that, under certain conditions, this problem has no spurious local minima and all saddle points are strict. In particular, we assumed that the number of hidden neurons is ï¬xed and limited by the input and output dimensions, speciï¬cally, n1 â‰¤ min{n0,n2}. In what follows, we relax this constraint on the network size and optimize over both the network size and weights. Arguably, this requires some form of regular- ization on the network weights that allows us to control the growth of the network size. A commonly used regularizer is weight decay, Î˜(U,V) = âˆ¥Uâˆ¥2 F + âˆ¥Vâˆ¥2 F, also known as Tykhonov regularization. Here, we use weight decay to regularize (4.33) in the particular case where X = I for simplicity of presentation,9 min n1âˆˆN+ min UâˆˆRn2Ã—n1 VâˆˆRn0Ã—n1 h1 2 âˆ¥Y âˆ’UVâŠ¤âˆ¥2 F + Î» 2(âˆ¥Uâˆ¥2 F + âˆ¥Vâˆ¥2 F) i , (4.34) where Î» > 0 is a regularization parameter. 9 Note that with this simpliï¬cation the problem becomes an unsupervised learning problem (matrix factorization) instead of the original supervised learning problem (linear network training). 216 Vidal et al: Optimization Landscape of Neural Networks There are several reasons for considering this particular case. First, (4.34) can be understood as a matrix factorization problem where, given a matrix Y âˆˆRn2Ã—n0, the goal is to factorize it as Y â‰ˆUVâŠ¤, where U âˆˆRn2Ã—n1 and V âˆˆRn0Ã—n1. Second, it is known that weight decay is closely connected to the nuclear norm of the product of the factorized matrices Z = UVâŠ¤; recall that the nuclear norm âˆ¥Zâˆ¥âˆ—is the sum of the singular values of a matrix Z, from the so-called variational form of the nuclear norm (Srebro et al., 2004): âˆ¥Zâˆ¥âˆ—= min n1âˆˆN+ min U,V: UVâŠ¤=Z 1 2(âˆ¥Uâˆ¥2 F + âˆ¥Vâˆ¥2 F), (4.35) where the above equation states that, given a matrix Z, if one considers all possible factorizations of Z into UVâŠ¤then a factorization Z = UVâŠ¤which minimizes the Tykhonov regularization on (U,V) will be equal to the nuclear norm of Z. Third, recall that the nuclear norm is a convex relaxation of the rank of the matrix which is known to encourage low-rank solutions. As a result, this allows us to control the network width, n1, and ensure capacity control via matrix factorization techniques without placing explicit constraints on n1. Indeed, owing to the variational deï¬nition of the nuclear norm in (4.35), the objective problem in (4.34) is closely related to a convex optimization problem with nuclear norm regularization: min Z h1 2 âˆ¥Y âˆ’Zâˆ¥2 F + Î»âˆ¥Zâˆ¥âˆ— i . (4.36) The strong similarity between (4.35) and the regularizer in (4.34) suggest we look at the convex problem in (4.36), whose solution can be found in closed form from the SVD of Y. Speciï¬cally, if Y = UYÎ£YVâŠ¤ Y is the SVD of Y then the global minimizer of (4.36) is given by the singular value thresholding operator Z = DÎ»(Y) = UY(Î£Y âˆ’Î»I)+VâŠ¤ Y, where the singular vectors of Y (the columns of UY and VY) are maintained while the singular values of Y (the diagonal entries of Î£Y) are shrunk by Î» and then thresholded at zero, i.e., a+ = max(a,0). But how do these results for the convex problem in (4.36) relate to solutions to the non-convex problem in (4.34)? First, observe that (4.35) implies that the convex problem provides a global lower bound for the non-convex problem. Speciï¬cally, for any (U,V,Z) such that Z = UVâŠ¤, (4.35) implies that 1 2 âˆ¥Y âˆ’Zâˆ¥2 F + Î»âˆ¥Zâˆ¥âˆ—â‰¤1 2 âˆ¥Y âˆ’UVâŠ¤âˆ¥2 F + Î» 2(âˆ¥Uâˆ¥2 F + âˆ¥V2âˆ¥2 F). (4.37) Additionally, this lower bound is always tight once n1 becomes suï¬ƒciently large, in the sense that for any Z one can ï¬nd a (U,V) such that Z = UVâŠ¤and the inequality above becomes an equality. As a result, a global minimum (U,V) of the non-convex problem (4.34) gives a global minimum Z = UVâŠ¤for the convex problem (4.36), 4.4 Optimization Landscape of Nonlinear Networks 217 and owing to the global lower bound in (4.37), this further implies that we have a global minimum of both the convex and non-convex problems, as we show next. Theorem 4.5. Let ui and vi be the ith columns of U and V, respectively. If (U,V,n1) is a local minimum of (4.34) such that for some i âˆˆ[n1] we have ui = 0 and vi = 0 then (i) Z = UVâŠ¤is a global minimum of (4.36) and (ii) (U,V,n1) is a global minimum of (4.34) and (4.35). Proof Recall that the Fenchel conjugate of a function â„¦is deï¬ned as â„¦âˆ—(Q) = supZâŸ¨Q,ZâŸ©âˆ’â„¦(Z), leading to Fenchelâ€™s inequality âŸ¨Q,ZâŸ©â‰¤â„¦(Z) + â„¦âˆ—(Q). Also recall that the subgradient of a convex function â„¦at Z is deï¬ned as âˆ‚â„¦(Z) = {Q: â„¦(Z) â‰¥â„¦(Z) + âŸ¨Q,Z âˆ’ZâŸ©, for all Z} = {Q: âŸ¨Q,ZâŸ©= â„¦(Z) + â„¦âˆ—(Q)}. Applying this to the nuclear norm â„¦(Z) = âˆ¥Zâˆ¥âˆ—and using (4.35), we obtain â„¦âˆ—(Q) = sup Z âŸ¨Q,ZâŸ©âˆ’âˆ¥Zâˆ¥âˆ—= sup n1âˆˆN+ sup ËœU, ËœV âŸ¨Q, ËœU ËœVâŠ¤âŸ©âˆ’1 2(âˆ¥ËœUâˆ¥2 F + âˆ¥ËœVâˆ¥2 F) = sup n1âˆˆN+ sup ËœU, ËœV n1 Ã• i=1  ËœuâŠ¤ i QËœvi âˆ’1 2(âˆ¥Ëœuiâˆ¥2 2 + âˆ¥Ëœviâˆ¥2 2)  = ( 0 if uâŠ¤Qv â‰¤1 2(âˆ¥uâˆ¥2 2 + âˆ¥vâˆ¥2 2) for all (u,v) âˆ else, (4.38) which then implies that âˆ‚âˆ¥Zâˆ¥âˆ—=  Q: âŸ¨Q,ZâŸ©= âˆ¥Zâˆ¥âˆ—and uâŠ¤Qv â‰¤1 2(âˆ¥uâˆ¥2 2 + âˆ¥vâˆ¥2 2) for all(u,v) . (4.39) To prove (i), we need to show that 0 âˆˆZ âˆ’Y + Î»âˆ‚âˆ¥Zâˆ¥âˆ—or Y âˆ’Z âˆˆÎ»âˆ‚âˆ¥Zâˆ¥âˆ—. Let us ï¬rst show that Q = Yâˆ’Z Î» satisï¬es the inequality in (4.39). Assume, without loss of generality, that the last columns of U and V are zero, choose any u âˆˆRn2, v âˆˆRn0, and Ïµ > 0, and let UÏµ = U + Ïµ1/2ueâŠ¤ n1 and VÏµ = V + Ïµ1/2veâŠ¤ n1, so that ZÏµ = UÏµVâŠ¤ Ïµ = UVâŠ¤+ ÏµuvâŠ¤= Z + ÏµuvâŠ¤. Since (U,V,n1) is a local minimum of (4.34), for all (u,v) there exists Î´ > 0 such that for all Ïµ âˆˆ(0,Î´) we have: 1 2 âˆ¥Y âˆ’ZÏµ âˆ¥2 F + Î» 2(âˆ¥UÏµ âˆ¥2 F + âˆ¥VÏµ âˆ¥2 F) âˆ’1 2 âˆ¥Y âˆ’Zâˆ¥2 F âˆ’Î» 2(âˆ¥Uâˆ¥2 F + âˆ¥Vâˆ¥2 F) â‰¥0, 1 2(âˆ’2âŸ¨Y,ZÏµ âˆ’ZâŸ©+ âŸ¨ZÏµ + Z,ZÏµ âˆ’ZâŸ©) + Î» 2(âˆ¥UÏµ âˆ¥2 F âˆ’âˆ¥Uâˆ¥2 F + âˆ¥VÏµ âˆ¥2 F âˆ’âˆ¥Vâˆ¥2 F) â‰¥0, Ïµ 2(âˆ’2âŸ¨Y,uvâŠ¤âŸ©+ âŸ¨2Z + ÏµuvâŠ¤,uvâŠ¤âŸ©) + Î»Ïµ 2 (âˆ¥uâˆ¥2 2 + âˆ¥vâˆ¥2 2) â‰¥0, uâŠ¤(Z âˆ’Y)v + Ïµ 2 âˆ¥uâˆ¥2 2âˆ¥vâˆ¥2 2 + Î» 2(âˆ¥uâˆ¥2 2 + âˆ¥vâˆ¥2 2) â‰¥0. 218 Vidal et al: Optimization Landscape of Neural Networks Letting Ïµ â†˜0, gives uâŠ¤(Yâˆ’Z) Î» v â‰¤1 2(âˆ¥uâˆ¥2 2 + âˆ¥vâˆ¥2 2), for all (u,v) as claimed. Let us now demonstrate that Q = Yâˆ’Z Î» satisï¬es the equality in (4.39). Because the inequality in (4.39) holds, we know that â„¦âˆ—(Q) = 0, which together with Fenchelâ€™s inequality gives âŸ¨Q,ZâŸ©â‰¤âˆ¥Zâˆ¥âˆ—. Then, since Z = UVâŠ¤, it follows from (4.35) that âˆ¥Zâˆ¥âˆ—â‰¤1 2(âˆ¥Uâˆ¥2 F + âˆ¥Vâˆ¥2 F). Therefore, to show that âŸ¨Q,ZâŸ©= âˆ¥Zâˆ¥âˆ—it suï¬ƒces to show that âŸ¨Q,ZâŸ©= 1 2(âˆ¥Uâˆ¥2 F + âˆ¥Vâˆ¥2 F). For this particular problem it is possible to prove that this equality holds simply by considering the ï¬rst-order optimality conditions, which must be satisï¬ed since (U,V,n1) is a local minimum: âˆ’(Y âˆ’UVâŠ¤)V + Î»U = 0 and âˆ’(Y âˆ’UVâŠ¤)âŠ¤U + Î»V = 0. (4.40) It follows that UâŠ¤(Y âˆ’UVâŠ¤)V = Î»UâŠ¤U and UâŠ¤(Y âˆ’UVâŠ¤)VâŠ¤= Î»VâŠ¤V. (4.41) Summing, taking the trace, and dividing by Î» gives the desired result: âŸ¨Yâˆ’UVâŠ¤ Î» ,UVâŠ¤âŸ©= 1 2(âˆ¥Uâˆ¥2 F + âˆ¥Vâˆ¥2 F) = âˆ¥Zâˆ¥âˆ—. (4.42) As a consequence, W âˆˆâˆ‚âˆ¥Zâˆ¥âˆ—and hence Z = UVâŠ¤is a global minimum of the convex problem in (4.36), thus concluding the proof of (i). As an alternative to the above approach, to develop an intuition for more general results, we will also provide a diï¬€erent proof of the equality in (4.42) without relying on the objective being diï¬€erentiable with respect to (U,V) but only requiring the loss function to be diï¬€erentiable with respect to Z = UVâŠ¤. In particular, let UÏ„ = (1 + Ï„)1/2U, VÏ„ = (1 + Ï„)1/2V, and ZÏ„ = UÏ„VâŠ¤ Ï„ = (1 + Ï„)UVâŠ¤= (1 + Ï„)Z. Again, since (U,V,n1) is a local minimum for Ï„ > 0 suï¬ƒciently small we have: 1 2 âˆ¥Y âˆ’ZÏ„âˆ¥2 F + Î» 2(âˆ¥UÏ„âˆ¥2 F + âˆ¥VÏ„âˆ¥2 F) âˆ’1 2 âˆ¥Y âˆ’Zâˆ¥2 F âˆ’Î» 2(âˆ¥Uâˆ¥2 F + âˆ¥Vâˆ¥2 F) â‰¥0, 1 2 âˆ¥Y âˆ’(1 + Ï„)Zâˆ¥2 F âˆ’1 2 âˆ¥Y âˆ’Zâˆ¥2 F + Î» 2(Ï„âˆ¥Uâˆ¥2 F + Ï„âˆ¥Vâˆ¥2 F) â‰¥0, 1 Ï„ 1 2 âˆ¥Y âˆ’Z âˆ’Ï„Zâˆ¥2 F âˆ’1 2 âˆ¥Y âˆ’Zâˆ¥2 F  â‰¥âˆ’Î» 2(âˆ¥Uâˆ¥2 F + âˆ¥Vâˆ¥2 F). Taking the limit Ï„ â†˜0 (recalling that the above limit on the left-hand side is the directional derivative of the loss in the direction Z) gives: âŸ¨Z âˆ’Y,ZâŸ©â‰¥âˆ’Î» 2(âˆ¥Uâˆ¥2 F + âˆ¥Vâˆ¥2 F) =â‡’âŸ¨Yâˆ’UVâŠ¤ Î» ,UVâŠ¤âŸ©â‰¤1 2(âˆ¥Uâˆ¥2 F + âˆ¥Vâˆ¥2 F). If we let UÏ„ = (1 âˆ’Ï„)1/2U, VÏ„ = (1 âˆ’Ï„)1/2V, and ZÏ„ = UÏ„V âŠ¤ Ï„ = (1 âˆ’Ï„)Z then, 4.4 Optimization Landscape of Nonlinear Networks 219 by repeating an identical set of arguments to those above, we obtain the opposite inequality: 1 2 âˆ¥Y âˆ’ZÏ„âˆ¥2 F + Î» 2(âˆ¥UÏ„âˆ¥2 F + âˆ¥VÏ„âˆ¥2 F) âˆ’1 2 âˆ¥Y âˆ’Zâˆ¥2 F âˆ’Î» 2(âˆ¥Uâˆ¥2 F + âˆ¥Vâˆ¥2 F) â‰¥0, 1 2 âˆ¥Y âˆ’(1 âˆ’Ï„)Zâˆ¥2 F âˆ’1 2 âˆ¥Y âˆ’Zâˆ¥2 F âˆ’Î» 2(Ï„âˆ¥Uâˆ¥2 F + Ï„âˆ¥Vâˆ¥2 F) â‰¥0, 1 Ï„ 1 2 âˆ¥Y âˆ’Z + Ï„Zâˆ¥2 F âˆ’1 2 âˆ¥Y âˆ’Zâˆ¥2 F  â‰¥Î» 2(âˆ¥Uâˆ¥2 F + âˆ¥Vâˆ¥2 F). Taking the limit Ï„ â†˜0 implies âŸ¨Z âˆ’Y,âˆ’ZâŸ©â‰¥Î» 2(âˆ¥Uâˆ¥2 F + âˆ¥Vâˆ¥2 F) =â‡’âŸ¨Yâˆ’UVâŠ¤ Î» ,UVâŠ¤âŸ©â‰¥1 2(âˆ¥Uâˆ¥2 F + âˆ¥Vâˆ¥2 F). Thus we have again shown that (4.42) must be true, without relying on the diï¬€er- entiability of the objective with respect to (U,V), but only on the diï¬€erentiablity of the loss function with respect to Z when we take the limits as Ï„ â†˜0. Finally, to see that claim (ii) is true, observe that the equality on the right-hand side of (4.42) implies that (U,V,n1) is an optimal factorization, i.e. a global minimum of (4.35). Finally, since the convex problem in (4.36) is a global lower bound for the non-convex problem in (4.34) and since Z = UVâŠ¤is a global minimum of the convex problem, it follows that (U,V,n1) must be a global minimum of the non-convex problem. â–¡ In summary, we have shown that the non-convex matrix-factorization problem in (U,V) admits a global lower bound in the product space Z = UVâŠ¤. Moreover, the lower bound is a convex function of Z and the global minima agree, i.e. if (U,V,n1) is a global minimum of the non-convex problem then UVâŠ¤is a global minimum of the convex problem. In addition, Theorem 4.5 provides a characterization of the local minima of the non-convex problem which are also global: they are the local minima with one column of U and the corresponding column of V equal to zero. Such a statement can be easily extended to local minima (U,V,n1) that are rank deï¬cient, i.e. there exists e , 0 such that Ue = 0 and Ve = 0, since the only part of the proof that depends on columns of U and V being zero is the deï¬nition of UÏµ and VÏµ, which can be readily replaced by UÏµ = U + Ïµ1/2ueâŠ¤and VÏµ = V + Ïµ1/2veâŠ¤ with âˆ¥eâˆ¥2 = 1. In addition, observe that the proof of Theorem 4.5 relies only on the following necessary and suï¬ƒcient conditions for the global optimality of any (U,V,n1). Corollary 4.6. (U,V,n1) is a global minimum of (4.34) if and only if it satisï¬es the following conditions: (i) âŸ¨Y âˆ’UVâŠ¤,UVâŠ¤âŸ©= Î» 2(âˆ¥Uâˆ¥2 F + âˆ¥Vâˆ¥2 F); 220 Vidal et al: Optimization Landscape of Neural Networks (ii) uâŠ¤(Y âˆ’UVâŠ¤)v â‰¤Î» 2(âˆ¥uâˆ¥2 2 + âˆ¥vâˆ¥2 2) for all (u,v). Recall that the global minimum of the convex problem (4.36) is given by the singular value thresholding of Y, Z = DÎ»(Y) = UY(Î£Y âˆ’Î»I)+VâŠ¤ Y, where Y = UYÎ£YVâŠ¤ Y is the SVD of Y. It follows that a global minimum of (4.34) can be obtained as U = UY(Î£Y âˆ’Î»I)1/2 + and V = VY(Î£Y âˆ’Î»I)1/2 + . In practice, while a globally optimal solution to (4.36) can be found using linear algebraic techniques, computing the SVD of Y is highly ineï¬ƒcient for large matrices Y. Therefore, we may still be interested in solving (4.36) using, e.g., gradient descent. In this case, we may need to use Corollary 4.6 to check that a global minimum has been found. Observe from the proof of Theorem 4.5 that condition (i) is satisï¬ed by any ï¬rst-order point and that optimization methods are often guaranteed to converge to ï¬rst-order points. Therefore, condition (ii) is the important one to check. It can be shown that that condition is equivalent to Ïƒmax(Yâˆ’UVâŠ¤) â‰¤Î», which involves computing only the largest singular value of a matrix. But what if condition (ii) is violated? In this case, one might wonder whether condition (ii) may be used to escape the non-global local minimum. Indeed, if condition (ii) is violated, then there exists (u,v) such that uâŠ¤(Y âˆ’UVâŠ¤)v > Î» 2(âˆ¥uâˆ¥2 2 + âˆ¥vâˆ¥2 2). Then it follows from the proof of Theorem 4.5 that, if we choose UÏµ = [U Ïµ1/2u] and VÏµ = [V Ïµ1/2v] for Ïµ small enough, then we can reduce the objective problem. This suggest an algorithm for minimizing (4.34) which consists of the following two steps. (i) For a ï¬xed n1, use a local descent strategy to minimize (4.34) with respect to U and V until convergence to a ï¬rst-order point occurs. (ii) Check whether condition (ii) is satisï¬ed, which is equivalent to solving the following optimization problem (called the polar problem): max u,v uâŠ¤(Y âˆ’UVâŠ¤)v âˆ¥uâˆ¥2 2 + âˆ¥vâˆ¥2 2 â‰¤Î» 2 â‡â‡’ max u,v uâŠ¤(Y âˆ’UVâŠ¤)v âˆ¥uâˆ¥2âˆ¥vâˆ¥2 â‰¤Î». (4.43) If the condition holds then a global minimum has been found. If not, let (u,v) be a solution to the polar problem,10 augment U and V with one additional column to give UÏµ = [U Ïµ1/2u] and VÏµ = [V Ïµ1/2v] for some Ïµ > 0, and go to (i). We refer the reader to Haeï¬€ele and Vidal (2019) for a more precise and detailed description of this meta-algorithm. 10 Note that a solution to the polar problem is given by the left and right singular vectors of Y âˆ’UVâŠ¤associated with its largest singular value. 4.4 Optimization Landscape of Nonlinear Networks 221 4.4.2 Positively Homogeneous Networks The above discussion on matrix factorization can be extended to neural networks with one hidden layer by adjusting the deï¬nitions of the maps Î¦ and Î˜ appropriately. In the above matrix-factorization example (returning to the use of W to notate the model parameters), Î¦ and Î˜ can be rewritten as Î¦(X,W[1],W[2]) = W[2](W[1])âŠ¤= n1 Ã• i=1 w[2] i (w[1] i )âŠ¤, and Î˜(W[1],W[2]) = 1 2(âˆ¥W[1]âˆ¥2 F + âˆ¥W[2]âˆ¥2 F) = n1 Ã• i=1 1 2(âˆ¥w[1] i âˆ¥2 2 + âˆ¥w[2] i âˆ¥2 2), (4.44) where w[1] i and w[2] i are the ith columns of W[1] and W[2], respectively. A key observation is that the map Î¦ and regularization Î˜ decompose as sums of functions over the columns of W[1] and W[2]. Further, these functions are both positively homogeneous11 with degree 2. Turning to single-hidden-layer neural networks, if we again let n1 denote the number of neurons in the hidden layer this motivates the following more general deï¬nitions for Î¦ and Î˜: Î¦n1(X,W[1],W[2]) = n1 Ã• i=1 Ï†(X,w[1] i ,w[2] i ), and Î˜n1(W[1],W[2]) = n1 Ã• i=1 Î¸(w[1] i ,w[2] i ), (4.45) where Ï†(X,w[1],w[2]) and Î¸(w[1],w[2]) are functions which are both positively ho- mogeneous and of the same degree p > 0 with respect to (w[1],w[2]). Clearly, Ï†(X,w[1],w[2]) = w[1](w[2])âŠ¤and Î¸(w[1],w[2]) = 1 2(âˆ¥w[1]âˆ¥2 2 + âˆ¥w[2]âˆ¥2 2) satisfy this property, with p = 2. But notice that it is also satisï¬ed, for example, by the map Ï†(X,w[1],w[2]) = w[2]ReLU((w1)âŠ¤X), where we recall that ReLU(z) = max(z,0) is a ReLU applied to each entry of (w[1])âŠ¤X. The fundamental observation is that both linear transformations and ReLU nonlinearities12 are positively homogeneous functions, and so the composition of such functions is also positively homogeneous. With these deï¬nitions, it is easy to see that the output of a two-layer neural network with ReLU nonlinearity on the hidden units, such as the network illustrated in the left-hand panel of Figure 4.3, can be expressed by the map Î¦ in (4.45). This same approach can be generalized beyond single-hidden-layer networks to 11 Recall that a function f is said to be positively homogeneous with degree-p if, for all Î± â‰¥0 one has f (Î±x) = Î±p f (x). 12 Notice that many other neural network operators such as max-pooling, leaky ReLUs, ones that raise to a polynomial power, and convolution are also positively homogeneous. 222 Vidal et al: Optimization Landscape of Neural Networks the more general multi-layer parallel network shown in the right-hand panel of Figure 4.3, by considering more general Ï† and Î¸ functions. In particular, we deï¬ne the mapping of the multi-layer parallel network and its corresponding regularization function as the sum of the corresponding mappings and regularization functions for r parallel subnetworks with identical architectures but possibly diï¬€erent weights. Speciï¬cally, we deï¬ne the mapping of the multi-layer parallel network and its regularization function as Î¦r(X,W[1],. . .,W[L]) = rÃ• i=1 Ï†(X,W[1] i ,. . .,W[L] i ), and Î˜r(W[1],. . .,W[L]) = rÃ• i=1 Î¸(W[1] i ,. . .,W[L] i ), (4.46) where W[l] i denotes the weight parameters for the lth layer of the ith subnetwork, W[l] = {W[l] i }r i=1 is the set of weight parameters for the lth layer of all r subnet- works, and the network mapping Ï†(X,W[1] i ,. . .,W[L] i ) and regularization function Î¸(W[1] i ,. . .,W[L] i ) are positively homogeneous functions of degree p > 0 on the weights of the ith subnetwork (W[1] i ,. . .,W[L] i ).13 Therefore we can write the train- ing problem for a network which consists of the sum of parallel subnetworks (where we also search over the number of subnetworks, r) as min r âˆˆN+ min W[1],...,W[L]  L(Y,Î¦r(X,W[1],. . .,W[L])) + Î»Î˜r(W[1],. . .,W[L]). (4.47) Note that this problem is typically non-convex owing to the mapping Î¦r, regardless of the choice of loss and regularization functions, L and Î˜, respectively. Therefore, to analyze this non-convex problem, we deï¬ne a generalization of the variational form of the nuclear norm in (4.35) for neural networks which consist of the sum of parallel subnetworks as: â„¦Ï†,Î¸(Z) = min r âˆˆN+ min W[1],...,W[L] : Î¦r (X,W[1],...,W[L])=Z Î˜r(W[1],. . .,W[L]), (4.48) with the additional condition that â„¦Ï†,Î¸(Z) = âˆif Î¦r(X,W[1],. . .,W[L]) , Z for all (W[1],. . .,W[L],r). The intuition behind the above problem is that, given an output Z generated by the network for some input X, we wish to ï¬nd the number of subnetworks (or the number of hidden units in the single-hidden-layer case) and weights (W[1],. . .,W[L]) that produce the output Z. Then, among all possible choices of sizes and weights, we prefer those that minimize Î˜r(W[1],. . .,W[L]). Note that the function â„¦Ï†,Î¸ is completely speciï¬ed once one chooses the functions 13 Note that Î¸ could additionally depend on X, but we omit that possibility here for notational simplicity. 4.4 Optimization Landscape of Nonlinear Networks 223 Ï† and Î¸ in (4.46), so for â„¦Ï†,Î¸ to be well-posed, Ï† and Î¸ must satisfy the following conditions.14 Deï¬nition 4.7. We will say that (Ï†,Î¸) is a nondegenerate pair if for any set of weights for one subnetwork (W [1],. . .,W [L]) the functions satisfy the following three conditions: (i) Both Ï† and Î¸ are positively homogeneous functions of the weights with the same degree p > 0: Ï†(X,Î±W [1],. . .,Î±W [L]) = Î±pÏ†(X,W [1],. . .,W [L]) and Î¸(Î±W [1],. . .,Î±W [L]) = Î±pÎ¸(W [1],. . .,W [L]) for all Î± â‰¥0. (ii) Î¸ is positive semideï¬nite: Î¸(W [1],. . .,W [L]) â‰¥0. (iii) The set {Ï†(X,W [1],. . .,W [L]): Î¸(W [1],. . .,W [L]) â‰¤1} is compact. As a concrete example, choosing Ï†(X,w[1],w[2]) = w[2]ReLU((w[1])âŠ¤X) as above and Î¸(w[1],w[2]) = 1 2(âˆ¥w[1]âˆ¥2 2 + âˆ¥w[2]âˆ¥2 2) satisï¬es the above requirements and cor- responds to a single-hidden-layer fully connected network (as Î¦n1(X,W[1],W[2]) = W[2]ReLU((W[1])âŠ¤X)) with â„“2 weight decay on the parameters. From these prelim- inaries, one can show that â„¦Ï†,Î¸ satisï¬es the following properties. Proposition 4.8 (Haeï¬€ele and Vidal, 2015, 2017). Given a nondegenerate pair of functions (Ï†,Î¸) then â„¦Ï†,Î¸(Z) has the following properties: (i) It is positive deï¬nite: â„¦Ï†,Î¸(Z) > 0 for all Z , 0, and â„¦Ï†,Î¸(0) = 0. (ii) It is positively homogeneous with degree 1: â„¦Ï†,Î¸(Î±Z) = Î±â„¦Ï†,Î¸(Z), for all Î± â‰¥0 and Z. (iii) It satisï¬es the triangle inequality: â„¦Ï†,Î¸(Q + Z) â‰¤â„¦Ï†,Î¸(Q) + â„¦Ï†,Î¸(Z), for all Q and Z. (iv) It is convex with respect to Z. (v) The inï¬mum in (4.48) can be achieved with r â‰¤card(Z), for all Z. Further, if, for any choice of weights for one subnetwork W [1],. . .,W [L], there exists a vector s = {âˆ’1,1}L such that Ï† X, s1W [1],. . ., sLW [L] = âˆ’Ï† X,W [1],. . .,W [L] and Î¸ s1W [1],. . ., sLW [L] = Î¸ W [1],. . .,W [L] then â„¦Ï†,Î¸(Z) is also a norm on Z. Note that regardless of whether â„¦Ï†,Î¸ is a norm or not, we always have that â„¦Ï†,Î¸(Z) is convex on Z. Therefore, if the loss L is convex on Z, so is the problem min Z L(Y,Z) + Î»â„¦Ï†,Î¸(Z). (4.49) 14 The ï¬rst two conditions are typically easy to verify, while the third condition is needed to avoid trivial situations such as â„¦Ï†,Î¸(Z) = 0, for all Z. 224 Vidal et al: Optimization Landscape of Neural Networks Further, just as in the previous matrix factorization example, we have that for any (Z,W[1],. . .,W[L]) such that Z = Î¦r(X,W[1],. . .,W[L]) the following global lower bound exists: L(Y,Z) + Î»â„¦Ï†,Î¸(Z) â‰¤L(Y,Î¦r(X,W[1],. . .,W[L])) + Î»Î˜r(W[1],. . .,W[L]), (4.50) and the lower bound is tight in the sense that for any Z such that â„¦Ï†,Î¸(Z) , âˆ there exists (W[1],. . .,W[L],r) such that Z = Î¦r(X,W[1],. . .,W[L]) and the above inequality becomes an equality. As a result, using a very similar analysis as that used to prove Theorem 4.5, one can show the following result. Theorem 4.9 (Haeï¬€ele and Vidal, 2015, 2017). Given a nondegenerate pair of functions (Ï†,Î¸), if (W[1],. . .,W[L],r) is a local minimum of (4.47) such that for some i âˆˆ[r] we have W[1] i = 0,. . .,W[L] i = 0, then (i) Z = Î¦r(X,W[1],. . .,W[L]) is a global minimum of (4.49) and (ii) (W[1],. . .,W[L],r) is a global minimum of (4.47) and (4.48). We refer to Haeï¬€ele and Vidal (2015, 2017) for the full proof, but it closely follows the sequences of arguments from the proof of Theorem 4.5. In particular, we note that the key property which is needed to generalize the proof of Theorem 4.5 is that Ï† and Î¸ are positively homogeneous functions of the same degree. Additionally, building on the discussion of the meta-algorithm from Â§4.4.1, it can also be shown (in combination with Theorem 4.9) that if the network is suï¬ƒciently large (as measured by the number of subnetworks, r) then there always exists a path from any initialization to a global minimizer which does not require the value of the objective to be increased. Theorem 4.10 (Haeï¬€ele and Vidal, 2015, 2017). Given a nondegenerate pair of functions (Ï†,Î¸), let |Ï†| denote the number of elements in the output of the function Ï†. Then if r > |Ï†| for the optimization problem min W[1],...,W[L]  L(Y,Î¦r(X,W[1],. . .,W[L])) + Î»Î˜r(W[1],. . .,W[L]), (4.51) a nonincreasing path to a global minimizer will always exist from any initialization. Again we refer to Haeï¬€ele and Vidal (2015, 2017) for the complete proof, but the key idea is that once one arrives at a local minimum if either the condition of Theorem 4.9 is satisï¬ed or the outputs of the subnetworks are linearly dependent. As a result, by positive homogeneity, one can traverse a ï¬‚at surface of the objective landscape until reaching a point that does satisfy the condition of Theorem 4.9. This point is either a local minimum (and hence a global minimum from the theorem) or else a descent direction must exist. 4.5 Conclusions 225 4.5 Conclusions We have studied the optimization landscape of neural network training for two classes of networks: linear networks trained with a squared loss and without reg- ularization, and positively homogeneous networks with parallel structure trained with a convex loss and positively homogeneous regularization. In the ï¬rst case, we derived conditions on the inputâ€“output covariance matrices under which all critical points are either global minimizers or saddle points. In the second case, we showed that when the networks is suï¬ƒciently wide, the non-convex objective on the weights can be lower bounded by a convex objective on the network mapping, and we derived conditions under which local minima of the non-convex objective yield global minima of both objectives. Future avenues for research include extending the results presented here to other classes of deep architectures. In particular, the current results are limited to parallel architectures whose size is measured by the number of parallel subnetworks of ï¬xed depth and width. This motivates extending the framework to cases in which both the depth and width of the network are varied. Moreover, the landscape of the objective is only one ingredient for explaining the role of optimization in deep learning. As discussed in the introduction, other in- gredients are the development of eï¬ƒcient algorithms for ï¬nding a global minimum and the study of the implicit regularization and generalization performance of such algorithms. We refer the reader to other chapters in this book for recent results on these fascinating subjects. Acknowledgments The authors acknowledge partial support by the NSF-Simons Research Collabo- rations on the Mathematical and Scientiï¬c Foundations of Deep Learning (NSF grant 2031985), the NSF HDR TRIPODS Institute for the Foundations of Graph and Deep Learning (NSF grant 1934979), NSF grants 1704458 and 2008460, and ARO grant MURI W911NF-17-1-0304. References Allen-Zhu, Zeyuan, Li, Yuanzhi, and Song, Zhao. 2019. A convergence theory for deep learning via over-parameterization. Pages 242â€“252 of: Proc. Interna- tional Conference on Machine Learning. Arora, Sanjeev, Cohen, Nadav, Hu, Wei, and Luo, Yuping. 2019. Implicit regu- larization in deep matrix factorization. Pages 7413â€“7424 of: Proc. Neural Information Processing Systems. Baldi, P., and Hornik, K. 1989. Neural networks and principal component analysis: Learning from examples without local minima. Neural Networks, 2(1), 53â€“58. 226 Vidal et al: Optimization Landscape of Neural Networks CandÃ¨s, E., and Tao, T. 2010. The power of convex relaxation: Near-optimal matrix completion. IEEE Transactions on Information Theory, 56(5), 2053â€“2080. Cavazza, Jacopo, Haeï¬€ele, Benjamin D., Lane, Connor, Morerio, Pietro, Murino, Vittorio, and Vidal, Rene. 2018. Dropout as a low-rank regularizer for matrix factorization. Pages 435â€“444 of: Proc. International Conference on Artiï¬cial Intelligence and Statistics, vol. 84. Choromanska, Anna, Henaï¬€, Mikael, Mathieu, Michael, Ben Arous, Gerard, and LeCun, Yann. 2015. The loss surfaces of multilayer networks. Pages 192â€“204 of: Proc. International Conference on Artiï¬cial Intelligence and Statistics. Dauphin, Yann N., Pascanu, Razvan, Gulcehre, Caglar, Cho, Kyunghyun, Ganguli, Surya, and Bengio, Yoshua. 2014. Identifying and attacking the saddle point problem in high-dimensional non-convex optimization. Pages 2933â€“2941 of: Proc. Neural Information Processing Systems. Donoho, David L. 2006. For most large underdetermined systems of linear equations the minimal â„“1-norm solution is also the sparsest solution. Communications on Pure and Applied Mathematics, 59(6), 797â€“829. Du, Simon, Lee, Jason, Li, Haochuan, Wang, Liwei, and Zhai, Xiyu. 2019. Gradient descent ï¬nds global minima of deep neural networks. Pages 1675â€“1685 of: Proc. International Conference on Machine Learning. Duchi, J., Hazan, E., and Singer, Y. 2017. Adaptive subgradient methods of online learning and stochastic optimization. Journal of Machine Learning Research, 12, 2121â€“2159. Ge, Rong, Huang, Furong, Jin, Chi, and Yuan, Yang. 2015. Escaping from saddle points â€“ online stochastic gradient for tensor decomposition. Pages 797â€“842 of: Proc. Conference on Learning Theory. Gori, M., and Tesi, A. 1991. Backpropagation converges for multi-layered net- works and linearly-separable patterns. Page 896 of: Proc. International Joint Conference on Neural Networks, vol. 2. Gori, M., and Tesi, A. 1992. On the problem of local minima in backpropagation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 14(1), 76â€“ 86. Gunasekar, Suriya, Woodworth, Blake E., Bhojanapalli, Srinadh, Neyshabur, Behnam, and Srebro, Nati. 2017. Implicit regularization in matrix factor- ization. Pages 6151â€“6159 of: Proc. Neural Information Processing Systems. Gunasekar, Suriya, Lee, Jason, Soudry, Daniel, and Srebro, Nathan. 2018a. Charac- terizing implicit bias in terms of optimization geometry. In: Proc. International Conference on Machine Learning. Gunasekar, Suriya, Lee, Jason D., Soudry, Daniel, and Srebro, Nati. 2018b. Implicit bias of gradient descent on linear convolutional networks. Pages 9461â€“9471 of: Advances in Neural Information Processing Systems. Haeï¬€ele, Benjamin D., and Vidal, RenÃ©. 2015. Global optimality in tensor factor- ization, deep learning, and beyond. ArXiv preprint arXiv:1506.07540. References 227 Haeï¬€ele, Benjamin D., and Vidal, Rene. 2017. Global optimality in neural network training. Pages 7331â€“7339 of: Proc. IEEE Conference on Computer Vision and Pattern Recognition. Haeï¬€ele, Benjamin D., and Vidal, Rene. 2019. Structured low-rank matrix factor- ization: Global optimality, algorithms, and applications. IEEE Transactions on Pattern Analysis and Machine Intelligence, 42(6), 1468â€“1482. Kawaguchi, Kenji. 2016. Deep learning without poor local minima. Pages 586â€“594 of: Proc. Neural Information Processing Systems. Kingma, Diederik, and Ba, Jimmy. 2014. Adam: A method for stochastic optimiza- tion. Proc. International Conference on Learning Representations. Laurent, Thomas, and Brecht, James. 2018. Deep linear networks with arbitrary loss: All local minima are global. Pages 2902â€“2907 of: Proc. International Conference on Machine Learning. Lee, Jason D., Panageas, Ioannis, Piliouras, Georgios, Simchowitz, Max, Jordan, Michael I., and Recht, Benjamin. 2019. First-order methods almost always avoid strict saddle points. Mathematical Programming, 176, 311â€“337. Lu, Haihao, and Kawaguchi, Kenji. 2017. Depth creates no bad local minima. ArXiv preprint arXiv:1702.08580. Mairal, Julien, Bach, Francis, Ponce, Jean, and Sapiro, Guillermo. 2010. Online learning for matrix factorization and sparse coding. Journal of Machine Learning Research, 11, 19â€“60. Mianjy, Poorya, Arora, Raman, and Vidal, RenÃ©. 2018. On the implicit bias of dropout. In: Proc. International Conference on Machine Learning. Nesterov, Y. 1983. A method of solving a convex programming problem with convergence rate O(1/k2). Soviet Math. Doklady, 27(2), 372â€“376. Nouiehed, Maher, and Razaviyayn, Meisam. 2018. Learning deep models: Critical points and local openness. ArXiv preprint arXiv:1803.02968. Pal, Ambar, Lane, Connor, Vidal, RenÃ©, and Haeï¬€ele, Benjamin D. 2020. On the regularization properties of structured dropout. Pages 7671â€“7679 of: Proc. IEEE Conference on Computer Vision and Pattern Recognition. Robbins, H., and Monro, S. 1951. A stochastic approximation method. Annals of Mathematical Statistics, 22, 400â€“407. Rumelhart, David E., Hinton, Geoï¬€rey E., and Williams, Ronald J. 1986. Learning representations by back-propagating errors. Nature, 323, 533â€“536. Srebro, Nathan, Rennie, Jason D. M., and Jaakkola, Tommi S. 2004. Maximum- margin matrix factorization. Pages 1329â€“1336 of: Proc. Neural Information Processing Systems, vol. 17. Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., and Salakhutdinov, R. 2014. Dropout: A simple way to prevent neural networks from overï¬tting. Journal of Machine Learning Research, 15(1), 1929â€“1958. Sun, Ju, Qu, Qing, and Wright, John. 2017. Complete dictionary recovery over the sphere I: Overview and the geometric picture. IEEE Transactions on Information Theory, 63(2), 853â€“884. 228 Vidal et al: Optimization Landscape of Neural Networks Sun, Ju, Qu, Qing, and Wright, John. 2018. A geometric analysis of phase retrieval. Found. Comput. Math., 18, 1131â€“1198. Werbos, P.J. 1974. Beyond Regression: New Tools for Predictions and Analysis in the Behavioral Sciences. Ph.D. thesis, Harvard University. Wright, Stephen J., and Nocedal, Jorge. 1999. Numerical Optimization, vol. 2. Springer. Xu, Yangyang, and Yin, Wotao. 2013. A block coordinate descent method for regularized multiconvex optimization with applications to nonnegative tensor factorization and completion. SIAM Journal on Imaging Sciences, 6(3), 1758â€“ 1789. Zhang, Yuqian, Kuo, Han-Wen, and Wright, John. 2018. Structured local minima in sparse blind deconvolution. Pages 2322â€“2331 of: Advances in Neural Information Processing Systems. Zhu, Zhihui, Soudry, Daniel, Eldar, Yonina C., and Wakin, Michael B. 2020. The global optimization geometry of shallow linear neural networks. Journal of Mathematical Imaging and Vision, 62, 279â€“292. 5 Explaining the Decisions of Convolutional and Recurrent Neural Networks Wojciech Samek, Leila Arras, Ahmed Osman, GrÃ©goire Montavon, Klaus-Robert MÃ¼ller Abstract: The ability to explain and understand the prediction behaviour of complex machine learning (ML) models such as deep neural networks is of great interest to developers, users and researchers. It allows them to verify the systemâ€™s decision making and gain new insights into the data and the model, including the detection of any malfunctioning. Moreover, it can also help to improve the overall training process, e.g., by removing detected biases. However, owing to the large complexity and highly nested structure of deep neural networks, it is non-trivial to obtain these interpretations for most of todayâ€™s models. This chapter describes layer-wise relevance propagation (LRP), a propagation-based explanation technique that can explain the decisions of a variety of ML models, including state-of-the-art convolutional and recurrent neural networks. As the name suggests, LRP imple- ments a propagation mechanism that redistributes the prediction outcome from the output to the input, layer by layer through the network. Mathematically, the LRP algorithm can be embedded into the framework of deep Taylor decomposition and the propagation process can be interpreted as a succession of ï¬rst-order Taylor expansions performed locally at each neuron. The result of the LRP computation is a heatmap visualizing how much each input variable (e.g., pixel) has contributed to the prediction. This chapter will discuss the algorithmic and theoretical under- pinnings of LRP, apply the method to a complex model trained for the task of visual question answering (VQA) and demonstrate that it produces meaningful ex- planations, revealing interesting details about the modelâ€™s reasoning. We conclude the chapter by commenting on the general limitations of the current explanation techniques and interesting future directions. 5.1 Introduction Over the years machine learning (ML) models have steadily grown in complexity, gaining predictivity often at the expense of interpretability. Deep neural networks 229 230 Samek et al: Explaining the Decisions of Networks are a prime example of this development. These models typically contain millions of parameters and tens or even hundreds of non-linearly interwoven layers of increasing abstraction. After being fed with vast amounts of data, these models can achieve record performances on complex tasks such as vision (CireÅŸan et al., 2011; Lu and Tang, 2015), language understanding (Bahdanau et al., 2014; Devlin et al., 2018), strategic game playing (Mnih et al., 2015; Silver et al., 2017; MoravÄÃ­k et al., 2017), medical diagnosis (Esteva et al., 2017; Hannun et al., 2019; Jurmeister et al., 2019) and scientiï¬c data analysis (Baldi et al., 2014; Mayr et al., 2016; SchÃ¼tt et al., 2017). The complexity of state-of-the-art convolutional neural networks (CNNs) can be illustrated by the popular VGG-16 model (Simonyan and Zisserman, 2014). This model consists of 16 layers of increasing abstraction and 138 million weight parameters; moreover, it requires 15.5 billion elementary operations (MACs) to classify a single 224 Ã— 224 image. Clearly, for such a large model it becomes very diï¬ƒcult to explain why and how it arrived at its decision, i.e., to ï¬nd the relevant parts in the input (e.g., pixels) which have triggered an individual decision. Such an analysis becomes even more complicated for models with an internal state, e.g., long short-term memory (LSTM) networks (Hochreiter and Schmidhuber, 1997), which are often used to process time series data or textual input, because the internal state inï¬‚uences the modelâ€™s decision making in complex ways. Unsurprisingly, until recently, state-of-the-art CNNs, LSTMs and deep models in general have been commonly regarded as â€œblack boxesâ€. Although the wish to understand and interpret the decision-making process of an AI system is as old as the technology itself, the ï¬eld of explainable AI (XAI) has seen a signiï¬cant revival in the last years. With the many advances in the area of deep learning, various methods have been proposed recently to make these models more interpretable (Samek et al., 2019). While some of these methoda aim to construct and train ML models that are by design more interpretable, e.g., by incorporating sparsity priors or disentangling the learned representation, other methods have been proposed that are capable of explaining a given (trained) ML model post hoc. This chapter will focus exclusively on the latter methods. We will consider the problem of visual question answering (VQA), where a deep model is trained to answer questions about a given image. In order to do so, the model needs to consist of LSTM and CNN parts, which process the question and the image, respectively. Thus, explaining decisions of the VQA model requires an XAI technique which can explain both types of models. In this chapter we will introduce such a technique, termed layer-wise relevance propagation (LRP) (Bach et al., 2015); it is a generic method for explaining individual predictions of ML models by meaningfully redistributing the decisions backwards onto the input variables. The result of this explanation process is a â€œheatmapâ€, visualizing how much each input variable has contributed to the prediction. In the context of VQA, LRP will 5.2 Why Explainability? 231 thus highlight the relevant words in the question and the part in the image which is relevant for answering the question. Analysing and interpreting these explanations allows us to get insights into the functioning of the VQA model, and in particular helps us to understand its failure modes. In the remainder of this chapter we motivate the need for explainability (Â§5.2), formalize the explanation problem for simple linear models and discuss the diï¬ƒcul- ties occurring when extending this concept to deep neural networks (Â§5.3). Then, in Â§5.4 we introduce LRP for convolutional neural networks, demonstrate that it can be theoretically embedded into the framework of deep Taylor decomposition (Montavon et al., 2017), and present extensions of the method suited for LSTM ar- chitectures. In Â§5.5 we demonstrate experimentally that LRP provides meaningful explanations and helps to understand the reasoning of a model that was trained on a visual question answering task. We conclude the chapter in Â§5.6 with a discussion of the limitations of current explanation techniques and open research questions. 5.2 Why Explainability? This section motivates the need for explainability in machine learning and shows that being able to explain and understand the reasoning of an ML model is highly beneï¬cial for at least three diï¬€erent reasons. For a detailed discussion on the risks and challenges of transparency we refer the interested reader to Weller (2019). 5.2.1 Practical Advantages of Explainability Explanations can be seen as an additional tool for inspecting and debugging ML models. In this role they help us to identify models which are malfunctioning or have learned a strategy which does not match our expectations. In the extreme case, this may lead to an unmasking of â€œClever Hansâ€ predictors,1 i.e., models that rely on spurious correlations and artifacts in the data and do not solve the task they were trained for (Lapuschkin et al., 2019). Such models appear to perform well, but would fail if put to a real test. The use of explanations facilitates quickly identifying such invalid prediction strategies and ï¬nding biases in the training data (Anders et al., 2020), which would be very cumbersome or even practically impossible if one were using standard performance metrics such as cross-validation. Overall, being able to explain predictions is of high value to ML practitioners. 1 Clever Hans was a horse that was supposed to be able to perform simple calculations. However, as it turned out, Hans was not doing the math, but selecting the correct answers by watching the body language of the human who asked the questions and presented the candidate answers. Similar behaviours have been reported for a competition-winning ML model, which turned out to classify images of a horse by the presence of a copyright tag (Lapuschkin et al., 2016a); since then, many further Clever Hans incidents have been observed across the board of various deep learning and other ML models (Lapuschkin et al., 2019). 232 Samek et al: Explaining the Decisions of Networks 5.2.2 Social and Legal Role of Explainability The acceptance of AI technology in sensitive domains such as medicine or au- tonomous driving may depend very much on the ability of the AI to explain its decisions. Not only may users (e.g., patients) mistrust a decision or feel uncom- fortable if confronted with a black box system: for experts (e.g., medical doctors) also, comprehending the reasoning of an AI-based assistant system and being able to verify its prediction is an important requirement for accepting new technologies, in particular if it is they who are ultimately responsible. But, even for a perfect AI system, explanations are very valuable simply because they are an integral part of humanâ€“machine interaction and enable the users to make informed decisions. Also, from a legal perspective, explainability is of utmost importance because it concerns anti-discrimination and fairness aspects. For instance, explanations are a direct way to check that the model does not base its predictions on certain features (e.g., age or social status) or that it implements consistent prediction strategies for diï¬€erent groups (e.g., men and women). The EUâ€™s General Data Protection Reg- ulation (GDPR) explicitly mentions the right to explanation for users subjected to decisions of an automated processing system (Goodman and Flaxman, 2017). 5.2.3 Theoretical Insights Through Explainability Explanations also provide new insights into the data and the model. For instance, Lapuschkin et al. (2019) analysed the neural network training process on a rein- forcement learning task (the Atari Breakout game) and found that the depth of the architecture and the size of the replay memory both have a strong eï¬€ect on the ef- fectivity of the network to learn to focus on the relevant elements (i.e., the ball and paddle) of the game. Apart from providing insights into the learned strategies of a model, explanations also allow us to better understand the data. For instance, know- ing that the network is focusing on particular features (e.g., genes) when predicting the survival times of a patient may help us discover unknown causal relationships between these two. The ability of machine learning to uncover such hidden patterns in data has been demonstrated in the past, e.g., in the context of the game of Go (Silver et al., 2016). Since explanations make learned prediction strategies acces- sible for human experts, they have the potential to lead to new scientiï¬c insights. Therefore, XAI methods have already been used in various scientiï¬c disciplines (e.g., Thomas et al., 2019; Horst et al., 2019; SchÃ¼tt et al., 2017; von Lilienfeld et al., 2020; Reyes et al., 2018). 5.3 From Explaining Linear Models to General Model Explainability 233 5.3 From Explaining Linear Models to General Model Explainability This section will introduce the problem of explanation and discuss a simple and mathematically well founded technique for explaining the predictions of linear (and mildly nonlinear) ML models. After explaining why the generalization of this simple XAI technique to deep neural networks fails, we will conclude this section with a brief review of state-of-the-art explanation techniques for deep learning models. 5.3.1 Explainability of Linear Models Let us consider a simple binary classiï¬cation setting. Assume we are given a trained ML model with parameters Î¸, i.e., fÎ¸ : Rd â†’R, (5.1) which estimates the class membership of a given input vector X = [x1,. . ., xd]âŠ¤âˆˆ Rd by the sign of the output, i.e., inputs with fÎ¸(X) > 0 are classiï¬ed as â€œclass 1â€ and other inputs are classiï¬ed as â€œclass 2â€. In the following, we assume that fÎ¸ is a linear classiï¬er trained to solve the binary classiï¬cation task. Thus, fÎ¸ has the form fÎ¸(X) = d Ã• i=1 wixi + b, (5.2) with parameters Î¸ = (w1,. . .,wd, b) âˆˆRd+1. Problem of wxplanation. Explaining the prediction of the model for a speciï¬c input X means of determining how much the ith input feature xi (e.g., pixel) has contributed to the classiï¬cation decision fÎ¸(X). Thus, an explanation (or heatmap) is a collection of relevance values having the same dimensionality as the input, i.e., [R1,. . ., Rd] âˆˆRd. We want explanations to have some faithfulness (Swartout and Moore, 1993; Samek et al., 2017) with respect to the function fÎ¸(X): in other words, each relevance value Ri âˆˆR should indicate how much the ith input feature xi has contributed to the overall classiï¬cation decision fÎ¸(X). Although there may be diï¬€erent ways to measure such a contribution, the explanation must identify and highlight the features actually used by the model. Consequently, for faithful expla- nations we may assume that a perturbation of the relevant features has a detrimental eï¬€ect on the modelâ€™s prediction (Samek et al., 2017). Speciï¬c quantiï¬able aspects of the explanation that relate to faithfulness are as follows. (i) Conservation. The conservation property, e.g. Bach et al. (2015), establishes a connection between the explanation and the modelâ€™s output. Thus, a conservative 234 Samek et al: Explaining the Decisions of Networks explanation lets us interpret the summed relevance values as the total evidence at the output of the network: d Ã• i=1 Ri â‰ˆfÎ¸(X). (5.3) Note that the approximate, rather than strict, equality accounts for potentially unexplainable elements of the function such as biases in the linear model in Eq. (5.2). (ii) Modelâ€“data interaction. Furthermore, an explanation should reï¬‚ect the inter- action between a feature and its usage by the model. That means that for one data point X(1) the ith feature may be present and used by the model (i.e., it is relevant to the decision) whereas for another data point X(2) it may be present but not used by the model (i.e., it is irrelevant to the decision). Thus, in contrast to feature selection (Guyon and Elisseeï¬€, 2003), an input feature cannot per se be regarded as relevant or irrelevant. This property leads to individual explanations. (iii) Signed relevance. Finally, we want relevance values to have a meaningful sign. More precisely, a positive value of Ri should indicate a feature xi which is rele- vant to the prediction whereas a negative Ri should mark a feature contradicting the decision. For example, when classifying images as urban or non-urban, vi- sual features such as buildings, cars, and pedestrians would be assigned positive relevance scores, whereas trees or wild animals would be assigned negative scores as they would tend to contradict the predicted class. For linear models we can easily ï¬nd an explanation that fulï¬lls all the above properties if we deï¬ne the contribution of the ith feature as the ith summand in (5.2), i.e., Ri = wixi. (5.4) Numerous other desirable properties of explanations have been proposed in the literature (Swartout and Moore, 1993; Baehrens et al., 2010; Shrikumar et al., 2016; Montavon et al., 2018; Robnik-Å ikonja and Bohanec, 2018); however, a commonly accepted mathematical deï¬nition of what explanations are and which axioms they need to fulï¬l is still lacking. Therefore, we do not aim to introduce a rigorous axiomatic deï¬nition of an explanation in this chapter. However, we would like to note that many popular explanation techniques do not fulï¬ll the properties described above, even for linear models. For instance, sensitivity analysis (e.g., Baehrens et al., 2010; Simonyan et al., 2013) computes values Ri indicating how much changes in an input feature translate into changes in the output. For linear models the sensitivity score for the ith input feature is simply wi, i.e., it is constant with respect to the input X. Thus, sensitivity-based explanations neither reï¬‚ect the 5.3 From Explaining Linear Models to General Model Explainability 235 interaction between the feature and the model nor are they conservative or providing a meaningful interpretation of the sign. 5.3.2 Generalizing Explainability to Nonlinear Models Practical machine learning models are often nonlinear. For example, kernel ma- chines expose the input features through a nonlinear feature map and deep neural networks interleave linear projections with nonlinear activation functions. The re- sulting output is a complex, interwoven mixture of those input features that are more capable of learning the prediction task. When the function is nonlinear but remains locally linear, explanations can be obtained using the well-known Taylor expansion. The latter oï¬€ers a generic tool for decomposing the prediction in terms of elements of a linear sum. If the function fÎ¸ : Rd â†’R is twice continuously diï¬€erentiable at the reference point eX âˆˆRd, then it can be decomposed as follows: fÎ¸(X) = fÎ¸(eX) + d Ã• i=1 (xi âˆ’exi) Â· [âˆ‡fÎ¸(eX)]i | {z } Ri + Îµ (5.5) with Îµ = O(âˆ¥X âˆ’eXâˆ¥ 2) as X â†’eX, (5.6) where the relevance contributions Ri can be identiï¬ed from the ï¬rst-order terms, and where Îµ denotes the higher-order terms. For a well-chosen reference point with fÎ¸(eX) = 0 (i.e., the root point) and small enough higher-order terms, we can explain the predictions of the nonlinear model in terms of Ri, while retaining approximately the conservation property fÎ¸(X) â‰ˆ d Ã• i=1 Ri, (5.7) which we also had for the linear case (up to the linear functionâ€™s bias term). Note that the solution in (5.4) can be regarded as a special case of the Taylor-based decomposition with reference point eX = 0. Thus, Taylor expansion oï¬€ers a generic mathematical tool to decompose a predic- tion into dimension-wise contributions. But does this method produce meaningful explanations for any nonlinear function? Unfortunately this is not the case. The Taylor approach provides meaningful decompositions only for simple (e.g., locally linear) nonlinear models. It fails to produce meaningful explanations for functions which are highly nonlinear. For example, the function f (x, y) = xÂ·y is dominated by second-order terms near the origin (x and y only matter jointly). Also, for piecewise-linear models, e.g., deep 236 Samek et al: Explaining the Decisions of Networks neural networks with ReLU activations, this naive Taylor-based decomposition has been shown to provide low-quality explanations (Montavon et al., 2018; Samek et al., 2020). The ï¬rst problem arises from the complexity of the neural network function and the high-dimensional space to which it is applied. In these high dimensions many root points eX are potentially reachable; however, only a few of them are truly meaningful (those that are close enough to the input data point to explain X, lying on the data manifold etc.). Deep neural networks are known to lack robustness to inputs lying outside the data manifold (i.e., â€œadversarialâ€ examples); thus, selecting a reference point which lies even slightly outside this complex manifold can lead to unintended behaviour of the model, resulting in noisy and uninformative explanations. For ReLU-based networks without bias, choosing a reference point eX = Î´X is valid2 for any positive value of Î´, and the non-explainable zero-order term vanishes in the limit Î´ â†’0, hence leading to an explanation that satisï¬es conservation. We note however that the reference point is in most cases quite far from the input sample X, and hence does not suï¬ƒciently contextualize the prediction, causing spurious negative relevance scores. Furthermore, owing to the multiscale and distributed nature of a neural network representation, its predictions are a combination of local and global eï¬€ects. The combination of the two eï¬€ects introduces a nonlinearity that is impossible to capture in one single linear expansion. Finally, the high complexity of todayâ€™s deep models often results in a shattered- gradients eï¬€ect (Balduzzi et al., 2017). Thus, methods relying on gradients (as the Taylor expansion in (5.5)) will systematically produce noisy explanations. In summary, one can say that Taylor-based decomposition provides a principled way to decompose a function into dimension-wise contributions, but it only works well for relatively simple (nonlinear) functions. 5.3.3 Short Survey on Explanation Methods To address the challenge of explaining nonlinear models and to overcome the diï¬ƒ- culties mentioned above, a number of approaches have been proposed. We now give a brief overview of the diï¬€erent approaches proposed in the context of deep neural networks. As in Samek and MÃ¼ller (2019) we categorize the methods into three classes: surrogate-based, perturbation-based, and propagation-based explanation methods. We saw in Â§5.3.1 that simple linear classiï¬ers are intrinsically explainable, because they can readily be decomposed as a sum over individual dimensions. Surrogate-based XAI methods utilize this property and explain the predictions of complex classiï¬ers by locally approximating them with a simple surrogate func- 2 The resulting reference point eX lies on the same linear region as the point X (Montavon et al., 2018). 5.3 From Explaining Linear Models to General Model Explainability 237 tion that is interpretable. Local interpretable model-agnostic explanations (LIME) (Ribeiro et al., 2016) is the most popular explanation technique falling into this category. While LIME has the advantage of being model-agnostic, i.e., it applies to any black-box model without requiring access to its internal structure, one needs to collect a sample of inputâ€“output pairs to build the surrogate. This can be computa- tionally expensive and the result may depend on the sample. The second class of XAI methods constructs explanations from the modelâ€™s response to local changes, e.g. to some coarse perturbations (Zeiler and Fergus, 2014; Zintgraf et al., 2017; Lundberg and Lee, 2017), or directly to chnages in the gradient, which can be computed cheaply (Baehrens et al., 2010; Simonyan et al., 2013; Shrikumar et al., 2016). To address the gradient noise and its locality, averaging approaches such as SmoothGrad (Smilkov et al., 2017) or integrated gradients (Sundararajan et al., 2017) have been proposed that improve explanation quality compared with a sole gradient, although averaging incurs an additional computational cost. Additionally, the perturbation approach can also be expressed as an optimization problem (Fong and Vedaldi, 2017; Chang et al., 2018; Macdonald et al., 2019), for example by trying to identify a minimal set of relevant input features, such as features that leave the expected classiï¬er score nearly constant when randomizing the remaining (non-relevant) features. Finally, propagation-based methods, such as layer-wise relevance [ropagation (Bach et al., 2015) and its deep Taylor decomposition theoretical framework (Mon- tavon et al., 2017) explain decisions of neural networks by utilizing the internal structure of the model, speciï¬cally, by running a purpose-designed backward pass in the network. They give robust explanations and can be computed quickly in a time of the order of a single backward pass. Furthermore, they overcome many dis- advantages of the other explanation techniques, e.g., they do not require sampling and do not have the problem of gradient shattering. The price to pay for these advan- tageous properties is the reliance on the structure of the model (i.e., these methods are not model-agnostic and require careful design of the backward pass to take into account the speciï¬city of each layer in the network). More recent work has focused on systematizing the design of the backward pass (Montavon et al., 2017, 2019) and on extending the approach beyond neural networks classiï¬ers (Kauï¬€mann et al., 2019, 2020; Eberle et al., 2021; Ruï¬€et al., 2021). Besides developing novel explanation methods, several works have also focused on comparing existing XAI techniques using objective evaluation criteria. For instance, Samek et al. (2017) measured the quality of explanations using â€œpixel ï¬‚ippingâ€. Some authors have considered proxy tasks for evaluation (Doshi-Velez and Kim, 2017), e.g., using an explanation for localization tasks (Zhang et al., 2018). Other authors have evaluated explanations using ground-truth information from a synthetic dataset (Osman et al., 2020). Yet other works assess explanation 238 Samek et al: Explaining the Decisions of Networks methods based on the fulï¬lment of certain axioms (Sundararajan et al., 2017; Lundberg and Lee, 2017; Montavon, 2019). Although the evaluation of XAI is still an ongoing research topic, propagation-based explanation techniques have already demonstrated their strength in various practical settings. A recent trend is also to move from individual explanations to dataset-wide explanations, which are presented to the user in a way that provides insight into the internal representation or the set of learned behaviours of the classiï¬er. For instance, Lapuschkin et al. (2019) proposed a technique for (semi-)automatically searching for interesting patterns in a set of explanations. Other works project explanations of the neural network onto more abstract semantic concepts that can be understood by a human (Bau et al., 2017; Kim et al., 2018) or strive for an interaction with the human user (e.g. Baehrens et al., 2010; Hansen et al., 2011). 5.4 Layer-Wise Relevance Propagation: Faithful Explanations by Leveraging Structure Layer-wise relevance propagation (LRP) (Bach et al., 2015; Montavon et al., 2019) is an XAI method which assumes that the ML model has a layered neural network structure, and leverages this structure to produce robust explanations at low compu- tational cost. The LRP method implements a reverse propagation procedure from the output of the network to the input features (Lapuschkin et al., 2016b; Alber et al., 2019). The propagation is implemented as a collection of redistribution rules applied at each layer of the network. In contrast with many other XAI methods, which rely on sampling or optimization, LRP computes explanations in a time of the order of one backward pass. This allows for fast GPU-based implementations (Alber et al., 2019), where hundreds of explanations can be computed per second. Furthermore, as we will see in Â§5.4.2, the propagation procedure used by LRP can be embedded in the framework of deep Taylor decomposition (Montavon et al., 2017), which views the backward pass as performing a multitude of Taylor ex- pansions at each layer. Each of these Taylor expansions is simple and tractable, and consequently this procedure conceptually overcomes the various diï¬ƒculties (of ï¬nding good reference points or of gradient shattering) that are encountered with the global Taylor expansion approach presented in Â§5.3.2. By viewing LRP from the perspective of Taylor expansions, it is natural that certain desirable properties of an explanation such as conservation will be inherited. In fact, LRP propagation rules will be shown to enforce the conservation locally at each neuron and, by extension, this also ensures conservation at a coarser level, e.g., between two consecutive layers, and more globally from the neural network output to the neural network input.3 Thus, LRP ensures that the total evidence for the 3 In practice, conservation holds only approximately owing to the possible presence of bias terms at each 5.4 Layer-Wise Relevance Propagation 239 decision taken by the model is redistributed, i.e., no evidence is added or removed through the propagation process. Mathematically, we can express this layer-wise conservation property of LRP as f (X) â‰ˆ nL Ã• k=1 R[L] k â‰ˆÂ· Â· Â· â‰ˆ n0 Ã• i=1 R[0] i , (5.8) where [l] and nl denote the superscript index and input dimension of the considered layer l, for l = 0,. . ., L. In the following we will show how this redistribution process is implemented in convolutional and recurrent neural networks and how it can be embedded in the framework of deep Taylor decomposition. 5.4.1 LRP in Convolutional Neural Networks Figure 5.1 illustrates the propagation procedure implemented by LRP for a convo- lutional neural network. First, the neural network classiï¬es the input, e.g., an image of a rooster. In order to do so, it passes the individual pixel values through a set of convolutional layers, before the resulting activation pattern is classiï¬ed using fully connected layers. At every layer i, activations are computed as x[i] k = Ïƒ  niâˆ’1 Ã• j=1 x[iâˆ’1] j w[i] jk + b[i] k  , (5.9) where Ïƒ(x) = max (0, x) is the ReLU activation function and the sum runs over all lower-layer activations x[iâˆ’1] j , plus an extra bias term b[i] k . The activation(s) at the output layer can be interpreted as the total evidence for the presence of a given class. If we aim to explain the networkâ€™s decision, then the prediction output (pre- softmax) is used to initialize the last layerâ€™s relevance value R[L] k . If the networkâ€™s prediction is wrong or if we want to investigate the modelâ€™s view on alternative decisions (e.g., identify features R[0] i > 0 speaking for the presence of a speciï¬c class), it may be useful to initialize the last layerâ€™s relevance value in a diï¬€erent way (e.g., by using the ground-truth label). In the example illustrated in Figure 5.1 the classiï¬cation decision is correct (â€œroosterâ€), thus we initialize the last layerâ€™s relevance value with the pre-softmax value of the neuron associated with the class â€œroosterâ€. In the next step, the backward-propagation procedure starts, i.e., the relevance values from the upper-layer neurons are redistributed to the lower-layer ones. The layer. The latter receive some share of the redistributed evidence (we could explicitly represent the relevance assigned to the biases at each layer by adding a 0th summation index in (5.8), resulting in an exact conservation throughout layers). Also, a certain LRP rule dissipates relevance by design to improve the signal-to-noise ratio of the explanation (the LRP-Ïµ rule; the details are given later). 240 Samek et al: Explaining the Decisions of Networks LRP method redistributes the relevance values proportionally to the contribution of neurons in the forward pass, i.e., R[iâˆ’1] j = ni Ã• k=1 z[i] jk Ãniâˆ’1 l=1 z[i] lk + b[i] k R[i] k , (5.10) where R[i] k denotes the relevance values of the upper-layer neurons (which are already known) and R[iâˆ’1] j stands for the newly computed relevance values of the lower-layer neurons. In this generic formula z[i] jk represents the extent to which neuron j from layer i âˆ’1 has contributed to make neuron k at layer i relevant. The abstract redistribution rule described in (5.10) is general enough to be ap- plicable to almost any ML model, including convolutional and recurrent neural networks. In practice, the choice of propagation rule must be made carefully for the explanation method to handle the diï¬€erent layer types properly (Montavon et al., 2018, 2019). Table 5.1 summarizes the diï¬€erent redistribution rules proposed for CNNs. Since the last two layers of the network depicted in Figure 5.1 are fully con- nected layers, we instantiate the general formulation with the LRP-Ïµ redistribution rule. Additionally, for the lower convolutional layers we use a diï¬€erent redistribu- tion rule, namely the LRP-Î±Î² rule with Î± = 1 and Î² = 0. A justiï¬cation of this layer-type speciï¬city of LRP will be provided in Â§5.4.2. The result of the redistribu- tion process is a heatmap highlighting the pixels which have contributed the most to the modelâ€™s decision that the image belongs to the class â€œroosterâ€. We see that the roosterâ€™s head and comb are the most relevant features for this decision. In the following we will discuss speciï¬c instances of the redistribution rule for diï¬€erent layer types (fc, conv, input layer) of a convolutional neural network. Section 5.4.3 will separately treat the redistribution of relevance through product layers, which are present in LSTM networks. The basic rule (LRP-0) and epsilon rule (LRP-Ïµ) (Bach et al., 2015) redistribute the relevance in proportion to two factors, namely the activations of lower-layer neurons and the connection strengths (weights). The intuition behind these rules is that the neurons that are more activated during prediction encode something about the input, e.g., features present in the image such as the roosterâ€™s comb, and should therefore receive a larger share of relevance than neurons which are not activated (note that with ReLU nonlinearities activations are positive or zero). Since the activation patterns will vary for diï¬€erent inputs, these redistribution rules will produce individual explanations for each input. However, since features may be present but irrelevant for the task, activations are not the only criterion to guide the relevance redistribution process. The connections between neurons (the weights) reï¬‚ect the integration of the encoded low-level features into higher-level concepts (and ï¬nally into the prediction) and should be therefore also taken into account 5.4 Layer-Wise Relevance Propagation 241 LRP backward pass forward pass LRP backward pass forward pass LRP- redistribution rule LRP- redistribution rule forward pass Fully-connected layers forward pass Convolutional layers prediction "rooster" Input Heatmap Figure 5.1 Illustration of the LRP procedure for a CNN. Each neuron redistributes as much relevance to the lower layer as it has received from the higher layer, i.e., no relevance is lost or artiï¬cially added in the explanation process. when explaining model decisions. Therefore, both rules redistribute relevance in proportion to the activations of lower-layer neurons and the connection strengths (the modelâ€“dataâ€“interaction). The LRP-Ïµ rule includes a small non-zero term Ïµ in the denominator. This additional term stabilizes the redistribution process by absorbing some relevance when the contributions to the activation of neuron k are weak or contradictory. Two rules which distinguish between the supporting (positive relevance) and contradicting (negative relevance) explanatory factors are LRP-Î±Î² (Bach et al., 2015) and LRP-Î³ (Montavon et al., 2019). These rules are also based on the strength of the activations and the weight values. By setting the Î±, Î², and Î³ parameters 242 Samek et al: Explaining the Decisions of Networks appropriately, we can control how much positive relevance is redistributed relative to the negative relevance (and vice versa). Note that as the Î³ parameter goes to inï¬nity, LRP-Î³ approaches LRP-Î±Î² with Î± = 1 and Î² = 0. The other rules shown in Table 5.1 are ï¬‚at redistribution and two redistribution rules satisfying diï¬€erent requirements on the ï¬rst layer. While the ï¬‚at redistribution rule can be used to reduce the spatial resolution of heatmaps (by simply uniformly redistributing relevance from some intermediate layer onto the input), the zB-rule and w2-rule are derived from the deep Taylor decomposition framework (DTD) (Montavon et al., 2017). Almost all the redistribution rules in Table 5.1 can be embedded into the DTD framework and interpreted as performing a local Taylor decomposition with speciï¬c assumptions on the reference point (see Â§5.4.2). This theoretical foundation of LRP is a strength of the method, because it allows one to design speciï¬c rules for diï¬€erent neural network layers (see the discussion in Montavon et al., 2019, and Kohlbrenner et al., 2020). Finally, there are two types of layers that are also commonly encountered in practical convolutional neural networks: pooling layers and batch normalization layers. For max-pooling layers, we can redistribute on the basis of a simple winner-take-all scheme. For average pooling layers, we note that they are a particular instance of a linear layer with positive constant weights and, therefore, the rules that have been deï¬ned for linear layers can also be used. Batch-normalization layers are typically treated by fusing them with the fully connected or convolution layer to which they connect (Montavon et al., 2019). This fusing is done after training but before applying LRP, so that LRP sees only an alternation of convolution or fully connected, ReLU and pooling layers. 5.4.2 Theoretical Interpretation of the LRP Redistribution Process We now present the embedding of LRP into the theoretical framework of deep Taylor decomposition (DTD) (Montavon et al., 2017) and show that the redistribution rules introduced above correspond to a particular Taylor expansion performed locally at the neuron. This match between the algorithmic view (LRP) and the theoretical view (DTD) on the explanation problem gives us the possibility to design redistribution rules tailored to each speciï¬c layer type of the CNN. We have seen that Taylor expansion lets us represent a function as a sum of zeroth-, ï¬rst- and higher-order terms (cf. Eq. (5.5)). If the reference point eX is chosen wisely, one can use the expansion to decompose the function value in terms of dimension-wise contributions (Eq. (5.7)). However, as discussed in Â§5.3.2 most of such Taylor decompositions are unreliable for deep neural networks owing to the gradient-shattering problem, to a combination of local and global eï¬€ects and to the diï¬ƒculty of ï¬nding a good reference point. Thus, although Taylor decomposition 5.4 Layer-Wise Relevance Propagation 243 Table 5.1 Overview of diï¬€erent LRP redistribution rules for CNNs. For better readability we include a 0th-index term with x[iâˆ’1] 0 B 1 and w[i] 0k B b[i] k . We use the notation (x)+ = max(0, x) and (x)âˆ’= min(0, x). Li and Hi denote the minimum and maximum pixel values. Name Redistribution Rule Usage DTD LRP-0 (Bach et al., 2015) R[iâˆ’1] j = ni Ã• k=1 x[iâˆ’1] j w[i] jk Ãniâˆ’1 l=0 x[iâˆ’1] l w[i] lk R[i] k fc layers âœ“ LRP-Ïµ (Bach et al., 2015) R[iâˆ’1] j = ni Ã• k=1 x[iâˆ’1] j w[i] jk Ïµk + Ãniâˆ’1 l=0 x[iâˆ’1] l w[i] lk R[i] k fc layers âœ“ LRP-Î³ (Montavon et al., 2019) R[iâˆ’1] j = ni Ã• k=1 x[iâˆ’1] j (w[i] jk + Î³(w[i] jk)+) Ãniâˆ’1 l=0 x[iâˆ’1] l (w[i] lk + Î³(w[i] lk )+) R[i] k conv layers âœ“ LRP-Î±Î² (s.t. Î± âˆ’Î² = 1) (Bach et al., 2015) R[iâˆ’1] j = ni Ã• k=1  Î± (x[iâˆ’1] j w[i] jk)+ Ãniâˆ’1 l=0 (x[iâˆ’1] l w[i] lk )+ âˆ’Î² (x[iâˆ’1] j w[i] jk)âˆ’ Ãniâˆ’1 l=0 (x[iâˆ’1] l w[i] lk )âˆ’  R[i] k conv layers Ã— (except Î± = 1, Î² = 0) ï¬‚at (Lapuschkin et al., 2019) R[iâˆ’1] j = ni Ã• k=1 1 niâˆ’1 R[i] k decrease resolution Ã— w2-rule (Montavon et al., 2017) R[0] i = n1 Ã• j=1 (w[1] ij )2 Ãn0 l=1(w[1] lj )2 R[1] j ï¬rst layer (Rd) âœ“ zB-rule (Montavon et al., 2017) R[0] i = n1 Ã• j=1 x[0] i w[1] ij âˆ’Li Â· (w[1] ij )+ âˆ’Hi Â· (w[1] ij )âˆ’ Ãn0 l=1 x[0] l w[1] lj âˆ’Li Â· (w[1] lj )+ âˆ’Hi Â· (w[1] lj )âˆ’R[1] j ï¬rst layer (pixels) âœ“ is an appropriate mathematical tool for determining dimension-wise contributions, it does not work reliably when we are trying to explain the deep neural network function in one step, i.e., when trying to linearly approximate its complex inputâ€“ output relation. The DTD method makes use of Taylor expansions, but leverages the deep layered structure of the model. Speciï¬cally, DTD uses Taylor expansions not to attribute directly the network output to its input (which we have shown to be unstable) but at each layer to attribute the relevance scores R[i] k to the neurons x[iâˆ’1] in the layer just below. This local attribution task can be interpreted as providing relevance messages that ï¬‚ow between the neurons of the two consecutive layers. Technically, DTD provides the theoretical framework for LRP in the following ways: (1) the relevance R[i] k at each layer is mainly driven by local activations and we can therefore approximate it locally using a relevance function R[i] k = brk(x[iâˆ’1]); 244 Samek et al: Explaining the Decisions of Networks (2) a ï¬rst-order Taylor expansion of the relevance function leads to a layer-wise redistribution scheme that corresponds to various LRP rules for appropriate choices of reference points. These two aspects of DTD are described in detail now. Relevance Function The relevance R[i] k of neuron k at layer i is a deterministic function rk of the activations x[iâˆ’1] at the lower-level layer, i âˆ’1, i.e., R[i] k = rk(x[iâˆ’1]). (5.11) This expresses the fact that both the forward propagation through the neural network as well as the backward relevance redistribution are deterministic processes. We need to know only the activations at layer iâˆ’1 in order to compute all the activations and relevance values from layer i to the output layer L. As discussed in Â§5.3.2, Taylor decomposition oï¬€ers a principled way to determine the dimension-wise contributions of the neurons at layer i âˆ’1 to the relevance value R[i] k . However, Taylor-based redistribution works reliably only for simple functions rk(x[iâˆ’1]). The function rk(x[iâˆ’1]) is in fact quite complex, because it includes the forward propagation from layer i to the output layer L as well as the relevance redistribution from the output layer L to layer i. A key insight of DTD is that the relevance function rk(x[iâˆ’1]) can be approximated locally by the simple relevance model brk(x[iâˆ’1]) deï¬ned as brk(x[iâˆ’1]) = x[i] k Â· c[i] k = max  0,Ãniâˆ’1 j=1 x[iâˆ’1] j w[i] jk + b[i] k  Â· c[i] k , (5.12) where c[i] k is a constant. This approximation holds for the last layer L, where the relevance of the output neuron is initialized to be the prediction (pre-softmax) output (R[L] k = x[L] k ). Here the constant c[L] k is simply 1. Thus, for the last layer we can safely use the Taylor-based redistribution approach. The question arises whether the approximation still holds for the lower layers, assuming that we have applied LRP in the higher layers. Take for example the LRP-Î³ rule. If the approximation holds in the layer above, the application of LRP-Î³ results in the following relevance scores: R[iâˆ’1] j = ni Ã• k=1 x[iâˆ’1] j  w[i] jk + Î³ w[i] jk + Ãniâˆ’1 l=0 x[iâˆ’1] l  w[i] lk + Î³ w[i] lk + R[i] k = x[iâˆ’1] j ni Ã• k=1  w[i] jk + Î³ w[i] jk + max  0,Ãniâˆ’1 l=0 x[iâˆ’1] l w[i] lk  Ãniâˆ’1 l=0 x[iâˆ’1] l  w[i] lk + Î³ w[i] lk + c[i] k 5.4 Layer-Wise Relevance Propagation 245 where we observe that the activation x[iâˆ’1] j is multiplied by a factor whose depen- dence on activations is diluted by two nested sums. This argument supports the modelling of that term as constant in that lower layer as well. Then, by induction, the approximation continues to hold at each layer. A similar argument can be made for other propagation rules, e.g., LRP-0 and LRP-Ïµ, as well as for LRP-Î±Î² with Î± = 0 and Î² = 1. Taylor-Based Redistribution and LRP Rules Having shown that the application of LRP at each layer produces the desired relevance structure, we now focus on the attribution of this relevance to the layer below by means of a Taylor expansion. The relevance function brk(x[iâˆ’1]) shown in Eq. (5.12) is a rescaled ReLU neuron taking the lower-layer activations as input. Hence, it consists of two linear pieces corresponding to the activated and deactivated domains. For the deactivated domain, there is no relevance to redistribute. For the activated domain, we consider some reference point ex[iâˆ’1] (also on the activated domain), and a Taylor expansion at this point gives R[i] k â‰ˆbrk  x[iâˆ’1] = brk  ex[iâˆ’1] + niâˆ’1 Ã• j=1  x[iâˆ’1] j âˆ’ex[iâˆ’1] j  Â· [âˆ‡brk  ex[iâˆ’1])]j | {z } R[i]â†’[iâˆ’1] kâ†’j  + Îµ |{z} 0 . (5.13) The ï¬rst-order terms, R[i]â†’[iâˆ’1] kâ†’j , determine how much of R[i] k should be redistributed to the neuron j of the lower layer. Thus, this formulation is equivalent to that in Eq. (5.5); however, here we are determining how the relevance should be redis- tributed between two adjacent layers (i and i âˆ’1) whereas in (5.5) we tried to determine how the relevance should be redistributed between the output and input of the network. The redistribution process between two adjacent layers turns out to be much simpler than that between output and input (i.e., there is no gradient shattering, the root point is easy to ï¬nd, etc.). Note that, because of the linearity of the ReLU function on its activated domain (see (5.12)), higher-order terms vanish (i.e., Îµ = 0). It can be easily seen that the ï¬rst-order terms reduce to R[i]â†’[iâˆ’1] kâ†’j =  x[iâˆ’1] j âˆ’ex[iâˆ’1] j  Â· w[i] jk c[i] k . Finally to obtain the relevance of neuron j in the lower-layer i âˆ’1, one needs only to pool all incoming relevance messages from layer i: R[iâˆ’1] j = ni Ã• k=1 R[i]â†’[iâˆ’1] kâ†’j . 246 Samek et al: Explaining the Decisions of Networks From this equation, various LRP propagation rules can be recovered. For example, choosing the reference pointex[iâˆ’1] = Î´ Â·x[iâˆ’1], with Î´ a small positive number, gives the LRP-Ïµ rule with Î´ = Ïµ(x[i] k +Ïµ)âˆ’1 (Montavon et al., 2019): we recover the LRP-0 rule in the limit Î´ â†’0. The LRP-Î³ rule is obtained by searching the root point on the line {x[iâˆ’1] âˆ’tx[iâˆ’1] âŠ™(1 + Î³1w[i] k âª°0),t âˆˆR}, where âŠ™denotes point-wise multiplication (Montavon et al., 2019). This connection between the LRP propagation rules and the choice of root points in the DTD framework gives a diï¬€erent perspective on how to choose propagation rules at each layer. These LRP rules no longer appear to be heuristically deï¬ned. For example, we can show that LRP-0/Ïµ/Î³ all correspond to a reference point whose components are positive. Hence, these rules can be justiï¬ed as being suitable for use in layers that receive positive quantities, e.g., ReLU activations, as input. Furthermore, LRP rules can be designed directly from the DTD framework, e.g., by choosing root points that satisfy membership of particular domains. For example, the zB-rule and the w2-rule (shown in Table 5.1) were originally derived from the DTD framework, where the root point is chosen in the ï¬rst case to satisfy pixel-value boxconstraints, or, in the second case, to be not subject to any domain constraint. More details on DTD, including proofs, can be found in (Montavon et al., 2017), where diï¬€erent LRP rules were derived for ReLU-activated neural networks with negative biases. Choosing LRP Rules in Practice The LRP-0 rule (Bach et al., 2015) is the simplest provided by LRP. It is conservative (except for biases) and gives equal treatment to positive and negative contributions. In practice, LRP-0 leads to explanations that closely follow the function and its gradient (see also Shrikumar et al., 2016 or Montavon, 2019). When the network becomes complex and deep, however, LRP-0 becomes exposed to the problem of shattered gradients, which causes the explanation to become noisy. In the DTD framework, such noisy behaviour can be explained by LRP-0 being associated with a root point at the origin, which is far from the actual data point, and thus is likely to bring irrelevant factors into the explanation. Hence, LRP-0 should be reserved for simple functions only (e.g., the very top layers of a CNN). The LRP-Ïµ rule (Bach et al., 2015) stabilizes the redistribution process by adding a constant to the denominator. This has a sparsiï¬cation eï¬€ect, where the relevance of neurons with weak net contributions is driven to zero by the stabilization term. We have observed that LRP-Ïµ works well for redistribution in the fully connected layers present in the top layers of the neural network and also in the topmost convolution layers. In the DTD framework, the gain in stability can be explained by the fact that the root point lies now closer to the data point and thus provides a better contextualization for the explanation. 5.4 Layer-Wise Relevance Propagation 247 The convolutional layers of a CNN exhibit strong levels of nonlinearity, especially due to the stacking of many of these layers. In practice, it can be diï¬ƒcult to fully identify the individual positive or negative eï¬€ects of each pixel. Instead, it is more practical to assign relevance to a collection of pixels, modelling their combined relevance. This behaviour of the explanation can be induced by imposing a preference for positive contributions over negative ones. Two rules that impose a diï¬€erent treatment between the positive and negative contributions are LRP-Î±Î² (Bach et al., 2015) and LRP-Î³ (Montavon et al., 2019), and they are both suitable for these lower-level layers; LRP-Î³ in particular has a DTD interpretation and the corresponding root point is at the ReLU hinge and relatively close to the data point. Hence, as with LRP-Ïµ, the beneï¬t of LRP-Î³ can again be understood as a better contextualization of the explanation. The LRP-Î³ and LRP-Î±Î² rules assume positive inputs. This is the case for most convolution layers building on ReLU neurons, but not in the ï¬rst layer where the input can be, e.g., pixel scores, which potentially are negative. For these special input layers, purpose-designed propagation rules, such as the w2-rule or the zB-rule (Montavon et al., 2017) (see Table 5.1), are more suitable. Finally, if we simply want to highlight a receptive ï¬eld rather than the contributing features within the receptive ï¬eld, the â€œï¬‚atâ€ redistribution rule (Lapuschkin et al., 2019) is appropriate. Now, while here and in Â§5.4.1 we have highlighted the layer-speciï¬c application of the LRP rules on a typical CNN for image classiï¬cation (according to Montavon et al., 2019 and Kohlbrenner et al., 2020), another conï¬guration of LRP rules might be more appropriate for other network architectures or tasks. For example, the LRP-Ïµ rule was found to work well on a word-embedding-based CNN for text classiï¬cation (Arras et al., 2017b). Furthermore, while LRP-Î±1Î²0 tends to produce explanations with a selectivity that is somewhat too low on a typical CNN for image classiï¬cation, the same rule has been shown to work very well in the context of the explanation-based pruning of CNNs (Yeom et al., 2019), as well as for the CNN-based relation network model (Osman et al., 2020) used for visual question answering in our experimental section, Â§5.5. More generally, when faced with a novel neural network model or task, and if the network is built upon ReLU activations, a default conï¬guration that can be tried is to apply the LRP-Î±1Î²0 rule in every hidden layer. The LRP-Î±Î² rule, with parameters ï¬xed as Î± = 1 and Î² = 0, considers only positively contributing neurons and delivers positive-valued relevances; compared with LRP-Ïµ and LRP-Î³ it has the advantage of having no free hyperparameter. Furthermore, if an objective assessment of explanation quality is available for a given task then it is possible to try various combinations of LRP rules and hyperparameters and run an actual hyperparameter selection procedure to optimize explanation for that particular task. 248 Samek et al: Explaining the Decisions of Networks In that sense, the multiple hyperparameters provided by LRP can prove very useful to address the exact applicationâ€™s needs. We now turn to the application of LRP to LSTM networks. 5.4.3 Extending LRP to LSTM Networks Long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997) networks are popular deep models for processing sequential inputs such as biomedical time series, genetic sequences, or textual data. These models consist of a memory cell storing an internal state. Figure 5.2 displays such an LSTM memory cell. In the forward pass this cell processes the input and sends it through the input gate, zt = g (Wzxt + Uzytâˆ’1 + bz) cell input, (5.14) it = Ïƒ (Wixt + Uiytâˆ’1 + bi) input gate, (5.15) where xt is the input vector at time t and zt and it are the corresponding cell input and input gate activations, respectively. The input gate, together with the forget gate, control, and update, the cell state, ft = Ïƒ (Wfxt + Ufytâˆ’1 + bf) forget gate, (5.16) ct = it âŠ™zt + ft âŠ™ctâˆ’1 cell state, (5.17) and the cell state itself has an inï¬‚uence on the output of the LSTM cell: ot = Ïƒ (Woxt + Uoytâˆ’1 + bo) output gate, (5.18) yt = ot âŠ™h (ct) cell output. (5.19) Thus, the LSTM output yt depends on the actual input xt as well as on the cell state ct. Both factors are connected through point-wise multiplication (denoted as âŠ™). The activation functions g, h are typically tanh or sigmoid, and the gate activation, Ïƒ, is a sigmoid. For applying the LRP propagation principle to LSTMs, the same redistribution rules which were proposed for convolutional neural networks with ReLU activation (as introduced in Table 5.1) can also be employed in the LSTM to redistribute the relevance through linear layers followed by an element-wise activation (even though the LSTM network typically uses activation functions that diï¬€er from ReLU). In particular, previous works have relied on the LRP-Ïµ rule for that purpose (Arras et al., 2017a; Ding et al., 2017; Arjona-Medina et al., 2019), i.e., the rule which is recommended for dense layers in CNNs and which provides a signed relevance. This means that in practice linear layers redirect the higher-layer relevance proportionally to the neuron forward-pass contributions (i.e., the neuronâ€™s activated value times the connection weight), while through element-wise activation layers the relevance 5.4 Layer-Wise Relevance Propagation 249 + + forget gate input gate cell input + output gate LSTM cell input recurrent input recurrent input recurrent output recurrent cell output ... ... ... ... ... ... + + ... ... input recurrent ... ... h g Legend gate activation function (usually sigmoid) g input activation function (usually tanh or sigmoid) h output activation function (usually tanh or sigmoid) + sum over all inputs branching point multiplication feedforward data flow recurrent data flow recurrent weights feedforward weights y o c f i z g g g LRP-all (Arras et al. 2017) LRP-prop (Ding et al. 2017) LRP-half (Arjona-Medina et al. 2018) i z i z i z = = = = = = Figure 5.2 Illustration of the LRP procedure for an LSTM. Each neuron, including the product neurons, redistributes to the input neurons as much as it has received from the higher layer (schematic diagram of the LSTM cell from Arras et al., 2019b, reprinted with permission of Springer Nature). is backward propagated identically (there is no relevance redirection through such layers). Besides, a novel challenge arises when trying to explain LSTM networks (as well as other gated recurrent networks): it comes from the point-wise multiplicative connections. To handle such nonlinearities in the LRP backward redistribution process, authors have proposed several redistribution rules verifying the layer-wise relevance conservation property of LRP (Arras et al., 2017a; Ding et al., 2017; Arjona-Medina et al., 2019). Denoting by xk such a product neuron in the forward pass, we note that it has the form xk = xg Â· xj, (5.20) where xg denotes a sigmoid activated gate neuron and xj, the remaining neuron, is a signal neuron. As per the design of the LSTM network (Hochreiter and Schmid- huber, 1997), the role of the gate is to control the ï¬‚ow of information in the LSTM cell (let the information pass if open, or refuse passage if closed), while the signal neuron carries the information itself; this information is either conveyed by the cell input or stored in the LSTM cellâ€™s memory via the cell state. A similar conï¬guration 250 Samek et al: Explaining the Decisions of Networks for products can also be found in other popular gated recurrent networks such as GRUs (Cho et al., 2014). Given the upper-layer relevance Rk of the neuron xk, three alternatives rules can be used to redistribute this quantity onto the input neurons xg and xj. These rules are illustrated at the right Figure 5.2 for the particular case of the product between input gate i and cell input z. LRP-all: One rule proposed by Arras et al. (2017a) assigns all the relevance to the signal neuron, i.e., it sets Rj = Rk and Rg = 0. This redistribution follows a signal- take-all strategy. It is based on the idea that gates could be considered as connection weights (although their value is not constant). Accordingly, they inï¬‚uence the LRP redistribution process through the computation of Rk (which, via the relevance redistribution in the next higher linear layer, is proportional to xk and thus also depends on xg), but they do not get a relevance value assigned per se, since their expected role in the LSTM forward pass is only to reweight the value of the signal xj. LRP-prop: The rule proposed by Ding et al. (2017) redistributes the relevance proportionally to the neuronsâ€™ activated values, i.e., Rj = xj xj+xg Rk and Rg = xg xj+xg Rk. LRP-half: A rule proposed by Arjona-Medina et al. (2019) redistributes the rele- vance equally, i.e., Rj = Rg = 0.5Rk. Arras et al. (2019a,b) carefully compared these three redistribution rules in simulations and experiments with real-world natural language processing (NLP) data on commonly used LSTMs (Greï¬€et al., 2017) that follow the structure given by equations (5.14)â€“(5.19) and that use the tanh nonlinearity for the cell input and output (the functions g and h in (5.14) and (5.19)). The qualitative and quantitative results showed a clear superiority of the LRP-all rule4 (Arras et al., 2017a) over LRP-prop and LRP-half. Independent works also successfully applied the LRP-all redistribution rule to LSTMs on a synthetic task, in the medical domain (Yang et al., 2018), as well as in NLP (Poerner et al., 2018) and in computer security (Warnecke et al., 2020), demonstrating the resulting LRP explanations are superior to other explanation methods for recurrent neural networks. DTD Interpretation In the following we will show that the LRP-all rule can be further motivated under the theoretical framework of deep Taylor decomposition (Montavon et al., 2017), 4 Note, though, that on non-standard customized LSTM models, as were introduced in Arjona-Medina et al. (2019); Arras et al. (2019b), other LRP rules for product neurons can become advantageous over LRP-all. 5.5 Explaining a Visual Question Answering Model 251 as was proposed in Arras et al. (2019b). Let us introduce the neuron pre-activations zg and zj, for neurons xg and xj, respectively, and consider a product of neurons where the signal is tanh activated: xk = sigm(zg) Â· tanh(zj). Suppose the relevance of the product neuron Rk(zg, zj), as a function of the pre- activations, can be approximated by a simple relevance model of the form bRk(zg, zj) = sigm(zg) Â· tanh(zj) Â· ck = xk Â· ck, where ck is a constant, such that Rk(zg, zj) = bRk(zg, zj) locally. This is the generic relevance form assumed by the DTD framework (Montavon et al., 2017). By per- forming a Taylor expansion of this relevance model at a root point (ezg,ezj), we obtain bRk(zg, zj) =bRk(ezg,ezj) (= 0) + sigmâ€²(ezg) Â· tanh(ezj) Â· ck Â· (zg âˆ’ezg) (= Rg) + sigm(ezg) Â· tanhâ€²(ezj) Â· ck Â· (zj âˆ’ezj) (= Rj) + Ïµ . Using the nearest root point in the space of pre-activations (Arras et al., 2019b), which is given by ezj = 0 and ezg = zg, it follows the LRP-all rule: Rg = 0 and Rj = bRk. Moreover, using this root point, higher-order terms of the form (zg âˆ’ezg)k as well as the interaction terms (zg âˆ’ezg)...(zj âˆ’ezj)... vanish; thus the error term Ïµ depends only on the signal variable zj. In other words, if the tanh activation is working near its linear regime then the error term Ïµ in the LRP decomposition is negligible (no matter whether the gateâ€™s sigmoid activation is saturated or not). Lastly, if instead of using a tanh activation for the signal neuron, one used a ReLU activation, for example, then the approximation error would be exactly zero in this case (i.e., Ïµ = 0). For more details on the various DTD relevance models for diï¬€erent signal activation functions we refer to Arras et al. (2019b). 5.5 Explaining a Visual Question Answering Model This section demonstrates the ability of LRP to explain the decisions of a com- plex model trained for visual question answering (VQA) with the CLEVR dataset (Johnson et al., 2017a). Visual question answering is a multi-modal task at the intersection between nature language processing and computer vision: the model is fed with an image and a textual question about that image, and is asked to predict the answer to that question, either with a single word or in free-form text. 252 Samek et al: Explaining the Decisions of Networks Dataset and Model The CLEVR dataset is a synthetic VQA task which was proposed by Johnson et al. (2017a) to avoid the biases present in VQA benchmarks based on real-world images (Antol et al., 2015; Goyal et al., 2017): its primary goal was to diagnose the visual reasoning capabilities of state-of-the-art VQA models. It is based on images of 3D rendered objects â€“ more precisely, geometric shapes positioned on a plane surface with a grey background. Each object has four types of attributes (spread among eight colours, two materials, two sizes, and three shapes). The questions are designed to test the following reasoning abilities: spatial relationships between objects, com- parison and recognition of object attributes, object counting, and the comparison of object sets sizes. The data consists of 70,000/15,000/15,000 training/validation/test images, and respectively 699,989/149,991/149,988 questions (i.e., there are roughly 10 questions per image). The prediction problem is framed as a 28-way classiï¬cation task (the output is a single word among a vocabulary of size 28). Early work on this task used complex models, e.g., with stacked attention (Yang et al., 2016) or with an explicit representation of the question-generation program (Johnson et al., 2017b), but Santoro et al. (2017) proposed a simpler architecture based on a relation network (RN) that performs even better in terms of prediction accuracy. We re-implemented their model and trained it as described in Santoro et al. (2017). Our trained model reaches a test accuracy of 93.3% on the CLEVR dataset (the original authors reported a performance of 95.5%). The model architecture is displayed in Figure 5.3. It consists of a four-layer CNN to extract feature maps from the image, a LSTM network to process the question, and a pairwise concatenation of visual â€œobjectsâ€ together with the question representation to fuse the visual and textual information. Here the â€œobjectsâ€ are simply pixels in the CNN last-layer feature maps. Each layer of the CNN has the following structure: conv â†’relu â†’batchnorm, with 24 kernels of size 3 Ã— 3 and stride 2 (no padding). The LSTM part of the network is a unidirectional LSTM with word embeddings of size 32, and a hidden layer of size 128. The paired representations (the object pair concatenated with the LSTM ï¬nal hidden state) are passed to a RN, which is made of a four-layer MLP of fully connected layers of size 256, each followed by ReLU activation, and a ï¬nal element-wise summation layer. The resulting representation is passed to a three-layer MLP with fully connected layers of size 256, each followed by ReLU activation. The output layer is a fully connected layer of size 28. For preprocessing the questions, we removed punctuation and performed lower-casing; the resulting input vocabulary has size 80. For preprocessing the images, we resized them to 128 Ã— 128 pixels and rescaled the pixel values to the range [0,1]. Training was done with the Adam optimizer. 5.5 Explaining a Visual Question Answering Model 253 Figure 5.3 VQA model from Santoro et al. (2017) used in our experiments. Methods for Explaining VQA In order to get insights into the modelâ€™s prediction strategies, we computed relevance values for the question (explaining the LSTM part of the model) and the image (explaining the CNN part of the model) using LRP. For the LSTM part of the network, we employed the LRP-all rule for product layers, and the LRP-Ïµ rule (with Ïµ = 0.001) for the remaining layers. For the CNN and RN parts of the network (i.e., on the image-processing side), we used the LRP-Î±Î² rule with Î± = 1 and Î² = 0, as this variant was shown to perform better, according to quantitative experiments performed in Osman et al. (2020), compared with a composite application of the LRP-Î±Î²/LRP-Ïµ rules for diï¬€erent layers. In addition, we computed relevances using two simple baseline methods, namely Gradient Ã— Input (Shrikumar et al., 2016) and Occlusion (Li et al., 2016). The ï¬rst method computes the relevance of an input feature by simply using the partial derivative of the prediction function (pre-softmax) multiplied by the featureâ€™s value, i.e., R[0] i = âˆ‚fc âˆ‚x[0] i (x[0]) Â· x[0] i (where fc is the prediction function for the target class c, and x[0] is the input), whereas the latter method computes the relevance value using a diï¬€erence of probabilities R[0] i = Pc(x[0]) âˆ’Pc(x|x[0] i =0) (where Pc are the predicted softmax probabilities, respectively, for the original unmodiï¬ed input and for the input where the feature of interest x[0] i has been set to zero). For all methods we explained the decision for the modelâ€™s predicted class. For the question heatmaps, in order to get one relevance value per word, we summed the relevances over the word embedding dimensions and took the absolute value for visualization. For the image heatmaps, we summed the relevances across the channels, took the absolute value, and applied gaussian blurring (with standard deviation 0.02 times the image size) for visualization of the original image overlayed 254 Samek et al: Explaining the Decisions of Networks Question Type Gradient Ã— Input Occlusion LRP count 0 10000 20000 30000 the objects things 5220 6329 6555 0 10000 20000 30000 how number what 8701 10030 15063 0 10000 20000 30000 many how what 5063 7975 9363 equal_shape 0 2000 4000 6000 8000 thing the object 2423 2588 2613 0 2000 4000 6000 8000 object the shape 1740 2176 3001 0 2000 4000 6000 8000 there the shape 1239 2210 5534 query_colour 0 10000 20000 the what color 3241 5566 9473 0 10000 20000 that what color 2203 11296 12506 0 10000 20000 is what color 5384 6392 11858 Figure 5.4 The most important words for three CLEVR question types, sorted by decreasing frequency over the validation set. A word is chosen as important if it appears in the top three relevant words of a question. with the heatmap. For raw heatmap visualization we just summed the relevances across the channels. Results and Insights In order to see which words are the most relevant per question type, we performed a word relevance statistic over the CLEVR validation set. For each question we iden- tiï¬ed the three most relevant words, and compiled the selected words by question type. Then we retrieved the words with the highest frequencies for a few question types in Figure 5.4. We manually highlighted words that were directly related to the question type, regardless of the image, in green. Note that the network does not have access to the question types during prediction. One can see that the nouns shape and colour have been identiï¬ed (except by Gradient Ã— Input) as being relevant to answer questions about the shape or colour of objects. For the question type count, where the goal is to count objects in a set, Occlusion and LRP both selected meaningful words, while Gradient Ã— Input attributed high relevance to generic words such as the, things, objects. Similar words were selected by Gradient Ã— Input for the question type equal_shape, which suggests that the latter method is less suited to ï¬nd important words related to the question type. Additionally, we visualized relevance heatmaps for individual data points in Figures 5.5 and 5.6, using Gradient Ã— Input and LRP (we did not compute heatmaps for Occlusion, since this method is very expensive to compute on the image side). The exemplary data points were automatically selected in the following way: we conducted a search over both correctly and falsely predicted CLEVR validation points with speciï¬c question types, and retrieved the three points with the highest predicted probabilities, i.e., the points where the model is very conï¬dent with its prediction. Indeed we expected the corresponding explanations to be more 5.5 Explaining a Visual Question Answering Model 255 focused and insightful on such data points, while on data points where the model is hesitating the heatmaps might be more diï¬€use and less informative. For correctly predicted data points (Figure 5.5), we considered all questions that query an objectâ€™s attribute, i.e., we used the question types query_material, query_colour, query_size, query_shape. For falsely predicted data points (Figure 5.6) we used only the question type query_colour. From the heatmap visualizations of the textual questions in Figure 5.5 and Fig. 5.6 we could not make a clear-cut statement about which explanation method was qualitatively better. Both methods seem to deliver equally sparse explanations. When the questions are about an objectâ€™s material (Figure 5.5), the word material is often highlighted (except in the last question for LRP); when they are about a colour (Figure 5.6), the word colour is always highlighted by LRP, and highlighted once by Gradient Ã— Input. However, it seems that Gradient Ã— Input attributes higher relevances to the last words in the question compared with those at the beginning of the question (which is probably due to a vanishing-gradient eï¬€ect induced by the forget gate in the unidirectional LSTM), while LRP is able to assign a high relevance, e.g., to the word colour (Figure 5.6), even when it appears at the beginning of the question. On the image side however, we clearly see that the LRP heatmaps in Figures 5.5 and 5.6 are qualitatively better than those delivered by Gradient Ã— Input: the LRP explanations are generally less noisier, more concentrated on objects, and provide pertinent clues to the modelâ€™s visual reasoning (as we will see below), while Gradient Ã— Input seems to attribute (partly) spurious relevances to background areas. Let us take a closer look at the LRP heatmaps in Figure 5.5. In the ï¬rst question, the target object of the question is the grey metal sphere at the top left of the scene. In the VQA question this object is referenced through its spatial location relatively to the brown sphere. And correspondingly, the LRP visual explanation both highlights the target object, as well as the referring object of the question. The second question has a similar pattern, where the target object is the yellow cube, and the referring object is the small blue sphere. Again the LRP heatmap reveals the two important objects of the question. In contrast, the Gradient Ã— Input heatmaps are not helpful for understanding the modelâ€™s decisions for question 1 and 2. Finally, in the third question, the target object of the VQA question is the red cylinder. Here the spatial relationships formulated in the VQA question are more intricate and involve multiple objects, including a big cyan cube and a small rubber sphere. Thus the LRP explanation is more diï¬€use: it highlights several objects, namely the two cyan cubes as well as all objects on their right. Intuitively it makes sense that the model is focusing on all these objects to answer the given more complex question. 256 Samek et al: Explaining the Decisions of Networks Question Answer Gradient Ã— Input LRP there is a large sphere on the left side of the big brown shiny object; what material is it ? there is a large sphere on the left side of the big brown shiny object; what material is it ? there is a large sphere on the left side of the big brown shiny object; what material is it ? True: metal Predicted: metal there is a big thing left of the small sphere; what material is it ? there is a big thing left of the small sphere; what material is it ? there is a big thing left of the small sphere; what material is it ? True: metal Predicted: metal there is a large object right of the rubber thing that is in front of the big cyan metallic cube behind the tiny gray matte sphere; what is its material ? there is a large object right of the rubber thing that is in front of the big cyan metallic cube behind the tiny gray matte sphere; what is its material ? there is a large object right of the rubber thing that is in front of the big cyan metallic cube behind the tiny gray matte sphere; what is its material ? True: metal Predicted: metal Figure 5.5 Heatmaps for three CLEVR questions which are correctly predicted. On the image side, we visualizeed the heatmap overlayed with the original image, as well as the raw heatmap (at bottom left of the images). In the top row, the VQA question is a query about the material (metal vs. rubber) of the grey sphere situated in the top left of the scene, and on the left side from the brown sphere. In the middle row, the queried object is the yellow cube, which is situated left of the small blue rubber sphere. In the bottom question, the query is about the material of the red cylinder, which is referenced with respect to the small grey rubber sphere and the cyan cube in the background of the scene. In all cases, we observe that the generated LRP explanations are informative and consistently highlight the important objects of the VQA question that the model relies on to predict the correct answer, while the gradient input heatmaps are noisier and less helpful for understanding the predictions. Also the target object of the question (the red cylinder) is identiï¬ed by the LRP heatmap. 5.5 Explaining a Visual Question Answering Model 257 Question Answer Gradient Ã— Input LRP the metal object on the left side of the blue cylinder is what color ? the metal object on the left side of the blue cylinder is what color ? the metal object on the left side of the blue cylinder is what color ? True: purple Predicted: gray what is the color of the object that is both on the left side of the cyan metal cylinder and in front of the large green rubber ball ? what is the color of the object that is both on the left side of the cyan metal cylinder and in front of the large green rubber ball ? what is the color of the object that is both on the left side of the cyan metal cylinder and in front of the large green rubber ball ? True: green Predicted: blue what is the color of the object that is to the right of the blue object and in front of the tiny gray matte cylinder ? what is the color of the object that is to the right of the blue object and in front of the tiny gray matte cylinder ? what is the color of the object that is to the right of the blue object and in front of the tiny gray matte cylinder ? True: gray Predicted: yellow Figure 5.6 Heatmaps for three CLEVR questions which were falsely predicted. On the image side, we visualized the heatmap overlayed with the original image, as well as the raw heatmap (at bottom left of the images). In the top question, the query is about the colour of the purple cylinder located left from the blue cylinder, and both of these objects are largely occluded by the big grey cube. Here the model obviously mistakenly interpreted the grey cube as being the target object of the question, as it provided â€œgrayâ€ as an answer, while the correct answer was â€œpurpleâ€. In the middle row, the queried object is the small green rubber cube, which is both situated left from the small cyan cylinder and in front of the big green rubber sphere. Here the model also made a mistake, and identiï¬ed the little blue metal cube as the target object. Lastly in the bottom question, the queried object is the grey metal ball, which is situated on the right of the blue cube and in front of the grey cylinder, however the model mistakenly identiï¬ed the yellow sphere as being the target object of the question. In all cases, the LRP heatmaps are coherent with the modelâ€™s predicted wrong answers, while the gradient input heatmaps are more diï¬€use and less helpful for understanding the modelâ€™s decisions. 258 Samek et al: Explaining the Decisions of Networks Other useful insights can be revealed by the LRP explanations in order to under- stand misclassiï¬ed examples in Figure 5.6. In these questions the colour of a given object is asked. And, in all cases, the object with the highest relevance, as identiï¬ed by the LRP heatmap, is consistent with the modelâ€™s predicted answer, while the Gradient Ã— Input heatmaps are less distinct. In the ï¬rst question, the target object of the question (the small purple object to the left of the grey cube), as well as the referring object (the small blue object behind the grey cube) are highly occluded by the grey cube, which explains why the model did not correctly identify the target object of the question and made a false prediction by focusing on the grey cube instead, as revealed by the LRP heatmap. In the second question the target object of the VQA question was the green cube; however, according to the LRP heatmap, the model identiï¬ed instead the blue cube as being the target object of the ques- tion. Thus, the model mistakenly interpreted the spatial relationships deï¬ned by the VQA question. A similar phenomenon can be observed in the third question: the target object of the VQA question is the grey metal sphere, but the LRP heatmap reveals that the model instead identiï¬ed the yellow metal sphere as being the target object, which again conï¬rms that the model has some diï¬ƒculty in catching subtle spatial relationships: here it obviously did not correctly recognize the â€œin front ofâ€ relationship. Overall, the LRP visual explanations helped to understand why the VQA model generated a particular answer. A further interesting point to note is that the neural network model we use â€“ which is based on a relation network (Santoro et al., 2017) â€“ does not contain any explicit attention mechanism in its architecture. Still, the LRP heatmaps were able to reveal image regions that are important to the modelâ€™s decision, and that resemble typical attention heatmaps (as provided, e.g., by highly customized models for the CLEVR dataset such as the model from Mascharka et al., 2018). 5.6 Discussion This chapter has presented LRP as a powerful technique for explaining predictions of complex ML models such as convolutional and recurrent neural networks. The redistribution rules of LRP were derived from ï¬rst principles using the deep Taylor decomposition framework. It was shown that a careful choice of the redistribution rules (respectively, root points in DTD) is very important when applying LRP to composite architectures, i.e., models that combine layers with diï¬€erent properties. A naive choice of redistribution rule, e.g., LRP-Ïµ for all layers of an image classi- ï¬cation CNN, or LRP-prop for a standard LSTM model, can produce poor results. However, when applied properly, LRP can produce meaningful explanations and 5.6 Discussion 259 lead to interesting insights, even for complex and multimodal models such as that used for the VQA task. As well as the successful application of LRP to diï¬€erent scenarios, progress has recently been made on the development of criteria to objectively assess the quality and utility of explanations (Samek et al., 2017; Lundberg and Lee, 2017; Montavon, 2019; Osman et al., 2020). Here, several metrics as well as axiomatic approaches have been developed, yet many challenges remain. For instance, while several general frameworks for XAI have been developed, e.g., based on deep Taylor decomposition (Montavon et al., 2017), Shapley values (Lundberg and Lee, 2017) or rate-distortion theory (Macdonald et al., 2019), a uniï¬ed theory of explanation is still lacking. A limitation of todayâ€™s methods is the low semantic level of explanations. For in- stance, heatmaps do let us distinguish whether a group of features is jointly relevant for a given classiï¬cation decision or whether each feature in this group contributes individually. Also, pixel-level heatmaps do not provide any information about the underlying concepts in the data, e.g., objects in the image. The resulting interpreta- tion gap (i.e., relating relevant pixels to the relevant object) has to be closed by the recipient of the explanations, which can be diï¬ƒcult and erroneous. Furthermore, the vulnerability of explanations to adversarial manipulations (Dombrowski et al., 2019) is a serious challenge, because it may undermine trust in the explanation results. Promising future research topics in the ï¬eld of XAI are the use of explanations beyond interpretability. For instance, Yeom et al. (2019) demonstrated that LRP relevance scores can be used to prune a neural network. Other applications of XAI, e.g., in the detection of adversarial attacks or general model improvement (e.g., by self-supervision) are interesting future research directions. Also, the ï¬eld of humanâ€“machine interaction oï¬€ers various opportunities for explainability re- search. For instance, questions such as â€œwhat type of explanation is most useful for humansâ€, â€œhow can we move towards interactive explainabilityâ€ or â€œhow can we prevent misleading explanations from harming performanceâ€ need to be inves- tigated in order to allow for reliable and eï¬ƒcient humanâ€“machine interaction; see e.g., Baehrens et al. (2010) and Hansen et al. (2011) for early studies in this vein. Finally, bringing the concepts of XAI to other types of models (Lundberg et al., 2020) or ML tasks (Kauï¬€mann et al., 2019) remains an active area of research. Acknowledgements This work was supported by the German Ministry for Edu- cation and Research (BMBF) as BIFOLD â€“ Berlin Institute for the Foundations of Learning and Data (ref. 01IS18025A and ref. 01IS18037A). KRM also received support from the Institute of Information & Communications Technology Planning 260 Samek et al: Explaining the Decisions of Networks & Evaluation (IITP) grants funded by the Korea Government (No. 2019-0-00079, Artiï¬cial Intelligence Graduate School Program, Korea University). References Alber, Maximilian, Lapuschkin, Sebastian, Seegerer, Philipp, HÃ¤gele, Miriam, SchÃ¼tt, Kristof T., Montavon, GrÃ©goire, Samek, Wojciech, MÃ¼ller, Klaus- Robert, DÃ¤hne, Sven, and Kindermans, Pieter-Jan. 2019. iNNvestigate neural networks! Journal of Machine Learning Research, 20(93), 1â€“8. Anders, Christopher J., Neumann, David, Marinc, Talmaj, Samek, Wojciech, MÃ¼ller, Klaus-Robert, and Lapuschkin, Sebastian. 2020. XAI for analyzing and unlearning spurious correlations in ImageNet. Proc. ICML 2020 Workshop â€“ XXAI: Extending Explainable AI Beyond Deep Models and Classiï¬ers. Antol, Stanislaw, Agrawal, Aishwarya, Lu, Jiasen, Mitchell, Margaret, Batra, Dhruv, Zitnick, C. Lawrence, and Parikh, Devi. 2015. VQA: Visual question answer- ing. Pages 2425â€“2433 of: Proc. International Conference on Computer Vision. Arjona-Medina, Jose A., Gillhofer, Michael, Widrich, Michael, Unterthiner, Thomas, Brandstetter, Johannes, and Hochreiter, Sepp. 2019. RUDDER: Re- turn decomposition for delayed rewards. Pages 13566â€“13577 of: Advances in Neural Information Processing Systems. Arras, Leila, Montavon, GrÃ©goire, MÃ¼ller, Klaus-Robert, and Samek, Wojciech. 2017a. Explaining recurrent neural network predictions in sentiment snalysis. Pages 159â€“168 of: Proc. EMNLPâ€™17 Workshop on Computational Approaches to Subjectivity, Sentiment & Social Media Analysis (WASSA). Arras, Leila, Horn, Franziska, Montavon, GrÃ©goire, MÃ¼ller, Klaus-Robert, and Samek, Wojciech. 2017b. â€œWhat is relevant in a text document?â€: An inter- pretable machine learning approach. PLoS ONE, 12(7), e0181142. Arras, Leila, Osman, Ahmed, MÃ¼ller, Klaus-Robert, and Samek, Wojciech. 2019a. Evaluating recurrent neural network explanations. Pages 113â€“126 of: Proc. ACLâ€™19 BlackboxNLP Workshop: Analyzing and Interpreting Neural Networks for NLP. Arras, Leila, Arjona-Medina, JosÃ©, Widrich, Michael, Montavon, GrÃ©goire, Gill- hofer, Michael, MÃ¼ller, Klaus-Robert, Hochreiter, Sepp, and Samek, Wojciech. 2019b. Explaining and interpreting LSTMs. Pages 211â€“238 of: Explainable AI: Interpreting, Explaining and Visualizing Deep Learning. Lecture Notes in Computer Science, vol. 11700. Bach, S., Binder, A., Montavon, G., Klauschen, F., MÃ¼ller, K.-R., and Samek, W. 2015. On pixel-wise explanations for non-linear classiï¬er decisions by layer-wise relevance propagation. PLoS ONE, 10(7), e0130140. Baehrens, David, Schroeter, Timon, Harmeling, Stefan, Kawanabe, Motoaki, Hansen, Katja, and MÃ¼ller, Klaus-Robert. 2010. How to explain individual References 261 classiï¬cation decisions. Journal of Machine Learning Research, 11, 1803â€“ 1831. Bahdanau, Dzmitry, Cho, Kyunghyun, and Bengio, Yoshua. 2014. Neural ma- chine translation by jointly learning to align and translate. ArXiv preprint arXiv:1409.0473. Baldi, Pierre, Sadowski, Peter, and Whiteson, Daniel. 2014. Searching for exotic particles in high-energy physics with deep learning. Nature Communications, 5, 4308. Balduzzi, David, Frean, Marcus, Leary, Lennox, Lewis, J.P., Ma, Kurt Wan-Duo, and McWilliams, Brian. 2017. The shattered gradients problem: If RESNETs are the answer, then what is the question? Pages 342â€“350 of: Proc. 34th International Conference on Machine Learning. Bau, David, Zhou, Bolei, Khosla, Aditya, Oliva, Aude, and Torralba, Antonio. 2017. Network dissection: Quantifying interpretability of deep visual repre- sentations. Pages 6541â€“6549 of: Proc. IEEE Conference on Computer Vision and Pattern Recognition. Chang, Chun-Hao, Creager, Elliot, Goldenberg, Anna, and Duvenaud, David. 2018. Explaining image classiï¬ers by counterfactual generation. ArXiv preprint arXiv:1807.08024. Cho, Kyunghyun, van Merrienboer, Bart, Gulcehre, Caglar, Bahdanau, Dzmitry, Bougares, Fethi, Schwenk, Holger, and Bengio, Yoshua. 2014. Learning phrase representations using RNN encoderâ€“decoder for statistical machine translation. Pages 1724â€“1734 of: Proc. 2014 Conference on Empirical Meth- ods in Natural Language Processing. CireÅŸan, Dan, Meier, Ueli, Masci, Jonathan, and Schmidhuber, JÃ¼rgen. 2011. A committee of neural networks for traï¬ƒc sign classiï¬cation. Pages 1918â€“1921 of: Proc. International Joint Conference on Neural Networks. Devlin, Jacob, Chang, Ming-Wei, Lee, Kenton, and Toutanova, Kristina. 2018. BERT: Pre-training of deep bidirectional transformers for language under- standing. ArXiv preprint arXiv:1810.04805. Ding, Yanzhuo, Liu, Yang, Luan, Huanbo, and Sun, Maosong. 2017. Visualizing and understanding neural machine translation. Pages 1150â€“1159 of: Proc. 55th Annual Meeting of the Association for Computational Linguistics. Dombrowski, Ann-Kathrin, Alber, Maximillian, Anders, Christopher, Ackermann, Marcel, MÃ¼ller, Klaus-Robert, and Kessel, Pan. 2019. Explanations can be manipulated and geometry is to blame. Pages 13567â€“13578 of: Advances in Neural Information Processing Systems. Doshi-Velez, Finale, and Kim, Been. 2017. Towards a rigorous science of inter- pretable machine learning. ArXiv preprint arXiv:1702.08608. Eberle, Oliver, BÃ¼ttner, Jochen, KrÃ¤utli, Florian, MÃ¼ller, Klaus-Robert, Valleriani, Matteo, and Montavon, GrÃ©goire. 2021. Building and interpreting deep simi- larity models. IEEE Transactions on Pattern Analysis & Machine Intelligence. 262 Samek et al: Explaining the Decisions of Networks Esteva, Andre, Kuprel, Brett, Novoa, Roberto A., Ko, Justin, Swetter, Susan M., Blau, Helen M., and Thrun, Sebastian. 2017. Dermatologist-level classiï¬cation of skin cancer with deep neural networks. Nature, 542(7639), 115. Fong, Ruth C., and Vedaldi, Andrea. 2017. Interpretable explanations of black boxes by meaningful perturbation. Pages 3429â€“3437 of: Proc. IEEE Conference on Computer Vision and Pattern Recognition. Goodman, Bryce, and Flaxman, Seth. 2017. European Union regulations on al- gorithmic decision-making and a â€œright to explanationâ€. AI Magazine, 38(3), 50â€“57. Goyal, Yash, Khot, Tejas, Summers-Stay, Douglas, Batra, Dhruv, and Parikh, Devi. 2017. Making the V in VQA matter: Elevating the role of image understanding in visual question answering. In: Proc. IEEE Conference on Computer Vision and Pattern Recognition. Greï¬€, Klaus, Srivastava, Rupesh K., KoutnÃ­k, Jan, Steunebrink, Bas R., and Schmid- huber, JÃ¼rgen. 2017. LSTM: A search space odyssey. IEEE Transactions on Neural Networks and Learning Systems, 28(10), 2222â€“2232. Guyon, Isabelle, and Elisseeï¬€, AndrÃ©. 2003. An introduction to variable and feature selection. Journal of Machine Learning Research, 3, 1157â€“1182. Hannun, Awni Y., Rajpurkar, Pranav, Haghpanahi, Masoumeh, Tison, Geoï¬€rey H., Bourn, Codie, Turakhia, Mintu P., and Ng, Andrew Y. 2019. Cardiologist- level arrhythmia detection and classiï¬cation in ambulatory electrocardiograms using a deep neural network. Nature Medicine, 25(1), 65. Hansen, Katja, Baehrens, David, Schroeter, Timon, Rupp, Matthias, and MÃ¼ller, Klaus-Robert. 2011. Visual interpretation of kernel-based prediction models. Molecular Informatics, 30(9), 817â€“826. Hochreiter, Sepp, and Schmidhuber, JÃ¼rgen. 1997. Long short-term memory. Neural Computation, 9(8), 1735â€“1780. Horst, Fabian, Lapuschkin, Sebastian, Samek, Wojciech, MÃ¼ller, Klaus-Robert, and SchÃ¶llhorn, Wolfgang I. 2019. Explaining the unique nature of individual gait patterns with deep learning. Scientiï¬c Reports, 9, 2391. Johnson, Justin, Hariharan, Bharath, van der Maaten, Laurens, Fei-Fei, Li, Zitnick, C. Lawrence, and Girshick, Ross B. 2017a. CLEVR: A diagnostic dataset for compositional language and elementary visual reasoning. Pages 1988â€“1997 of: Proc. IEEE Conference on Computer Vision and Pattern Recognition. Johnson, Justin, Hariharan, Bharath, van der Maaten, Laurens, Hoï¬€man, Judy, Fei-Fei, Li, Lawrence Zitnick, C., and Girshick, Ross. 2017b. Inferring and executing programs for visual reasoning. In: Proc. IEEE International Con- ference on Computer Vision (ICCV). Jurmeister, Philipp, Bockmayr, Michael, Seegerer, Philipp, Bockmayr, Teresa, Treue, Denise, Montavon, GrÃ©goire, Vollbrecht, Claudia, Arnold, Alexander, Teichmann, Daniel, Bressem, Keno, SchÃ¼ller, Ulrich, von Laï¬€ert, Maximil- ian, MÃ¼ller, Klaus-Robert, Capper, David, and Klauschen, Frederick. 2019. Machine learning analysis of DNA methylation proï¬les distinguishes primary References 263 lung squamous cell carcinomas from head and neck metastases. Science Translational Medicine, 11(509). Kauï¬€mann, Jacob, Esders, Malte, Montavon, GrÃ©goire, Samek, Wojciech, and MÃ¼ller, Klaus-Robert. 2019. From clustering to cluster explanations via neural networks. ArXiv preprint arXiv:1906.07633. Kauï¬€mann, Jacob, MÃ¼ller, Klaus-Robert, and Montavon, GrÃ©goire. 2020. Towards explaining anomalies: a deep Taylor decomposition of one-class models. Pat- tern Recognition, 101, 107198. Kim, Been, Wattenberg, Martin, Gilmer, Justin, Cai, Carrie, Wexler, James, Viegas, Fernanda, and Sayres, Rory. 2018. Interpretability beyond feature attribution: Quantitative testing with concept activation vectors (TCAV). Pages 2668â€“2677 of: Proc. International Conference on Machine Learning. Kohlbrenner, Maximilian, Bauer, Alexander, Nakajima, Shinichi, Binder, Alexan- der, Samek, Wojciech, and Lapuschkin, Sebastian. 2020. Towards best practice in explaining neural network decisions with LRP. Pages 1â€“7 of: Proc. IEEE International Joint Conference on Neural Networks. Lapuschkin, Sebastian, Binder, Alexander, Montavon, GrÃ©goire, MÃ¼ller, Klaus- Robert, and Samek, Wojciech. 2016a. Analyzing classiï¬ers: Fisher vectors and deep neural networks. Pages 2912â€“2920 of: Proc. IEEE Conference on Computer Vision and Pattern Recognition. Lapuschkin, Sebastian, Binder, Alexander, Montavon, GrÃ©goire, MÃ¼ller, Klaus- Robert, and Samek, Wojciech. 2016b. The LRP toolbox for artiï¬cial neural networks. Journal of Machine Learning Research, 17(1), 3938â€“3942. Lapuschkin, Sebastian, WÃ¤ldchen, Stephan, Binder, Alexander, Montavon, GrÃ©- goire, Samek, Wojciech, and MÃ¼ller, Klaus-Robert. 2019. Unmasking Clever Hans predictors and assessing what machines really learn. Nature Communi- cations, 10, 1096. Li, Jiwei, Monroe, Will, and Jurafsky, Dan. 2016. Understanding neural networks through representation erasure. ArXiv preprint arXiv:1612.08220. Lu, Chaochao, and Tang, Xiaoou. 2015. Surpassing human-level face veriï¬cation performance on LFW with GaussianFace. Pages 3811â€“3819 of: Proc. 29th AAAI Conference on Artiï¬cial Intelligence. Lundberg, Scott M., and Lee, Su-In. 2017. A uniï¬ed approach to interpreting model predictions. Pages 4765â€“4774 of: Advances in Neural Information Processing Systems. Lundberg, Scott M., Erion, Gabriel, Chen, Hugh, DeGrave, Alex, Prutkin, Jor- dan M., Nair, Bala, Katz, Ronit, Himmelfarb, Jonathan, Bansal, Nisha, and Lee, Su-In. 2020. From local explanations to global understanding with ex- plainable AI for trees. Nature Machine Intelligence, 2(1), 2522â€“5839. Macdonald, Jan, WÃ¤ldchen, Stephan, Hauch, Sascha, and Kutyniok, Gitta. 2019. A rate-distortion framework for explaining neural network decisions. ArXiv preprint arXiv:1905.11092. 264 Samek et al: Explaining the Decisions of Networks Mascharka, D., Tran, P., Soklaski, R., and Majumdar, A. 2018. Transparency by design: closing the gap between performance and interpretability in visual reasoning. Pages 4942â€“4950 of: Proc. IEEE Conference on Computer Vision and Pattern Recognition. Mayr, Andreas, Klambauer, GÃ¼nter, Unterthiner, Thomas, and Hochreiter, Sepp. 2016. DeepTox: toxicity prediction using deep learning. Frontiers in Environ- mental Science, 3, 80. Mnih, Volodymyr, Kavukcuoglu, Koray, Silver, David, Rusu, Andrei A., Veness, Joel, Bellemare, Marc G., Graves, Alex, Riedmiller, Martin, Fidjeland, An- dreas K., Ostrovski, Georg, et al. 2015. Human-level control through deep reinforcement learning. Nature, 518(7540), 529â€“533. Montavon, GrÃ©goire. 2019. Gradient-based vs. propagation-based explanations: an axiomatic comparison. Pages 253â€“265 of: Explainable AI: Interpreting, Ex- plaining and Visualizing Deep Learning. Lecture Notes in Computer Science, vol. 11700. Springer. Montavon, GrÃ©goire, Lapuschkin, Sebastian, Binder, Alexander, Samek, Wojciech, and MÃ¼ller, Klaus-Robert. 2017. Explaining nonlinear classiï¬cation decisions with deep Taylor decomposition. Pattern Recognition, 65, 211â€“222. Montavon, GrÃ©goire, Samek, Wojciech, and MÃ¼ller, Klaus-Robert. 2018. Meth- ods for interpreting and understanding deep neural networks. Digital Signal Processing, 73, 1â€“15. Montavon, GrÃ©goire, Binder, Alexander, Lapuschkin, Sebastian, Samek, Woj- ciech, and MÃ¼ller, Klaus-Robert. 2019. Layer-wise relevance propagation: An overview. Pages 193â€“209 of: Explainable AI: Interpreting, Explaining and Visualizing Deep Learning. Lecture Notes in Computer Science, vol. 11700. Springer. MoravÄÃ­k, Matej, Schmid, Martin, Burch, Neil, LisÃ½, Viliam, Morrill, Dustin, Bard, Nolan, et al. 2017. DeepStack: Expert-level artiï¬cial intelligence in heads-up no-limit poker. Science, 356(6337), 508â€“513. Osman, Ahmed, Arras, Leila, and Samek, Wojciech. 2020. Towards ground truth evaluation of visual explanations. ArXiv preprint arXiv:2003.07258. Poerner, Nina, SchÃ¼tze, Hinrich, and Roth, Benjamin. 2018. Evaluating neural network explanation methods using hybrid documents and morphosyntactic agreement. Pages 340â€“350 of: Proc. 56th Annual Meeting of the Association for Computational Linguistics. Reyes, Esteban, EstÃ©vez, Pablo A., Reyes, Ignacio, Cabrera-Vives, Guillermo, Hui- jse, Pablo, Carrasco, Rodrigo, and Forster, Francisco. 2018. Enhanced rota- tional invariant convolutional neural network for supernovae detection. Pages 1â€“8 of: Proc. International Joint Conference on Neural Networks. Ribeiro, Marco Tulio, Singh, Sameer, and Guestrin, Carlos. 2016. Why should I trust you?: Explaining the predictions of any classiï¬er. Pages 1135â€“1144 of: Proc. ACM International Conference on Knowledge Discovery and Data Mining. References 265 Robnik-Å ikonja, Marko, and Bohanec, Marko. 2018. Perturbation-based explana- tions of prediction models. Pages 159â€“175 of: Human and Machine Learning, J. Zhou and F. Chen (eds). Springer. Ruï¬€, Lukas, Kauï¬€mann, Jacob R., Vandermeulen, Robert A., Montavon, GrÃ©goire, Samek, Wojciech, Kloft, Marius, Dietterich, Thomas G., and MÃ¼ller, Klaus- Robert. 2021. A unifying review of deep and shallow anomaly dtection. Proceedings of the IEEE, 109, 756â€“795. Samek, Wojciech, and MÃ¼ller, Klaus-Robert. 2019. Towards explainable artiï¬cial intelligence. Pages 5â€“22 of: Explainable AI: Interpreting, Explaining and Visualizing Deep Learning. Lecture Notes in Computer Science, vol. 11700. Springer. Samek, Wojciech, Binder, Alexander, Montavon, GrÃ©goire, Lapuschkin, Sebastian, and MÃ¼ller, Klaus-Robert. 2017. Evaluating the visualization of what a deep neural network has learned. IEEE Transactions on Neural Networks and Learning Systems, 28(11), 2660â€“2673. Samek, Wojciech, Montavon, GrÃ©goire, Vedaldi, Andrea, Hansen, Lars Kai, and MÃ¼ller, Klaus-Robert. 2019. Explainable AI: Interpreting, explaining and visualizing deep learning. Lecture Notes in Computer Science, vol. 11700. Springer. Samek, Wojciech, Montavon, GrÃ©goire, Lapuschkin, Sebastian, Anders, Christo- pher J., and MÃ¼ller, Klaus-Robert. 2020. Toward interpretable machine learning: Transparent deep neural networks and beyond. ArXiv preprint arXiv:2003.07631. Santoro, Adam, Raposo, David, Barrett, David G., Malinowski, Mateusz, Pascanu, Razvan, Battaglia, Peter, and Lillicrap, Tim. 2017. A simple neural network module for relational reasoning. Pages 4967â€“4976 of: Advances in Neural Information Processing Systems. SchÃ¼tt, Kristof T., Arbabzadah, Farhad, Chmiela, Stefan, MÃ¼ller, Klaus R., and Tkatchenko, Alexandre. 2017. Quantum-chemical insights from deep tensor neural networks. Nature Communications, 8, 13890. Shrikumar, Avanti, Greenside, Peyton, Shcherbina, Anna, and Kundaje, Anshul. 2016. Not just a black box: Learning important features through propagating activation diï¬€erences. ArXiv preprint arXiv:1605.01713. Silver, David, Huang, Aja, Maddison, Chris J., Guez, Arthur, Sifre, Laurent, Van Den Driessche, George, et al. 2016. Mastering the game of Go with deep neural networks and tree search. Nature, 529(7587), 484â€“489. Silver, David, Schrittwieser, Julian, Simonyan, Karen, Antonoglou, Ioannis, Huang, Aja, Guez, Arthur, Hubert, Thomas, Baker, Lucas, Lai, Matthew, Bolton, Adrian, et al. 2017. Mastering the game of Go without human knowledge. Nature, 550(7676), 354â€“359. Simonyan, Karen, and Zisserman, Andrew. 2014. Very deep convolutional networks for large-scale image recognition. ArXiv preprint arXiv:1409.1556. 266 Samek et al: Explaining the Decisions of Networks Simonyan, Karen, Vedaldi, Andrea, and Zisserman, Andrew. 2013. Deep inside convolutional networks: Visualising image classiï¬cation models and saliency maps. ArXiv preprint arXiv:1312.6034. Smilkov, Daniel, Thorat, Nikhil, Kim, Been, ViÃ©gas, Fernanda, and Wattenberg, Martin. 2017. Smoothgrad: removing noise by adding noise. ArXiv preprint arXiv:1706.03825. Sundararajan, Mukund, Taly, Ankur, and Yan, Qiqi. 2017. Axiomatic attribution for deep networks. Pages 3319â€“3328 of: Proc. International Conference on Machine Learning. Swartout, William R., and Moore, Johanna D. 1993. Explanation in Second Gener- ation Expert Systems. Pages 543â€“585 of: Second Generation Expert Systems. Springer. Thomas, Armin W., Heekeren, Hauke R., MÃ¼ller, Klaus-Robert, and Samek, Wo- jciech. 2019. Analyzing neuroimaging data through recurrent deep learning models. Frontiers in Neuroscience, 13, 1321. von Lilienfeld, O. Anatole, MÃ¼ller, Klaus-Robert, and Tkatchenko, Alexandre. 2020. Exploring chemical compound space with quantum-based machine learning. Nat. Rev. Chem., 4, 347â€”-358. Warnecke, Alexander, Arp, Daniel, Wressnegger, Christian, and Rieck, Konrad. 2020. Evaluating explanation methods for deep learning in security. Pages 158â€“174 of: Proc. 2020 IEEE European Symposium on Security and Privacy. Weller, Adrian. 2019. Transparency: Motivations and challenges. Pages 23â€“40 of: Explainable AI: Interpreting, Explaining and Visualizing Deep Learning. Lecture Notes in Computer Science, vol. 11700. Springer. Yang, Yinchong, Tresp, Volker, Wunderle, Marius, and Fasching, Peter A. 2018. Explaining therapy predictions with layer-wise relevance propagation in neu- ral networks. Pages 152â€“162 of: Proc. IEEE International Conference on Healthcare Informatics. Yang, Zichao, He, Xiaodong, Gao, Jianfeng, Deng, Li, and Smola, Alexander J. 2016. Stacked attention networks for image question answering. Pages 21â€“29 of: Proc. IEEE Conference on Computer Vision and Pattern Recognition. Yeom, Seul-Ki, Seegerer, Philipp, Lapuschkin, Sebastian, Wiedemann, Simon, MÃ¼ller, Klaus-Robert, and Samek, Wojciech. 2019. Pruning by explain- ing: A novel criterion for deep neural network pruning. ArXiv preprint arXiv:1912.08881. Zeiler, Matthew D., and Fergus, Rob. 2014. Visualizing and understanding convo- lutional networks. Pages 818â€“833 of: Proc. ECCV 2014. Zhang, J., Lin, Z.L., Brandt, J., Shen, X., and Sclaroï¬€, S. 2018. Top-down neural attention by excitation backprop. International Journal of Computer Vision, 126, 1084â€“1102. Zintgraf, Luisa M., Cohen, Taco S., Adel, Tameem, and Welling, Max. 2017. Visualizing deep neural network decisions: Prediction diï¬€erence analysis. In: Proc. International Conference on Learning Representations. 6 Stochastic Feedforward Neural Networks: Universal Approximation Thomas Merkh and Guido MontÃºfar Abstract: In this chapter we take a look at the universal approximation question for stochastic feedforward neural networks. In contrast with deterministic neural net- works, which represent mappings from a set of inputs to a set of outputs, stochastic neural networks represent mappings from a set of inputs to a set of probability dis- tributions over the set of outputs. In particular, even if the sets of inputs and outputs are ï¬nite, the class of stochastic mappings in question is not ï¬nite. Moreover, while for a deterministic function the values of all output variables can be computed inde- pendently of each other given the values of the inputs, in the stochastic setting the values of the output variables may need to be correlated, which requires that their values are computed jointly. A prominent class of stochastic feedforward networks which has played a key role in the resurgence of deep learning is the class of deep belief networks. The representational power of these networks has been studied mainly in the generative setting as models of probability distributions without an input, or in the discriminative setting for the special case of deterministic mappings. We study the representational power of deep sigmoid belief networks in terms of compositions of linear transformations of probability distributions, Markov kernels, which can be expressed by the layers of the network. We investigate diï¬€erent types of shallow and deep architectures, and the minimal number of layers and units per layer that are suï¬ƒcient and necessary for the network to be able to approximate any given stochastic mapping from the set of inputs to the set of outputs arbitrarily well. The discussion builds on notions of probability sharing and mixtures of product distributions, focusing on the case of binary variables and conditional probabilities given by the sigmoid of an aï¬ƒne map. After reviewing existing results, we present a detailed analysis of shallow networks and a uniï¬ed analysis for a variety of deep networks. Most of the results were previously unpublished or are new. 267 268 Merkh and MontÃºfar: Stochastic Feedforward Neural Networks 6.1 Introduction Obtaining detailed comparisons between deep and shallow networks remains a topic of theoretical and practical importance as deep learning continues to grow in popularity. The success of deep networks exhibited in many recent applications has sparked much interest in such comparisons (Larochelle et al., 2007; Bengio, 2009; Delalleau and Bengio, 2011; Pascanu et al., 2014; MontÃºfar et al., 2014; Mhaskar and Poggio, 2016; Eldan and Shamir, 2016; Poggio et al., 2017; Lu et al., 2017; Raghu et al., 2017; Yarotsky, 2017, 2018; BÃ¶lcskei et al., 2019; Gribonval et al., 2019) and in the development of the theory of deep architectures. Despite the acclaim, guidelines for choosing the most appropriate model for a given problem have remained elusive. One approach to obtaining such guidance is to analyze the representational and approximation capabilities of diï¬€erent types of architectures. The representational power of neural networks poses a number of interesting and important questions, even if it might not capture other important and complex aspects that impact the performance in practice. In particular, we note that the choice of network architecture deï¬nes a particular parametrization of the representable functions, which in turn has an eï¬€ect on the shape of the parameter optimization landscape. This chapter examines one aspect of this subject matter; namely, how do deep and shallow stochastic feedforward networks compare in terms of the number of com- putational units and parameters that are suï¬ƒcient to approximate a target stochastic function to a given accuracy? In contrast with deterministic neural networks, which represent mappings from a set of inputs to a set of outputs, stochastic neural net- works represent mappings from a set of inputs to a set of probability distributions over the set of outputs. As such, stochastic networks can be used to model the probability distribution of a given set of training examples. This type of problem, which is an instance of parametric density estimation, is a core problem in statistics and machine learning. When trained on a set of unlabeled examples, a stochastic network can learn to map an internal hidden variable to new examples which follow a similar probability distribution to that of the training examples. They can also be trained to generate examples which follow probability distributions conditioned on given inputs. For instance, the input might specify a value â€œcatâ€ or â€œdogâ€, and the outputs could be images of the corresponding animals. Generative modeling is a very active area of research in contemporary machine learning. In recent years, a particularly popular approach to generative modeling is the generative adversarial network (Goodfellow et al., 2014) and its many variants. The distinguishing prop- erty of this approach is that the training loss is formulated in terms of the ability of a discriminator to tell apart the generated examples and the training examples. Aside from utilizing this particular type of loss, these models are implemented in the same 6.1 Introduction 269 general way, as a sequence of mappings that take an internal source to values in a desired domain. The distinguishing property of stochastic neural networks is that each layer can implement randomness. Learning stochastic feedforward networks has been an important topic of research for years (Neal, 1990; Ngiam et al., 2011; Tang and Salakhutdinov, 2013; Raiko et al., 2014; Lee et al., 2017). Stochastic neural networks have found applications not only as generative models, but also in unsupervised feature learning (Hinton and Salakhutdinov, 2006; Ranzato et al., 2007), semantic hashing (Salakhutdinov and Hinton, 2009), and natural language understanding (Sarikaya et al., 2014), among others. Unsupervised training with stochastic networks can be used as a parameter initialization strategy for subsequent supervised learning, which was a key technique in the rise of deep learning in the decade after 2000 (Hinton et al., 2006; Bengio et al., 2007; Bengio, 2009). We study the representational power of stochastic feedforward networks from a class that is known as Bayesian sigmoid belief networks (Neal, 1992). These are special types of directed graphical models, also known as Bayesian networks (Lauritzen, 1996; Pearl, 1988). We consider a spectrum of architectures (network topologies) in relation to universal approximation properties. When viewing net- works as approximators of elements from a speciï¬c class, they can be quantiï¬ed and compared by measures such as the worst-case error for the class. If a suï¬ƒciently large network is capable of approximating all desired elements with arbitrary ac- curacy, it can be regarded as a universal approximator. The question of whether a certain network architecture is capable of universal approximation and, if so, how many computational units and trainable parameters suï¬ƒce, has been studied for a variety of stochastic networks (see, e.g., Sutskever and Hinton, 2008; Le Roux and Bengio, 2010; Bengio and Delalleau, 2011; MontÃºfar and Ay, 2011; MontÃºfar et al., 2011; MontÃºfar, 2014a; MontÃºfar et al., 2015; MontÃºfar, 2014b, 2015; MontÃºfar and Rauh, 2017; MontÃºfar, 2015). Most of the existing works on the representational power of stochastic feedfor- ward networks focus on the generative setting with no inputs, modeling a single probability distribution over the outputs, or the discriminative setting modeling a deterministic mapping from inputs to outputs. Models of stochastic functions, which are also referred to as Markov kernels or conditional probability distribu- tions, are more complex than models of probability distributions. Rather than a single probability distribution, they need to approximate a probability distribution for each possible input. Universal approximation in this context inherently requires more complexity as compared with generative models with no inputs. There is also a wider variety of possible network architectures, each with virtually no guidance on how one compares with another. Nonetheless, as we will see, the question of the universal approximation of Markov kernels can be addressed using similar tools as those previously developed for studying the universal approximation of probability 270 Merkh and MontÃºfar: Stochastic Feedforward Neural Networks distributions with deep belief networks (Sutskever and Hinton, 2008; Le Roux and Bengio, 2010; MontÃºfar and Ay, 2011; MontÃºfar, 2014b). We will also draw on unpublished studies of shallow stochastic feedforward networks (MontÃºfar, 2015). The overall idea of universal approximation that we consider here is as follows. For each possible input x âˆˆ{0,1}d to the network, there will be a target condi- tional distribution p(Â· | x) over the outputs y âˆˆ{0,1}s, which the network attempts to learn. Note that while there is only a ï¬nite number (2s)2d of deterministic mappings from inputs to outputs, there is a continuum (âˆ†{0,1}s)2d of mappings from inputs to probability distributions over outputs, where âˆ†{0,1}s is the (2s âˆ’1)-dimensional simplex of probability distributions over {0,1}s. As the number of hidden units of the network grows, the network gains the ability to better approximate the target conditional distributions. At a certain threshold, the model will have suï¬ƒcient com- plexity to approximate each conditional distribution with arbitrarily high precision. The precise threshold is unknown except in special cases, and thus upper bounds for universal approximation are generally used to quantify a networkâ€™s representational capacity. Since feedforward networks operate sequentially, each layer can be seen as a module that is able to implement certain operations that share or diï¬€use the probability mass away from the distribution at the previous layer and toward the target distribution. This is referred to as probability mass sharing. Depending on the size of the layers, the types of possible operations varies. The composition of operations layer by layer is a key diï¬€erence between deep and shallow networks. We prove suï¬ƒciency bounds for universal approximation with shallow networks and with a spectrum of deep networks. The proof methods for the deep and shallow cases diï¬€er in important ways owing to the compositional nature of deep architec- tures. This is especially so when restrictions are imposed on the width of the hidden layers. We extend the ideas put forth by Sutskever and Hinton (2008), Le Roux and Bengio (2010), and MontÃºfar and Ay (2011), where universal approximation bounds were proven for deep belief networks. Our main results can be stated as follows. â€¢ A shallow sigmoid stochastic feedforward network with d binary inputs, s binary outputs, and a hidden layer of width 2d(2sâˆ’1 âˆ’1) is a universal approximator of Markov kernels. â€¢ There exists a spectrum of deep sigmoid stochastic feedforward networks with d binary inputs, s binary outputs, and 2dâˆ’j(2sâˆ’b + 2b âˆ’1) hidden layers of width 2j(s+dâˆ’j) that are universal approximators of Markov kernels. Here b âˆ¼log2(s), and the overall shape of each network is controlled by j âˆˆ{0,1,. . ., d}. Moreover, each of these networks can be implemented with a minimum of 2d(2sâˆ’1) trainable parameters. â€¢ For the networks in the previous item, if both the trainable and non-trainable 6.2 Overview of Previous Works and Results 271 parameters are restricted to have absolute values at most Î±, the approximation error for any target kernel can be bounded in inï¬nity norm by 1âˆ’Ïƒ( Î± 2 (d +s))N + 2Ïƒ(âˆ’Î± 2 (d + s)), where N is the total number of units of the network and Ïƒ is the standard logistic sigmoid function. The chapter is organized as follows. In Â§6.2 we discuss previous works for context and in Â§6.3 we consider preliminary notions and ï¬x notation. Then, in Â§6.4 we present an analysis of shallow networks; the proofs are contained in Â§6.5. In Â§6.6 we give the main results, describing universal approximation with a spectrum of deep networks and approximation with bounded weights. The proofs of these results are to be found in Â§6.7. Next, in Â§6.8 we discuss the lower bounds for universal approximation. Afterward, a brief comparison between architectures and numerical experiments is given in Â§6.9. Lastly, Â§6.10 oï¬€ers a conclusion and avenues for future research. 6.2 Overview of Previous Works and Results The universal approximation property has been studied in a variety of contexts in the past. The seminal work of Cybenko (1989) and Hornik et al. (1989) showed that deterministic multilayer feedforward networks with at least one suï¬ƒciently large hidden layer are universal approximators over a certain class of Borel measurable functions. An overview on universal approximation for deterministic networks was provided by Scarselli and Tsoi (1998). The case of stochastic functions was not covered by this analysis, and was studied later. Soon after Hinton et al. (2006) intro- duced a practical technique for training deep architectures, universal approximation for deep belief networks (DBNs) was shown by Sutskever and Hinton (2008). They found that a DBN consisting of 3(2s âˆ’1) + 1 layers of width s + 1 is suï¬ƒcient for approximating any distribution p âˆˆâˆ†s arbitrarily well. This suï¬ƒciency bound was improved upon twice, ï¬rst by Le Roux and Bengio (2010), then by MontÃºfar and Ay (2011). The former introduced the idea of using Gray codes to overlap proba- bility sharing steps, thereby reducing the number of layers down to âˆ¼2s/s, each having width s. The latter further reduced the number of layers to 2sâˆ’1/(s âˆ’b), with b âˆ¼log2(s), by improving previous results on the representational capac- ity of restricted Boltzmann machines (RBMs) and probability-sharing theorems. It is interesting to note that still further improvements have been made on the representational capabilities of RBMs (MontÃºfar and Rauh, 2017), but it remains unclear whether universal approximation bounds for DBNs can beneï¬t from such improvements. For a recent review of results on RBMs, see MontÃºfar (2018). Several stochastic networks in addition to DBNs have been shown to be univer- sal approximators. The undirected counterpart to DBNs, called a deep Boltzmann 272 Merkh and MontÃºfar: Stochastic Feedforward Neural Networks machine (DBM), was proven to be a universal approximator even if the hidden layers are restricted to have at most the same width as the output layer (MontÃºfar, 2014a). Here it was shown that DBMs could be analyzed similarly to feedforward networks under certain parameter regimes. This result veriï¬es the intuition that undirected graphical models are in some well-deï¬ned sense at least as powerful as their directed counterparts. For shallow stochastic networks, universal approxima- tion bounds have been obtained for feedforward networks, which will be discussed next, and for undirected networks called conditional restricted Boltzmann machines (CRBMs) (MontÃºfar et al., 2015). Both such architectures are capable of universal approximation and have similar suï¬ƒciency bounds. We note that not every network architecture is capable of providing universal approximation. For example, it is known that for an RBM, DBN, or DBM to be a universal approximator of dis- tributions over {0,1}s, the hidden layer immediately before the visible layer must have at least s âˆ’1 units (MontÃºfar, 2014a). In fact, if s is odd, at least s units are needed (MontÃºfar and Morton, 2015). In addition, necessary bounds for universal approximation exist for all of the previously mentioned architectures, though such bounds are generally harder to reï¬ne. Except for very small models, there exists a gap between the known necessary bounds and the suï¬ƒciency bounds. Last, it was recently shown that deterministic feedforward networks with hidden layer width at most equal to the input dimension are unable to capture functions with bounded level sets (Johnson, 2019). Such discoveries exemplify the importance of analyzing the approximation capabilities of diï¬€erent network architectures. As already mentioned in the introduction, the representational power of dis- criminative models has been studied previously. In particular, the representation of deterministic functions from {0,1}d â†’{0,1}, known as Boolean functions, by logical circuits or threshold gates has been studied for many years. Shannon (1949) showed that almost all d-input Boolean functions require a logic circuit of size at least (1 âˆ’o(1))2d/d. Lupanov (1956) showed that every d-input Boolean function can be expressed by a logic circuit of size at most (1 + o(1))2d/d. Other works on the representation of Boolean functions include Brunato and Battiti (2015); Huang et al. (2006); Muroga (1971); Neciporuk (1964); Wenzel et al. (2000). A particu- larly interesting result by Hastad and Goldmann (1991) shows that, when the depth of a threshold circuit is restricted, some Boolean functions require exponentially more units to be represented. Rojas (2003) showed that a suï¬ƒciently deep stack of perceptrons where each layer is connected to the input and feeds forward a single bit of information is capable of learning any d-input Boolean function. The equivalent network without skip connections to the input would be a network of width d + 1. In that work it was pointed out that there is a direct trade-oï¬€between the width and depth of the network, and this idea will surface again in the analysis of deep networks that 6.3 Markov Kernels and Stochastic Networks 273 follows. Le Roux and Bengio (2010) showed that a sigmoid belief network with 2dâˆ’1 + 1 layers of width d is suï¬ƒcient for representing any deterministic function f : {0,1}d â†’{0,1}. This was achieved by showing that the parameters of one layer can be chosen to map a single vector to some ï¬xed h0 âˆˆ{0,1}d. Then, considering two classes of vectors, those for which f (h) = 0 and those for which f (h) = 1, one may choose to map the smaller of the two classes of vectors to h0. This can be done in 2dâˆ’1 layers or less, and then the last layer can correctly label the inputs depending on whether the network mapped them to h0. This process diï¬€ers from that to be discussed in that these networks are not learning multivariate conditional distributions for each input, but rather labeling each input 0 or 1. While for a deterministic function the values of all output variables can be computed independently of each other given the values of the inputs, in the stochastic setting the values of the output variables may be correlated, which requires that their values are computed jointly. 6.3 Markov Kernels and Stochastic Networks 6.3.1 Binary Probability Distributions and Markov Kernels Let s âˆˆN and consider the set of vectors {0,1}s of cardinality 2s. A probability distribution over the set {0,1}s is a vector p âˆˆR2s with non-negative entries pi, i âˆˆ{1,. . .,2s} that add to one. The entries correspond to the probabilities p(y) that this distribution assigns to each y âˆˆ{0,1}s. The set of all such probability distributions is the set âˆ†s := n p âˆˆR2s : Ã•2s i=1 pi = 1, pi â‰¥0 for i = 1,2,. . .,2so . (6.1) This set is a simplex of dimension 2s âˆ’1. The vertices of âˆ†s are point distributions which assign full probability mass to a single y âˆˆ{0,1}s and no mass to {0,1}s \ y. Such distributions are denoted Î´y and are sometimes referred to as deterministic because there is no uncertainty in them. The support of a distribution p âˆˆâˆ†s is denoted by supp(p) and is the set of the vectors in {0,1}s to which p assigns non- zero probability. The support set of a deterministic distribution is a singleton. A probability model M is just a subset of âˆ†s. If a model M âŠ†âˆ†s satisï¬es M = âˆ†s then M is said to have the universal approximation property. Here M refers to the closure of M in the Euclidean topology. A stochastic map or Markov kernel with input space {0,1}d and output space {0,1}s is a map P: {0,1}d â†’âˆ†s. Such a Markov kernel can be seen as a 2d Ã— 2s matrix with non-negative entries and with rows that sum to 1. The ith row is the probability distribution over {0,1}s corresponding to the ith input. The set of all 274 Merkh and MontÃºfar: Stochastic Feedforward Neural Networks Markov kernels is written as âˆ†d,s := n P âˆˆR2dÃ—2s : Pij â‰¥0, Ã•2s j=1 Pij = 1 for all i = 1,2,. . .,2do . (6.2) One can see that âˆ†d,s is the 2d-fold Cartesian product of âˆ†s and thus is a polytope of dimension 2d(2s âˆ’1). A model of Markov kernels N is simply a subset of âˆ†d,s. If a model N ï¬lls this polytope, meaning N = âˆ†d,s, it is said to be a universal approximator of Markov kernels. An important class of probability distributions is that of the factorizable or independent distributions, for which the probability of observing a joint state z âˆˆ {0,1}d is just the product of the probabilities of observing each state zj âˆˆ{0,1}, j = 1,. . ., d, individually. These distributions can be written as p(z) = d Ã– j=1 pzj(zj) = d Ã– j=1 pzj j (1 âˆ’pj)1âˆ’zj, for all z = (z1,. . ., zd) âˆˆ{0,1}d, (6.3) where pzj is a probability distribution over {0,1} and pj = pzj(zj = 1) âˆˆ[0,1] is the probability of the event zj = 1, for j = 1,. . ., d. We denote the set of all factorizable distributions of d binary variables, of the form given above, by Ed. The Hamming distance âˆ¥a âˆ’bâˆ¥H between two vectors a and b is the number of positions where a and b diï¬€er. One notable property of Ed is that if xâ€²,xâ€²â€² âˆˆ{0,1}d have Hamming distance âˆ¥xâ€² âˆ’xâ€²â€²âˆ¥H = 1 then any probability distribution p âˆˆâˆ†d with supp(p) = {xâ€²,xâ€²â€²} is in Ed. Certain conï¬gurations of binary vectors will be important in our analysis. The set of d-bit binary vectors can be visualized as the vertex set of the d-dimensional hypercube. If xâ€²,xâ€²â€² âˆˆ{0,1}d have Hamming distance âˆ¥xâ€² âˆ’xâ€²â€²âˆ¥H = 1, they form an edge of the d-cube. For this reason, they are sometimes referred to as an edge pair. A codimension 0 â‰¤j â‰¤d face of the d-cube consists of the 2dâˆ’j vectors having the same j bits in common. 6.3.2 Stochastic Feedforward Networks We consider stochastic networks known as Bayesian sigmoid belief networks (Neal, 1992), which are Bayesian networks (Pearl, 1988; Lauritzen, 1996) with conditional distributions taking the form of a logistic sigmoid function applied to a linear combination of the parent variables. Details can be found in (Saul et al., 1996) and (Bishop, 2006, Section 8.1). Each unit of the network represents a random variable and edges capture depen- dencies between variables. In our discussion, all of the units of the network are binary. The graphs are directed and acyclic, so that the units can be arranged into a sequence of layers. We will focus on the case where consecutive layers are fully 6.3 Markov Kernels and Stochastic Networks 275 connected and there are no intralayer or skip connections. The units in each layer are conditionally independent of the state of the units in the previous layer. Figure 6.5 shows an example of such an architecture. We denote the binary inputs by x âˆˆ{0,1}d, and the outputs by y âˆˆ{0,1}s. The networkâ€™s computational units take states in {0,1} with activation probabilities given by the sigmoid function applied to an aï¬ƒne transformation of the previous layerâ€™s values. Speciï¬cally, given a state hlâˆ’1 âˆˆ{0,1}mlâˆ’1 of the mlâˆ’1 units in layer l âˆ’1, the jth unit of the lth layer activates, i.e. it takes state hl j = 1, with probability p(hl j = 1 | hlâˆ’1) = Ïƒ(Wl jhlâˆ’1 + bl j) = 1 1 + eâˆ’(Wl jhlâˆ’1+bl j) . (6.4) Here Wl j âˆˆR1Ã—mlâˆ’1 is a row vector of weights and bl j âˆˆR is a bias. The weights and biases of all units in layer l are collected in a matrix Wl âˆˆRml Ã— Rmlâˆ’1 and a vector bl = (bl 1,. . ., bl ml) âˆˆRml. We denote the parameters (weights and biases) of the entire network collectively by Î¸. Note that the inverse of the sigmoid function Ïƒ is known as the logit function Ïƒâˆ’1(x) = log ( x 1âˆ’x) = log(x) âˆ’log (1 âˆ’x). The units in layer l are conditionally independent of the state hlâˆ’1 of the units in the preceding layer. The probability of observing state hl = (hl 1,. . ., hl ml) âˆˆ{0,1}ml at layer l given hlâˆ’1 is p(hl | hlâˆ’1) = ml Ã– j=1 Ïƒ(Wl jhlâˆ’1 + bl j)hl j  1 âˆ’Ïƒ(Wl jhlâˆ’1 + bl j)1âˆ’hl j . (6.5) Given an input x, the conditional distribution of all units in a network with L + 2 layers (including input and output layers) can be written as p(h1,h2,. . .,hL,y | x) = p(y | hL)p(hL | hLâˆ’1) Â· Â· Â· p(h1 | x). (6.6) By marginalizing over the hidden layers, which are all the layers other than the input and output layers, one obtains the conditional distribution of the output given the input as p(y | x) = Ã• h1 Â· Â· Â· Ã• hL p(y | hL)p(hL | hLâˆ’1) Â· Â· Â· p(h1 | x). (6.7) In particular, a network with ï¬xed parameters represents a Markov kernel in âˆ†d,s. When we allow the networkâ€™s parameter Î¸ to vary arbitrarily, we obtain the set of all Markov kernels in âˆ†d,s that are representable by the particular network architecture. The architecture is fully determined by the number of layers and the sizes m1,. . .,mL of the hidden layers. We call a network shallow if L = 1, and deep if L > 1. We denote by Fd,s âŠ†âˆ†d,s the set of all Markov kernels that can be represented by a network module of the form (6.5) with an input layer of size d and an output layer of size s, with no hidden layers. Networks with L > 1 can be seen as 276 Merkh and MontÃºfar: Stochastic Feedforward Neural Networks the composition of L + 1 such network modules. We denote by Fd,m1,...,mL,s := FmL,s â—¦FmLâˆ’1,mL â—¦Â· Â· Â· â—¦Fm1,m2 â—¦Fd,m1 âŠ†âˆ†d,s the set of all Markov kernels of the form (6.7) representable by a network architecture with an input layer of size d, hidden layers of size ml for l = 1,. . ., L, and an output layer of size s. The general task in practice for our feedforward stochastic network is to learn a conditional distribution pâˆ—(Â· | x) for each given input x. In other words, when providing the network with input x, the goal is to have the outputs y distributed according to some target distribution pâˆ—(Â· | x). We will be interested in the question of which network architectures have the universal approximation property, meaning that they are capable of representing any Markov kernel in âˆ†d,s with arbitrarily high accuracy. Our analysis builds on previous works discussing closely related types of stochas- tic networks. For completeness, we now provide the deï¬nition of those networks. A restricted Boltzmann machine (RBM) with m hidden and d visible binary units is a probability model in âˆ†d consisting of the distributions p(x) = Ã• hâˆˆ{0,1}m 1 Z(W,b,c) exp(xâŠ¤Wh + xâŠ¤b + hâŠ¤c), for all x âˆˆ{0,1}d, (6.8) where W âˆˆRdÃ—m, b âˆˆRd, c âˆˆRm are weights and biases, and Z is deï¬ned in such a way that Ã xâˆˆ{0,1}d p(x) = 1 for any choice of the weights and biases. This is an undirected graphical model with hidden variables. A deep belief network (DBN) is a probability model constructed by composing a restricted Boltzmann machine and a Bayesian sigmoid belief network as described above. Such a DBN represents probability distributions of the form p(y) = Ã• xâˆˆ{0,1}d p(y | x)p(x), for all y âˆˆ{0,1}s, (6.9) where p(y | x) is a conditional probability distribution of the form (6.7) and p(x) is a probability distribution of the form (6.8). 6.4 Results for Shallow Networks In the case of shallow networks, which have an input layer, a single hidden layer, and an output layer, as shown in Figure 6.1, we are interested in the smallest size of hidden layer which will provide for a universal approximation capacity. The results in this section are taken from a technical report (MontÃºfar, 2015), with a few adjustments. The shallow network Fd,m,s = Fm,s â—¦Fd,m has a total of dm + m + sm + s free parameters. We will also consider a restricted case where the second module has ï¬xed weights, meaning that we ï¬x R âˆˆFm,s and consider the composition 6.4 Results for Shallow Networks 277 y1 y2 Â· Â· Â· ys h1 h2 h3 h4 h5 Â· Â· Â· hm x1 x2 x3 Â· Â· Â· xd W V c b Fd,m Fm,s Output layer Input layer Figure 6.1 Feedforward network with a layer of d input units, a layer of m hidden units, and a layer of s output units. Weights and biases are (V, c) for the hidden layer, and (W, b) for the output layer. Râ—¦Fd,m, which has only dm+m free parameters. By comparing the number of free parameters and the dimension of âˆ†d,s, which is 2d(2s âˆ’1), it is possible to obtain (see Theorem 6.21 in Â§6.8) the following lower bound on the minimal number of hidden units that suï¬ƒces for universal approximation: Proposition 6.1. Let d â‰¥1 and s â‰¥1. â€¢ If there is an R âˆˆFm,s with R â—¦Fd,m = âˆ†d,s then m â‰¥ 1 (d+1)2d(2s âˆ’1). â€¢ If Fd,m,s = âˆ†d,s then m â‰¥ 1 (s+d+1)(2d(2s âˆ’1) âˆ’s). In the following, we will bound the minimal number of hidden units of a universal approximator from above. First we consider the case where the output layer has ï¬xed weights and biases. Then we consider the case where all weights and biases are free parameters. 6.4.1 Fixed Weights in the Output Layer Theorem 6.2. Let d â‰¥1 and s â‰¥1. A shallow sigmoid stochastic feedforward network with d inputs, m units in the hidden layer, s outputs, and ï¬xed weights and biases in the output layer is a universal approximator of Markov kernels in âˆ†d,s whenever m â‰¥2dâˆ’1(2s âˆ’1). The theorem will be shown by constructing R âˆˆFm,s such that R â—¦Fd,m = âˆ†d,s whenever m â‰¥1 22d(2s âˆ’1). In view of the lower bound from Proposition 6.1, this upper bound is tight at least when d = 1. When there are no input units, i.e., d = 0, we may set F0,m = Em, the set of 278 Merkh and MontÃºfar: Stochastic Feedforward Neural Networks factorizable distributions of m binary variables (6.3), which has m free parameters, and âˆ†0,s = âˆ†s, the set of all probability distributions over {0,1}s. Theorem 6.2 generalizes to this case as: Proposition 6.3. Let s â‰¥2. There is an R âˆˆFm,s with R â—¦Em = âˆ†s whenever m â‰¥2s âˆ’1. This bound is always tight, since the network uses exactly 2s âˆ’1 parameters to approximate every distribution from âˆ†s arbitrarily well. For s = 1, one hidden unit is suï¬ƒcient and necessary for universal approximation. 6.4.2 Trainable Weights in the Output Layer When we allow for trainable weights and biases in both layers, we obtain a slightly more compact bound: Theorem 6.4. Let d â‰¥1 and s â‰¥2. A shallow sigmoid stochastic feedforward network with d inputs, m units in the hidden layer, and s outputs is a universal approximator of kernels in âˆ†d,s whenever m â‰¥2d(2sâˆ’1 âˆ’1). This bound on the number of hidden units is slightly smaller than that obtained for ï¬xed weights in the output layer. However, it always leaves a gap in the corre- sponding parameter-counting lower bound. As before, we can also consider the setting where there are no input units, d = 0, in which case the units in the hidden layer may assume an arbitrary product distribution, F0,m = Em. In this case we obtain: Proposition 6.5. Let s â‰¥1. Then Fm,s â—¦Em = âˆ†s, whenever m â‰¥2sâˆ’1 âˆ’1. For s = 1, the bias of the output unit can be adjusted to obtain any desired distribution, and hence no hidden units are needed. For s = 2, a single hidden unit, m = 1, is suï¬ƒcient and necessary for universal approximation, so that the bound is tight. For s = 3, three hidden units are necessary (MontÃºfar and Morton, 2015, Proposition 3.19), so that the bound is tight in this case also. 6.5 Proofs for Shallow Networks We ï¬rst give an outline of the proofs and then proceed with the analysis, ï¬rst for the case of ï¬xed weights in the output layer and then for the case of trainable weights in the output layer. Our strategy for proving Theorems 6.2 and 6.4 can be summarized as follows: â€¢ First we show that the ï¬rst layer of Fd,m,s can approximate Markov kernels arbitrarily well, which ï¬xes the state of some units, depending on the input, and 6.5 Proofs for Shallow Networks 279 y1 y2 Â· Â· Â· ys h1 Â· Â· Â· hN h1 Â· Â· Â· hN . . . h1 Â· Â· Â· hN x1 x2 x3 Â· Â· Â· xd W1 W2 W 2d 2 V1 V2 V 2d 2 Figure 6.2 Illustration of the construction used in our proof. Each block of hidden units is active on a distinct subset of possible inputs. The output layer integrates the activities of the block that was activated by the input, and produces corresponding activities of the output units. has an arbitrary product distribution over the states of the other units. The idea is illustrated in Figure 6.2. â€¢ Then we show that the second layer can approximate arbitrarily well deterministic kernels whose rows are copies of the point measures from âˆ†s, ordered in an appropriate way with respect to the diï¬€erent inputs. Note that the point measures are the vertices of the simplex âˆ†s. â€¢ Finally, we show that the set of product distributions of each block of hidden units is mapped to the convex hull of the rows of the kernel represented by the second layer, which is âˆ†s. â€¢ The output distributions of distinct sets of inputs is modeled individually by distinct blocks of hidden units, and so we obtain the universal approximation of Markov kernels. The goal of our analysis is to construct the individual pieces of the network so that they are as compact as possible. Lemma 6.6 will provide a trick that allows us to use each block of units in the hidden layer for a pair of distinct input vectors at the same time. This allows us to halve the number of hidden units that would be needed if each input had an individual block of active hidden units. Similarly, Lemma 6.11 gives a way of producing mixture components at the output layer that are more ï¬‚exible than simple point measures. This comes at the expense of allowing only one input per hidden block, but it allows us to nearly halve the number of hidden units per block, for only a slight reduction in the total number of hidden units. 6.5.1 Fixed Weights in the Output Layer The First Layer We start with the following lemma. 280 Merkh and MontÃºfar: Stochastic Feedforward Neural Networks Lemma 6.6. Let xâ€²,xâ€²â€² âˆˆ{0,1}d diï¬€er in only one entry, and let qâ€²,qâ€²â€² be any two distributions on {0,1}. Then Fd,1 can approximate the following arbitrarily well: p(Â· | x) = ï£±ï£´ï£´ï£² ï£´ï£´ï£³ qâ€², if x = xâ€² qâ€²â€², if x = xâ€²â€² Î´0, otherwise . Proof Given the input weights and bias, V âˆˆR1Ã—d and c âˆˆR, for each input x âˆˆ{0,1}d the output probability is given by p(z = 1 | x) = Ïƒ(Vx + c). (6.10) Since the two vectors xâ€²,xâ€²â€² âˆˆ{0,1}d diï¬€er in only one entry, they form an edge pair E of the d-dimensional unit cube. Let l âˆˆ[d] := {1,. . ., d} be the entry in which they diï¬€er, with xâ€² l = 0 and xâ€²â€² l = 1. Since E is a face of the cube, there is a supporting hyperplane of E. This means that there are ËœV âˆˆR1Ã—d and Ëœc âˆˆR with ËœVx + Ëœc = 0 if x âˆˆE and ËœVx + Ëœc < âˆ’1 if x âˆˆ{0,1}d \ E. Let Î³â€² = Ïƒâˆ’1(qâ€²(z = 1)) and Î³â€²â€² = Ïƒâˆ’1(qâ€²â€²(z = 1)). We deï¬ne c = Î± Ëœc + Î³â€² and V = Î± ËœV+(Î³â€²â€² âˆ’Î³â€²)eâŠ¤ l . Then, as Î± â†’âˆ, Vx + c = ï£±ï£´ï£´ï£² ï£´ï£´ï£³ Î³â€², if x = xâ€² Î³â€²â€², if x = xâ€²â€² âˆ’âˆ, otherwise . Substituting this into (6.10) proves the claim. â–¡ Given any binary vector x = (x1,. . ., xd) âˆˆ{0,1}d, let dec(x) := Ãd i=1 2iâˆ’1xi be its integer representation. Using the previous lemma, we obtain the following. Proposition 6.7. Let N â‰¥1 and m = 2dâˆ’1N. For each x âˆˆ{0,1}d, let p(Â· | x) be an arbitrary factorizing distribution from EN. The model Fd,m can approximate the following kernel from âˆ†d,m arbitrarily well: P(h | x) =Î´0(h0) Â· Â· Â· Î´0(hâŒŠdec(x)/2âŒ‹âˆ’1)p(hâŒŠdec(x)/2âŒ‹| x) Ã— Î´0(hâŒŠdec(x)/2âŒ‹+1) Â· Â· Â· Î´0(h2dâˆ’1âˆ’1), for all h âˆˆ{0,1}m,x âˆˆ{0,1}d, where hi = (hNi+1,. . ., hN(i+1)) for all i âˆˆ{0,1,. . .,2dâˆ’1 âˆ’1}. Proof We divide the set {0,1}d of all possible inputs into 2dâˆ’1 disjoint pairs with successive decimal values. The ith pair consists of the two vectors x with âŒŠdec(x)/2âŒ‹= i, for all i âˆˆ{0,. . .,2dâˆ’1 âˆ’1}. The kernel P has the property that, for the ith input pair, all output units are inactive with probability one, except those with index Ni + 1,. . ., N(i + 1). Given a joint distribution q let qj denote the corresponding marginal distribution on the states of the jth unit. By Lemma 6.6, 6.5 Proofs for Shallow Networks 281 we can set PNi+j(Â· | x) = ï£±ï£´ï£´ï£² ï£´ï£´ï£³ pj(Â· | x), if dec(x) = 2i pj(Â· | x), if dec(x) = 2i + 1 Î´0, otherwise for all i âˆˆ{0,. . .,2dâˆ’1 âˆ’1} and j âˆˆ{1,. . ., N}. â–¡ The Second Layer For the second layer we will consider deterministic kernels. Given a binary vector z, let l(z) := âŒˆlog2(dec(z) + 1)âŒ‰denote the largest j where zj = 1. Here we set l(0,. . .,0) = 0. Given an integer l âˆˆ{0,. . .,2s âˆ’1}, let bins(l) denote the s-bit representation of l; that is, the vector with dec(bins(l)) = l. Lastly, when applying any scalar operation to a vector, such as subtraction by a number, it should be understood as being performed pointwise on each vector element. Lemma 6.8. Let N = 2s âˆ’1. The set FN,s can approximate the following deter- ministic kernel arbitrarily well: Q(Â· | z) = Î´bins l(z)(Â·), for all z âˆˆ{0,1}N. In words, the zth row of Q indicates the largest non-zero entry of the binary vector z. For example, for s = 2 we have N = 3 and Q = 00 01 10 11 Â©Â­Â­Â­Â­Â­Â­Â­Â­Â­Â­Â­ Â« ÂªÂ®Â®Â®Â®Â®Â®Â®Â®Â®Â®Â® Â¬ 1 000 1 001 1 010 1 011 1 100 1 101 1 110 1 111 . Proof of Lemma 6.8 Given the input and bias weights, W âˆˆRsÃ—N and b âˆˆRs, for each input z âˆˆ{0,1}N the output distribution is the product distribution p(y | z) = 1 z(Wz+b) exp(yâŠ¤(Wz +b)) in Es with parameter Wz+b. If sgn(Wz+b) = sgn(xâˆ’1 2) for some x âˆˆ{0,1}s then the product distribution with parameters Î±(Wz + b), Î± â†’âˆ, tends to Î´x. We need to show only that there is a choice of W and b with sgn(Wz+b) = sgn( f (z)âˆ’1 2), f (z) = bins(l(z)), for all z âˆˆ{0,1}N. That is precisely the statement of Lemma 6.9 below. â–¡ We used the following lemma in the proof of Lemma 6.8. For l = 0,1,. . .,2s âˆ’1, 282 Merkh and MontÃºfar: Stochastic Feedforward Neural Networks b b 0 1 b b 2 b b b b 3 b b 0 1 b b2 b b b b 3 Figure 6.3 Illustration of Lemma 6.9 for s = 2, N = 2s âˆ’1 = 3. There is an ar- rangement of s hyperplanes which divides the vertices of a (2s âˆ’1)-dimensional cube as 0|1|2, 3|4, 5, 6, 7| Â· Â· Â· |22sâˆ’2, . . . , 22sâˆ’1 âˆ’1. the lth orthant of Rs is the set of all vectors r âˆˆRs with strictly positive or negative entries and dec(r+) = l, where r+ indicates the positive entries of r. Lemma 6.9. Let N = 2s âˆ’1. There is an aï¬ƒne map {0,1}N â†’Rs; z 7â†’Wz + b, sending {z âˆˆ{0,1}N : l(z) = l} to the lth orthant of Rs, for all l âˆˆ{0,1,. . ., N}. Proof Consider the aï¬ƒne map z 7â†’Wz + b, where b = âˆ’(1,. . .,1)âŠ¤and the lth column of W is 2l+1(bins(l)âˆ’1 2) for all l âˆˆ{1,. . ., N}. For this choice, sgn(Wz+b) = sgn(bins(l(z)) âˆ’1 2) lies in the lth orthant of Rs. â–¡ Lemma 6.9 is illustrated in Figure 6.3 for s = 2 and N = 2s âˆ’1 = 3. As another example, for s = 3 the aï¬ƒne map can be deï¬ned as z 7â†’Wz + b, where b = Â©Â­ Â« âˆ’1 âˆ’1 âˆ’1 ÂªÂ® Â¬ and W = Â©Â­ Â« 2 âˆ’4 8 âˆ’16 32 âˆ’64 128 âˆ’2 4 8 âˆ’16 âˆ’32 64 128 âˆ’2 âˆ’4 âˆ’8 16 32 64 128 ÂªÂ® Â¬ . Proposition 6.10. Let N = 2s âˆ’1 and let Q be deï¬ned as in Lemma 6.8. Then Q â—¦EN = âˆ†+ s , the interior of âˆ†s consisting of all strictly positive distributions. Proof Consider a strictly positive product distribution p âˆˆEN with p(z) = ÃN i=1 p1âˆ’zi i (1 âˆ’pi)zi for all z âˆˆ{0,1}N. Then pâŠ¤Q âˆˆâˆ†s is the vector q = (q0,q1,. . .,qN) with entries q0 = p(0) = ÃN j=1 pj and qi = Ã• z: l(z)=i p(z) = Ã• z1,...,ziâˆ’1  Ã– k<i p1âˆ’zk k (1 âˆ’pk)zk  (1 âˆ’pi)  Ã– j>i pj  = (1 âˆ’pi) Ã– j>i pj, 6.5 Proofs for Shallow Networks 283 for all i = 1,. . ., N. Therefore qi q0 = 1 âˆ’pi pi 1 Ãiâˆ’1 j=1 pj for all i = 1,. . ., N. (6.11) Since (1 âˆ’pi)/pi can be made arbitrary in (0,âˆ) by choosing an appropriate pi, independently of pj, for j < i, the quotient qi/q0 can be made arbitrary in (0,âˆ) for all i âˆˆ{1,. . ., N}. This implies that q can be made arbitrary in âˆ†+ s . In fact, each p âˆˆEN âˆ©âˆ†+ N is mapped uniquely to one q âˆˆâˆ†+ s . â–¡ Proof of Theorem 6.2 The statement follows from Propositions 6.7 and 6.10. â–¡ 6.5.2 Trainable Weights in the Second Layer In order to prove Theorem 6.4 we use the same construction of the ï¬rst layer as in the previous section, except that we use one block of hidden units for each input vector. The reason for not using a single block for a pair of inputs is that now the second layer will contribute to the modeling of the output distribution in a way that depends on the speciï¬c input of the block. For the second layer we will use the following reï¬nement of Lemma 6.8. Lemma 6.11. Let s â‰¥2 and N = 2sâˆ’1 âˆ’1. The set FN,s can approximate the following kernels arbitrarily well: Q(Â· | z) = Î»zÎ´bins 2l(z)(Â·) + (1 âˆ’Î»z)Î´bins 2l(z)+1(Â·), for all z âˆˆ{0,1}N, where Î»z are certain (not mutually independent) weights in [0,1]. Given any rl âˆˆR+, l âˆˆ{0,1,. . ., N}, it is possible to choose the Î»z such that Ã z: l(z)=l Î»z Ã z: l(z)=l(1 âˆ’Î»z) = rl, for all l âˆˆ{0,1,. . ., N}. In words, the zth row of Q is a convex combination of the indicators of 2l(z) and 2l(z) + 1, and, furthermore, the total weight assigned to 2l relative to 2l + 1 can be made arbitrary for each l. For example, for s = 3 we have N = 3 and Q = 000 001 010 011 000 001 010 011 Â©Â­Â­Â­Â­Â­Â­Â­Â­Â­Â­Â­ Â« ÂªÂ®Â®Â®Â®Â®Â®Â®Â®Â®Â®Â® Â¬ Î»000 (1 âˆ’Î»000) 000 Î»001 (1 âˆ’Î»001) 001 Î»010 (1 âˆ’Î»010) 010 Î»011 (1 âˆ’Î»011) 011 Î»100 (1 âˆ’Î»100) 100 Î»101 (1 âˆ’Î»101) 101 Î»110 (1 âˆ’Î»110) 110 Î»111 (1 âˆ’Î»111) 111 . 284 Merkh and MontÃºfar: Stochastic Feedforward Neural Networks The sum of all weights in any given even-numbered column can be made arbitrary, relative to the sum of all weights in the column right next to it, for all N + 1 such pairs of columns simultaneously. Proof of Lemma 6.11 Consider the sets Zl = {z âˆˆ{0,1}N : l(z) = l}, for l = 0,1,. . ., N. Let Wâ€² âˆˆR(sâˆ’1)Ã—N and bâ€² âˆˆRsâˆ’1 be the input weights and biases deï¬ned in Lemma 6.8. We deï¬ne W and b by appending a row (Âµ1,. . ., ÂµN) above Wâ€² and an entry Âµ0 above bâ€². If Âµj < 0 for all j = 0,1,. . ., N then z 7â†’Wz + b maps Zl to the 2lth orthant of Rs, for each l = 0,1,. . ., N. Consider now some arbitrary ï¬xed choice of Âµj, for j < l. Choosing Âµl < 0 with |Âµl| > Ã j<l |Âµl|, Zl is mapped to the 2lth orthant. If Âµl â†’âˆ’âˆthen Î»z â†’1 for all z with l(z) = l. As we increase Âµl to a suï¬ƒciently large positive value, the elements of Zl gradually are mapped to the (2l + 1)th orthant. If Âµl â†’âˆthen (1 âˆ’Î»z) â†’1 for all z with l(z) = l. By continuity, there is a choice of Âµl such that Ã z: l(z)=l Î»z Ã z: l(z)=l(1 âˆ’Î»z) = rl. Note that the images of Zj, for j < l, are independent of the ith columns of W for all i = l,. . ., N. Hence changing Âµl does not have any inï¬‚uence on the images of Zl, nor on Î»z for z: l(z) < l. Tuning Âµi sequentially, starting with i = 0, we obtain a kernel that approximates any Q of the claimed form arbitrarily well. â–¡ Let QN s be the collection of kernels described in Lemma 6.11. Proposition 6.12. Let s â‰¥2 and N = 2sâˆ’1 âˆ’1. Then QN s â—¦EN = âˆ†+ s . Proof Consider a strictly positive product distribution p âˆˆEN with p(z) = ÃN i=1 p1âˆ’zi i (1âˆ’pi)zi for allz âˆˆ{0,1}N. Then pâŠ¤Q âˆˆâˆ†s isavector(q0,q1,. . .,q2N+1) whose entries satisfy q0 + q1 = p(0) = ÃN j=1 pj and q2i + q2i+1 = (1 âˆ’pi) Ã– j>i pj, for all i = 1,. . ., N. As in the proof of Proposition 6.10, this implies that the vector (q0+q1,q2+q3,. . .,q2N +q2N+1) can be made arbitrary in âˆ†+ sâˆ’1. This is irrespective of the coeï¬ƒcients Î»0,. . .,Î»N. Now all we need to show is that we can make q2i arbitrary relative to q2i+1 for all i = 0,. . ., N. 6.5 Proofs for Shallow Networks 285 We have q2i = Ã• z: l(z)=i Î»zp(z) = Â©Â­ Â« Ã• z: l(z)=i Î»z  Ã– k<i p1âˆ’zk k (1 âˆ’pk)zk ÂªÂ® Â¬ (1 âˆ’pi)  Ã– j>i pj  , and q2i+1 = Ã• z: l(z)=i (1 âˆ’Î»z)p(z) = Â©Â­ Â« Ã• z: l(z)=i (1 âˆ’Î»z)  Ã– k<i p1âˆ’zk k (1 âˆ’pk)zk ÂªÂ® Â¬ (1 âˆ’pi)  Ã– j>i pj  . Therefore, q2i q2i+1 = Ã z: l(z)=i Î»z  Ã k<i p1âˆ’zk k (1 âˆ’pk)zk  Ã z: l(z)=i(1 âˆ’Î»z)  Ã k<i p1âˆ’zk k (1 âˆ’pk)zk  . By Lemma 6.11 it is possible to choose all Î»z arbitrarily close to zero for all z with l(z) = i and have them transition continuously to values arbitrarily close to one (independently of the values of Î»z, z: l(z) , i). Since all pk are strictly positive, this implies that the quotient q2i q2i+1 takes all values in (0,âˆ) as the Î»z, z: l(z) = i, transition from zero to one. â–¡ Proof of Theorem 6.4 This follows from a direct modiï¬cation of Proposition 6.7, so that it has a hidden block per input vector, and Proposition 6.12. â–¡ 6.5.3 Discussion of the Proofs for Shallow Networks We have proved upper bounds on the minimal size of shallow sigmoid stochastic feedforward networks that can approximate any stochastic function with a given number of binary inputs and outputs arbitrarily well. By our analysis, if all pa- rameters of the network are free, 2d(2sâˆ’1 âˆ’1) hidden units suï¬ƒce but, if only the parameters of the ï¬rst layer are free, 2dâˆ’1(2s âˆ’1) hidden units suï¬ƒce. It is interesting to compare these results with what is known about the universal approximation of Markov kernels by the undirected stochastic networks known as conditional restricted Boltzmann machines. For those networks, MontÃºfar et al. (2015) showed that 2dâˆ’1(2s âˆ’1) hidden units suï¬ƒce; if the number d of input units is large enough, 1 42d  2s âˆ’1 + 1 30  suï¬ƒce. A more recent work (MontÃºfar and Rauh, 286 Merkh and MontÃºfar: Stochastic Feedforward Neural Networks 2017) showed that an RBM is a universal approximator as soon as m â‰¥2(log(v) + 1) (v + 1) 2v âˆ’1, where v = d + s; such an RBM has a smaller asymptotic behavior. In the case of no input units, our bound 2sâˆ’1 âˆ’1 equals the bound for RBMs from MontÃºfar et al. (2011) but is larger than the bound from MontÃºfar and Rauh (2017). It has been observed that undirected networks can represent many kernels that can be represented by feedforward networks, especially when these are not too stochastic (MontÃºfar et al., 2015; MontÃºfar, 2014a). Verifying the tightness of the bounds re- mains an open problem, as well as a detailed comparison of directed and undirected architectures. 6.6 Results for Deep Networks We now consider networks with multiple layers of hidden units, i.e., L > 1. Since the dimension of âˆ†d,s is 2d(2s âˆ’1), a lower bound on the number of trainable parameters that a model needs for universal approximation is 2d(2s âˆ’1). Details on this are provided in Â§6.8. The following theorem provides suï¬ƒcient conditions for a spectrum of deep architectures to be universal approximators. Theorem 6.13. Let d, s âˆˆN, and assume that s = 2bâˆ’1 + b for some b âˆˆN. Then a deep sigmoid stochastic feedforward network with d binary inputs and s binary outputs is a universal approximator of Markov kernels in âˆ†d,s if it contains 2dâˆ’j(2sâˆ’b + 2b âˆ’1) hidden layers, each consisting of 2j(s + d âˆ’j) units, for any j âˆˆ{0,1,2,. . ., d}. We note that the indicated upper bounds on the width and depth hold for the universal approximation of âˆ†d,sâ€² for any sâ€² â‰¤s. Moreover, if j = d, we can save one layer. We will make use of this in our numerical example in Â§6.9. One can also use a simpliï¬ed construction with 2d+sâˆ’j hidden layers. The theorem indicates that there is a spectrum of networks capable of providing universal approximation such that if a network is made narrower, it must become proportionally deeper. The network topology with j = d has depth exponential in s and width exponential in d, whereas j = 0 has depth exponential in d and s, but width exponential only in d + s. See Figure 6.4 for an illustration of how j aï¬€ects the diï¬€erent network properties and shape. There may exist a spectrum of networks bridging the gap between the shallow universal approximators from Â§6.4 which have width exponential in d and s and only one hidden layer, and the j = d case, although no formal proof has been established. When considered as fully connected, the networks described in Theorem 6.13 6.6 Results for Deep Networks 287 vary greatly in their numbers of parameters. However, we will see that each of them can be implemented with the same minimal number of trainable parameters, equal to the dimension of âˆ†d,s. For the narrowest case j = 0, each input vector is carried to a speciï¬c block of hidden layers which create the corresponding output distribution; this is then passed downwards until the output layer. When j > 0 it will be shown that the ï¬rst layer can divide the input space into 2j sets, each to be handled by its own parallel section of the network. Each of the 2j sections can be thought of as running side by side and non-interacting, meanwhile creating the corresponding output distributions of the 2dâˆ’j diï¬€erent inputs. In the widest case, j = d, each input is processed by its own parallel section of the network and 2s/(2(s âˆ’b))+2(sâˆ’b)âˆ’1 hidden layers are suï¬ƒcient for creating the corresponding output distribution, which resembles the the bound for deep belief networks (MontÃºfar and Ay, 2011). x1 x2 x3 Â· Â· Â· xd ... y1 y2 y3 Â· Â· Â· ys Output layer Input layer j = 0 network topology Subsection 2d Subsection 1 (a) x1 x2 x3 Â· Â· Â· xd Â· Â· Â· y1 y2 y3 Â· Â· Â· ys Output layer Input layer j = d network topology Section 1 Section 2 Section 2d (b) (c) 0 0 d 2s+dâˆ’1   s+d s 2s+dâˆ’1 j increasing # of units (d) 0 d d + s s + d + log2   (s+d)2 s  s + 2d + log2   (s+2d)2 s  j increasing log2 # parameters Figure 6.4 (a) The deepest narrowest network architecture, j = 0. Here, there is a single section consisting of 2d subsections stacked on top of each other. (b) The widest deep architecture j = d. Here, there are 2d sections placed in parallel, each consisting of a single subsection. (c) A plot of how the number of units scales as a function of j, for ï¬xed s, d. (d) A log-scale plot of how the total number of network parameters scales with j; the lowest horizontal line shows the log of the rounded number of trainable parameters in our construction, which is independent of j. In some special instances, small reductions in size are possible. For instance, in 288 Merkh and MontÃºfar: Stochastic Feedforward Neural Networks the widest network topology considered, where j = d, universal approximation can be done with one layer less. 6.6.1 Parameter Count If we are to consider the networks as being fully connected and having all weight and bias parameters trainable, the number of parameters grows exponentially with j as |Î¸full| = O 2s+d+j(s + d âˆ’j)2 2(s âˆ’log2(s))  , suggesting that the deepest topology where j = 0 is the most eï¬ƒcient universal approximator considered. The number of units for these networks linearly decreases with j as # of units = O 2s+d(s + d âˆ’j) 2(s âˆ’log2(s))  , meaning that the widest topology, where j = d, uses the least number of units. However, if one counts only the parameters that cannot be ï¬xed prior to training according to the construction that we provide below in Â§6.7, one obtains |Î¸trainable| = 2d(2s âˆ’1). Each of the network topologies has the hidden units organized into 2j sections and a total of 2d subsections, each with 2s âˆ’1 trainable parameters. The ï¬rst hidden layer and the output layer have ï¬xed parameters. Each of the trainable parameters controls exactly one entry in the Markov kernel; the number of parameters is also necessary for universal approximation of âˆ†d,s, as we will show below in Â§6.8. 6.6.2 Approximation with Finite Weights and Biases The quality of approximation provided in our construction depends on the mag- nitudes of the network parameters. If this is allowed to increase unboundedly, a universal approximator will be able to approximate any Markov kernel with ar- bitrary accuracy. If the parameters are only allowed to have a certain maximal magnitude, the approximation is within an error bound described in the following theorem. Theorem 6.14. Let Ïµ âˆˆ(0,1/2s) and consider a target kernel pâˆ—in the non- empty set âˆ†Ïµ d,s := {P âˆˆâˆ†d,s : Ïµ â‰¤Pij â‰¤1 âˆ’Ïµ for all i, j}. There is a choice of the network parameters, bounded in absolute value by Î± = 2mÏƒâˆ’1(1 âˆ’Ïµ), where 6.7 Proofs for Deep Networks 289 m = max{j, s + (d âˆ’j)} â‰¤d + s, such that the conditional probabilities p(y | x) generated by the network are uniformly close to the target values, according to |p(y | x) âˆ’pâˆ—(y | x)| â‰¤1 âˆ’(1 âˆ’Ïµ)N + Ïµ, for all x âˆˆ{0,1}d,y âˆˆ{0,1}s, where N is the total number of units in the network excluding input units. If one considers an arbitrary target kernel pâˆ—, the error bound increases by Ïµ. The proof of Theorem 6.14 is presented in Â§6.7.4 after the proof of Theorem 6.13. It depends on explicit error bounds for the probability-sharing steps to be discussed next. 6.7 Proofs for Deep Networks The proof naturally splits into three steps. The ï¬rst step shows that the ï¬rst layer is capable of dividing the input space into 2j disjoint sets of 2dâˆ’j input vectors, sending each set to a diï¬€erent parallel-running section of the network. The 2dâˆ’j vectors in the Ï„th set will activate the Ï„th section, while the other sections take state zero with probability one. Second, it is shown that each of the 2j sections is capable of approximating the conditional distributions for the corresponding 2dâˆ’j inputs. The last step explicitly determines the parameters needed to copy the relevant units from the last hidden layer to the output layer. 6.7.1 Notation The integer j dictates the networkâ€™s topology. This index can be any number in {0,1,2,. . ., d}. For an input vector x, we denote the r through râ€² bits by x[r,râ€²]. The target conditional probability distribution given the input x is denoted by pâˆ—(Â· | x). The joint state of all units in the lth hidden layer is denoted by hl. The state of the rth unit of the lth layer is denoted by hl r. If a range is provided as superscript (subscript), it refers to a range of layers (units), i.e., h[l,l+2] refers to the hidden layers hl,hl+1,hl+2. The integer Ï„ = 1,2,. . .,2j is an index specifying a block of units in a layer. Each block consists of s + d âˆ’j consecutive units. The Ï„th block of units in a given layer comprises those indexed from (Ï„ âˆ’1)(s + d âˆ’j) + 1 to Ï„(s + d âˆ’j). The state of the Ï„th block of units of hidden layer l is hl (Ï„) := hl [(Ï„âˆ’1)(s+dâˆ’j)+1, Ï„(s+dâˆ’j)]. If a block activates, this means that it can take a state other than the zero vector with non-zero probability. If the block is inactive, it will take the zero state with arbitrarily high probability. Owing to their diï¬€erent functions in what follows, it is useful to denote by al (Ï„) and bl (Ï„) the ï¬rst s and the last d âˆ’j units of the Ï„th block at the lth layer. The ï¬rst unit of al (Ï„) is denoted as al (Ï„),1; it plays an important role in the ï¬rst layer of the network. 290 Merkh and MontÃºfar: Stochastic Feedforward Neural Networks For the second part of the proof, we will ï¬rst show that L = 2s+dâˆ’j hidden layers suï¬ƒce, and then reï¬ne this bound. Thus the focus will be on the entire Ï„th section of hidden units. The Ï„th section comprises the units in the Ï„th block over all the hidden layers, written without a superscript as h(Ï„) := h[1,2s+dâˆ’j] (Ï„) . Each section is broken into subsections, indexed by q = 1,2,. . .,2dâˆ’j. The qth subsection of the Ï„th section is h(q) (Ï„) := h[(qâˆ’1)2s+1,q2s] (Ï„) . Lastly, each subsection of the network will have a Gray code associated with it, i.e., a sequence of binary vectors in which subsequent vectors diï¬€er in only one bit. In actuality, since each subsection will be capable of performing not just one but rather multiple tasks per layer, it will have a set of partial Gray codes associated with it, to be deï¬ned in the following. See Figure 6.5 for an illustration of the notation described here. 6.7.2 Probability Mass Sharing For ï¬xed weights and a given input, a width-m layer of the network will exhibit a marginal distribution p âˆˆâˆ†m. A subsequent width-m layer determines a particular mapping of p to another distribution pâ€² âˆˆâˆ†m. For certain choices of the parameters, this mapping transforms p in such a way that a fraction of the mass of a given state g âˆˆ {0,1}m is transferred to some other state Ë†g âˆˆ{0,1}m, so that pâ€²(Ë†g) = p(Ë†g) + Î»p(g), pâ€²(g) = (1 âˆ’Î»)p(g), and pâ€²(z) = p(z) for all other states z. This mapping is referred to as probability mass sharing, and was exploited in the works of Sutskever and Hinton (2008), Le Roux and Bengio (2010), andMontÃºfar and Ay (2011). One important takeaway from these works is that probability mass sharing in one layer is restrictive, and the states g and Ë†g need to stand in a particular relation to each other, e.g., as Hamming neighbors. MontÃºfar and Morton (2012) gave a description of the mappings that are expressible by the individual layers of a Bayesian sigmoid belief network. In the following we describe ways in which probability mass sharing is possible. We deï¬ne Gray codes and partial Gray codes, as they will be useful in discussion of probability mass sharing sequences. A Gray code G for s bits is a sequence of vectors gi âˆˆ{0,1}s, i = 1,. . .,2s, such that âˆ¥gi âˆ’giâˆ’1âˆ¥H = 1 for all i âˆˆ{2,. . .,2s}, and Ã i{gi} = {0,1}s. One may visualize such a code as tracing a path along the edges of an s-cube without ever returning to the same vertex but covering all the vertices. A partial Gray code Si for s bits is a sequence of vectors Si,j âˆˆ{0,1}s, j = 1,. . .,r â‰¤2s, such that âˆ¥Si,j âˆ’Si,jâˆ’1âˆ¥H = 1 for all j âˆˆ{1,. . .,r}. We will be interested in collections of partial Gray codes which contain the same number of vectors and which partition {0,1}s. In the analysis, subsections of the network will activate to the values of diï¬€erent partial Gray codes with appropriate probabilities. The ï¬rst proposition states that, when considering two consecutive layers l âˆ’1 6.7 Proofs for Deep Networks 291 h(1) (1) h(2) b8 (1) a8 (1) a6 (1),1 a6 (2),1 Input layer Output layer Figure 6.5 The network architecture from Theorem 6.13 for j = 1, with d = 2 inputs and s = 2 outputs. The ï¬gure exempliï¬es the notation deï¬ned in Â§6.7.1. The white connections (which are present also in the white section of the ï¬gure) are set to zero, separating the hidden units into 2dâˆ’j parallel running sections. Here there are two sections indexed by Ï„ = 1, 2, and in each section there are two subsections indexed by q = 1, 2. The network has 2d subsections in total when they are added across all sections. Each output vector is generated with an appropriate conditional probability, given the input vector x, by mapping the input through a corresponding sequence of states of a(Ï„) with appropriate probabilities; information about the input is preserved throughout the hidden layers by b(Ï„). and l, there exists a weight vector Wl i and bias bl i such that hl i will be a copy of hlâˆ’1 i with arbitrarily high probability. See Sutskever and Hinton (2008, Section 3.2) or Le Roux and Bengio (2010, Section 3.3) for equivalent statements. Proposition 6.15. Fix Ïµ âˆˆ(0,1/2) and Î± = log (1 âˆ’Ïµ)âˆ’log (Ïµ). Choose the ith row of weights Wl i such that Wl ii = 2Î± and Wl ij = 0 for j , i, and choose the ith bias 292 Merkh and MontÃºfar: Stochastic Feedforward Neural Networks bl i = âˆ’Î±. Then Pr(hl i = hlâˆ’1 i | hlâˆ’1) = (1 âˆ’Ïµ) for all hlâˆ’1. Letting Î± â†’âˆallows the unit to copy the state of a unit in the previous layer with arbitrarily high probability. Proof For the given choice of weights and bias, the total input to the ith unit in layer l will be 2Î±hlâˆ’1 i âˆ’Î±, meaning that Pr(hl i = 1 | hlâˆ’1) = ( Ïƒ(Î±) when hlâˆ’1 i = 1 Ïƒ(âˆ’Î±) when hlâˆ’1 i = 0. (6.12) Since Î± = Ïƒâˆ’1(1 âˆ’Ïµ), one has that Pr(hl i = hlâˆ’1 i | hlâˆ’1) = (1 âˆ’Ïµ). â–¡ The next theorem states that the weights Wl i and bias bl i of the ith unit in layer l may be chosen such that if hlâˆ’1 matches any of two pre-speciï¬ed vectors g and Ë†g in {0,1}m then hl i ï¬‚ips hlâˆ’1 i with a pre-speciï¬ed probability. Otherwise, if hlâˆ’1 is not g or Ë†g, the bit is copied with arbitrarily high probability. This corresponds to Theorem 2 of Le Roux and Bengio (2010), adjusted to our notation. Theorem 6.16 (Theorem 2 of Le Roux and Bengio, 2010). Consider two adjacent layers l âˆ’1 and l of width m. Consider two vectors g and Ë†g in {0,1}m with âˆ¥gâˆ’Ë†gâˆ¥H = 1, diï¬€ering in entry j. Fix two probabilities Ï, Ë†Ï âˆˆ(0,1) and a tolerance Ïµ âˆˆ(0,1/2). For any i , j, there exist (Wl i, bl i) with absolute values at most Î± = 2m(Ïƒâˆ’1(1 âˆ’Ïµ)) such that Pr(hl i = 1 | hlâˆ’1 = g) = ÏÏµ, Pr(hl i = 1 | hlâˆ’1 = Ë†g) = Ë†ÏÏµ; otherwise, Pr(hl i = hlâˆ’1 i | hlâˆ’1 , g, Ë†g) = 1 âˆ’Ïµ. Here ÏÏµ = max{Ïµ,min{Ï,1 âˆ’Ïµ}}. If we ï¬x a maximum parameter magnitude Î± instead of a tolerance then we can substitute Ïµ = 1 âˆ’Ïƒ(Î±/2m). This theorem allows to have a given vector g âˆˆ{0,1}m map at the subsequent layer to itself or to a Hamming adjacent vector gâ€² âˆˆ{0,1}m with a pre-speciï¬ed probability Ï, with Pr(hl = g | hlâˆ’1 = g) = ÏÏµ(1 âˆ’Ïµ)mâˆ’1, (6.13) Pr(hl = gâ€² | hlâˆ’1 = g) = (1 âˆ’ÏÏµ)(1 âˆ’Ïµ)mâˆ’1, (6.14) and Pr(hl = hlâˆ’1 | hlâˆ’1 , g) = (1 âˆ’Ïµ)m, (6.15) where Ïµ can be made arbitrarily small if the maximum magnitude of weights and biases, Î± = 2m(Ïƒâˆ’1(1 âˆ’Ïµ)), is allowed to grow to inï¬nity. This mapping is referred 6.7 Proofs for Deep Networks 293 to as a probability mass sharing step, or a sharing step for short. This in turn allows the transfer of probability mass around the m-cube, one vertex at a layer, until the correct probability mass resides on each binary vector, to the given level of accuracy. The sharing path follows a Gray code, each pair of consecutive vectors having Hamming distance one. In fact, the theorem allows us to overlay multiple sharing paths, so long as the Gray codes are suï¬ƒciently separated. A collection of partial codes satisfying this requirement and covering the set of binary strings is described in the following theorem, which is Lemma 4 of MontÃºfar and Ay (2011). Theorem 6.17 (Lemma 4 of MontÃºfar and Ay, 2011). Let m = 2b 2 +b, b âˆˆN, b â‰¥1. There exist 2b = 2(m âˆ’b) sequences Si, 1 â‰¤i â‰¤2b, composed of length-m binary vectors Si,k, 1 â‰¤k â‰¤2mâˆ’b, satisfying the following: (i) {S1,. . .,S2b } is a partition of {0,1}m. (ii) The vectors S1,1,. . .,S2b,1 share the same values in their last m âˆ’b bits. (iii) The vector (0,. . .,0) is the last element S1,2mâˆ’b of the ï¬rst sequence. (iv) For all i âˆˆ{1,. . .,2b} and k âˆˆ{1,. . .,2mâˆ’bâˆ’1}, we have âˆ¥Si,k,Si,k+1âˆ¥H = 1. (v) For all i,r âˆˆ{1,. . .,2b} such that i , r and, for all k âˆˆ{1,. . .,2mâˆ’b âˆ’1}, the bit switched between Si,k and Si,k+1 and the bit switched between Sr,k and Sr,k+1 are diï¬€erent, unless âˆ¥Si,k âˆ’Sr,kâˆ¥H = 1. This theorem describes a schedule that allows for probability to be shared from 2(m âˆ’b) vectors per layer, starting from the vectors Si,1, i = 1,. . .,2(m âˆ’b). For every layer l, if hlâˆ’1 matches Si,lâˆ’1, probability mass will be shared onto Si,l, for each i = 1,. . .,2(m âˆ’b), and it will be copied unchanged otherwise. The accuracy of the transition probabilities depends on the maximum allowed magnitudes of the weights and biases, similarly to equation (6.15). 6.7.3 Universal Approximation The First Layer The ï¬rst step of the proof focuses on the ï¬‚exibility of the ï¬rst layer of the network. For ï¬xed d, s, j,there are 2j(s+dâˆ’j) units in h1 belonging to 2j consecutive blocks, indexed by Ï„ = 1,. . .,2j. Within each block, set the parameters according to Proposition 6.15 to copy the last d âˆ’j bits of the input x, so that b1 (Ï„) = x[j+1,d] with probability (1 âˆ’Ïµ)jâˆ’d and parameters of magnitude no more than Î± = 2Ïƒâˆ’1(1 âˆ’Ïµ). Within each block, a1 (Ï„) will activate with probability close to one for exactly 2dâˆ’j inputs x. This can be done by setting the parameters of the ï¬rst unit a1 (Ï„),1 of each block in such a way that the unit takes state 1 only if the ï¬rst j bits of x agree 294 Merkh and MontÃºfar: Stochastic Feedforward Neural Networks with the number Ï„ of the block. This is formalized in the following lemma. The remainder units a1 (Ï„),i, i = 2,. . ., s, are set to take the zero state with probability 1 âˆ’Ïµ, by choosing their weights to be zero and bias equal to âˆ’Ïƒâˆ’1(1 âˆ’Ïµ). Lemma 6.18. Fix Ï„ âˆˆ{1,2,. . .,2j} and Ïµ âˆˆ(0,1/2). Let S =  x âˆˆ{0,1}d : int(x) 2dâˆ’j  + 1 = Ï„ . Then there exist weights W âˆˆR1Ã—d and biases b âˆˆR for the ï¬rst unit of the Ï„th block having absolute values at most Î± = 2dÏƒâˆ’1(1 âˆ’Ïµ), such that Pr  a1 (Ï„),1 = 1 | x = ( 1 âˆ’Ïµ, x âˆˆS Ïµ, x < S. (6.16) Proof The probability of unit a1 (Ï„),1 activating is given by Pr  a1 (Ï„),1 = 1 | x = Ïƒ(Wx + b). (6.17) Note that S is the set of length-d binary vectors whose ï¬rst j bits equal the length- j binary vector g with integer representation Ï„. Geometrically, S is a (d âˆ’j)- dimensional face of the d-hypercube. In turn, there exists an aï¬ƒne hyperplane in Rd separating S from {0,1}d \ S. For instance, we may choose W = Î³(2(g âˆ’1 2),0)âŠ¤ and b = Î³(âˆ¥gâˆ¥1 âˆ’1 2), which gives Wx + b = 1 2Î³ for all x âˆˆS and Wx + b â‰¤âˆ’1 2Î³ for all < S. Choosing Î³ = 2Ïƒâˆ’1(1 âˆ’Ïµ) yields the claim. â–¡ Note that x âˆˆS is equivalent to x[1,j] = g, where g is the j-bit representation of Ï„. Following (6.13)â€“(6.15), the second bit can also be activated, as Pr  a1 (Ï„),2 = 1 | x = ( ÏÏµ, x[1,j] = g Ïµ, x[1,j] , g , (6.18) for any chosen Ï âˆˆ[0,1]. We will be able to use this type of initialization to save one layer when j = d, where there is only one subsection per section. The Hidden Layers In the second part of our construction, the focus is restricted to individual sections of the network, having width s +(d âˆ’j) and L = 2dâˆ’j(2sâˆ’b +2(s âˆ’b)âˆ’1) layers. To prevent separate sections from interfering with one another, set all weights between the units in sections Ï„ and Ï„â€² to zero, for all Ï„â€² , Ï„. The Ï„th section will only be contingent upon its parameters and h1 (Ï„), which can be regarded as the input to the section. Each section is responsible for approximating the target conditional distributions of 2dâˆ’j inputs. Each section should be thought of as consisting of 2dâˆ’j subsections in sequence, each consisting of 2sâˆ’b + 2(s âˆ’b) âˆ’1 consecutive layers. 6.7 Proofs for Deep Networks 295 Each subsection is responsible for approximating the target conditional distribu- tion of a single input x. The ï¬rst layer of any subsection copies the state from the previous layer, except for the very ï¬rst subsection, which we described above. Sub- section q will be â€œactivatedâ€ if al (Ï„) = (1,0,. . .,0) and bl (Ï„) takes the speciï¬c value bindâˆ’j(q), where l is the ï¬rst layer of the subsection. When a subsection is activated, it will carry out a sequence of sharing steps to generate the output distribution. This can be achieved in 2s layers by applying a single sharing step per layer, with sched- ule given by a Gray code with initial state (1,0,. . .,0) and ï¬nal state (0,. . .,0). If only single sharing steps are used then the parameters which need to be trainable are biases. Alternatively, we can arrange for the ï¬rst 2(s âˆ’b) layers of the subsection to conduct probability sharing to distribute the mass of al (Ï„) = (1,0,. . .,0) across the initial states Si,1, i = 1,. . .,2(s âˆ’b), of the partial Gray codes from Theorem 6.17. Following this, the subsection overlays the 2b = 2(s âˆ’b) sequences of sharing steps with the schedule from Theorem 6.17, to generate the output distribution. When the subsection is not activated, it copies the incoming vector downwards until its last layer. By the construction one can see that if a1 (Ï„) = 0, probability mass is never transferred away from this state, meaning that the last hidden layer of the section takes the state aL (Ï„) = 0 with probability close to one. Therefore the blocks of the ï¬nal hidden layer will be distributed as aL (Ï„) âˆ¼ ( pâˆ—(Â· | x), if  âŒŠint(x) 2dâˆ’j âŒ‹+ 1 = Ï„ Î´0, otherwise. (6.19) We can obtain a slight improvement when j = d and there is only one subsection per section. In this case, we can set two of the initial states Si,1, i = 1,. . .,2(s âˆ’b), as (1,0,0,. . .,0) and (1,1,0,. . .,0). We initialize the ï¬rst hidden layer by (6.18), which allows us to place probabilities ÏÏµ and 1 âˆ’ÏÏµ on these two states and so to save one of the 2(s âˆ’b) sharing steps that are used to initialize the partial Gray codes. The Output Layer The third and ï¬nal step of the proof speciï¬es how the last layer of the network will copy the relevant block of the ï¬nal hidden layer in such a way that the output layer exhibits the correct distribution. To this end, we just need the ith unit of the output layer to implement an or gate over the ith bits of all blocks, for i = 1,. . ., s. This can be achieved, with probability 1 âˆ’Ïµ, by setting the the bias of each output unit as âˆ’Î±, and the weight matrix W âˆˆRsÃ—2j(s+dâˆ’j) of the output layer as W = Î±  Is | Z | Is | Z | Â· Â· Â· | Is | Z  , (6.20) 296 Merkh and MontÃºfar: Stochastic Feedforward Neural Networks where Is is the s Ã— s identity matrix, Z is the s Ã— (d âˆ’j) zero matrix, and Î± = 2Ïƒâˆ’1(1 âˆ’Ïµ). This concludes the proof of Theorem 6.13. Input layer Output layer 1 0 0 1 1 0 0 1 0 0 0 0 1 0 1 1 1 1 0 1 1 0 0 1 Figure 6.6 The narrow network topology for d = 1, s = 2, j = 0, where only one probability- sharing step is utilized per layer, and the Gray codes specifying them are shown. This network is a universal approximator of Markov kernels in âˆ†1,2. The units shown in blue will copy the input state throughout the network. The ï¬rst hidden layer of the network will either be â€œ100â€ or â€œ101â€ depending on the input. If it is the former, only the ï¬rst subsection h[1,4] will perform probability mass sharing, resulting in h4 distributed as pâˆ—(Â· | x = 0). If it is the latter, probability mass sharing will only occur in h[5,8]. Only the non-zero weights are shown. This network requires only six trainable parameters, which are all biases, indicated by the bold outlined units. Example 6.19. Consider the case where d = 1 and s = 2 and the simple sharing scheme uses a Gray code. The narrowest case where j = 0 is a network consisting of 2 Ã— 22 = 8 layers, each of width 2 + 1. There is only one section, and it consists of two subsections, one for each possible input. The ï¬rst subsection is responsible for approximating pâˆ—(Â· | x = 0) and the second for pâˆ—(Â· | x = 1). See Figure 6.6 for an illustration of the network and the Gray codes used to specify the sharing steps. 6.7.4 Error Analysis for Finite Weights and Biases The proof construction demonstrates that we may conduct sequential probability sharing steps according to Gray codes which specify a unique path from every x to every y. Given an input x, the path generating y will occur with probability pâˆ—(y | x) as the size of the parameters becomes inï¬nitely large. If the magnitude of the parameters is bounded by Î± = 2m(Ïƒâˆ’1(1 âˆ’Ïµ)) and we consider a target kernel P âˆˆâˆ†Ïµ d,s := {P âˆˆâˆ†d,s : Ïµ â‰¤Pij â‰¤1 âˆ’Ïµ for all i, j}, we may utilize Theorem 6.16 to compute bounds on the probability of the path intended to generate y. Any other paths from x to y will have a low probability. The details are as follows. Proof of Theorem 6.14 Fix an Ïµ âˆˆ(0,1/2s) and pâˆ—âˆˆâˆ†Ïµ d,s. Suppose that a network from Theorem 6.13 has L hidden layers of width m. Without loss of generality, 6.7 Proofs for Deep Networks 297 assume that y = gr for some r âˆˆ{1,. . .,2s} where the sequence {gl} is the Gray code deï¬ning the sharing steps. Recall that p(y | x) may be written as p(y | x) = Ã• h1 Â· Â· Â· Ã• hL p(y | hL) Â· Â· Â· p(h1 | x) = Ã• h p(y | hL) Â· Â· Â· p(h1 | x). (6.21) Note that most terms in this sum are O(Ïµ) or smaller when using the proof construc- tion for Theorem 6.13. The one term that is larger than the rest is the term where the hidden layers activate as the sequence h1 = g1,. . .,hr = gr,hr+1 = gr,. . .,hL = gr. In particular, if the parameters in the network were inï¬nitely large, this sequence of hidden layer activations would occur with probability exactly pâˆ—(y | x) by con- struction. Denote this sequence by T and let p(y,T | x) denote the probability of observing this sequence, p(y,T | x) := p(y | hL = gr)p(hL = gr | hLâˆ’1 = gr) Â· Â· Â· p(hr = gr | hrâˆ’1 = grâˆ’1) Â· Â· Â· p(h1 = g1 | x). When the magnitude of the weights is bounded by 2mÏƒâˆ’1(1 âˆ’Ïµ), Theorem 6.16 provides the error terms for each p(hl = gl | hlâˆ’1 = glâˆ’1). Speciï¬cally, we have that p(h1 = g1 | x) = (1 âˆ’Ïµ)m, p(h2 = g2 | h1 = g1) = Ï[1](1 âˆ’Ïµ)mâˆ’1, ... ... p(hr = gr | hrâˆ’1 = grâˆ’1) = Ï[râˆ’1](1 âˆ’Ïµ)mâˆ’1, p(hr+1 = gr | hr = gr) = (1 âˆ’Ï[r])(1 âˆ’Ïµ)mâˆ’1, p(hr+2 = gr | hr+1 = gr) = (1 âˆ’Ïµ)m, ... ... p(hL = gr | hLâˆ’1 = gr) = (1 âˆ’Ïµ)m, p(y | hL = gr) = (1 âˆ’Ïµ)s, where Ï[l] are the transfer probabilities between layers l and l + 1 discussed in Theorem 6.16. We point out that for the output of the network to be y = gr, the complementary sharing probability must occur at layer l = r, i.e., it is (1 âˆ’Ï[r]). Additionally, we point out that pâˆ—(y | x) = Ï[1]Ï[2] Â· Â· Â· Ï[râˆ’1](1 âˆ’Ï[r]). With this, the 298 Merkh and MontÃºfar: Stochastic Feedforward Neural Networks bound in Theorem 6.14 may be derived as follows: |p(y | x) âˆ’pâˆ—(y | x)| = Ã• h1 Â· Â· Â· Ã• hL p(y | hL) Â· Â· Â· p(h1 | x) âˆ’pâˆ—(y | x) â‰¤|p(T,y | x) âˆ’pâˆ—(y | x)| + Ã• (h1,...,hL),T p(y | hL) Â· Â· Â· p(h1 | x) < |p(T,y | x) âˆ’pâˆ—(y | x)| + Ïµ = |Ï[1] Â· Â· Â· Ï[râˆ’1](1 âˆ’Ï[r])(1 âˆ’Ïµ)mLâˆ’r+s âˆ’pâˆ—(y | x)| + Ïµ = |pâˆ—(y | x)(1 âˆ’Ïµ)mLâˆ’r+s âˆ’pâˆ—(y | x)| + Ïµ = pâˆ—(y | x)|1 âˆ’(1 âˆ’Ïµ)mLâˆ’r+s| + Ïµ < 1 âˆ’(1 âˆ’Ïµ)mLâˆ’r+s + Ïµ. In the third line, the second term is upper bounded by Ïµ because each term in the sum has at least one factor Ïµ and the sum itself cannot be larger than 1. Since mL âˆ’r + s â‰¤N, where N is the total number of units in the network excluding the input units, for any r âˆˆ2s, we can uniformly bound the diï¬€erence in each probability using 1 âˆ’(1 âˆ’Ïµ)N. It remains to show that if pâˆ—âˆˆâˆ†Ïµ d,s then, for each x, the factorization pâˆ—(gr | x) = Ï[1] Â· Â· Â· Ï[râˆ’1](1 âˆ’Ï[r]) has factors in [Ïµ,1 âˆ’Ïµ] for each gr. Since pâˆ—(g1 | x) = (1âˆ’Ï[1]) â‰¥Ïµ, we have that Ï[1] â‰¤1âˆ’Ïµ. Similarly, since pâˆ—(g2 | x) = Ï[1](1âˆ’Ï[2]) â‰¥Ïµ and (1 âˆ’Ï[2]) â‰¤1, we have that Ï[1] â‰¥Ïµ. The same argument applies recursively for all r. Finally, for an arbitrary target kernel pâˆ—âˆˆâˆ†d,s, one ï¬nds an approximation pâˆ—,Ïµ âˆˆâˆ†Ïµ d,s with |pâˆ—(y | x) âˆ’pâˆ—,Ïµ(y | x)| â‰¤Ïµ and |p(y | x) âˆ’pâˆ—(y | x)| â‰¤|p(y | x) âˆ’pâˆ—,Ïµ(y | x)| + Ïµ. â–¡ 6.7.5 Discussion of the Proofs for Deep Networks Since universal approximation was shown for the shallow case, it follows that any stochastic feedforward network of width at least 2d(2sâˆ’1 âˆ’1) with s â‰¥2, d â‰¥1, and L â‰¥1 hidden layers is a universal approximator. The proof above reï¬nes this bound by showing that, as a network is made deeper, it may be made proportionally narrower while still remaining a universal approximator. This proof applies to network topologies indexed by j âˆˆ{0,1,. . ., d}, where the shallowest depth occurs when j = d; 2sâˆ’b + 2(s âˆ’b) hidden layers is suï¬ƒcient, with b âˆ¼log2(s). This leaves open whether there is a spectrum of networks between the j = d case and the shallow case which are also universal approximators. Beyond the narrow side of the spectrum, where j = 0, it is also open whether narrower universal approximators 6.8 Lower Bounds for Shallow and Deep Networks 299 exist. This is due to the proof technique, which relies on information about the input being passed from layer to layer. We point out that the universal approximation of âˆ†d,s requires unbounded pa- rameters. Indeed, if we want to express a conditional probability value of 1 as the product of conditional probabilities expressed by the network then some of these factors need to have entries 1. On the other hand it is clear that a sigmoid unit only approaches the value 1 as its total input tends to inï¬nity, which requires that the parameters tend to inï¬nity. We have provided bounds on the approximation errors when the parameters are bounded in magnitude. However, it is left open whether the universal approximation of kernels in âˆ†Ïµ d,s, with entries bounded away from 0 and 1, is possible with ï¬nite weights. The reason is that our proof technique relies on inducing nearly deterministic behavior in many of the computational units by sending the weights toward inï¬nity. Nonetheless, as shown in our bounds and illus- trated in Â§6.9, most of the approximation quality is already present for moderately sized weights. The networks that we have discussed here are optimal in the sense that they only utilize 2d(2s âˆ’1) trainable parameters. This follows from the observation that each probability-mass-sharing step has exactly one parameter that depends on the kernel being approximated. Further improvements on ï¬nding more compact universal approximators, in the sense that they have fewer units, may be possible. It remains of interest to determine the tightness of our theorems regarding the number of units. Lower bounds for the width and overall number of parameters of the network can be determined by the information-theoretic and geometric techniques to be discussed next. 6.8 Lower Bounds for Shallow and Deep Networks 6.8.1 Parameter Counting Lower Bounds The following theorem establishes a lower bound on the number of parameters needed in a network for universal approximation to be possible. It veriï¬es the intuition that the number of trainable parameters of the model needs to be at least as large as the dimension of the set of kernels that we want to approximate arbitrarily well. This result is needed in order to exclude the possibility of the type of lower- dimensional universal approximator that is a space-ï¬lling curve. The proof is based on ï¬nding a smooth parametrization of the closure of the model and then applying Sardâ€™s theorem (Sard, 1942). We start with the parametrization. Proposition 6.20. The closure of the set of kernels Fd,m1,...,mL,s âŠ†âˆ†d,s represented by any Bayesian sigmoid belief network can be parametrized in terms of a ï¬nite 300 Merkh and MontÃºfar: Stochastic Feedforward Neural Networks collection of smooth maps with compact parameter space of the same dimension as the usual parameter space. Proof Since the composition of units to create a network corresponds to taking matrix products and since marginalization corresponds to adding entries of a matrix, both of which are smooth maps, it suï¬ƒces to prove the statement for a single unit. Consider the set Fm,1 of kernels in âˆ†m,1 represented by a unit with m binary inputs. The usual parametrization takes w = (w0,w1,. . .,wm) âˆˆRm+1 to the kernel in âˆ†m,1 given by the 2 Ã— 2m matrix h Ïƒ  Ã•m j=0 wjhj  ,Ïƒ  âˆ’ Ã•m j=0 wjhj i hâˆˆ{1}Ã—{0,1}m, where any h = (h0, h1,. . ., hm) has ï¬rst entry h0 = 1. We split the parameter space Rm+1 into the 2m+1 closed orthants. Fix one of the orthants Rm+1 S , which is speciï¬ed by a partition of {0,1,. . .,m} into a set S of coordinates that are allowed to be negative and a complementary set L of coordinates that are allowed to be positive. Now consider the bijection w âˆˆRm+1 S â†’[Ï‰,Î³] âˆˆ(0,1]m+1 with Ï‰j = exp(wj) for each j âˆˆS and Î³j = exp(âˆ’wj) for each j < S. Then Ïƒ  Ã•m j=0 wjhj  = exp(Ãm j=0 wjhj) exp(Ãm j=0 wjhj) + 1 = Ãm j=0 exp(wjhj) Ãm j=0 exp(wjhj) + 1 = Ã jâˆˆS exp(wjhj) Ã jâˆˆS exp(wjhj) + Ã j<S exp(âˆ’wjhj) = Ã jâˆˆS Ï‰hj j Ã jâˆˆS Ï‰hj j + Ã j<S Î³hj j . This deï¬nes a smooth map ÏˆS : [0,1]m+1 â†’âˆ†m,1 (or a ï¬nite family of smooth maps over the relative interiors of the faces of [0,1]m+1). Since the map ÏˆS is continuous and its domain is compact and its co-domain is Hausdorï¬€, the image ÏˆS([0,1]m+1) is closed. In turn, the union over diï¬€erent orthants Ã S ÏˆS([0,1]m+1) is a closed set which contains Fm,1 as a dense subset, so that it is equal to Fm,1. â–¡ Theorem 6.21. Consider a stochastic feedforward network with d binary inputs and s binary outputs. If the network is a universal approximator of Markov kernels in âˆ†d,s, then necessarily the number of trainable parameters is at least 2d(2s âˆ’1). The space of Markov kernels is âˆ†d,s = âˆ†sÃ—Â· Â· Â·Ã—âˆ†s (2d times), and has dimension 2d(2s âˆ’1). This theorem states that at least one parameter is needed per degree of freedom of a Markov kernel. 6.8 Lower Bounds for Shallow and Deep Networks 301 Proof Consider one of the smooth and closed maps Ïˆ appearing in Proposi- tion 6.20 and denote its input space by â„¦= [0,1]k. Sardâ€™s theorem states that the set of critical values of a smooth map is a null set. If the input-space dimension is less than the output-space dimension then every point is a critical point and the image of the map is a null set. Therefore, we conclude that if dim(â„¦) = k is less than 2d(2s âˆ’1), the set Ïˆ(â„¦) = Ïˆ(â„¦) is a null set. Since the closure of the model is a ï¬nite union of such sets, it cannot possibly be a universal approximator if the dimension is of the parameter space is less than indicated. â–¡ 6.8.2 Minimum Width A universal approximator cannot have layers that are too narrow. We can show this by utilizing the data processing-inequality. Another proof uses the combinatorics of the tuples of factorizing distributions represented by a layer of stochastic units. We start with the approach based on the data-processing inequality. To be precise, consider the mutual information of two discrete random vectors X and Y, which is deï¬ned as MI(X; Y) = H(Y) âˆ’H(Y | X), (6.22) where H(Y) = âˆ’Ã y p(y) log p(y) stands for the entropy of the probability distribu- tion of Y and H(Y | X) = âˆ’Ã x p(x) Ã y p(y | x) log p(y | x) stands for the conditional entropy of Y given X. If the state spaces are X and Y then the maximum value of the mutual information is min{log |X|,log |Y|}. This value is attained by any joint distribution for which one of the variables is uniformly distributed and its state is fully determined by the observation of the other variable. The data processing inequality states that if a joint distribution satisï¬es the Markov chain p(x,h,y) = p(x)p(h | x)p(y | h) then the mutual information behaves monotonically in the sense that MI(X; Y) â‰¤MI(X; H). Note that this inequality is independent of how the conditional distributions are parametrized, and in special cases there might exist stronger inequalities. From this generic inequality we infer the following. Proposition 6.22. Consider a sigmoid stochastic feedforward network with d inputs and s outputs. If the network is a universal approximator of Markov kernels in âˆ†d,s then each hidden layer has at least min{d, s} units. Proof The network is a universal approximator of Markov kernels if and only if the model augmented to include arbitrary probability distributions over the inputs is a universal approximator of joint distributions over inputs and outputs. In view of the data-processing inequality, if any of the hidden layers has less than min{d, s} units then the joint distributions of inputs and outputs represented by the network 302 Merkh and MontÃºfar: Stochastic Feedforward Neural Networks satisfy non-trivial inequalities regarding the mutual information, meaning that an open non-trivial set of joint distributions is excluded. â–¡ We can strengthen this result further in the case where the number of inputs is smaller than the number of outputs. Proposition 6.23. Consider stochastic feedfoward networks with d inputs and s > d outputs. â€¢ If d â‰¥0 and s â‰¥2, the last hidden layer of a universal approximator has at least s âˆ’1 units when s is even and at least s units when s is odd. â€¢ If d â‰¥1, the last hidden layer of a universal approximator has at least s units. Proof Note that the output distribution is a mixture of the conditional distributions of the output layer given all possible values of the second-to-last layer, all of which are factorizing distributions. Further, note that if a factorizing distribution has support strictly contained in the set of even (or odd) parity strings then it must be a point measure. Consider d â‰¥0 and, as a desired output distribution, the uniform distribution on strings of even parity. In order for the the network to represent this, the last kernel needs to contain the 2sâˆ’1 point measures on even parity strings as rows. In turn, the last hidden layer must have at least s âˆ’1 units. The lower bound s results from the fact that the rows of the kernels are not arbitrary product distributions. Indeed, for a module with m input units, the 2m rows of the kernel are factorizing distributions with shared parameters of the form Wh + b, h âˆˆ{0,1}m. The parameter vector of a point measure that is concentrated on a given vector y âˆˆ{0,1}s is a vector on the yth orthant of Rs. The set of parameters Wh + b, h âˆˆ{0,1}m, intersects all even-parity orthants of Rs only if m â‰¥s (see MontÃºfar and Morton, 2015, Proposition 3.19). Now consider d â‰¥1 and, as a desired pair of output distributions for two diï¬€erent inputs, a distribution supported strictly on the even-parity strings and a distribution supported on the odd-parity strings. This requires that the last kernel has all 2s point measures as rows, and hence at least s inputs. â–¡ An example of Proposition 6.23 is shown in Figure 6.7. 6.9 A Numerical Example The previous theory is constructive in the sense that, given a speciï¬c Markov kernel P âˆˆâˆ†d,s, for any choice of the shape coeï¬ƒcient j the parameters are explicitly determined. To validate this for the d = 2, s = 2 case, 500 Markov kernels in âˆ†2,2 were generated uniformly at random by sampling from the Dirichlet distribution with parameter 1. For each kernel, a network consisting of the parameters speciï¬ed 6.9 A Numerical Example 303 Figure 6.7 By Proposition 6.23, this network is not a universal approximator of âˆ†1,4, although it has more than dim(âˆ†1,4) = 32 parameters. Table 6.1 10Ïµ Î± Error bound of Thm 6.14 Eavg Emax 2âˆ’2 14.65 0.4160 0.0522 0.1642 2âˆ’3 17.47 0.2276 0.0248 0.1004 2âˆ’4 20.28 0.1192 0.0134 0.0541 2âˆ’5 23.06 0.0610 0.0077 0.0425 2âˆ’6 25.84 0.0308 0.0060 0.0306 by the theory above was instantiated. We considered the architecture with j = d = 2. As the magnitude of the non-zero parameters grows, the network will converge to the target kernel according to Theorem 6.14. Let Pâˆ—be the target kernel and P the approximation represented by the network (for the relatively small number of variables it could be calculated exactly, but here we calculated it via 25,000 samples of the output for each input). The error is E = maxi,j |Pij âˆ’Pâˆ— ij|. In Table 6.1 we report the average error over 500 target kernels, Eavg = 1 500 Ã500 k=1 Ek and the maximum error Emax = maxk Ek, for the various values of the coeï¬ƒcient Ïµ from our theorem, along with the corresponding parameter magnitude bound Î±, and the error upper bound in Theorem 6.14 |p(y | x) âˆ’pâˆ—(y | x)| â‰¤1 âˆ’(1 âˆ’Ïµ)N + 2Ïµ. 304 Merkh and MontÃºfar: Stochastic Feedforward Neural Networks One section Input layer Output layer Figure 6.8 The network architecture used in the numerical example of Â§6.9 to demonstrate our results. The non-zero connections are shown in black. Notice that even within sections the connectivity does not need to be full. We now describe the explicit construction. Write a given kernel as P = Â©Â­Â­Â­ Â« p(0,0 | 0,0) p(0,1 | 0,0) p(1,0 | 0,0) p(1,1 | 0,0) p(0,0 | 0,1) p(0,1 | 0,1) p(1,0 | 0,1) p(1,1 | 0,1) p(0,0 | 1,0) p(0,1 | 1,0) p(1,0 | 1,0) p(1,1 | 1,0) p(0,0 | 1,1) p(0,1 | 1,1) p(1,0 | 1,1) p(1,1 | 1,1) ÂªÂ®Â®Â® Â¬ = Â©Â­Â­Â­ Â« p11 p12 p13 p14 p21 p22 p23 p24 p31 p32 p33 p34 p41 p42 p43 p44 ÂªÂ®Â®Â® Â¬ . Given a ï¬xed kernel of this form, the following choices of network parameters will make the network exactly approximate the kernel as we allow the maximum magnitude of the parameters Î± â†’âˆ. We consider the widest deep architecture with j = d. Since we have d = 2 inputs and s = 2 = 2bâˆ’1 + b outputs, with b = 1, the network has 2dâˆ’j  2s 2(s âˆ’b) + 2(s âˆ’b) âˆ’1  âˆ’1 = 2 hidden layers of width 2j(s + d âˆ’j) = 8. Here, since j = d, we can save one 6.9 A Numerical Example 305 layer in comparison with the general construction. This network consists of 2j = 4 independent sections, one for each possible input. See Figure 6.8 for an illustration of the overall network topology. The sharing schedule of the single subsection of each section follows the 2b = 2(d âˆ’b) = 2 partial Gray codes S1 = S1,1 S1,2  = 1 0 0 0  and S2 = S2,1 S2,2  = 1 1 0 1  . (6.23) For a given 0 < Ïµ < 1/4 we set Î³ = Ïƒâˆ’1(1 âˆ’Ïµ). The parameters have magnitude at most Î± = 2mÏƒâˆ’1(1 âˆ’Ïµ), where m = max{j, s + (d âˆ’j)} = 2. The weights and biases of the ï¬rst hidden layer are W1 = Â©Â­Â­Â­Â­Â­Â­Â­Â­Â­Â­Â­Â­ Â« âˆ’2Î³ âˆ’2Î³ âˆ’Î³ âˆ’Î³ âˆ’2Î³ 2Î³ âˆ’2Î³ 2Î³ 2Î³ âˆ’2Î³ 2Î³ âˆ’2Î³ 2Î³ 2Î³ Î³ Î³ ÂªÂ®Â®Â®Â®Â®Â®Â®Â®Â®Â®Â®Â® Â¬ , b1 = Â©Â­Â­Â­Â­Â­Â­Â­Â­Â­Â­Â­Â­ Â« Î³ Ïƒâˆ’1(p12 + p14) âˆ’Î³ Ïƒâˆ’1(p22 + p24) âˆ’2Î³ âˆ’Î³ Ïƒâˆ’1(p32 + p34) âˆ’2Î³ âˆ’3Î³ Ïƒâˆ’1(p41 + p44) âˆ’2Î³ ÂªÂ®Â®Â®Â®Â®Â®Â®Â®Â®Â®Â®Â® Â¬ . (6.24) This will map x = (0,0) to a1 (1) = (1,0) with probability (1 âˆ’Ïµ)(1 âˆ’(p12 + p14)) = (1 âˆ’Ïµ)(p11 + p13) and to a1 (1) = (1,1) with probability (1 âˆ’Ïµ)(p12 + p14). The other transitions are similar. When P âˆˆâˆ†Ïµ 2,2, one necessarily has that Ïµ â‰¤1/4; otherwise âˆ†Ïµ 2,2 would be empty, pij â‰¤1 âˆ’3Ïµ, and pij + pik â‰¤1 âˆ’2Ïµ for any i, j, k. This in turn would mean that the bias parameters in (6.24) are bounded by 3Î³. The second hidden layer has weights W2 = Â©Â­Â­Â­Â­Â­Â­Â­Â­Â­Â­Â­Â­ Â« Î³ Ï‰1 0 0 0 0 0 0 0 2Î³ 0 0 0 0 0 0 0 0 Î³ Ï‰2 0 0 0 0 0 0 0 2Î³ 0 0 0 0 0 0 0 0 Î³ Ï‰3 0 0 0 0 0 0 0 2Î³ 0 0 0 0 0 0 0 0 Î³ Ï‰4 0 0 0 0 0 0 0 2Î³ ÂªÂ®Â®Â®Â®Â®Â®Â®Â®Â®Â®Â®Â® Â¬ , (6.25) where Ï‰i = Ïƒâˆ’1  pi4 pi2 + pi4  âˆ’Ïƒâˆ’1  pi3 pi1 + pi3  , (6.26) 306 Merkh and MontÃºfar: Stochastic Feedforward Neural Networks for i = 1,2,3,4. The second-hidden-layer biases are chosen as b2 = Â©Â­Â­Â­Â­Â­Â­Â­Â­Â­Â­Â­Â­ Â« Ïƒâˆ’1(p13/(p11 + p13)) âˆ’Î³ âˆ’Î³ Ïƒâˆ’1(p23/(p21 + p23)) âˆ’Î³ âˆ’Î³ Ïƒâˆ’1(p33/(p33 + p31)) âˆ’Î³ âˆ’Î³ Ïƒâˆ’1(p43/(p43 + p41)) âˆ’Î³ âˆ’Î³ ÂªÂ®Â®Â®Â®Â®Â®Â®Â®Â®Â®Â®Â® Â¬ . (6.27) This will map a1 (1) = (1,0) to a2 (1) = (1,0) with probability p13/(p11 + p13)(1 âˆ’Ïµ) and to a2 (1) = (0,0) with probability (1 âˆ’p13/(p11 + p13))(1 âˆ’Ïµ). In particular, for input x = (0,0) we have Pr  a2 (1) = (0,0),a1 (1) = (1,0) | x = (0,0) =  1 âˆ’ p13 p11 + p13  (1 âˆ’Ïµ) Â· (1 âˆ’Ïµ)(p11 + p13) = p11(1 âˆ’Ïµ)2. Note that if Ïµ â‰¤pij â‰¤1âˆ’Ïµ for all i, j, then the factors p13/(p11+p13) and (p11+p13) are also between Ïµ and 1 âˆ’Ïµ. The other transitions are similar. The weights and biases of the output layer are W3 = 2Î³ 0 2Î³ 0 2Î³ 0 2Î³ 0 0 2Î³ 0 2Î³ 0 2Î³ 0 2Î³  , b3 = âˆ’Î³ âˆ’Î³  . (6.28) This will map, for example, h2 =  a2 (1),a2 (2),a2 (3),a2 (4)  = (0,0,0,. . .,0) to y = (0,0) with probability (1 âˆ’Ïƒ(âˆ’Î³))2 = (1 âˆ’Ïµ)2. Furthermore, to see that the parameters in (6.26) and (6.27) are bounded by 2Î³, we need to show that pi j pi j+pik â‰¤1 âˆ’Ïµ or 1 + pik pi j â‰¥(1 âˆ’Ïµ)âˆ’1. The smallest possible pik pi j is Ïµ 1âˆ’3Ïµ , and 1 1âˆ’Ïµ â‰¤1 + Ïµ 1âˆ’3Ïµ is easily veriï¬ed for Ïµ âˆˆ(0,1/4]. Note that exactly 22(22 âˆ’1) = 12 parameters depend on the values of the target Markov kernel itself, while the other parameters are ï¬xed and depend only on the desired level of accuracy. 6.10 Conclusion In this chapter we have made advances towards a more complete picture of the repre- sentational power of stochastic feedforward networks. We showed that a spectrum of sigmoid stochastic feedforward networks is capable of universal approximation. In the obtained results, a shallow architecture requires fewer hidden units than a deep architecture, while deep architectures achieve the minimum number of trainable 6.11 Open Problems 307 parameters necessary for universal approximation. At the extreme of the spectrum discussed is the j = 0 case, where a network of width s + d and depth approx 2d+s/2(s âˆ’b), b âˆ¼log2(s), is suï¬ƒcient for universal approximation. At the other end of the deep spectrum is the j = d case, which can be seen as an intermediate between the j = 0 case and the shallow universal approximator since its width is exponential in d and its depth is exponential in s. Further, we obtained bounds on the approximation errors when the network parameters are restricted in absolute value by some Î± > 0. In our construction, the error is then bounded by 1 âˆ’(1 âˆ’Ïµ)N + 2Ïµ where Ïµ is the error of each unit, which is bounded by Ïƒ(âˆ’Î±/2(d + s)). 6.11 Open Problems Here we collect a few open problems that we believe would be worth developing in the future. â€¢ Determination of the dimension of the set of distributions represented at layer l of a deep stochastic network. In this direction, the dimension of RBMs has been studied previously (Cueto et al., 2010; MontÃºfar and Morton, 2017). â€¢ Determination of the equations and inequalities that characterize the set of Markov kernels representable by a deep stochastic feedforward network. Work in this direction includes studies of Bayesian networks (Garcia et al., 2005), the RBM with three visible and two hidden binary variables (Seigal and MontÃºfar, 2018), and naive Bayes models with one hidden binary variable (Allman et al., 2019). â€¢ Obtaining maximum approximation error bounds for a network which is not a universal approximator, by measures such as the maximum KL-divergence, and evaluating how the behavior of diï¬€erent network topologies depends on the num- ber of trainable parameters. There is a deal of research in this direction, covering hierarchical graphical models (MatÃºÅ¡, 2009), exponential families (Rauh, 2011), RBMs and DBNs (Le Roux and Bengio, 2008; MontÃºfar et al., 2011, 2013, 2015). For the RBM with three visible and two hidden units mentioned above, Seigal and MontÃºfar (2018) obtained the exact value. â€¢ Finding out whether it is possible to obtain more compact families of universal approximators of Markov kernels than those we presented here. We constructed universal approximators with the minimal number of trainable weights, but which include a substantial number of non-zero ï¬xed weights. Is it possible to construct more compact universal approximators with a smaller number of units and non- zero weights? Can we reï¬ne the lower bounds for the minimum width and the minimum depth given a maximum width of the hidden layers of a universal 308 Merkh and MontÃºfar: Stochastic Feedforward Neural Networks approximator? This kind of problem has traditionally been more diï¬ƒcult than reï¬ning upper bounds. A few examples are listed in MontÃºfar and Rauh (2017). â€¢ Our construction uses sparsely connected networks to achieve the minimum pos- sible number of parameters. How does restricting the connectivity of a network aï¬€ect the distributions it can represent? Are advantages provided by sparsely connected networks over fully connected networks? â€¢ Generalization of the analysis to conditional distributions other than sigmoid and to non-binary variables. Eï¬€orts in this direction include the treatment of RBMs with non-binary units (MontÃºfar and Morton, 2015) and of DBNs with non-binary units (MontÃºfar, 2014b). â€¢ Another interesting research direction would be to consider the theoretical ad- vantages of stochastic networks in relation to deterministic networks, and to develop more eï¬€ective techniques for training stochastic networks. In this direc- tion, Tang and Salakhutdinov (2013) discussed multi-modality and combinations of deterministic and stochastic units. Acknowledgement We thank Nihat Ay for insightful discussions. This project has received funding from the European Research Council (ERC) under the European Unionâ€™s Horizon 2020 research and innovation programme (grant agreement no. 757983). References Allman, Elizabeth S., Cervantes, Hector BaÃ±os, Evans, Robin, HoÅŸten, Serkan, Kub- jas, Kaie, Lemke, Daniel, Rhodes, John A., and Zwiernik, Piotr. 2019. Maxi- mum likelihood estimation of the latent class model through model boundary decomposition. Journal of Algebraic Statistics, 10(1). Bengio, Yoshua. 2009. Learning deep architectures for AI. Foundations and Trends in Machine Learning, 2(1), 1â€“127. Bengio, Yoshua, and Delalleau, Olivier. 2011. On the expressive power of deep architectures. Pages 18â€“36 of: Algorithmic Learning Theory, Kivinen, Jyrki, SzepesvÃ¡ri, Csaba, Ukkonen, Esko, and Zeugmann, Thomas (eds). Springer. Bengio, Yoshua, Lamblin, Pascal, Popovici, Dan, and Larochelle, Hugo. 2007. Greedy layer-wise training of deep networks. Pages 153â€“160 of: Advances in Neural Information Processing Systems, vol. 19, SchÃ¶lkopf, B., Platt, J. C., and Hoï¬€man, T. (eds). Bishop, Christopher M. 2006. Pattern Recognition and Machine Learning Springer. BÃ¶lcskei, Helmut, Grohs, Philipp, Kutyniok, Gitta, and Petersen, Philipp. 2019. Optimal approximation with sparsely connected deep neural networks. SIAM Journal on Mathematics of Data Science, 1(1), 8â€“45. References 309 Brunato, Mauro, and Battiti, Roberto. 2015. Stochastic local search for direct training of threshold networks. Pages 1â€“8 of: Proc. 2015 International Joint Conference on Neural Networks. Cueto, MarÃ­a AngÃ©lica, Morton, Jason, and Sturmfels, Bernd. 2010. Geometry of the restricted Boltzmann machine. pages 135â€“153 of: Algebraic Methods in Statistics and Probability II, Viana, M. A. G. and Wynn, H. (eds). Amer. Math Soc. Cybenko, George. 1989. Approximation by superpositions of a sigmoidal function. Mathematics of Control, Signals and Systems, 2(4), 303â€“314. Delalleau, Olivier, and Bengio, Yoshua. 2011. Shallow vs. deep sum-product networks. Pages 666â€“674 of: Advances in Neural Information Processing Systems, vol. 24. Eldan, Ronen, and Shamir, Ohad. 2016. The power of depth for feedforward neu- ral networks. Pages 907â€“940 of: Proc. 29th Annual Conference on Learning Theory, Feldman, Vitaly, Rakhlin, Alexander, and Shamir, Ohad (eds). Pro- ceedings of Machine Learning Research, vol. 49. Garcia, Luis David, Stillman, Michael, and Sturmfels, Bernd. 2005. Algebraic geometry of Bayesian networks. Journal of Symbolic Computation, 39(3), 331â€“355. Goodfellow, Ian, Pouget-Abadie, Jean, Mirza, Mehdi, Xu, Bing, Warde-Farley, David, Ozair, Sherjil, Courville, Aaron, and Bengio, Yoshua. 2014. Genera- tive adversarial nets. Pages 2672â€“2680 of: Advances in Neural Information Processing Systems, vol. 27. Gribonval, RÃ©mi, Kutyniok, Gitta, Nielsen, Morten, and VoigtlÃ¤nder, Felix. 2019. Approximation spaces of deep neural networks. ArXiv preprint arXiv:1905.01208. Hastad, Johan, and Goldmann, Mikael. 1991. On the power of small-depth threshold circuits. Computational Complexity, 1(2), 113â€“129. Hinton, Geoï¬€rey E, and Salakhutdinov, Ruslan R. 2006. Reducing the dimension- ality of data with neural networks. Science, 313(5786), 504â€“507. Hinton, Geoï¬€rey E, Osindero, Simon, and Teh, Yee-Whye. 2006. A fast learning algorithm for deep belief nets. Neural Computation, 18(7), 1527â€“1554. Hornik, Kurt, Stinchcombe, Maxwell, and White, Halbert. 1989. Multilayer feedfor- ward networks are universal approximators. Neural Networks, 2(5), 359â€“366. Huang, Guang-Bin, Zhu, Qin-Yu, Mao, K. Z., Siew, Chee-Kheong, Saratchandran, Paramasivan, and Sundararajan, Narasimhan. 2006. Can threshold networks be trained directly? IEEE Transactions on Circuits and Systems II: Express Briefs, 53(3), 187â€“191. Johnson, Jesse. 2019. Deep, skinny neural networks are not universal aApproxima- tors. In: Proc. International Conference on Learning Representations. Larochelle, Hugo, Erhan, Dumitru, Courville, Aaron, Bergstra, James, and Bengio, Yoshua. 2007. An empirical evaluation of deep architectures on problems 310 Merkh and MontÃºfar: Stochastic Feedforward Neural Networks with many factors of variation. Pages 473â€“480 of: Proc. 24th International Conference on Machine Learning. Lauritzen, Steï¬€en L. 1996. Graphical Models. Oxford Statistical Science Series. Clarendon Press. Le Roux, Nicolas, and Bengio, Yoshua. 2008. Representational power of restricted Boltzmann machines and deep belief networks. Neural Computation, 20(6), 1631â€“1649. Le Roux, Nicolas, and Bengio, Yoshua. 2010. Deep belief networks are compact universal approximators. Neural Computation, 22(8), 2192â€“2207. Lee, Kimin, Kim, Jaehyung, Chong, Song, and Shin, Jinwoo. 2017. Simpliï¬ed stochastic feedforward neural networks. ArXiv preprint arXiv:1704.03188. Lu, Zhou, Pu, Hongming, Wang, Feicheng, Hu, Zhiqiang, and Wang, Liwei. 2017. The expressive power of neural networks: A view from the width. Pages 6231â€“6239 of: Advances in Neural Information Processing Systems, vol. 30. Lupanov, Oleg B. 1956. On rectiï¬er and switching-and-rectiï¬er circuits. Doklady Academii Nauk SSSR, 111(6), 1171â€“1174. MatÃºÅ¡, FrantiÅ¡ek. 2009. Divergence from factorizable distributions and matroid rep- resentations by partitions. IEEE Transactions on Information Theory, 55(12), 5375â€“5381. Mhaskar, Hrushikesh N., and Poggio, Tomaso. 2016. Deep vs. shallow networks: An approximation theory perspective. Analysis and Applications, 14(06), 829â€“848. MontÃºfar, Guido. 2014a. Deep narrow Boltzmann machines are universal approx- imators. In: Proc. International Conference on Learning Representations. MontÃºfar, Guido. 2014b. Universal approximation depth and errors of narrow belief networks with discrete units. Neural Computation, 26(7), 1386â€“1407. MontÃºfar, Guido. 2015. A comparison of neural network architectures. In: Proc. Deep Learning Workshop, ICML. MontÃºfar, Guido. 2015. Universal approximation of Markov kernels by shallow stochastic feedforward networks. ArXiv preprint arXiv:1503.07211. MontÃºfar, Guido. 2018. Restricted Boltzmann machines: Introduction and re- view. Pages 75â€“115 of: Information Geometry and Its Applications, Ay, Nihat, Gibilisco, Paolo, and MatÃºÅ¡, FrantiÅ¡ek (eds). Springer. MontÃºfar, Guido, and Ay, Nihat. 2011. Reï¬nements of universal approximation results for deep belief networks and restricted Boltzmann machines. Neural Computation, 23(5), 1306â€“1319. MontÃºfar, Guido, and Morton, Jason. 2012. Kernels and submodels of deep be- lief networks. In: Proc. Deep Learning and Unsupervised Feature Learning Workshop, NIPS. MontÃºfar, Guido, and Morton, Jason. 2015. Discrete restricted Boltzmann ma- chines. Journal of Machine Learning Research, 16, 653â€“672. References 311 MontÃºfar, Guido, and Morton, Jason. 2015. When does a mixture of products contain a product of mixtures? SIAM Journal on Discrete Mathematics, 29(1), 321â€“347. MontÃºfar, Guido, and Morton, Jason. 2017. Dimension of marginals of Kronecker product models. SIAM Journal on Applied Algebra and Geometry, 1(1), 126â€“ 151. MontÃºfar, Guido, and Rauh, Johannes. 2017. Hierarchical models as marginals of hierarchical models. International Journal of Approximate Reasoning, 88, 531â€“546. MontÃºfar, Guido, Rauh, Johannes, and Ay, Nihat. 2011. Expressive power and approximation errors of restricted Boltzmann machines. Pages 415â€“423 of: Advances in Neural Information Processing Systems, vol. 24. MontÃºfar, Guido, Rauh, Johannes, and Ay, Nihat. 2013. Maximal information divergence from statistical models deï¬ned by neural networks. Pages 759â€“766 of: Geometric Science of Information. Springer. MontÃºfar, Guido, Pascanu, Razvan, Cho, Kyunghyun, and Bengio, Yoshua. 2014. On the number of linear regions of deep neural networks. Pages 2924â€“2932 of: Advances in Neural Information Processing Systems, vol. 27. MontÃºfar, Guido, Ay, Nihat, and Ghazi-Zahedi, Keyan. 2015. Geometry and expres- sive power of conditional restricted Boltzmann machines. Journal of Machine Learning Research, 16(1), 2405â€“2436. Muroga, Saburo. 1971. Threshold Logic and its Applications. Wiley-Interscience. Neal, Radford M. 1990. Learning stochastic feedforward networks. Technical Report. CRG-TR-90-7, Deptment of Computer Science, University of Toronto. Neal, Radford M. 1992. Connectionist learning of belief networks. Artiï¬cial Intelligence, 56(1), 71â€“113. Neciporuk, E.I. 1964. The synthesis of networks from threshold elements. Problemy Kibernetiki, 11, 49â€“62. Ngiam, Jiquan, Chen, Zhenghao, Koh, Pang, and Ng, Andrew. 2011. Learning deep energy models. Pages 1105â€“1112 of: Proc. 28th International Conference on Machine Learning. Pascanu, Razvan, MontÃºfar, Guido, and Bengio, Yoshua. 2014. On the number of response regions of deep feed forward networks with piece-wise linear activations. In: Proc. International Conference on Learning Representations. Pearl, Judea. 1988. Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. Morgan Kaufmann Publishers Inc. Poggio, Tomaso, Mhaskar, Hrushikesh, Rosasco, Lorenzo, Miranda, Brando, and Liao, Qianli. 2017. Why and when can deep â€“ but not shallow â€“ networks avoid the curse of dimensionality: A review. International Journal of Automation and Computing, 14(5), 503â€“519. Raghu, Maithra, Poole, Ben, Kleinberg, Jon, Ganguli, Surya, and Sohl-Dickstein, Jascha. 2017. On the Expressive Power of Deep Neural Networks. Pages 312 Merkh and MontÃºfar: Stochastic Feedforward Neural Networks 2847â€“2854 of: Proc. 34th International Conference on Machine Learning. Proceedings of Machine Learning Research, vol. 70. Raiko, Tapani, Berglund, Mathias, Alain, Guillaume, and Dinh, Laurent. 2014. Techniques for learning binary stochastic feedforward neural networks. In: Proc. International Conference on Learning Representations. Ranzato, M., Huang, F. J., Boureau, Y., and LeCun, Y. 2007 (June). Unsupervised learning of invariant feature hierarchies with applications to object recogni- tion. Pages 1â€“8 of: Proc. IEEE Conference on Computer Vision and Pattern Recognition. Rauh, Johannes. 2011. Finding the maximizers of the information divergence from an exponential family. IEEE Transactions on Information Theory, 57(6), 3236â€“3247. Rojas, RaÃºl. 2003. Networks of width one are universal classiï¬ers. Pages 3124â€“ 3127 of: Proc. International Joint Conference on Neural Networks, vol. 4. Salakhutdinov, Ruslan, and Hinton, Geoï¬€rey. 2009. Semantic hashing. Interna- tional Journal of Approximate Reasoning, 50(7), 969â€“978. Sard, Arthur. 1942. The measure of the critical values of diï¬€erentiable maps. Bulletin of the American Mathematical Society, 48(12), 883â€“890. Sarikaya, Ruhi, Hinton, Geoï¬€rey E., and Deoras, Anoop. 2014. Application of deep belief networks for natural language understanding. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 22(4), 778â€“784. Saul, Lawrence K., Jaakkola, Tommi, and Jordan, Michael I. 1996. Mean ï¬eld theory for sigmoid belief networks. Journal of Artiï¬cial Intelligence Research, 4(1), 61â€“76. Scarselli, Franco, and Tsoi, Ah Chung. 1998. Universal approximation using feed- forward neural networks: A survey of some existing methods, and some new results. Neural Networks, 11(1), 15â€“37. Seigal, Anna, and MontÃºfar, Guido. 2018. Mixtures and products in two graphical models. Journal of Algebraic Statistics, 9(1). Shannon, Claude E. 1949. The synthesis of two-terminal switching circuits. The Bell System Technical Journal, 28(1), 59â€“98. Sutskever, Ilya, and Hinton, Geoï¬€rey E. 2008. Deep, narrow sigmoid belief net- works are universal approximators. Neural Computation, 20(11), 2629â€“2636. Tang, Yichuan, and Salakhutdinov, Ruslan R. 2013. Learning stochastic feedfor- ward neural networks. Pages 530â€“538 of: Advances in Neural Information Processing Systems, vol. 26. Wenzel, Walter, Ay, Nihat, and Pasemann, Frank. 2000. Hyperplane arrangements separating arbitrary vertex classes in n-cubes. Advances in Applied Mathe- matics, 25(3), 284â€“306. Yarotsky, Dmitry. 2017. Error bounds for approximations with deep ReLU net- works. Neural Networks, 94, 103â€“114. References 313 Yarotsky, Dmitry. 2018. Optimal approximation of continuous functions by very deep ReLU networks. Pages 639â€“649 of: Proc. 31st Conference On Learning Theory. Proceedings of Machine Learning Research, vol. 75. 7 Deep Learning as Sparsity-Enforcing Algorithms A. Aberdam and J. Sulam Abstract: Over the last few decades sparsity has become a driving force in the development of new and better algorithms in signal and image processing. In the context of the late deep learning zenith, a pivotal work by Papyan et al. showed that deep neural networks can be interpreted and analyzed as pursuit algorithms seeking for sparse representations of signals belonging to a multilayer-synthesis sparse model. In this chapter we review recent contributions showing that this observation is correct but incomplete, in the sense that such a model provides a symbiotic mixture of coupled synthesis and analysis sparse priors. We make this observation precise and use it to expand on uniqueness guarantees and stability bounds for the pursuit of multilayer sparse representations. We then explore a convex relaxation of the resulting pursuit and derive eï¬ƒcient optimization algorithms to approximate its solution. Importantly, we deploy these algorithms in a supervised learning formulation that generalizes feed-forward convolutional neural networks into recurrent ones, improving their performance without increasing the number of parameters of the model. 7.1 Introduction The search for parsimonious representations is concerned with the identiï¬cation of a few but fundamental components that explain a particular observable. In signal processing, sparsity has continuously been a driving force in the design of algorithms, transformations, and a plethora of applications in computer vision (see Elad (2010) and references therein). Broadly speaking, sparse representation modeling assumes that real signals (and among them, images) can be well approximated by a linear combination of only a few elements from a collection of atoms, termed a dictionary. More formally, a 314 7.1 Introduction 315 signal x[0] âˆˆRn0 can be expressed as x[0] = D[1]x[1] + v, (7.1) where D[1] âˆˆRn0Ã—n1 is an overcomplete dictionary (n0 < n1), x[1] âˆˆRn1 is the representations vector, and v âˆˆRn0 accounts for measurement noise and model deviations. Importantly, x[1] has only a few non-zero coeï¬ƒcients, as indicated by the â„“0 pseudo-norm, âˆ¥x[1]âˆ¥0 â‰¤s1 â‰ªn1. Over the last two decades, numerous works have studied the problem of estimating such sparse representations for a given dictionary (Mallat and Zhang, 1993; Tropp, 2004, 2006; Blumensath and Davies, 2009), and methods have even been proposed to adaptively ï¬nd some optimal dictionary for a particular task (Aharon et al., 2006; Mairal et al., 2010). The latter problem, termed dictionary learning, has enabled sparse enforcing methods to provide state-of-the-art solutions to a wide variety of problems in signal and image processing (Sulam et al., 2014; Romano et al., 2014; Mairal et al., 2010, 2009) and machine learning (Jiang et al., 2013; Patel et al., 2014; Shrivastava et al., 2014; Mairal et al., 2012). While convolutional neural networks (CNNs) were developed independently of these ideas (LeCun et al., 1990; Rumelhart et al., 1986), a recent work (Papyan et al., 2017a) has shown that these models can be interpreted as pursuit algorithms enforcing particular sparse priors on the neuronsâ€™ activations. This observation was formalized in terms of a multilayer and convolutional sparse coding (MLâ€“CSC) model, in which it was proposed that not only should we set x[0] = D[1]x[1] (for a convolutional dictionary D[1]) but also x[1] should admit a representation by yet another dictionary, x[1] = D[2]x[2], with x[2] also sparse. Such an assumption can be extended to a number, L, of layers, naturally resulting in a deep sparse model. Under this framework, the forward pass in a CNN â€“ the process of computing the deepest representation from a given input â€“ can be interpreted in terms of nested thresholding operators, resulting in coarse estimations of the underlying sparse representations at diï¬€erent layers. With this understanding, one can resort to well-established theoretical results in sparse approximation (Elad, 2010), together with recent extensions to the convolutional setting (Papyan et al., 2017b), to derive conditions under which the activations computed by convolutional networks are stable (Papyan et al., 2017a). More broadly, these ideas (ï¬rst presented in Papyan et al., 2017a, then extended in Sulam et al., 2018, and recently summarized in Papyan et al., 2018) are based on a cascade of synthesis sparse models across several layers. While insightful, such constructions lead to a series of baï¬„ing issues. (i) The resulting theoretical guarantees provide recovery bounds that become looser with the network depth. 316 Aberdam and Sulam: Deep Learning as Sparsity-Enforcing Algorithms (ii) The sparsity constraints imposed on the representations seem too stringent. What is more, the cardinality of the representations or activations is bound to grow (i.e., become denser) toward shallower layers. (iii) While such ideas can be employed to learn a multilayer model on real data (Sulam et al., 2018), the resulting algorithms are still far from achieving the computational eï¬ƒciency of deep learning models, and it is unclear how practical CNN architectures can be tied to the MLâ€“CSC model. In this chapter, we ï¬rst review several works at the intersection of sparse rep- resentations and neural networks and then move on to present some recent results (mainly those in Aberdam et al., 2019; Sulam et al., 2018; Romano and Elad, 2018) showing that the above multilayer model admits an alternative (and more useful) interpretation: while such a construction does impose a synthesis sparse prior on the deepest representation, the intermediate representations serve to enforce an analysis sparse prior on it. Hence, MLâ€“CSC puts forward a model in which these two complementary ideas (synthesis and analysis) interact in a symbiotic manner. This diï¬€erence might seem subtle, but its implications are far reaching. We will show that such an interpretation resolves the issues mentioned above, allowing for representations that are not as sparse and enabling the development of improved uniqueness and stability guarantees. Moreover, this analysis naturally leads to a problem formulation that is a generalization of the basis pursuit problem in a mul- tilayer setting. Importantly, we will show how the solution to this problem can be approximated by recurrent CNNs. The remainder of this chapter is organized as follows. In the next section we brieï¬‚y comment on related works that have explored the interplay between sparse priors and neural networks before reviewing some basic deï¬nitions and results needed to expand on the multilayer sparse model in Â§7.4. We then present a holistic interpretation of this multilayer construction in Â§7.5, combining synthesis and analysis sparse ideas. The resulting pursuit problem is then analyzed practically in Â§7.6 by means of a multilayer iterative shrinkage algorithm, before we conclude by commenting in Â§7.6.1 on ongoing and future research directions. 7.2 Related Work Neural networks and sparsity have a long and rich history of synergy, and researchers have borrowed elements and insights from one ï¬eld and have brought interesting results and applications to the other. Though an exhaustive review is beyond the scope of this chapter, we mention here a few important works at the intersection of these ï¬elds. One of the earliest and most notable connections between (biological) neural 7.3 Background 317 networks and sparse representations was made by Olshausen and Fields (1996, 1997), showing that an unsupervised approach that maximizes sparsity (i.e., sparse coding) can explain statistical properties of the receptive ï¬elds of neurons in the mammalian primary visual cortex. In the machine learning community, early neural networks algorithms were also proposed for maximizing some measure of sparsity in an overcomplete basis through some regularization strategy or built-in nonlinearities (Ranzato et al., 2007, 2008; Lee et al., 2008; Ng et al., 2011); see Goodfellow et al. (2016, Chapter 14) for an overview. Such unsupervised models are broadly known as autoencoders: algorithms that (approximately) compress an input into a simpler code, while being able to then decode it. Later work (Makhzani and Frey, 2013) required the activations in k-sparse autoencoders to be exactly sparse by means of a hard-thresholding operator. These ideas were extended in Makhzani and Frey (2015), imposing further sparsity constraints and deploying them for image classiï¬cation in a stacked fashion. On a diï¬€erent line of work, Gregor and LeCun showed that a particular neural network architecture with skip connections can be trained in a supervised manner to provide approximations to sparse codes resulting from the iterative soft-thresholding algorithm (ISTA) (Gregor and LeCun, 2010). Interestingly, only a few steps (or lay- ers) of their proposed network â€“ coined learned ISTA (LISTA) â€“ suï¬ƒces to produce accurate solutions, resulting in a much faster inference algorithm for sparse coding. This result has recently attracted considerable interest in the signal processing and machine learning communities, as it provides theoretical analyses and guarantees (Moreau and Bruna, 2017; Giryes et al., 2018; Chen et al., 2018; Liu et al., 2019). Sparsity continues to be a driving force in the design and analysis of deep neural networks, and it includes ideas from harmonic analysis (BÃ¶lcskei et al., 2019; Mallat, 2016) as well as probabilistic models (Patel et al., 2016; Ho et al., 2018). Our approach in this chapter makes further connections between deep learning architectures and a generative sparse multilayer model for natural signals, extending the results in Papyan et al. (2017a). As we will shortly show, this approach enables us to provide theoretical guarantees for particular network architectures, while still improving performance in practical applications over simpler baseline models. 7.3 Background Let us ï¬rst review some basic concepts of sparse modeling, and settle the notation for the reminder of the chapter. The problem of retrieving a sparse representation x[1] for a signal x[0] and dictionary D[1] can be formulated in terms of the following optimization problem: min x[1] âˆ¥x[1]âˆ¥0 s.t. âˆ¥x[0] âˆ’D[1]x[1]âˆ¥2 2 â‰¤Îµ. (7.2) 318 Aberdam and Sulam: Deep Learning as Sparsity-Enforcing Algorithms The allowed error, Îµ, can account for noisy measurements and model deviations, and can be replaced by an equality constraint (i.e., such that x[0] = D[1]x[1]) in an ideal case. The problem in (7.2) is non-convex and NP-hard in general, and thus one must resort to approximation algorithms in practice. One family of such methods employs a greedy approach by progressively building an estimation of the unknown support, such as matching pursuit (Mallat and Zhang, 1993) or variations thereof (e.g., Pati et al., 1993). A second alternative is to relax the non-convex and non-smooth â„“0 pseudo-norm for a convex surrogate, such as the â„“1 norm. This relaxation, termed basis pursuit1, is formulated in its Lagrangian form as min x[1] 1 2 âˆ¥x[0] âˆ’D[1]x[1]âˆ¥2 2 + Î»âˆ¥x[1]âˆ¥1  , (7.3) where the penalty parameter Î» provides a compromise between sparsity and re- construction error. This formulation has become very popular for sparse coding (Tropp, 2006) because the problem in (7.3) is convex and can be addressed by a large collection of optimization tools: see Boyd and Vandenberghe (2004) and references therein. The characteristics of the dictionary inï¬‚uence our ability to ï¬nd solutions to the problems above, and there exist several measures for quantifying the goodness of a matrix D[1]. One such notion is the spark of the dictionary, Î·(D[1]), deï¬ned as the smallest number of linearly dependent columns in D[1]. The spark immediately enables the formulation of uniqueness guarantees, since if there exists a represen- tation x[1] for a signal x[0] (i.e., x[0] = D[1]x[1]) such that x[1] 0 < Î·(D[1]) 2 , then this solution is necessarily the sparsest solution possible (Donoho and Elad, 2003) to the problem in (7.2) when Îµ = 0. From a practical perspective, the mutual coherence Âµ(D[1]) is a more useful measure since, unlike the spark, it is trivial to compute. Assuming hereafter that the columns of D[1] are normalized to unit length (âˆ¥djâˆ¥2 = 1), the mutual coherence is deï¬ned by Âµ(D[1]) = max i,j dT i dj . (7.4) One may then bound the spark with the mutual coherence (Donoho and Elad, 2003), as Î·(D[1]) â‰¥1 + 1 Âµ(D[1]). In this way, a suï¬ƒcient condition for uniqueness is to require that x[1] 0 < 1 2  1 + 1 Âµ(D[1])  . 1 More precisely, the problem in (7.3) is a basis pursuit denoising (BPDN) formulation, also known as least the absolute shrinkage and selection operator (LASSO) in the statistical learning community. 7.3 Background 319 As described, this model is commonly referred to as a synthesis sparsity model, because the linear operator D[1] synthesizes the signal x[0] from its representation x[1]. An alternative, slightly more recent version, is that of an analysis sparse model, in which it is assumed that real signals can be analyzed by an operator â„¦âˆˆRn1Ã—n0, resulting in a representations that is co-sparse (Nam et al., 2013); i.e. âˆ¥â„¦x[0]âˆ¥0 â‰¤n1 âˆ’l1, where l1 is the co-sparsity level. The corresponding pursuit problem over a variable Î± âˆˆRn0, min Î± âˆ¥x[0] âˆ’Î±âˆ¥2 2 such that âˆ¥â„¦Î±âˆ¥0 â‰¤n1 âˆ’l1, (7.5) is NP-complete (Nam et al., 2013), as is the problem in (7.2), and analogous approximation techniques exist. We will combine these two models in a synergetic way later, in Â§7.5. These sparse models are typically deployed on small image patches, mainly owing to computational and complexity constraints (Sulam et al., 2016). In order to model large natural images, the synthesis sparse model has been recently studied in a convolutional setting (Papyan et al., 2017b) by constraining the dictionary D[1] to be a concatenation of circulant banded matrices determined by low-dimensional atoms di âˆˆRnd. Formally, this model assumes that x[0] = m Ã• i=1 di âŠ›x[1] i + v, (7.6) where the signal dimension is typically much larger than the dimension of the atoms, n0 â‰«nd, and the corresponding representations (or feature maps) x[1] i are sparse. When periodic extensions are employed, the expression above can be written equivalently in terms of the redundant matrix D[1] âˆˆRn0Ã—n1 and the concatenation of all x[1] i into the vector x[1], resulting in a generative (yet structured) model, x[0] = D[1]x[1] + v. In this convolutional setting, a local measure of sparsity, or rather density, was shown to be more appropriate than the traditional â„“0. The work in Papyan et al. (2017b) proposed an â„“0,âˆmeasure which accounts for the cardinality of the densest stripe of x[1] â€“ the set of coeï¬ƒcients coding for any nd-dimensional patch in the n0- dimensional signal x[0]. With these local notions, Papyan et al. (2017b) formulated the convolutional pursuit problem just as in (7.2), though replacing the â„“0 norm by the â„“0,âˆnorm. Interestingly, the â„“1 norm still serves as a surrogate for this density measure, in the sense that the basis pursuit problem in (7.3) is guaranteed to recover the solution to the convolutional pursuit problem for signals that are â„“0,âˆ-sparse (Papyan et al., 2017b). In order to keep our exposition succinct, we will not dwell on convolutional sparsity any further, though we will mention it again when appropriate. 320 Aberdam and Sulam: Deep Learning as Sparsity-Enforcing Algorithms 7.4 Multilayer Sparse Coding Papyan et al. (2017a) put forward a multilayer extension of sparse models by assum- ing that, just as x[0] â‰ˆD[1]x[1], the representation x[1] can be further approximated by yet another dictionary D[2] and representation x[2], i.e., x[1] â‰ˆD[2]x[2]. This model was ï¬rst proposed in the context of convolutional sparse representations, under the assumption that all dictionaries are convolutional and that the representations are â„“0,âˆ-sparse. We will employ the more general multilayer sparse coding (MLâ€“ SC) formulation following Aberdam et al. (2019) and state that, given dictionaries {D[i]}L i=1 of appropriate dimensions, a signal x[0] âˆˆRn0 admits a representation in terms of the MLâ€“SC model if x[0] = D[1]x[1], x[1] 0 â‰¤s1, x[1] = D[2]x[2], x[2] 0 â‰¤s2, ... x[Lâˆ’1] = D[L]x[L], x[L] 0 â‰¤sL. (7.7) Denoting the eï¬€ective dictionary from layer i to j as D[i,j] = D[i]D[i+1] Â· Â· Â· D[jâˆ’1]D[j], one can concisely2 write x[0] = D[1,i]x[i], with âˆ¥x[i]âˆ¥0 â‰¤si, for all 1 â‰¤i â‰¤L. This construction imposes a synthesis sparsity model on representations at diï¬€er- ent layers. As such, one can resort to the vast theoretical guarantees for this model (Elad, 2010) and apply them in a layer-by-layer fashion. This enabled Papyan et al. (2017a) to formulate theoretical guarantees for the nested sparse vectors x[i]. As an example, we state here the uniqueness guarantees from that work: Theorem 7.1 (Theorem 4 from Papyan et al., 2017a). If a set of representations {x[i]}L i=1 exists such that x[iâˆ’1] = D[i]x[i], for all 1 â‰¤i â‰¤L (7.8) and x[i] 0 = si < 1 2  1 + 1 Âµ(D[i])  , for all 1 â‰¤i â‰¤L, (7.9) then these representations are the sparsest for the signal x[0]. This result relies on each representation being the sparsest for its respective layer, and thus uniqueness is guaranteed only if all the representations are sparse enough. We should wonder whether we could do better â€“ we will revisit this uniqueness guarantee in Â§7.5. For the formal proof of this result (as well as those in the following sections), we refer the reader to the respective references. 2 Note that, for simplicity, we employ a slight abuse of notation by denoting D[i,i] = D[i]. 7.4 Multilayer Sparse Coding 321 7.4.1 MLâ€“SC Pursuit and the Forward Pass In practice, one is given measurements x[0] = x+v, where x admits a representation in terms of the MLâ€“SC model (with cardinalities si), and v is assumed Gaussian. The corresponding pursuit for the set of L representations, dubbed deep pursuit (Papyan et al., 2017a), seeks to ï¬nd a set of sparse x[i] that approximately reconstruct each layer. Formally, this problem is given by: Find {x[i]}L i=1 such that âˆ¥x[iâˆ’1] âˆ’D[i]x[i]âˆ¥2 2 â‰¤Îµi, âˆ¥x[i]âˆ¥0 â‰¤si for all1 â‰¤i â‰¤L. (7.10) Arguably the simplest algorithm that can provide an approximate solution to this problem is given in terms of layered thresholding operators. A soft-thresholding operator with threshold Î² is the scalar function given by TÎ²(x) =  sign(x)(|x| âˆ’Î²) if |x| â‰¥Î² 0 if |x| < Î². (7.11) Such a soft-thresholding operator results from the proximal of the â„“1 norm, em- ployed as a relaxation of the â„“0 norm. Alternatively, a hard-thresholding function provides the corresponding proximal for the latter option. With such a function at hand, an estimate for the ï¬rst representation can be obtained by performing Ë†x[1] = TÎ²[1](D[1]Tx[0]). Then, one can further compute Ë†x[i] = TÎ²[i](D[i]T Ë†x[iâˆ’1]), for all 2 â‰¤i â‰¤L. (7.12) The central observation in Papyan et al. (2017a) was that, if a non-negativity assumption is also imposed on the representations, and denoting W[i] = D[i]T, such a layered soft-thresholding algorithm can be written equivalently in terms of the rectiï¬er linear unit (ReLU), or non-negative projection, Ïƒ(x) = max{0, x} as Ë†x[i] = Ïƒ(W[i]Ë†x[iâˆ’1] + b[i]), for all 2 â‰¤i â‰¤L. (7.13) The equivalence is exact when the biases b[i] are set according to3 the thresholds Î²[i]. In other words, the forward pass algorithm in (7.13) â€“ computing the inner- most representation from an input signal (or image) x[0] through a sequence of aï¬ƒne transformations and the ReLU activation function â€“ can be interpreted as a coarse pursuit seeking to estimate the representations x[i] for that signal. This is formalized in the following result. Theorem 7.2 (Stability of the forward pass â€“ layered soft-thresholding algorithm, 3 Note that this expression is more general, in that it allows for diï¬€erent thresholds per atom, than the expression in (7.12). The latter can be recovered by setting every entry in the bias vector equal to âˆ’Î²[i]. 322 Aberdam and Sulam: Deep Learning as Sparsity-Enforcing Algorithms Papyan et al., 2017a, Theorem 10). Suppose that an MLâ€“SC signal is contaminated with a bounded noise âˆ¥vâˆ¥2 â‰¤Ïµ0, so that x[0] = D[1,i]x[i] + v, and suppose too that âˆ¥x[i]âˆ¥0 = si < 1 2  1 + ci Âµ(D[i])  âˆ’ Ïµiâˆ’1 Âµ(D[i]) ki, for all 1 â‰¤i â‰¤L, (7.14) where ci = |x[i] min|/|x[i] max| and ki = 1/|x[i] max| are constants depending on the maximal and minimal absolute values of the non-zero values in x[i]. Then, for proper thresholds4 Î²[i] the layered soft-thresholding algorithm recovers a set of representations {Ë†x[i]}L i=1 with the correct supports and that satisfy âˆ¥x[i] âˆ’Ë†x[i]âˆ¥2 â‰¤Ïµi = âˆšsi  Ïµiâˆ’1 + Î²[i] + Âµ(D[i])kâˆ’1 i (si âˆ’1), for all 1 â‰¤i â‰¤L. (7.15) In simple terms, Theorem 7.2 guarantees that, under sparse assumptions on the representations (or activations) x[i] and small mutual coherence of the dictionaries Âµ(D[i]), the vectors computed by the forward pass, Ë†x[i], provide approximations to such underlying sparse representations. These estimates Ë†x[i] are stable, in the sense that the correct support is recovered and their distance to the original x[i] is bounded in an â„“2 sense. Note that these guarantees based on Âµ(D[i]) result from a worst-case scenario analysis, and so they are typically quite pessimistic. Networks satisfying these conditions were constructed in Papyan et al. (2017a). Interestingly, later work (Cisse et al., 2017) showed that enforcing ï¬lters to behave like Parseval frames â€“ tightly related to a small mutual coherence â€“ provides improved stability against adversarial attacks. We will comment on this connection later in this chapter. A result such as Theorem 7.2 is illuminating, as it demonstrates the potential of employing results in sparse approximation to the study of deep learning. The mere application of these guarantees in a layered-synthesis manner provides bounds that become looser with the depth of the network, as can be observed in (7.15), so it is unclear to what extent the layer-wise constraints really contribute to an improved estimation of x[i] or what their underlying beneï¬t is. Papyan et al. (2017a) also suggested an improvement to the above strategy by replacing simple thresholding with the complete solution to basis pursuit at every layer in a sequential manner, and named it layered basis pursuit: Ë†x[i] = arg min x[i] 1 2 âˆ¥Ë†x[iâˆ’1] âˆ’D[i]x[i]âˆ¥2 2 + Î»iâˆ¥x[i]âˆ¥1. (7.16) Unlike the forward pass, this allows recovery of the representations exactly in an ideal (noiseless) setting. Still, note that no global reconstruction is possible in general, as the solution to the above problem yields only approximations Ë†x[iâˆ’1] â‰ˆ D[i]Ë†x[i], and it is thus unclear how to reconstruct Ë†x[0] from {Ë†x[i]}L i=1 in a stable way. 4 The proper thresholds depend on the values of the representations x[i], the mutual coherence of the dictionaries and the noise contamination. Further details can be found in Papyan et al., 2017a, Appendix E. 7.4 Multilayer Sparse Coding 323 7.4.2 MLâ€“SC: A Projection Approach These shortcomings led Sulam et al. (2018) to propose a projection alternative to the deep pursuit problem from (7.10). Given measurements x[0] = x + v, they proposed the following optimization problem: min x[i] âˆ¥x[0] âˆ’D[1,L]x[L]âˆ¥2 2 such that  x[iâˆ’1] = D[i]x[i], for all 2 â‰¤i â‰¤L, âˆ¥x[i]âˆ¥0 â‰¤si, for all 1 â‰¤i â‰¤L. (7.17) In words, this seeks to obtain the closest signal x to the measurements x[0] that satisï¬es the model constraints. To solve this problem, one can resort to a global pursuit stage (ï¬nding the deepest Ë†x[L] that minimizes the â„“2 data term) followed by a back-tracking step that ensures the sparsity constraints are met and modiï¬es them otherwise. This scheme has the advantage that the stability bounds for the estimates Ë†x[i] do not necessarily increase with the depth of the model (Sulam et al., 2018). More interestingly, this projection alternative provided a ï¬rst dictionary learning approach for this multilayer (and convolutional) model. The authors leverage the sparsity of the intermediate dictionaries D[i] as proxies for the sparsity intermediate representations. Indeed, âˆ¥x[iâˆ’1]âˆ¥0 â‰¤âˆ¥D[i,L]âˆ¥0âˆ¥x[L]âˆ¥0, where âˆ¥D[i,L]âˆ¥0 counts the maximal number of non-zeros in any column of D[i,L] â€“ it forms an induced â„“0 pseudo-norm. Using this approach, for a collection of N training signals x[0] i , Sulam et al. (2018) proposed the following multilayer dictionary learning problem: min x[L] i ,D[l] 1 2N N Ã• i=1 âˆ¥x[0] i âˆ’D[1,L]x[L] i âˆ¥2 2 + L Ã• l=2 Î»[l]âˆ¥D[l]âˆ¥0 + Î¹ L Ã• l=1 âˆ¥D[l]âˆ¥2 F + Î»âˆ¥x[L] i âˆ¥1, (7.18) explicitly enforcing sparsity on the atoms in all intermediate dictionaries. Such a non-convex problem can be minimized empirically by an alternating minimization approach, solving for x[L] i while keeping the dictionaries ï¬xed and then vice versa. Moreover, this can be carried out in an online way, minimizing (7.18) one sam- ple (or a mini-batch of samples) at a time. This is the approach for training such a model on digit images (MNIST), yielding the convolutional multilayer mode shown in Figure 7.1. The features (or representations) that result from this unsupervised learning scheme were also shown to be useful, providing comparable classiï¬cation performance to competing modern auto-encoder alternatives (Makhzani and Frey, 2013, 2015). Nonetheless, this framework is restricted to sparse dictionaries, and the resulting algorithm does not scale as well as other convolutional network im- plementations. Such an alternating minimization scheme deviates from traditional autoencoders, which minimize the reconstruction loss through back-propagation. 324 Aberdam and Sulam: Deep Learning as Sparsity-Enforcing Algorithms Figure 7.1 A three-layer convolutional model trained on MNIST digits, taken from Papyan et al. (2018). 7.5 The Holistic Way Both approaches presented above, the layer-wise and the projection alternatives, consist of relatively separate pursuits for each representation layer. As a conse- quence, in the layer-wise approach the recovery error increases with the network depth (Papyan et al., 2017a), while in the projection case this error is essentially the same as that found using the eï¬€ective model as a single-layer model (Sulam et al., 2018). These results contradict the intuition that the estimation of the representa- tions in a MLâ€“CS model should improve, since this imposes further constraints than the traditional case. Extra constraints convey additional information, which the previous methods seem to be incapable of leveraging. In this section we oï¬€er a holistic perspective that aims to analyze the deep pursuit problem wholly, enforcing all constraints simultaneously. In doing so, we will show that this model imposes a synergetic coupling between synthesis and analysis sparse models, leading to answers to the baï¬„ing questions posed previously. To solve all the constraints in the deep pursuit problem at the same time we shall ï¬rst rewrite it as a function of the deepest layer. Let us express the intermediate representations as linear functions of x[L]; namely, x[iâˆ’1] = D[i,L]x[L]. Thus, (7.17) becomes min x[L] âˆ¥x[0] âˆ’D[1,L]x[L]âˆ¥2 2 such that  âˆ¥x[L]âˆ¥0 â‰¤sL, âˆ¥D[i,L]x[L]âˆ¥0 â‰¤si, for all 2 â‰¤i â‰¤L. (7.19) Written in this way, it is clear that from a global perspective the eï¬€ective dictionary 7.5 The Holistic Way 325 Figure 7.2 Illustration of the MLâ€“SC model for a two-layer decomposition. From Aberdam et al. (2019). Â©2019 Society for Industrial and Applied Mathematics. Reprinted with permission. All rights reserved. D[1,L] imposes a synthesis-sparse prior on the deepest representation x[L]. However, there are extra constraints on it: x[L] should be not only sparse but also produce zeros in the intermediate layers x[iâˆ’1] when multiplied by D[i,L]. In other words, the intermediate dictionaries D[i,L] act as analysis operators in a co-sparse model, as depicted in Figure 7.2 for a two-layer case. This particular coupling of both sparse priors sheds light on the dimensionality of the space spanned by the MLâ€“SC model. From a synthesis point of view, an sL-sparse signal x[L] lives in a union of sL-dimensional subspaces. However, and following Figure 7.2, when requiring the intermediate representations to be sparse we are enforcing x[L] to be orthogonal to certain rows from D[2]. Denoting such rows by Î¦, we conclude that x[2] must be in the kernel of Î¦. Letting Î› denote the support of x[2], this boils down to the condition that Î¦Î›x[2] Î› = 0. As shown in Aberdam et al. (2019), this extra constraint reduces the degrees of freedom of the deepest representation, and so the MLâ€“SC signals eï¬€ectively lies in a union of (sL âˆ’r)-dimensional subspaces, where r = rank{Î¦Î›}. This new understanding of the dimensionality of the signals in the MLâ€“SC model leads to several corollaries. For instance, one can now evaluate whether the model is empty simply by observing the kernel space of Î¦Î›. If this subspace contains only the zero vector (i.e., sL â‰¤r) then no signal exists satisfying the support constraints. Otherwise, one could sample signals from the model by randomly selecting representations x[L] in the kernel of Î¦Î› â€“ easily done through a singular value decomposition. A second relevant conclusion is that the representation layers should not be too sparse, providing more generous bounds. Indeed, under a full- rank assumption for the intermediate dictionaries, every additional zero at any intermediate layer increases r, thus reducing the eï¬€ective dimension of the signal. This is in sharp contrast to the analysis from Â§7.4. The previous pursuit schemes 326 Aberdam and Sulam: Deep Learning as Sparsity-Enforcing Algorithms required the dictionaries to be sparse in order to satisfy the stringent sparsity constraints. Such a requirement is not needed under this holistic interpretation. An important consequence of the holistic approach is a signiï¬cant improvement in uniqueness guarantees. Recall from Theorem 7.1 that in the layered synthesis interpretation, every representation at every layer should be sparse enough to be the unique (i.e., sparsest) representation for its respective layer. However, thanks to the analysisâ€“synthesis understanding, the set of representations in an MLâ€“SC model is guaranteed to be unique if the deepest representation satisï¬es âˆ¥x[L]âˆ¥0 â‰¤ Î·(D[1,L])âˆ’1 2 + r. In other words, the traditional bound on the allowed cardinality is extended by r, the rank of Î¦Î›. This is formalized in the following result: Theorem 7.3 (Aberdam et al., 2019, Theorem 5.1). Consider an MLâ€“SC signal x[0] and a set of dictionaries {D[i]}L i=1. If a set of representations {x[i]}L i=1 satisï¬es âˆ¥x[L]âˆ¥0 = sL < 1 2  1 + 1 Âµ(D[1,L])  + r, (7.20) where r = rank{Î¦Î›}, then, for a set of dictionaries {D[i]}L i=1 in general position,5, this set is the unique MLâ€“SC representation for x[0]. Other results can be derived using similar arguments, and we give here one ï¬nal example from Aberdam et al. (2019). Consider the oracle denoising case, in which the supports of the representations at every layer are known and the goal is to provide an estimate of Ë†x[i] given noisy measurements x[0] = D[1,i]x[i] + v. Assuming Gaussian noise with standard deviation Ïƒv and disregarding the analysis- sparse prior in the model, one can show that the expected error in the deepest representation is upper bounded as follows: Eâˆ¥Ë†x[L] âˆ’x[L]âˆ¥2 2 â‰¤ Ïƒ2 v 1 âˆ’Î´sL sL, (7.21) where Î´sL is the restricted isometric property (RIP) for the eï¬€ective dictionary D[1,L] and cardinality of sL. This property measures how far a redundant matrix is from an isometry for sparse vectors, small values being preferred. If one also leverages the information contained in the analysis priors of the intermediate layers, however, one can show that this error is reduced, resulting in: Eâˆ¥Ë†x[L] âˆ’x[L]âˆ¥2 2 â‰¤ Ïƒ2 v 1 âˆ’Î´sL (sL âˆ’r) . (7.22) As it is now clear, this dual prior interpretation provides a signiï¬cant advantage in 5 The assumption of the dictionaries being in general position â€“ i.e. not containing non-trivial linear dependencies among its rows â€“ follows from the analysis in Nam et al. (2013) and further details can be found in Aberdam et al. (2019). Such an assumption holds for almost every dictionary set in Lebesgue measure. 7.6 Multilayer Iterative Shrinkage Algorithms 327 terms of theoretical guarantees, showing the important beneï¬ts of the intermediate sparse representations. How can these theoretical beneï¬ts be brought to practice by some pursuit algorithm? A ï¬rst answer was provided by Aberdam et al. (2019) who proposed a holistic pursuit that alternates between application of the synthesis priors and of the analysis priors, following a greedy strategy. This algorithm6 provides recovery guarantees that have the same ï¬‚avor as those commented on in this section, i.e., an error that is proportional to sL âˆ’r. However, such greedy approaches are still far from being scalable and eï¬ƒcient algorithms that can bring these ideas into the practical deep learning world. This is what we study next. 7.6 Multilayer Iterative Shrinkage Algorithms Our approach to reducing the hard optimization problem from (7.19) to an easier form relies, naturally, on an â„“1 relaxation of both synthesis and analysis â„“0 priors. We propose the following multilayer extension of a basis pursuit problem: MLâ€“BP : min x[L] 1 2 âˆ¥x[0] âˆ’D[1,L]x[L]âˆ¥2 2 + L Ã• l=2 Î»[lâˆ’1]âˆ¥D[l,L]x[L]âˆ¥1 + Î»[L]âˆ¥x[L]âˆ¥1. (7.23) Note that setting all but the last penalty parameters to zero (Î»[l] = 0 for 1 â‰¤ l < L, and Î»[L] > 0) results in a traditional â„“1 pursuit for the eï¬€ective model D[1,L]. The problem above is related to other approaches in the compressed sensing literature, such as the analysis lasso (Candes et al., 2010) and robust sparse analysis regularization (Vaiter et al., 2013). Yet the coupling of the synthesis and analysis operators makes MLâ€“BP diï¬€erent from other previous formulations. The optimization problem in (7.23) is convex, and a wide variety of solvers could be employ to ï¬nd a minimizer (Boyd and Vandenberghe, 2004). Nonetheless, we are interested in ï¬rst-order methods that only incur matrixâ€“vector multiplication and entry-wise operations, which scale to large-dimensional problems. Iterative shrinkage algorithms (ISTAs) and their accelerated versions (Fast ISTAs, or FIS- TAs) (Beck and Teboulle, 2009) are attractive, but not directly applicable to problem (7.23). ISTA minimizes the sum of functions f +g, where f is convex and L-smooth while g is convex but possible non-smooth, by means of a proximal gradient ap- proach that iterates the updates: xk+1 = prox 1 L g  xk âˆ’1 L âˆ‡f (xk)  . (7.24) One might consider letting f be the â„“2 data term in problem (7.23) and g be the 6 Code is available at https://github.com/aaberdam/Holistic_Pursuit. 328 Aberdam and Sulam: Deep Learning as Sparsity-Enforcing Algorithms sum of the â„“1 penalty functions. This, however, is not very useful as calculating the proximal of a sum of functions is problematic (Combettes and Pesquet, 2011). As an alternative, Sulam et al. (2020) put forward a proximal gradient-mapping algorithm. Given functions f and g, the gradient-mapping operator is deï¬ned as Gf ,g L (x) = L  x âˆ’prox 1 L g  x âˆ’1 L âˆ‡f (x)  . (7.25) This operator is a sort of generalization of gradient, since Gf ,g L (xâˆ—) = 0 if and only if xâˆ—is a minimizer of f + g, and Gf ,g L (x) = âˆ‡f (x) if g â‰¡0 (Beck, 2017). For simplicity, let us consider a two-layer model and the associated pursuit problem min x[2] F(x[2]) â‰œ1 2 âˆ¥x[0] âˆ’D[1]D[2]x[2]âˆ¥2 2 | {z } f (D[2]x[2]) + Î»[1]âˆ¥D[2]x[2]âˆ¥1 | {z } g1(D[2]x[2]) + Î»[2]âˆ¥x[2]âˆ¥1 | {z } g2(x[2]) . (7.26) Note that we have chosen to express the overall loss as F = f + g1 + g2. All the functions are convex, but only f is smooth. With these, the multilayer ISTA (MLâ€“ ISTA) from Sulam et al. (2020) employs a sequence of nested proximal gradient mappings. In this way, and employing the auxiliary variable x[1] = D[2]x[2], the updates of MLâ€“ISTA for the problem in (7.26) can be written as: x[2] k+1 = proxtg2  x[2] k âˆ’tD[2]TGf ,g1 1/Ï  x[1] k  . (7.27) In the particular case where g1 and g2 are the â„“1 norms, their prox is the soft-thresh- olding operator, and so the above update can be written as x[2] k+1 = TtÎ»[2]  x[2] k âˆ’t ÏD[2]T  x[1] k âˆ’TÏÎ»[1](x[1] k âˆ’ÏD[1]T(D[1]x[1] k âˆ’x[0]))  . (7.28) Though seemingly intricate, this algorithm simply performs a series of proximal gradient updates per layer, albeit in a global and holistic way, that require only matrixâ€“vector multiplications and entry-wise operators. Moreover, when the dic- tionaries are convolutional, one can leverage fast GPU implementations to the MLâ€“BP problem on high-dimensional data. Proximal gradient-mapping algorithms have not been proposed before, let alone analyzed. In fact, by noting that gradient-mapping is, in general, not the gradient of any primitive function, one naturally asks: Does such an update even minimize the problem MLâ€“BP in (7.26)? We give here the convergence result from Sulam et al. (2020), presenting some ï¬rst theoretical insights. In a nutshell, it states that if the iterates are close enough, in the sense that they are Îµ-ï¬xed points, âˆ¥x[2] k+1âˆ’x[2] k âˆ¥2 < tÎµ, 7.6 Multilayer Iterative Shrinkage Algorithms 329 then these iterates can get arbitrarily close to the optimum of F(x[2]) in the function value. Theorem 7.4 (Sulam et al., 2020, Corollary 2.21). Suppose that {x[2] k } is the sequence generated by MLâ€“ISTA with Ï âˆˆ  0, 1 âˆ¥D[1] âˆ¥2 2  and t âˆˆ  0, 4Ï 3âˆ¥D[2] âˆ¥2  . If âˆ¥x[2] k+1 âˆ’x[2] k âˆ¥2 â‰¤tÎµ then F(x[2] k+1) âˆ’Fopt â‰¤Î·Îµ + (Î² + Îºt)Ï, (7.29) where Î·, Î² and Îº are constants not depending on t, Ï,Ïµ. One can state an analogous result for an accelerated version of the above algo- rithm, a fast MLâ€“ISTA (Sulam et al., 2020). However, further work is required to provide a complete convergence analysis of these multilayer ISTA approaches that provide convergence rates. In addition, the above result is limited to a two-layer model, and an extension to further layers is not trivial. These questions constitute exciting avenues of current research. 7.6.1 Towards Principled Recurrent Neural Networks To test the ideas presented in the last two sections in a real data application, and through this tie them to practical implementations of deep neural networks, we propose the following experiment. Train a convolutional MLâ€“SC model in a supervised learning setting for image classiï¬cation by considering a training set of N pairs of images x[0] i and their corresponding labels yi, i.e., {(x[0] i , yi)}N i=1. Denote by Î¶Î¸(Ë†x[L] i ) a classiï¬er parameterized by Î¸ acting on representations Ë†x[L] i , computed from each example x[0] i . We propose to address the following optimization problem: min Î¸,{D[i],Î»[i]} 1 N N Ã• 1=1 L  yi, Î¶Î¸(Ë†x[L] i )  such that (7.30) Ë†x[L] i = arg min x[L] 1 2 âˆ¥x[0] i âˆ’D[1,L]x[L]âˆ¥2 2 + Lâˆ’1 Ã• i=1 Î»[i]âˆ¥D[i+1,L]x[L]âˆ¥1 + Î»[L]âˆ¥x[L]âˆ¥1. In other words, we employ a classiï¬cation loss L (such as the cross-entropy func- tion) to train a classiï¬er Î¶Î¸(Ë†x[L] i ), where the representation vector Ë†x[L] i is a solution to the multilayer basis pursuit problem. Solving this bi-level problem while com- puting an exact minimizer of the MLâ€“BP is challenging. However, this diï¬ƒculty can be alleviated by employing the MLâ€“ISTA iterates as approximations to the representation vectors at diï¬€erent iterations, (Ë†x[L] i )k. In this way, we can replace the analytic expressions for such iterates in the loss function L, enabling minimiza- tion with respect to the parameters by back-propagation. Notably, this provides a 330 Aberdam and Sulam: Deep Learning as Sparsity-Enforcing Algorithms D[1] W[2] D[2] W[1] TÏÎ»1 TtÎ»2 x[0] Figure 7.3 Recurrent network interpretation of the MLâ€“ISTA algorithm in (7.28). Note that the ï¬lters are given by W[i] = D[i]T , and the thresholding operators TÎ² can be equivalently written in terms of a ReLU activations function, Ïƒ, and appropriate biases b. Adapted from Sulam et al. (1980). Â©1980 IEEE. Reprinted with permission of IEEE. learning algorithm that is closer to more traditional autoencoders. Nonetheless, the sparse-promoting norms (at every layer) are made explicit in (7.30). This implies that a pursuit involving a single thresholding operation per layer (such as the for- ward pass) is sub-optimal, as it will not provide minimizers of the bilevel problem above. The formulation in (7.30) generalizes feed-forward convolutional neural net- works. Indeed, if only one iteration of the MLâ€“ISTA is used (and initially we once again takes x[2] 0 = 0, for a two-layer model, for simplicity) we simply compute the representations by Ë†x[2] = TtÎ»[2]  t ÏD[2]T TÏÎ»[1]  ÏD[1]Tx[0] . (7.31) Once again, denoting the networkâ€™s ï¬lters by W[i] = D[i]T and under a non- negative assumption on the representations x[i], the above can be written simply as Ë†x[2] = Ïƒ  t ÏW[2]Ïƒ  ÏW[1]x[0] + b[1] + b[2] , i.e., by computing the forward pass. More generally, however, further iterations of MLâ€“ISTA compute more iterates from the recurrent network in Figure 7.3. Alas, unlike other architectures popular in the deep learning community, such as Resnet (He et al., 2016), Densenet (Huang et al., 2017), these â€œskip connectionsâ€ are nothing other than the implementation of an optimization algorithm minimizing the multilayer basis pursuit problem. Im- portantly, while further iterations of the MLâ€“ISTA eï¬€ectively implement â€œdeeperâ€ networks, the number of parameters in the model remains unchanged. Before moving onto the numerical demonstration of these algorithms, a comment about the LISTA approach from Gregor and LeCun (2010) is appropriate. Note that in our case the representations are obtained as those estimated by MLâ€“ISTA from (7.27). LISTA, on the other hand, factorizes the update of ISTA into two new 7.6 Multilayer Iterative Shrinkage Algorithms 331 matrices, for a single layer or dictionary, and it learns those instead. As a result, the number of parameters is doubled. An extension of our approach in the spirit of Gregor and LeCun (2010) should in the ï¬rst place extend LISTA to the multilayer setting presented in this chapter, and some preliminary results in this direction can be found in Sulam et al. (2020). We studied the problem in (7.30) for three common datasets: MNIST, SVHN and CIFAR10. For the ï¬rst two cases, we construct a three layer convolutional neural network with ReLU activations (i.e., employing non-negativity constraints in the representations) followed by a ï¬nal linear (fully connected) classiï¬er Î¶Î¸(x). For CIFAR10, we implemented an analogous three layer convolutional MLâ€“SC model, and deï¬ne Î¶Î¸(x) as a three-layer convolutional network with pooling operators.7 The models were trained with stochastic gradient descent (SGD), without any other optimization tricks (batch normalization, dropout, etc.), so as to provide a clear experimental setting. For the same reason, we excluded from this comparison other â€“ more sophisticated â€“ models that can be trained with these tools. For reference, the reader should note that such state-of-the-art methods obtain around 92%â€“95% accuracy in the CIFAR10 dataset (Huang et al., 2017). Our goal here is not to provide a best-performing architecture but, rather, to verify the practical validity of the MLâ€“SC model through a pursuit algorithm that serves it better, and to explore the beneï¬ts of MLâ€“ISTA over feed-forward networks. Further implementation details can be found in Sulam et al. (2020), and code is provided to reproduce these experiments.8 The results are shown in Figure 7.4, which give, for comparison, the networks resulting from applying MLâ€“ISTA and MLâ€“FISTA with, as baseline, a feed-forward CNN. We also include in these experiments the layered basis pursuit approach from (7.16). These four networks (the feed-forward CNN, MLâ€“ISTA, MLâ€“FISTA, and MLâ€“BP) all have exactly the same number of model parameters, namely for dictionaries, bias vectors, and the ï¬nal classiï¬er. The latter constructions, however, are networks that are eï¬€ectively deeper, as they â€œunrollâ€ the iterations of diï¬€erent multilayer pursuit algorithms. A natural question is, then, how well would a network with the same depth and general recurrent architecture perform if the parameters (weights) were not tied but rather free to be learned? This is the model denoted by â€œall freeâ€ in Figure 7.4. Somewhat surprisingly, performance does not improve any further, despite â€“ or perhaps because â€“ a signiï¬cant increase in the numbers of parameters. 7 Note that MLâ€“SC is a linear generative model. As such, in order to include the shift-invariant properties that are important for very heterogeneous images in CIFAR, we included pooling operators in the classiï¬er part. 8 Code is available at https://github.com/jsulam/ml-ista. 332 Aberdam and Sulam: Deep Learning as Sparsity-Enforcing Algorithms 0 50 100 150 Epoch 70 72 74 76 78 80 82 84 CIFAR CNN ML-ISTA ML-FISTA LBP All Free 0 20 40 60 80 100 Epoch 97.0 97.5 98.0 98.5 99.0 99.5 MNIST CNN ML-ISTA ML-FISTA LBP All Free 0 50 100 150 Epoch 88 89 90 91 92 93 94 95 SVHN CNN ML-ISTA ML-FISTA LBP All Free Figure 7.4 Comparison of diï¬€erent architectures on the SVHN dataset, with a feed-forward network as baseline. All networks have the same number of parameters. Adapted from Sulam et al. (1980). Â© 1980 IEEE. Reprinted, with permission, from IEEE. 7.7 Final Remarks and Outlook This chapter has two main objectives. The ï¬rst consists in exploring the new mul- tilayer model for sparse signals, analysing the dimensionality of such sparse repre- sentations and brieï¬‚y presenting some theoretical results regarding uniqueness and stability guarantees. The second objective is to employ this understanding of the MLâ€“SC model to design tractable pursuit formulations, as well as eï¬ƒcient algo- rithms to solve them. In doing so, we also showed how these ï¬rst-order algorithms generalise feed-forward CNNs, improving their performance while maintaining the number of parameters in the model constant. Several exciting research directions exist in both avenues of work. An important question that remains unanswered is the following: how stable are these solutions to the MLâ€“BP problem? Unlike the layered and projections approaches from Â§7.4, there are currently no bounds known for the distance between the estimates recov- ered by the MLâ€“ISTA and the original representations generating these signals. Such results would provide stability bounds that depende on the amount of unfold- ings of the recurrent network in Figure 7.3 and would have feed-forward networks as a particular case. A second major issue that we have not explored in this chapter is the learnability of the dictionaries in the MLâ€“SC model. How do the model constraints aï¬€ect or aid the sample complexity of such a model? How would this answer depend on the accuracy of the estimates for the representations Ë†x[L]? Providing answers to these questions would likely shed light on the sample complexity of particular classes of recurrent network architectures. As the careful reader might have noticed, the experimental results in the ï¬nal section reveal something rather curious: modifying the mapping from the input space to the representation space, A(x[0]): Rn0 â†’RnL, while maintaining the References 333 structure of the generative model improved classiï¬cation when minimizing the supervised loss L(yi, Î¶Î¸(A(x[0] i )). Why does it happen? What does this imply about the aptitude of the obtained Ë†x[L] i as representations for a signal x[0] i in the context of supervised learning? Seeking explanations to this phenomenon, in particular from a learning theory perspective, is also a highly promising avenue of work. Finally, what else can we learn from MLâ€“SC as a generative model for deep learning? In this sense, improving the stability and robustness of deep classiï¬ers is a clear path. In the theoretical analysis of the model, and of the pursuits involved, we have ignored the fact that the ultimate task was to employ the obtained sparse representations for classiï¬cation. How do these eventually aï¬€ect the subsequent classiï¬er? Romano and Elad (2018) studied precisely this point, showing that the stability of the obtained Ë†x[L] directly aï¬€ects the stability of the (linear) classiï¬er acting on them. In particular, they showed that the coherence of the dictionaries in the MLâ€“SC model inï¬‚uences the margin of the ï¬nal classiï¬er and in turn controls the robustness to adversarial attacks. Further work is required to tie their analysis to the holistic approach presented here in Â§7.5; this opens exciting research directions. References Aberdam, Aviad, Sulam, Jeremias, and Elad, Michael. 2019. Multi-layer sparse coding: The holistic way. SIAM Journal on Mathematics of Data Science, 1(1), 46â€“77. Aharon, Michal, Elad, Michael, and Bruckstein, Alfred. 2006. K-SVD: An algo- rithm for designing overcomplete dictionaries for sparse representation. IEEE Transactions on Signal Processing, 54(11), 4311â€“4322. Beck, Amir. 2017. First-Order Methods in Optimization. SIAM. Beck, Amir, and Teboulle, Marc. 2009. A fast iterative shrinkage-thresholding algorithm for linear inverse problems. SIAM Journal on Imaging Sciences, 2(1), 183â€“202. Blumensath, Thomas, and Davies, Mike E. 2009. Iterative hard thresholding for compressed sensing. Applied and Computational Harmonic Analysis, 27(3), 265â€“274. BÃ¶lcskei, Helmut, Grohs, Philipp, Kutyniok, Gitta, and Petersen, Philipp. 2019. Optimal approximation with sparsely connected deep neural networks. SIAM Journal on Mathematics of Data Science, 1(1), 8â€“45. Boyd, Stephen, and Vandenberghe, Lieven. 2004. Convex Optimization. Cambridge University Press. Candes, Emmanuel J., Eldar, Yonina C., Needell, Deanna, and Randall, Paige. 2010. Compressed sensing with coherent and redundant dictionaries. ArXiv preprint arXiv:1005.2613. Chen, Xiaohan, Liu, Jialin, Wang, Zhangyang, and Yin, Wotao. 2018. Theoretical 334 Aberdam and Sulam: Deep Learning as Sparsity-Enforcing Algorithms linear convergence of unfolded ista and its practical weights and thresholds. Pages 9061â€“9071 of: Advances in Neural Information Processing Systems. Cisse, Moustapha, Bojanowski, Piotr, Grave, Edouard, Dauphin, Yann, and Usunier, Nicolas. 2017. Parseval networks: Improving robustness to adversarial exam- ples. Pages 854â€“863 of: Proc. 34th International Conference on Machine Learning. Combettes, Patrick L., and Pesquet, Jean-Christophe. 2011. Proximal splitting methods in signal processing. Pages 185â€“212 of: Fixed-Point Algorithms for Inverse Problems in Science and Engineering. Springer. Donoho, David L., and Elad, Michael. 2003. Optimally sparse representation in general (nonorthogonal) dictionaries via â„“1 minimization. Proc. National Academy of Sciences, 100(5), 2197â€“2202. Elad, Michael. 2010. Sparse and Redundant Representations â€“ From Theory to Applications in Signal and Image Processing. Springer. Giryes, Raja, Eldar, Yonina C., Bronstein, Alex M., and Sapiro, Guillermo. 2018. Tradeoï¬€s between convergence speed and reconstruction accuracy in inverse problems. IEEE Transactions on Signal Processing, 66(7), 1676â€“1690. Goodfellow, Ian, Bengio, Yoshua, and Courville, Aaron. 2016. Deep Learning. MIT Press. http://www.deeplearningbook.org. Gregor, Karol, and LeCun, Yann. 2010. Learning fast approximations of sparse cod- ing. Pages 399â€“406 of: Proc. 27th International Conference on International Conference on Machine Learning. Omnipress. He, Kaiming, Zhang, Xiangyu, Ren, Shaoqing, and Sun, Jian. 2016. Deep residual learning for image recognition. Pages 770â€“778 of: Proc. IEEE Conference on Computer Vision and Pattern Recognition. Ho, Nhat, Nguyen, Tan, Patel, Ankit, Anandkumar, Anima, Jordan, Michael I., and Baraniuk, Richard G. 2018. Neural rendering model: Joint generation and prediction for semi-supervised learning. ArXiv preprint arXiv:1811.02657. Huang, Gao, Liu, Zhuang, Weinberger, Kilian Q., and van der Maaten, Laurens. 2017. Densely connected convolutional networks. Page 3 of: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, vol. 1. Jiang, Zhuolin, Lin, Zhe, and Davis, Larry S. 2013. Label consistent K-SVD: Learning a discriminative dictionary for recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence, 35(11), 2651â€“2664. LeCun, Yann, Boser, Bernhard E., Denker, John S., Henderson, Donnie, Howard, Richard E., Hubbard, Wayne E., and Jackel, Lawrence D. 1990. Handwrit- ten digit recognition with a back-propagation network. Pages 396â€“404 of: Advances in Neural Information Processing Systems. Lee, Honglak, Ekanadham, Chaitanya, and Ng, Andrew Y. 2008. Sparse deep belief net model for visual area V2. Pages 873â€“880 of: Advances in Neural Information Processing Systems. References 335 Liu, Jialin, Chen, Xiaohan, Wang, Zhangyang, and Yin, Wotao. 2019. ALISTA: An- alytic weights are as good as learned weights in LISTA. In: Proc. International Conference on Learning Representations. Mairal, Julien, Bach, Francis, Ponce, Jean, Sapiro, Guillermo, and Zisserman, Andrew. 2009. Non-local sparse models for image restoration. Pages 2272â€“ 2279 of: proc. 12th International Conference on Computer Vision. IEEE. Mairal, Julien, Bach, Francis, Ponce, Jean, and Sapiro, Guillermo. 2010. Online learning for matrix factorization and sparse coding. Journal of Machine Learning Research, 11, 19â€“60. Mairal, Julien, Bach, Francis, and Ponce, Jean. 2012. Task-driven dictionary learn- ing. IEEE Transactions on Pattern Analysis and Machine Intelligence, 34(4), 791â€“804. Makhzani, Alireza, and Frey, Brendan. 2013. K-sparse autoencoders. ArXiv preprint arXiv:1312.5663. Makhzani, Alireza, and Frey, Brendan J. 2015. Winner-take-all autoencoders. Pages 2791â€“2799 of: Advances in Neural Information Processing Systems. Mallat, StÃ©phane. 2016. Understanding deep convolutional networks. Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences, 374(2065), 20150203. Mallat, StÃ©phane G, and Zhang, Zhifeng. 1993. Matching pursuits with timeâ€“ frequency dictionaries. IEEE Transactions on Signal Processing, 41(12), 3397. Moreau, Thomas, and Bruna, Joan. 2017. Understanding trainable sparse coding via matrix factorization. In: Proc. International Conference on Learning Representations. Nam, Sangnam, Davies, Mike E., Elad, Michael, and Gribonval, RÃ©mi. 2013. The cosparse analysis model and algorithms. Applied and Computational Har- monic Analysis, 34(1), 30â€“56. Ng, Andrew, et al. 2011. Sparse autoencoder. CS294A Lecture Notes, Stanford University. Olshausen, Bruno A., and Field, David J. 1996. Emergence of simple-cell recep- tive ï¬eld properties by learning a sparse code for natural images. Nature, 381(6583), 607. Olshausen, Bruno A., and Field, David J. 1997. Sparse coding with an overcomplete basis set: A strategy employed by V1? Vision Research, 37(23), 3311â€“3325. Papyan, Vardan, Romano, Yaniv, and Elad, Michael. 2017a. Convolutional neu- ral networks analyzed via convolutional sparse coding. Journal of Machine Learning Research, 18(1), 2887â€“2938. Papyan, Vardan, Sulam, Jeremias, and Elad, Michael. 2017b. Working locally think- ing globally: Theoretical guarantees for convolutional sparse coding. IEEE Transactions on Signal Processing, 65(21), 5687â€“5701. Papyan, Vardan, Romano, Yaniv, Sulam, Jeremias, and Elad, Michael. 2018. The- oretical foundations of deep learning via sparse representations: A multilayer 336 Aberdam and Sulam: Deep Learning as Sparsity-Enforcing Algorithms sparse model and its connection to convolutional neural networks. IEEE Signal Processing Magazine, 35(4), 72â€“89. Patel, Ankit B., Nguyen, Minh Tan, and Baraniuk, Richard. 2016. A probabilis- tic framework for deep learning. Pages 2558â€“2566 of: Advances in Neural Information Processing Systems. Patel, Vishal M., Chen, Yi-Chen, Chellappa, Rama, and Phillips, P. Jonathon. 2014. Dictionaries for image and video-based face recognition. Journal of the Optical Society of America A, 31(5), 1090â€“1103. Pati, Yagyensh Chandra, Rezaiifar, Ramin, and Krishnaprasad, Perinkulam Samba- murthy. 1993. Orthogonal matching pursuit: Recursive function approxima- tion with applications to wavelet decomposition. Pages 40â€“44 of: Conference Record of the 27th Asilomar Conference on Signals, Systems and Computers, 1993. IEEE. Ranzato, MarcÃurelio, Poultney, Christopher, Chopra, Sumit, LeCun, Yann, et al. 2007. Eï¬ƒcient learning of sparse representations with an energy-based model. Pages 1137â€“1144 of: Advances in Neural Information Processing Systems. Ranzato, MarcÃurelio, Boureau, Y-Lan, and LeCun, Yann. 2008. Sparse feature learning for deep belief networks. Pages 1185â€“1192 of: Advances in Neural Information Processing Systems. Romano, Yaniv, and Elad, Michael. 2018. Classiï¬cation stability for sparse-modeled signals. ArXiv preprint arXiv:1805.11596. Romano, Yaniv, Protter, Matan, and Elad, Michael. 2014. Single image interpolation via adaptive nonlocal sparsity-based modeling. IEEE Transactions on Image Processing, 23(7), 3085â€“3098. Rumelhart, David E., Hinton, Geoï¬€rey E., and Williams, Ronald J. 1986. Learning representations by back-propagating errors. Nature, 323(6088), 533. Shrivastava, Ashish, Patel, Vishal M., and Chellappa, Rama. 2014. Multiple kernel learning for sparse representation-based classiï¬cation. IEEE Transactions on Image Processing, 23(7), 3013â€“3024. Sulam, J., Aberdam, A., Beck, A. and Elad, M. 1980. On multi-layer basis pursuit, eï¬ƒcient algorithms and convolutional neural networks. IEEE Transactions on Pattern Analysis and Machine Intelligence, 42(8), 1968â€“1980. Sulam, Jeremias, Ophir, Boaz, and Elad, Michael. 2014. Image denoising through multi-scale learnt dictionaries. Pages 808â€“812 of: Proc. International Confer- ence on Image Processing. IEEE. Sulam, Jeremias, Ophir, Boaz, Zibulevsky, Michael, and Elad, Michael. 2016. Trainlets: Dictionary learning in high dimensions. IEEE Transactions on Signal Processing, 64(12), 3180â€“3193. Sulam, Jeremias, Papyan, Vardan, Romano, Yaniv, and Elad, Michael. 2018. Mul- tilayer convolutional sparse modeling: Pursuit and dictionary learning. IEEE Transactions on Signal Processing, 66(15), 4090â€“4104. Sulam, Jeremias, Aberdam, Aviad, Beck, Amir, and Elad, Michael. 2020. On multi- layer basis pursuit, eï¬ƒcient algorithms and convolutional neural networks. References 337 IEEE Transactions on Pattern Recognition and Machine Intelligence, 42(8), 1968â€“1980. Tropp, Joel A. 2004. Greed is good: Algorithmic results for sparse approximation. IEEE Transactions on Information theory, 50(10), 2231â€“2242. Tropp, Joel A. 2006. Just relax: Convex programming methods for identifying sparse signals in noise. IEEE Transactions on Information Theory, 52(3), 1030â€“1051. Vaiter, Samuel, PeyrÃ©, Gabriel, Dossal, Charles, and Fadili, Jalal. 2013. Robust sparse analysis regularization. IEEE Transactions on Information Theory, 59(4), 2001â€“2016. 8 The Scattering Transform Joan Bruna Abstract: In this chapter we present scattering representations, a signal repre- sentation built using wavelet multiscale decompositions with a deep convolutional architecture. Its construction highlights the fundamental role of geometric stability in deep learning representations, and provides a mathematical basis to study con- volutional neural networks (CNNs). We describe its main mathematical properties, its applications to computer vision, speech recognition and physical sciences, as well as its extensions to Lie groups and non-Euclidean domains. Finally, we discuss recent applications to the modeling of high-dimensional probability densities. 8.1 Introduction Understanding the success of deep learning in challenging data domains, such as computer vision, speech recognition or natural language processing,remainsa major unanswered question that requires a tight integration of diï¬€erent theoretical aspects of the learning algorithm: approximation, estimation and optimization. Amongst the many pieces responsible for such success, an important element comes from the extra structure built into the neural architecture as a result of the input signal structure. Images, sounds and text are signals deï¬ned over low-dimensional domains such as grids or their continuous Euclidean counterparts. In these domains one can articulate speciï¬c priors of data distributions and tasks, which are leveraged in neural networks through convolutional layers. This requires developing a signal processing theory of deep learning. In order to gain a mathematical understanding of the interplay between geometric properties of the input domain and convolutional architectures, in this chapter we set aside the op- timization and data-adaptivity pieces of the puzzle and take an axiomatic approach to building high-dimensional signal representations with prescribed properties that make them amenable to complex recognition and classiï¬cation tasks. The ï¬rst step is to develop the notion of geometric stability (Â§8.2). In essence, 338 8.2 Geometric Stability 339 a signal representation deï¬ned on a metric domain is geometrically stable if small perturbations in the metric structure result in small changes in the output features. In Euclidean domains, geometric stability can be expressed in terms of diï¬€eo- morphisms, which model many naturally occurring transformations in computer vision and speech recognition, such as changes in viewpoint, local translations or musical pitch transpositions. Stability with respect to the action of diï¬€eomorphisms is achieved by sepa- rating scales, leading to multiscale signal decompositions. Section 8.3 describes scattering representations on the Euclidean translation group. First introduced in Mallat (2012), they combine wavelet multiscale decompositions with pointwise modulus activation functions. We describe their main mathematical properties and applications to computer vision. Scattering transforms are natural generalizations of multiscale representations of stochastic processes, in which classical high-order polynomial moments are replaced by stable non-linear transforms. Section 8.4 reviews stochastic scattering representations and their main applications to multi- fractal analysis. Euclidean scattering representations serve as a mathematical basis for studying CNNs on image and audio domains. In many areas of physical and social sci- ences, however, data are rarely deï¬ned over regular Euclidean domains. As it turns out, one can extend the formalism of geometric stability and wavelet scattering representations in two important directions: ï¬rst, to more general Lie groups of transformations (Â§8.5), and then to graphs and manifolds (Â§8.5.3). We conclude this chapter by focusing on two important applications of scattering representations. Thanks to their ability to capture key geometrical properties of high- dimensional signals with stability guarantees, they may be used in unsupervised learning to perform high-dimensional density estimation and implicit modeling, as described in Â§8.6. 8.2 Geometric Stability This ection describes the notion of geometric stability in signal representations. We begin with the Euclidean setting (Â§8.2.1), where this stability is expressed in terms of diï¬€eomorphisms of the signal domain. We discuss how to extend this notion to general metric domains in Â§8.2.3, and then highlight the limitations of several standard high-dimensional signal representations in regards to geometric stability (Â§8.2.4). 340 Bruna: The Scattering Transform 8.2.1 Euclidean Geometric Stability Consider a compact d-dimensional Euclidean domain â„¦= [0,1]d âŠ‚Rd on which square-integrable functions x âˆˆL2(â„¦) are deï¬ned (for example, in image analysis applications, images can be thought of as functions on the unit square â„¦= [0,1]2). We consider a generic supervised learning setting, in which an unknown function f : L2(â„¦) â†’Y is observed on a training set {xi âˆˆL2(â„¦), fi = f (xi)}iâˆˆI. In the vast majority of computer vision and speech analysis tasks, the unknown function f satisï¬es crucial regularity properties expressed in terms of the signal domain â„¦. Global translation invariance. Let Tvx(u) = x(u âˆ’v), with u,v âˆˆâ„¦, be a trans- lation operator1 acting on functions x âˆˆL2(â„¦). Our ï¬rst assumption is that the function f is either invariant, i.e., f (Tvx) = f (x) for any x âˆˆL2(â„¦) and v âˆˆâ„¦, or equivariant, i.e., f (Tvx) = Tv f (x), with respect to translations, depending on the task. Translation invariance is typical in object classiï¬cation tasks, whereas equiv- ariance arises when the output of the model is a space in which translations can act upon (for example, in problems of object localization, semantic segmentation, or motion estimation). The notions of global invariance and equivariance can be easily extended to other transformation groups beyond translations. We discuss in Â§8.5 one such extension, to the group of rigid motions generated by translations and rotations in â„¦. However, global invariance is not a strong prior in the face of high-dimensional estimation. Ineed, global transformation groups are typically low dimensional; in particular, in signal processing, they often correspond to subgroups of the aï¬ƒne group Aï¬€(â„¦), with dimension O(d2). A much stronger prior may be deï¬ned by specifying how the function f behaves under geometric perturbations of the domain which are â€˜nearbyâ€™ these global transformation groups. Local deformations and scale separation. In particular, given a smooth vector ï¬eld Ï„: â„¦â†’â„¦, a deformation by Ï„ acts on L2(â„¦) as xÏ„(u) := x(u âˆ’Ï„(u)). Deformations can model local translations, changes in viewpoint, rotations and frequency transpositions (Bruna and Mallat, 2013) and have been extensively used as models of image variability in computer vision (Jain et al., 1996; Felzenszwalb et al., 2010; Girshick et al., 2014). Most tasks studied in computer vision are not only translation invariant/equivariant but also stable with respect to local deformations (Mallat, 2016; Bruna and Mallat, 2013). In tasks that are translation invariant, the above prior may be expressed informally as | f (xÏ„) âˆ’f (x)| â‰ˆâˆ¥Ï„âˆ¥, (8.1) 1 Assuming periodic boundary conditions to ensure that the operation is well deï¬ned over L2(â„¦). 8.2 Geometric Stability 341 for all x,Ï„. Here, âˆ¥Ï„âˆ¥measures the distance of the associated diï¬€eomorphism Ï•(u) := u âˆ’Ï„(u) to the translation group; we will see in the next section how to specify this metric in the space of diï¬€eomorphisms. In other words, the target to be predicted does not change much if the input image is slightly deformed. In tasks that are translation equivariant, we have | f (xÏ„)âˆ’fÏ„(x)| â‰ˆâˆ¥Ï„âˆ¥instead. The deformation stability property is much stronger than that of global invariance, since the space of local deformations has high dimensionality, as opposed to the group of global invariants. Besides this deformation stability, another key property of target functions arising from the physical world is that long-range dependences may be broken into mul- tiscale local interaction terms, leading to hierarchical models in which the spatial resolution is progressively reduced. To illustrate this principle, denote by q(z1, z2; v) = Prob(x(u) = z1 and x(u + v) = z2) (8.2) the joint distribution of two image pixels at an oï¬€set v from each other, where we have assumed a stationary statistical model for natural images (hence q does not depend upon the location u). In presence of long-range dependencies, this joint distribution will not be separable for any v. However, the deformation stability prior states that q(z1, z2; v) â‰ˆq(z1, z2; v(1 + Ïµ)) for small Ïµ. In other words, whereas long-range dependencies do indeed exist in natural images and are critical to object recognition, they can be captured and downsampled at diï¬€erent scales. This principle of stability with respect to local deformations has been exploited in the computer vision community in models other than CNNs, for instance, deformable-parts models (Felzenszwalb et al., 2010), as we will review next. In practice, the Euclidean domain â„¦is discretized using a regular grid with n points; the translation and deformation operators are still well-deï¬ned so the above prop- erties hold in the discrete setting. 8.2.2 Representations with Euclidean Geometric Stability Motivated by the previous geometric stability prior, we are interested in building signal representations that are compatible with such a prior. Speciï¬cally, suppose our estimation for f , the target function, takes the form Ë†f (x) := âŸ¨Î¦(x),Î¸âŸ©, (8.3) where Î¦: L2(â„¦) â†’RK corresponds to the signal representation and Î¸ âˆˆRK to the classiï¬cation or regression coeï¬ƒcients, respectively. In a CNN, one would associate Î¦ with the operator that maps the input to the ï¬nal hidden layer, and Î¸ with the ï¬nal output layer of the network. The linear relationship between Î¦(x) and Ë†f (x) above implies that geometric 342 Bruna: The Scattering Transform stability in the representation is suï¬ƒcient to guarantee a predictor which is also geometrically stable. Indeed, if we assume that for all x,Ï„ , âˆ¥Î¦(x) âˆ’Î¦(xÏ„)âˆ¥â‰²âˆ¥xâˆ¥âˆ¥Ï„âˆ¥, (8.4) then by Cauchyâ€“Schwarz, it follows that | Ë†f (x) âˆ’Ë†f (xÏ„)| â‰¤âˆ¥Î¸âˆ¥âˆ¥Î¦(x) âˆ’Î¦(xÏ„)âˆ¥â‰²âˆ¥Î¸âˆ¥âˆ¥xâˆ¥âˆ¥Ï„âˆ¥. This motivates the study of signal representations where one can certify (8.4), while ensuring that Î¦ captures enough information that âˆ¥Î¦(x) âˆ’Î¦(xâ€²)âˆ¥is large whenever | f (x) âˆ’f (xâ€²)| is large. In this setting, a notorious challenge to achieving (8.4) while keeping enough discriminative power in Î¦(x) is to transform the high- frequency content of x in such a way that it becomes stable. In recognition tasks, one may want to consider not only geometric stability but also stability with respect to the Euclidean metric in L2(â„¦): for all x,xâ€² âˆˆL2(â„¦), âˆ¥Î¦(x) âˆ’Î¦(xâ€²)âˆ¥â‰²âˆ¥x âˆ’xâ€²âˆ¥. (8.5) This stability property ensures that additive noise in the input will not drastically change the feature representation. The stability desiderata (8.4) and (8.5) may also be interpreted in terms of robustness to adversarial examples (Szegedy et al., 2014). Indeed, the general setup of adversarial examples consists in producing small perturbations xâ€² of a given input x (measured by appropriate norms) such that |âŸ¨Î¦(x) âˆ’Î¦(xâ€²),Î¸âŸ©| is large. Stable representations certify that those adversarial examples cannot be obtained with small additive or geometric perturbations. 8.2.3 Non-Euclidean Geometric Stability Whereas Euclidean domains may be used to model many signals of interest, such as images, videos or speech, a wide range of high-dimensional data across the physical and social sciences is naturally deï¬ned on more general geometries. For example, signals measured on social networks have rich geometrical structure, encoding locality and multiscale properties, yet they have a non-Euclidean geometry. An important question is thus how to extend the notion of geometrical stability to more general domains. Deformations provide the natural framework for describing geometric stability in Euclidean domains, but their generalization to non-Euclidean, non-smooth domains is not straightforward. Let x âˆˆL2(X) be a signal deï¬ned on a domain X. If X is embedded into a low-dimension Euclidean space â„¦âŠ‚Rd, such as a 2-surface within a three-dimensional space, then one can still deï¬ne meaningful deformations on X via extrinsic deformations of â„¦. Indeed, if Ï„: Rd â†’Rd is a smooth ï¬eld with 8.2 Geometric Stability 343 Ï•(v) = v âˆ’Ï„(v) the corresponding diï¬€eomorphism (assuming âˆ¥Ï„âˆ¥< 1/2), then we can deï¬ne xÏ„ âˆˆL2(XÏ„) as xÏ„(u) := x(Ï•âˆ’1(u)), u âˆˆX . Such deformation models were studied in Kostrikov et al. (2018) with applications in surface representation in which the notion of geometric stability relies on its ambient Euclidean structure. In more general applications, however, we may be interested in intrinsic notions of geometric stability that do not necessarily rely on a pre-existing low-dimensional embedding of the domain. The change of variables Ï•(u) = u âˆ’Ï„(u) deï¬ning the deformation can be seen as a perturbation of the Euclidean metric in L2(Rd). Indeed, âŸ¨xÏ„,yÏ„âŸ©L2(Rd,Âµ) = âˆ« Rd xÏ„(u)yÏ„(u)dÂµ(u) = âˆ« Rd x(u)y(u)|I âˆ’âˆ‡Ï„(u)|dÂµ(u) = âŸ¨x,yâŸ©L2(Rd, ËœÂµ) , with d ËœÂµ(u) = |I âˆ’âˆ‡Ï„(u)|dÂµ(u) and |I âˆ’âˆ‡Ï„(u)| â‰ˆ1 if âˆ¥âˆ‡Ï„âˆ¥is small, where I is the identity. Therefore, a possible way to extend the notion of deformation stability to general domains L2(X) is to think of X as a metric space and reason in terms of the stability of Î¦: L2(X) â†’RK to metric changes in X. This requires a representation that can be deï¬ned on generic metric spaces, as well as a criterion for comparing how close two metric spaces are to each other. We will describe a general approach for discrete metric spaces that is based on diï¬€usion operators in Â§8.5.3. 8.2.4 Examples Kernel Methods Kernel methods refer to a general theory in the machine learning framework, whose main purpose consists in embedding data in a high-dimensional space, in order to express complex relationships in terms of linear scalar products. For a generic input space Z (which can be thought of as Z = L2(X), corre- sponding to the previous discussion), a feature map Î¦: Z âˆ’â†’H maps data into a Hilbert space H with the reproducing property: for each f âˆˆH and x âˆˆZ, f (x) = âŸ¨f,Î¦(x)âŸ©. Linear classiï¬cation methods access the transformed data Î¦(x) only through scalar products of the form (Shawe-Taylor and Cristianini, 2004) âŸ¨Î¦(x),Î¦(xâ€²)âŸ©. Rather than building the mapping explicitly, the popular â€˜kernel trickâ€™ exploits Mercerâ€™s theorem. This theorem states that a continuous, symmetric and positive deï¬nite kernel K : Z Ã— Z â†’R deï¬nes an integral operator of L2(Z), which 344 Bruna: The Scattering Transform diagonalizes in an orthonormal basis (Minh et al., 2006) {Ï†n}n of L2(Z), with non-negative eigenvalues. As a result, K(x,xâ€²) admits a representation K(x,xâ€²) = Ã• nâ‰¥1 Î»nÏ†n(x)Ï†n(xâ€²) , which yields K(x,xâ€²) = âŸ¨Î¦(x),Î¦(xâ€²)âŸ©, with Î¦(x) = (Î»1/2 n Ï†n(x))n. For kernel methods it is thus suï¬ƒcient to construct positive deï¬nite kernels K on Z2 in order to extend linear classiï¬cation tools to more complex relationships. Despite their success and eï¬€ectiveness in a number of machine learning tasks, the high-dimensional embeddings induced by kernel methods do not automatically enjoy stability with respect to additive noise or deformations. The kernel needs to be chosen accordingly. Convolutional kernels networks (Mairal et al., 2014; Bietti and Mairal, 2019a) have been developed to capture geometric stability properties and to oï¬€er empirical performance competitive with modern deep architectures. These kernels contrast with another recent family, that of neural tangent kernels (Jacot et al., 2018), which linearize a generic deep architecture around its parameter initialization but which do not oï¬€er the same amount of geometric stability (Bietti and Mairal, 2019b). Power Spectra, Autocorrelation and Registration Invariants Translation invariant representations can be obtained from registration, auto-correl- ation or Fourier modulus operators. However, the resulting representations are not Lipschitz continuous to deformations. A representation Î¦(x) is translation invariant if it maps global translations, xc(u) = x(u âˆ’c), with c âˆˆRd, of any function x âˆˆL2(Rd) to the same image: for all x âˆˆL2(Rd), for all c âˆˆRd, Î¦(xc) = Î¦(x) . (8.6) The Fourier transform modulus is an example of a translation invariant repre- sentation. Let Ë†x(Ï‰) be the Fourier transform of x(u) âˆˆL2(Rd). Since b xc(Ï‰) = eâˆ’ic.Ï‰ Ë†x(Ï‰), it follows that | b xc| = |Ë†x| does not depend upon c. A Fourier modulus is translation invariant and stable to additive noise, but unsta- ble to small deformations at high frequencies (Mallat, 2012), as is illustrated with the following dilation example. Let Ï„(u) = su denote a linear displacement ï¬eld, where |s| is small, and let x(u) = eiÎ¾uÎ¸(u) be a modulated version of a low-pass window Î¸(u). Then the dilation xÏ„(u) = L[Ï„]x(u) = x((1 + s)u) moves the central frequency of Ë†x from Î¾ to (1+ s)Î¾. If Ïƒ2 Î¸ = âˆ« |Ï‰|2| Ë†Î¸(Ï‰)|2 dÏ‰ measures the frequency 8.2 Geometric Stability 345 spread of Î¸ then Ïƒ2 x = âˆ« |Ï‰ âˆ’Î¾|2| Ë†x(Ï‰)|2dÏ‰ = Ïƒ2 Î¸ , and Ïƒ2 xÏ„ = (1 + s)âˆ’d âˆ« (Ï‰ âˆ’(1 + s)Î¾)2| Ë†x((1 + s)âˆ’1Ï‰)|2 dÏ‰ = âˆ« |(1 + s)(Ï‰ âˆ’Î¾)|2| Ë†x(Ï‰)|2 dÏ‰ = (1 + s)2Ïƒ2 x . It follows that if the distance between the central frequencies of x and xÏ„, sÎ¾, is large compared with their frequency spreads, (2 + s)ÏƒÎ¸, then the frequency supports of x and xÏ„ are nearly disjoint and hence âˆ¥|Ë†xÏ„| âˆ’|Ë†x|âˆ¥âˆ¼âˆ¥xâˆ¥, which shows that Î¦(x) = |Ë†x| is not Lipschitz continuous to deformations, since Î¾ can be arbitrarily large. Ï‰ |Ë†x| |Ë†xÏ„| Î¾ (1 + s)Î¾ Ïƒx (1 + s)Ïƒx Figure 8.1 Dilation of a complex bandpass window. If Î¾ â‰«Ïƒxsâˆ’1 then the supports are nearly disjoint. The autocorrelation of x, Rx(v) = âˆ« x(u)xâˆ—(u âˆ’v) du, is also translation invariant: Rx = Rxc. Since Rx(v) = x â‹†x(v), with x(u) = xâˆ—(âˆ’u), it follows that c Rx(Ï‰) = | Ë†x(Ï‰)|2 . The Plancherel formula thus proves that it has the same instabilities as a Fourier transform: âˆ¥Rx âˆ’RxÏ„ âˆ¥= (2Ï€)âˆ’1 |Ë†x|2 âˆ’|Ë†xÏ„|2 . Besides deformation instabilities, the Fourier modulus and the autocorrelation lose too much information. For example, a Dirac Î´(u) and a linear chirp eiu2 are 346 Bruna: The Scattering Transform two signals having Fourier transforms whose moduli are equal and constant. Very diï¬€erent signals may not be discriminated from their Fourier moduli. A canonical invariant (Keysers et al., 2007; Soatto, 2009) Î¦(x) = x(u âˆ’a(x)) registers x âˆˆL2(Rd) with an anchor point a(x), which is translated when x is translated: a(xc) = a(x) + c . It thus deï¬nes a translation invariant representation: Î¦xc = Î¦x. For example, the anchor point may be a ï¬ltered maximum a(x) = arg maxu |xâ‹†h(u)|, for some ï¬lter h(u). A canonical invariant Î¦x(u) = x(u âˆ’a(x)) carries more information than a Fourier modulus, and characterizes x up to a global absolute position (Soatto, 2009). However, it has the same high-frequency instability as a Fourier modulus transform. Indeed, for any choice of anchor point a(x), applying the Plancherel formula proves that âˆ¥x(u âˆ’a(x)) âˆ’xâ€²(u âˆ’a(xâ€²))âˆ¥â‰¥(2Ï€)âˆ’1 âˆ¥|Ë†x(Ï‰)| âˆ’|Ë†xâ€²(Ï‰)|âˆ¥. If xâ€² = xÏ„, the Fourier transform instability at high frequencies implies that Î¦x = x(u âˆ’a(x)) is also unstable with respect to deformations. 8.3 Scattering on the Translation Group This section reviews the scattering transform on the translation group and its math- ematical properties. In Â§8.3.1 we discuss windowed scattering transforms and its construction from Littlewoodâ€“Paley wavelet decompositions. In Â§8.3.2 we intro- duce the scattering metric and review the scattering energy conservation property, and in Â§8.3.3 we describe the Lipschitz continuity property of scattering transforms with respect to deformations. Section 8.3.4 presents algorithmic aspects and im- plementation, and ï¬nally Â§8.3.5 illustrates scattering properties in computer vision applications. 8.3.1 Windowed Scattering Transform A wavelet transform is deï¬ned by dilating a mother wavelet Ïˆ âˆˆL2(Rd) with scale factors {aj}jâˆˆZ for a > 1. In image processing applications one usually sets a = 2, whereas audio applications need smaller dilation factors, typically a â‰¤21/8. Wavelets can be not only dilated but also rotated along a discrete rotation group G of Rd. As a result, a dilation by aj and a rotation by r âˆˆG of Ïˆ produce Ïˆa jr(u) = aâˆ’djÏˆ(aâˆ’jrâˆ’1u) . (8.7) 8.3 Scattering on the Translation Group 347 Wavelets are thus normalized in L1(Rd), so that âˆ¥Ïˆa jr âˆ¥1 = âˆ¥Ïˆâˆ¥1, which means that their Fourier transforms satisfy Ë†Ïˆa jr(Ï‰) = Ë†Ïˆ(ajrÏ‰). In order to simplify notation, we write Î» = ajr âˆˆaZ Ã— G and |Î»| = aj and deï¬ne ÏˆÎ»(u) = aâˆ’djÏˆ(Î»âˆ’1u). This notation will be used throughout the rest of the chapter. Scattering operators can be deï¬ned for general mother wavelets, but of particular interest are the complex wavelets that can be written as Ïˆ(u) = eiÎ·uÎ¸(u) , where Î¸ is a lowpass window whose Fourier transform is real and has a bandwidth of the order of Ï€. As a result, after a dilation and a rotation, Ë†ÏˆÎ»(Ï‰) = Ë†Î¸(Î»Ï‰ âˆ’Î·) is centered at Î»âˆ’1Î· and has a support size proportional to |Î»|âˆ’1. In Â§8.3.4 we shall specify the wavelet families used in all numerical experiments. A Littlewoodâ€“Paley wavelet transform is a redundant representation which com- putes the following ï¬lter bank, without subsampling: for all u âˆˆRd and Î» âˆˆaZÃ—G, WÎ»x(u) = xâ‹†ÏˆÎ»(u) = âˆ« x(v)ÏˆÎ»(uâˆ’v) dv . (8.8) If x is real and the wavelet is chosen such that Ë†Ïˆ is also real then Wâˆ’Î»x = WÎ»xâˆ—, which implies that in this case one can assimilate a rotation r with its negative version âˆ’r into an equivalence class of positive rotations G+ = G/{Â±1}. A wavelet transform with a ï¬nite scale 2J considers only the subbands Î» sat- isfying |Î»| â‰¤2J. The low frequencies which are not captured by these wavelets are recovered by a lowpass ï¬lter Ï†J whose spatial support is proportional to 2J: Ï†J(u) = 2âˆ’dJÏ†(2âˆ’Ju). The wavelet transform at scale 2J thus consists of the ï¬lter bank WJx = {x â‹†Ï†J,(WÎ»x)Î»âˆˆÎ›J } , where Î›J = {ajr : r âˆˆG+, |Î»| â‰¤2J}. Its norm is deï¬ned as âˆ¥WJxâˆ¥2 = âˆ¥x â‹†Ï†J âˆ¥2 + Ã• Î»âˆˆÎ›J âˆ¥WÎ»xâˆ¥2 . Thus WJ is a linear operator from L2(Rd) to a product space generated by copies of L2(Rd). It deï¬nes a frame of L2(Rd), whose bounds are characterized by the following Littlewoodâ€“Paley condition: Proposition 8.1. If there exists Ïµ > 0 such that, for almost all Ï‰ âˆˆRd and all J âˆˆZ, 1 âˆ’Ïµ â‰¤| Ë†Ï†(2JÏ‰)|2 + 1 2 Ã• j â‰¤J Ã• r âˆˆG | Ë†Ïˆ(2jrÏ‰)|2 â‰¤1, 348 Bruna: The Scattering Transform then WJ is a frame with bounds given by 1 âˆ’Ïµ and 1: (1 âˆ’Ïµ)âˆ¥xâˆ¥2 â‰¤âˆ¥WJxâˆ¥2 â‰¤âˆ¥xâˆ¥2, x âˆˆL2(Rd) . (8.9) In particular, this Littlewoodâ€“Paley condition implies that Ë†Ïˆ(0) = 0 and hence that the wavelet must have at least a vanishing moment. When Ïµ = 0, the wavelet decomposition preserves the Euclidean norm and we say that it is unitary. Wavelet coeï¬ƒcients are not translation invariant but translate as the input is translated, and their average âˆ« WÎ»x(u) du does not produce any information since wavelets have zero mean. A translation invariant measure which is also stable to the action of diï¬€eomorphisms can be extracted out of each wavelet sub-band Î», by introducing a non-linearity which restores a non-zero, informative, average value. This is for instance achieved by computing the complex modulus and averaging the result: âˆ« |x â‹†ÏˆÎ»|(u) du . (8.10) Although many other choices of non-linearity are algorithmically possible, the complex modulus preserves the signal energy and enables overall energy conserva- tion; see the next section, and ï¬nally in Â§8.7, how half-rectiï¬ed alternatives provide further insights into the signal through the phase harmonics. The information lost by the averaging in (8.10) is recovered by a new wavelet decomposition {|x â‹†ÏˆÎ»| â‹†ÏˆÎ»â€²}Î»â€²âˆˆÎ›J of |x â‹†ÏˆÎ»|, which produces new invariants by iterating the same procedure. Let U[Î»]x = |x â‹†ÏˆÎ»| denote the wavelet modulus operator corresponding to the subband Î». Any sequence p = (Î»1,Î»2,. . .,Î»m) deï¬nes a path, i.e., the ordered product of non-linear and non-commuting operators U[p]x = U[Î»m] Â· Â· Â· U[Î»2]U[Î»1]x = | Â· Â· Â· ||x â‹†ÏˆÎ»1| â‹†ÏˆÎ»2| Â· Â· Â· | â‹†ÏˆÎ»m| , with U[âˆ…]x = x. As with frequency variables, path variables p = (Î»1,. . .,Î»m) can be manipulated in a number of ways. The scaling and rotation by alg âˆˆaZ Ã— G+ of a path p is denoted by algp = (algÎ»1,. . .,algÎ»m), and the concatenation of two paths is written as p + pâ€² = (Î»1,. . .,Î»m,Î»â€² 1,. . .,Î»â€² mâ€²). Many applications in image and audio recognition require representations that are locally translation invariant, but which keep spatial or temporal information beyond a certain scale 2J. A windowed scattering transform computes a locally translation invariant representation by applying a low-pass ï¬lter at scale 2J with Ï†2J (u) = 2âˆ’2JÏ†(2âˆ’Ju). Deï¬nition 8.2. For each path p = (Î»1,. . .,Î»m) with Î»i âˆˆÎ›J and x âˆˆL1(Rd) we 8.3 Scattering on the Translation Group 349 m= 0 m= 1 m= 2 m= 3 f U [Î»1]f SJ[;]f = f ?Ï†J U [Î»1;Î»2]f SJ[Î»1]f SJ[Î»1;Î»2]f Figure 8.2 Convolutional structure of the windowed scattering transform. Each layer is computed from the previous one by applying a wavelet modulus decomposition U to each envelope U[p]x. The outputs of each layer are obtained via a low-pass ï¬lter Ï†J. deï¬ne the windowed scattering transform as SJ[p]x(u) = U[p]x â‹†Ï†2J (u) = âˆ« U[p]x(v)Ï†2J (u âˆ’v) dv . A scattering transform has the structure of a convolutional network, but its ï¬lters are given by wavelets instead of being learned. Thanks to this structure, the resulting transform is locally translation invariant and stable to deformations, as will be discussed in Â§8.3.3. The scattering representation enjoys several appealing properties that are described in the following subsections. 8.3.2 Scattering Metric and Energy Conservation The windowed scattering representation is obtained by cascading a basic propagator operator, UJx = {x â‹†Ï†J,(U[Î»]x)Î»âˆˆÎ›J } . (8.11) The ï¬rst layer of the representation applies UJ to the input function, whereas suc- cessive layers are obtained by applying UJ to each output U[p]x. Since U[Î»]U[p] = U[p + Î»] and U[p]x â‹†Ï†J = SJ[p]x, it follows that UJU[p]x = {SJ[p]x,(U[p + Î»]x)Î»âˆˆÎ›J } . (8.12) If Î›m J denotes the set of paths of length or order m, it follows from (8.12) that the (m+1)th layer, given by Î›m+1 J , is obtained from the previous layer via the propagator UJ. We denote by PJ the set of paths of any order up to scale 2J, PJ = Ã m Î›m J . 350 Bruna: The Scattering Transform The propagator UJ is non-expansive since the wavelet decomposition WJ is non-expansive, from (8.9), and the modulus is also non-expansive. As a result, âˆ¥UJx âˆ’UJxâ€²âˆ¥2 = âˆ¥x â‹†Ï†J âˆ’xâ€² â‹†Ï†J âˆ¥2 + Ã• Î»âˆˆÎ›J âˆ¥|WÎ»x| âˆ’|WÎ»xâ€²|âˆ¥2 â‰¤âˆ¥x âˆ’xâ€²âˆ¥2 . Moreover, if the wavelet decomposition is unitary then the propagator UJ is also unitary. For any path set â„¦, the Euclidean norm deï¬ned by the scattering coeï¬ƒcients SJ[p], p âˆˆâ„¦, is âˆ¥SJ[â„¦]xâˆ¥2 = Ã• pâˆˆâ„¦ âˆ¥SJ[p]xâˆ¥2 . Since SJ[PJ] is constructed by cascading the non-expansive operator UJ, it follows that SJ[PJ] is also non-expansive: Proposition 8.3. The windowed scattering transform is non-expansive: for all x,xâ€² âˆˆL2(Rd), âˆ¥SJ[PJ]x âˆ’SJ[PJ]xâ€²âˆ¥â‰¤âˆ¥x âˆ’xâ€²âˆ¥. (8.13) The windowed scattering thus deï¬nes a metric which is continuous with respect to the L2(Rd) Euclidean metric, and thus is stable to additive noise. Let us now consider the case where the wavelet decomposition is unitary, i.e., Ïµ = 0 in (8.9). One can easily verify by induction on the path order m = |p| that for all m, âˆ¥xâˆ¥2 = Ã• |p|<m âˆ¥SJ[p]xâˆ¥2 + Ã• |p|=m âˆ¥U[p]xâˆ¥2 . This decomposition expresses the signal energy âˆ¥xâˆ¥2 in terms of coeï¬ƒcients captured by the ï¬rst m layers of the scattering network and a residual energy RJ,x(m) := Ã pâˆˆPJ ;|p|=m âˆ¥U[p]xâˆ¥2. An important question with practical im- plications is how to understand the energy decay RJ,x(m) as m grows, since this determines how many layers of processing are eï¬€ectively needed to repre- sent the input. In particular, the scattering representation is energy preserving if limmâ†’âˆRJ,x(m) = 0. This was established, under mild assumptions on the wavelet decomposition, for the univariate case x âˆˆL2(R) in Waldspurger (2017): Theorem 8.4 (Waldspurger, 2017, Theorem 3.1). Let {Ïˆj}jâˆˆZ be a family of wavelets satisfying the Littlewoodâ€“Paley condition (8.9) and such that for all j,Ï‰ > 0, | Ë†Ïˆj(âˆ’Ï‰)| â‰¤| Ë†Ïˆj(Ï‰)| , with strict inequality for each Ï‰ for at least one scale. Finally, we assume for some Ïµ > 0 that Ë†Ïˆ(Ï‰) = O(|Ï‰|1+Ïµ) . 8.3 Scattering on the Translation Group 351 Then, for any J âˆˆZ, there exists r > 0, a > 1 such that for all m â‰¥2 and f âˆˆL2(R) it holds that RJ,x(m) â‰¤âˆ¥xâˆ¥2 âˆ’âˆ¥x â‹†Ï‡ram âˆ¥2 , (8.14) where Ï‡s is the Gaussian window Ï‡s(t) = âˆšÏ€s exp(âˆ’(Ï€st)2). In particular, this result establishes energy conservation, owing to the square integrability of Ë†x âˆˆL2(R). But, importantly, it also provides a quantitative rate at which the energy decays within the network: the energy in the input signal carried by frequencies around 2k disappears after O(k) layers, leading to exponential energy decay. An earlier version of the energy conservation was established in Mallat (2012) for general input dimensions but under more restrictive admissibility conditions for the wavelet, and without the rate of convergence. A similar energy conservation result, also with an exponential convergence rate, has been established for extensions of the scattering transform where the wavelet decomposition is replaced by other frames. Czaja and Li (2017) studied energy conservation for uniform covering frames, obtaining exponential convergence too. Wiatowski et al. (2017) generalized this result to more general frames that are also allowed to vary from one layer to the next. 8.3.3 Local Translation Invariance and Lipschitz Continuity with Respect to Deformations The windowed scattering metric deï¬ned in the previous section is non-expansive, which gives stability to additive perturbations. In this subsection we review its geometric stability to the action of deformations, and its asymptotic translation invariance, as the localization scale 2J increases. Each choice of such a localization scale deï¬nes a metric dJ(x,xâ€²) := âˆ¥SJ[PJ]x âˆ’SJ[PJ]xâ€²âˆ¥. An induction argument over the non-expansive Littlewoodâ€“Paley property (8.9) shows that the limit of dJ as J â†’âˆis well deï¬ned, thanks to the following non-expansive property: Proposition 8.5 (Mallat, 2012, Proposition 2.9). For all x,xâ€² âˆˆL2(Rd) and J âˆˆZ, âˆ¥SJ+1[PJ+1]x âˆ’SJ+1[PJ+1]xâ€²âˆ¥â‰¤âˆ¥SJ[PJ]x âˆ’SJ[PJ]xâ€²âˆ¥. As a result, the sequence (âˆ¥SJ[PJ]xâˆ’SJ[PJ]xâ€²âˆ¥)J is positive and non-increasing as J increases, and hence it converges. In fact, under mild assumptions, this limit metric is translation invariant: 352 Bruna: The Scattering Transform Theorem 8.6 (Mallat, 2012, Theorem 2.10). Let xv(u) = x(u âˆ’v). Then for ad- missible scattering wavelets satisfying the assumptions of Theorem 8.4 it holds that for all x âˆˆL2(Rd), c âˆˆRd, lim Jâ†’âˆâˆ¥SJ[PJ]x âˆ’SJ[PJ]xvâˆ¥= 0 (8.15) for d = 1. Remark 8.7. This result is proved in Mallat (2012) for general dimensions d under stronger assumptions on the wavelets (admissibility condition (2.28) in Mallat, 2012). However, these stronger assumptions can be made unnecessary, by extending the result in Waldspurger (2017) to arbitrary d. Remark 8.8. Wiatowski and BÃ¶lcskei (2017) described an interesting extension of Theorem 8.6 which holds for more general decomposition frames than wavelets that is based on the notion of vertical translation invariance. This refers to the asymptotic translation invariance enjoyed by mth-layer coeï¬ƒcients of the network, as m grows. The translation invariance of the overall representation is based on two funda- mental properties: (i) the equivariance of wavelet modulus decomposition operators with respect to translation, UJTvx = TvUJx, and (ii) the invariance provided by the local averaging operator AJx := x â‹†Ï†J. Indeed, scattering coeï¬ƒcients up to order m are obtained by composing UJ up to m times followed by AJ. It follows that the translation invariance measured at order m is expressed as âˆ¥SJ[Î›m J ]Tvx âˆ’SJ[Î›m J ]xâˆ¥= âˆ¥AJTvU[Î›m J ]x âˆ’AJU[Î›m J ]xâˆ¥ â‰¤âˆ¥U[Î›m J ]xâˆ¥âˆ¥AJTv âˆ’AJ âˆ¥. As well as asymptotic translation invariance, the windowed scattering transform deï¬nes a stable metric with respect to the action of diï¬€eomorphisms, which can model non-rigid deformations. A diï¬€eomorphism maps a point u âˆˆRd to u âˆ’Ï„(u), where Ï„(u) is a vector displacement ï¬eld satisfying âˆ¥âˆ‡Ï„âˆ¥âˆ< 1, where âˆ¥âˆ‡Ï„âˆ¥is the operator norm. As described in Â§8.2.1, it acts on functions x âˆˆL2(Rd) by composition: xÏ„(u) = x(u âˆ’Ï„(u)). The following central theorem computes an upper bound of âˆ¥SJ[PJ]xÏ„ âˆ’SJ[PJ]xâˆ¥. For that purpose, we assume an admissible scattering wavelet,2 and deï¬ne the auxiliary norm âˆ¥U[PJ]xâˆ¥1 = Ã• mâ‰¥0 âˆ¥U[Î›m J ]xâˆ¥. 2 Again, as mentioned in Remark 8.7, such admissible wavelet conditions can be relaxed by extending the energy conservation results from Waldspurger (2017). 8.3 Scattering on the Translation Group 353 Theorem 8.9 (Mallat, 2012, Theorem 2.12). There exists C such that every x âˆˆ L2(Rd) with âˆ¥U[PJ]xâˆ¥1 < âˆand Ï„ âˆˆC2(Rd) with âˆ¥âˆ‡Ï„âˆ¥âˆâ‰¤1/2 satisfy âˆ¥SJ[PJ]xÏ„ âˆ’SJ[PJ]xâˆ¥â‰¤Câˆ¥U[PJ]xâˆ¥1K(Ï„) , (8.16) with K(Ï„) = 2âˆ’J âˆ¥Ï„âˆ¥âˆ+ âˆ¥âˆ‡Ï„âˆ¥âˆmax  1,log supu,uâ€² |Ï„(u) âˆ’Ï„(uâ€²)| âˆ¥âˆ‡Ï„âˆ¥âˆ  + âˆ¥HÏ„âˆ¥âˆ, and, for all m â‰¥0, if PJ,m = Ã n<m Î›n J then âˆ¥SJ[PJ,m]xÏ„ âˆ’SJ[PJ,m]xâˆ¥â‰¤Cmâˆ¥xâˆ¥K(Ï„) . (8.17) This theorem shows that a diï¬€eomorphism produces in the scattering domain an error bounded by a term proportional to 2âˆ’J âˆ¥Ï„âˆ¥âˆ, which corresponds to the local translation invariance, plus a deformation error proportional to âˆ¥âˆ‡Ï„âˆ¥âˆ. Whereas rigid translations Tv commute with all the convolutional or pointwise operators deï¬ning the scattering representation, non-rigid deformations no longer commute with convolutions. The essence of the proof is thus to control the commutation error between the wavelet decomposition and the deformation. If LÏ„ denotes the deformation operator LÏ„x = xÏ„, Mallat (2012) proved that âˆ¥[WJ,LÏ„]âˆ¥= âˆ¥WJLÏ„ âˆ’LÏ„WJ âˆ¥â‰²âˆ¥âˆ‡Ï„âˆ¥, thanks to the scale-separation properties of wavelet decompositions. The norm âˆ¥U[PJ]xâˆ¥1 measures the decay of the scattering energy across depth. Again, in the univariate case it was shown in Waldspurger (2017) that for all m, âˆ¥U[Î›m J ]xâˆ¥â‰¤ âˆ« |Ë†x(Ï‰)|2hm(Ï‰) dÏ‰ 1/2 , with hm(Ï‰) = 1 âˆ’exp(âˆ’2(Ï‰/(ram))2) and a > 1. Denote by F =  x; âˆ« |Ë†x(Ï‰)|2 log(1 + |Ï‰|) dÏ‰ < âˆ  the space of functions whose Fourier transform is square integrable against a loga- rithmic scaling. This corresponds to a logarithmic Sobolev class of functions having an average modulus of continuity in L2(Rd). In that case, for x âˆˆF , we have: Proposition 8.10. If x âˆˆF then âˆ¥U[PJ]xâˆ¥1 < âˆ. This implies that the geometric stability bound from Theorem 8.9 applies to such functions, with an upper bound that does not blow up with depth. When x has compact support, the following corollary shows that the windowed scattering metric is Lipschitz continuous under the action of diï¬€eomorphisms: 354 Bruna: The Scattering Transform Corollary 8.11 (Mallat, 2012, Corollary 2.15). For any compact set â„¦âŠ‚Rd there exists C such that, for all x âˆˆL2(Rd) supported in â„¦with âˆ¥U[PJ]xâˆ¥1 < âˆand for all Ï„ âˆˆC2(Rd) with âˆ¥âˆ‡Ï„âˆ¥âˆâ‰¤1/2, it holds that âˆ¥SJ[PJ,m]xÏ„ âˆ’SJ[PJ,m]xâˆ¥â‰¤Câˆ¥U[PJ]xâˆ¥1  2âˆ’J âˆ¥Ï„âˆ¥âˆ+ âˆ¥âˆ‡Ï„âˆ¥âˆ+ âˆ¥HÏ„âˆ¥âˆ  . (8.18) The translation error term, proportional to 2âˆ’J âˆ¥Ï„âˆ¥âˆ, can be reduced to a second- order error term, 2âˆ’2J âˆ¥Ï„âˆ¥2 âˆ, by considering a ï¬rst-order Taylor approximation of each SJ[p]x (Mallat, 2012). As mentioned earlier, Czaja and Li (2017) and Wiatowski and BÃ¶lcskei (2017) developed extensions of scattering representations by replacing scattering wavelets with other decomposition frames, also establishing deformation stability bounds. However, an important diï¬€erence between these results and Theorem 8.9 is that no bandlimited assumption is made on the input signal x, but rather the weaker condition that âˆ¥U[PJ]xâˆ¥1 < âˆ. For appropriate wavelets leading to exponential energy decay, such a quantity is bounded for x âˆˆL1 âˆ©L2. Finally, another relevant work that connects the above geometric stability results with kernel methods is Bietti and Mairal (2019a), in which a convolutional kernel is constructed that enjoys provable deformation stability. 8.3.4 Algorithms We now describe algorithmic aspects of the scattering representation, in particular the choice of scattering wavelets and the overall implementation as a speciï¬c CNN architecture. Scattering Wavelets The Littlewoodâ€“Paley wavelet transform of x, {x â‹†ÏˆÎ»(u)}Î», deï¬ned in (8.8), is a redundant transform with no orthogonality property. It is stable and invertible if the wavelet ï¬lters Ë†ÏˆÎ»(Ï‰) cover the whole frequency plane. On discrete images, to avoid aliasing, one should capture frequencies only in the circle |Ï‰| â‰¤Ï€ inscribed in the image frequency square. Most camera images have negligible energy outside this frequency circle. As mentioned in Â§8.3.1, one typically considers near-analytic wavelets, meaning that | Ë†Ïˆ(âˆ’Ï‰)| â‰ª| Ë†Ïˆ(Ï‰)| for Ï‰ lying on a predeï¬ned half-space of R2. The reason is hinted at in Theorem 8.4, namely the complex envelope of analytic wavelets is smoother than that of a real wavelet, and therefore more energy will be captured at earlier layers of the scattering representation. Let u Â· uâ€² and |u| denote the inner product and norm in R2. A Morlet wavelet Ïˆ 8.3 Scattering on the Translation Group 355 (a) (b) (c) Figure 8.3 Complex Morlet wavelet. (a) Real part of Ïˆ(u). (b) Imaginary part of Ïˆ(u). (c) Fourier modulus | Ë†Ïˆ(Ï‰)|. is an example of a complex wavelet given by Ïˆ(u) = Î± (eiuÂ·Î¾ âˆ’Î²) eâˆ’|u|2/(2Ïƒ2) , where Î² â‰ª1 is adjusted so that âˆ« Ïˆ(u) du = 0. Its real and imaginary parts are nearly quadrature phase ï¬lters. Figure 8.3 shows the Morlet wavelet, with Ïƒ = 0.85 and Î¾ = 3Ï€/4, as used in all classiï¬cation experiments. The Morlet wavelet Ïˆ shown in Figure 8.3 together with Ï†(u) = exp(âˆ’|u|2/(2Ïƒ2))/(2Ï€Ïƒ2) for Ïƒ = 0.7 satisï¬es (8.9) with Ïµ = 0.25. Cubic spline wavelets constitute an important family of unitary wavelets satis- fying the Littlewoodâ€“Paley condition (8.9) with Ïµ = 0. They are obtained from a cubic-spline orthogonal Battleâ€“LemairÃ© wavelet, deï¬ned from the conjugate mirror ï¬lter (Mallat, 2008) Ë†h(Ï‰) = s S8(Ï‰) 28S8(2Ï‰), with Sn(Ï‰) = âˆ Ã• k=âˆ’âˆ 1 (Ï‰ + 2kÏ€)n , which in the case n = 8 simpliï¬es to the expression S8(2Ï‰) =5 + 30 cos2(Ï‰) + 30 sin2(Ï‰) cos2(Ï‰) 10528 sin8(Ï‰) + 70 cos4(Ï‰) + 2 sin4(Ï‰) cos2(Ï‰) + 2 3 sin6(Ï‰) 105 Ã— 28 sin8(Ï‰) . In two dimensions, Ë†Ïˆ is deï¬ned as a separable product in frequency polar coor- dinates Ï‰ = |Ï‰|Î·, where Î· is a unit vector: for all |Ï‰|, Î· âˆˆR+ Ã— S1, Ë†Ïˆ(Ï‰) = Ë†Ïˆ1(|Ï‰|)Î³(Î·), with Î³ designed such that for all Î·, Ã• r âˆˆG+ |Î³(râˆ’1Î·)|2 = 1 . 356 Bruna: The Scattering Transform Figure 8.4 shows the corresponding two-dimensional ï¬lters obtained with spline wavelets on setting both Ë†Ïˆ1 and Î³ to be cubic splines. (a) (b) (c) Figure 8.4 Complex cubic spline wavelet. (a) Real part of Ïˆ(u). (b) Imaginary part of Ïˆ(u). (c) Fourier modulus | Ë†Ïˆ(Ï‰)|. Fast Scattering Computations with Scattering Convolutional Network A scattering representation is implemented with a CNN having a very speciï¬c architecture. As opposed to standard CNNs, output scattering coeï¬ƒcients are pro- duced by each layer as opposed to just the last layer. Filters are not learned from the data but are predeï¬ned wavelets. If p = (Î»1,. . .,Î»m) is a path of length m then the windowed scattering coeï¬ƒcients SJ[p]x(u) of order m are computed at layer m of a convolution network which is speciï¬ed. We will describe a fast scattering implementation over frequency-decreasing paths, where most of the scattering energy is concentrated. A frequency-decreasing path p = (2âˆ’j1r1,. . .,2âˆ’jmrm) satisï¬es 0 < jk â‰¤jk+1 â‰¤J. If the wavelet transform is computed over K rotation angles then the total number of frequency-decreasing paths of length m is Km  J m . Let N be the number of pixels of the image x. Since Ï†2J is a low-pass ï¬lter scaled by 2J, SJ[p]x(u) = U[p]x â‹†Ï†2J (u) is uniformly sampled at intervals Î±2J, with Î± = 1 or Î± = 1/2. Each SJ[p]x is an image with Î±âˆ’22âˆ’2JN coeï¬ƒcients. The total number of coeï¬ƒcients in a scattering network of maximum depth m is thus P = N Î±âˆ’2 2âˆ’2J m Ã• m=0 Km  J m  . (8.19) If m = 2 then P â‰ƒÎ±âˆ’2 N2âˆ’2JK2J2/2. It decreases exponentially when the scale 2J increases. Algorithm 8.1 describes the computations of scattering coeï¬ƒcients on sets Pm â†“of frequency-decreasing paths of length m â‰¤m. The initial set P0 â†“= {âˆ…} corresponds to the original image U[âˆ…]x = x. Let p + Î» be the path which begins at p and ends at Î» âˆˆP. If Î» = 2âˆ’jr then U[p + Î»]x(u) = |U[p]x â‹†ÏˆÎ»(u)| has energy at 8.3 Scattering on the Translation Group 357 frequencies mostly below 2âˆ’jÏ€. To reduce computations we can thus subsample this convolution at intervals Î±2j, with Î± = 1 or Î± = 1/2 to avoid aliasing. Algorithm 8.1 Fast Scattering Transform for m = 1 to m do for all p âˆˆPmâˆ’1 â†“ do Output SJ[p]x(Î±2Jn) = U[p]x â‹†Ï†2J (Î±2Jn) end for for all p + Î»m âˆˆPm â†“with Î»m = 2âˆ’jmrm do Compute U[p + Î»m]x(Î±2jmn) = |U[p]x â‹†ÏˆÎ»m(Î±2jmn)| end for end for for all p âˆˆPmax â†“ do Output SJ[p]x(Î±2Jn) = U[p]x â‹†Ï†2J (Î±2Jn) end for At layer m there are Km  J m  propagated signals U[p]x with p âˆˆPm â†“. They are sampled at intervals Î±2jm which depend on p. One can verify by induction on m that layer m has a total number of samples equal to Î±âˆ’2 (K/3)m N. There are also Km  J m  scattering signals S[p]x but they are subsampled by 2J and thus have many fewer coeï¬ƒcients. The number of operations to compute each layer is therefore driven by the O((K/3)m N log N) operations needed to compute the internal propagated coeï¬ƒcients using FFTs. For K > 3, the overall computational complexity is thus O((K/3)m N log N). The package Kymatio (Andreux et al., 2018) provides a modern implementation of scattering transforms leveraging eï¬ƒcient GPU-optimized routines. 8.3.5 Empirical Analysis of Scattering Properties To illustrate the properties of scattering representations, let us describe a visualiza- tion procedure. For a ï¬xed position u, windowed scattering coeï¬ƒcients SJ[p]x(u) of order m = 1,2 are displayed as piecewise constant images over a disk representing the Fourier support of the image x. This frequency disk is partitioned into sectors {â„¦[p]}pâˆˆPm indexed by the path p. The image value is SJ[p]x(u) on the frequency sectors â„¦[p], as shown in Figure 8.5. For m = 1, the scattering coeï¬ƒcient SJ[Î»1]x(u) depends upon the local Fourier transform energy of x over the support of Ë†ÏˆÎ»1. Its value is displayed over a sector â„¦[Î»1] which approximates the frequency support of Ë†ÏˆÎ»1. For Î»1 = 2âˆ’j1r1, there are 358 Bruna: The Scattering Transform â„¦[Î»1] â„¦[Î»1;Î»2] (a) (b) Figure 8.5 To display scattering coeï¬ƒcients, the disk covering the image frequency support is partitioned into sectors â„¦[p], which depend upon the path p. (a): For m = 1, each â„¦[Î»1] is a sector rotated by r1 which approximates the frequency support of Ë†ÏˆÎ»1. (b): For m = 2, all â„¦[Î»1, Î»2] are obtained by subdividing each â„¦[Î»1]. K rotated sectors located in an annulus of scale 2âˆ’j1, corresponding to each r1 âˆˆG, as shown by Figure 8.5(a). Their areas are proportional to âˆ¥ÏˆÎ»1âˆ¥2 âˆ¼Kâˆ’1 2âˆ’j1. The second-order scattering coeï¬ƒcients SJ[Î»1,Î»2]x(u) are computed with a sec- ond wavelet transform, which performs a second frequency subdivision. These coeï¬ƒcients are displayed over frequency sectors â„¦[Î»1,Î»2] which subdivide the sec- tors â„¦[Î»1] of the ï¬rst wavelets Ë†ÏˆÎ»1, as illustrated in Figure 8.5(b). For Î»2 = 2âˆ’j2r2, the scale 2j2 divides the radial axis and the resulting sectors are subdivided into K angular sectors corresponding to the diï¬€erent values of r2. The scale and angu- lar subdivisions are adjusted so that the area of each â„¦[Î»1,Î»2] is proportional to âˆ¥|ÏˆÎ»1| â‹†ÏˆÎ»2âˆ¥2. A windowed scattering SJ is computed with a cascade of the wavelet modulus operators U deï¬ned in (8.11), and its properties thus depend upon the wavelet transform properties. In Â§Â§8.3.3 and 8.3.2 conditions were given for wavelets to de- ï¬ne a scattering transform that is non-expansive and preserves the signal norm. The scattering energy conservation shows that âˆ¥SJ[p]xâˆ¥decreases quickly as the length of p increases, and is non-negligible only over a particular subset of frequency- decreasing paths. Reducing computations to these paths deï¬nes a convolution net- work with many fewer internal and output coeï¬ƒcients. Theorem 8.4 shows that the energy captured by the mth layer of the scattering convolutional network, Ã |p|=m âˆ¥SJ[p]xâˆ¥2, converges to 0 as m â†’âˆ. This scatter- ing energy conservation also proves that the more sparse the wavelet coeï¬ƒcients, the more energy propagates to deeper layers. Indeed, when 2J increases, one can verify that, at the ï¬rst layer, SJ[Î»1]x = |x â‹†ÏˆÎ»1| â‹†Ï†2J converges to âˆ¥Ï†âˆ¥2 âˆ¥x â‹†ÏˆÎ»âˆ¥2 1. The more sparse xâ‹†ÏˆÎ», the smaller âˆ¥xâ‹†ÏˆÎ»âˆ¥1 and hence the more energy is propagated to deeper layers to satisfy the global energy conservation of Theorem 8.4. Figure 8.6 shows two images having same ï¬rst-order scattering coeï¬ƒcients, but 8.3 Scattering on the Translation Group 359 (a) (b) (c) (d) Figure 8.6 (a) Two images x(u). (b) Fourier modulus | Ë†x(Ï‰)|. (c) First-order scattering coef- ï¬cients SJ x[Î»1] displayed over the frequency sectors of Figure 8.5(a). They are the same for both images. (d) Second-order scattering coeï¬ƒcients SJ x[Î»1, Î»2] over the frequency sectors of Figure 8.5(b). They are diï¬€erent for each image. the top image is piecewise regular and hence has wavelet coeï¬ƒcients which are much more sparse than those of the uniform texture at the bottom. As a result the top image has second-order scattering coeï¬ƒcients of larger amplitude than those of the bottom image. Higher-order coeï¬ƒcients are not displayed because they have negligible energy. For typical images, as in the CalTech101 dataset (Fei-Fei et al., 2004), Table 8.1 shows that the scattering energy has an exponential decay as a function of the path length m. The scattering coeï¬ƒcients are computed with cubic spline wavelets, which deï¬ne a unitary wavelet transform and satisfy the scattering admissibility condition for energy conservation. As expected, the energy of the scattering coeï¬ƒcients converges to 0 as m increases, and it is already below 1% for m â‰¥3. The propagated energy âˆ¥U[p]xâˆ¥2 decays because U[p]x is a progressively lower- frequency signal as the path length increases. Indeed, each modulus computes a regular envelope of oscillating wavelet coeï¬ƒcients. The modulus can thus be interpreted as a non-linear â€˜demodulatorâ€™ which pushes the wavelet coeï¬ƒcient energy towards lower frequencies. As a result, an important portion of the energy of U[p]x is then captured by the low-pass ï¬lter Ï†2J , which outputs SJ[p]x = U[p]x â‹†Ï†2J . Hence less energy is propagated to the next layer. Another result is that the scattering energy propagates only along a subset of frequency-decreasing paths. Since the envelope |x â‹†ÏˆÎ»| is more regular than x â‹† ÏˆÎ», it follows that |x â‹†ÏˆÎ»(u)| â‹†ÏˆÎ»â€² is non-negligible only if ÏˆÎ»â€² is located at lower frequencies than ÏˆÎ» and, hence, if |Î»â€²| < |Î»|. Iterating on wavelet modulus 360 Bruna: The Scattering Transform Table 8.1 Percentage of energy Ã pâˆˆPm â†“âˆ¥SJ[p]xâˆ¥2/âˆ¥xâˆ¥2 of scattering coeï¬ƒcients on frequency-decreasing paths of length m, with dependence upon J. These average values are computed on the Caltech-101 database, with zero mean and unit variance images. J m = 0 m = 1 m = 2 m = 3 m = 4 m â‰¤3 1 95.1 4.86 â€“ â€“ â€“ 99.96 2 87.56 11.97 0.35 â€“ â€“ 99.89 3 76.29 21.92 1.54 0.02 â€“ 99.78 4 61.52 33.87 4.05 0.16 0 99.61 5 44.6 45.26 8.9 0.61 0.01 99.37 6 26.15 57.02 14.4 1.54 0.07 99.1 7 0 73.37 21.98 3.56 0.25 98.91 operators thus propagates the scattering energy along frequency-decreasing paths p = (Î»1,. . .,Î»m) where |Î»k| < |Î»kâˆ’1| for 1 â‰¤k < m. We denote by Pm â†“the set of frequency-decreasing (or equivalently scale-increasing) paths of length m. The scattering coeï¬ƒcients along other paths have a negligible energy. This is veriï¬ed by Table 8.1, which shows not only that the scattering energy is concentrated on low-order paths, but also that more than 99% of the energy is absorbed by frequency-decreasing paths of length m â‰¤3. Numerically, it is therefore suï¬ƒcient to compute the scattering transform along frequency-decreasing paths. It deï¬nes a much smaller convolution network. In Â§8.3.4 we will show that the resulting coeï¬ƒcients are computed with O(N log N) operations. Signal recovery versus energy conservation. Preserving energy does not imply that the signal information is preserved. Since a scattering transform is calculated by iteratively applying U, inverting SJ requires inverting U. The wavelet transform W is a linear invertible operator, so inverting Uz = {zâ‹†Ï†2J, |zâ‹†ÏˆÎ»|}Î»âˆˆP amounts to recovering the complex phases of wavelet coeï¬ƒcients removed by the modulus. The phases of the Fourier coeï¬ƒcients cannot be recovered from their moduli, but wavelet coeï¬ƒcients are redundant, as opposed to Fourier coeï¬ƒcients. For particular wavelets, it has been proved that the phases of the wavelet coeï¬ƒcients can indeed be recovered from their moduli, and that U has a continuous inverse (Waldspurger, 2012). Still, one cannot invert SJ exactly because we discard information when comput- ing the scattering coeï¬ƒcients SJ[p]x = U[p]xâ‹†Ï†2J of the last layer Pm. Indeed, the propagated coeï¬ƒcients |U[p]x â‹†ÏˆÎ»| of the next layer are eliminated, because they are not invariant and have negligible total energy. The number of such coeï¬ƒcients 8.3 Scattering on the Translation Group 361 Figure 8.7 Signal reconstruction from scattering coeï¬ƒcients SJx with J = log N. Top: original images. Middle: reconstruction from only ï¬rst-order coeï¬ƒcients. Bottom: reconstruction using ï¬rst- and second-order coeï¬ƒcients. is larger than the total number of scattering coeï¬ƒcients kept at previous layers. Initializing the inversion by setting these small coeï¬ƒcients to zero produces an error. This error is further ampliï¬ed as the inversion of U progresses across layers from m to 0. Yet, under some structural assumptions on the signal x, it is possible to recover the signal from its scattering coeï¬ƒcients z = SJx. For instance, if x admits a sparse wavelet decomposition, Bruna and Mallat (2018) showed that important geometrical information of x is preserved in SJx. Figure 8.7 illustrates signal recovery using either m = 1 or m = 2 with J = log N. The recovery is obtained using gradient descent on the energy E(x) = âˆ¥SJx âˆ’zâˆ¥2 as will be described in Â§8.6. In this case, ï¬rst-order scattering provides a collection of â„“1 norms {âˆ¥x â‹†ÏˆÎ»âˆ¥1}Î», which recover the overall regularity of the signal but fail to reconstruct its geometry. Adding second-order coeï¬ƒcients results in O(log N2) coeï¬ƒcients and substantially improves the reconstruction quality. In essence, the sparsity in these images creates no scale interactions on a large subset of scattering coeï¬ƒcients, which reduces the loss of information caused by the removal of the wavelet phases. For natural images with weaker sparsity, Figure 8.8 shows reconstructions from second-order scattering coeï¬ƒcients for diï¬€erent values of J, using the same recov- ery algorithm. When the scale 2J is such that the number of scattering coeï¬ƒcients is comparable with the dimensionality of x, we observe good perceptual quality. When dim(SJx) â‰ªdim(x), the scattering coeï¬ƒcients deï¬ne an underlying gen- erative model based on a microcanonical maximum entropy principle, as will be described in Â§8.6. 362 Bruna: The Scattering Transform Figure 8.8 Samples from â„¦J ,Ïµ for diï¬€erent values of J using the gradient descent algorithm described in Â§8.6.3. Top row: original images. Second row: J = 3. Third row: J = 4. Fourth row: J = 5. Fifth row: J = 6. The visual quality of the reconstruction is nearly perfect for J = 3 and degrades for larger values of J. 8.3.6 Scattering in Modern Computer Vision Thanks to their provable deformation stability and ability to preserve important geometric information, scattering representations are suitable as feature extractors in many computer vision pipelines. First demonstrated in Bruna and Mallat (2013) on handwritten digit classiï¬- cation and texture recognition, scattering-based image classiï¬cation models have been further developed in Oyallon and Mallat (2015), Oyallon et al. (2017), and Oyallon et al. (2018b), by extending the wavelet decomposition to other transfor- mation groups (see Â§8.5) and by integrating them within CNN architectures as preprocessing stages. In particular, the results from Oyallon et al. (2018b) demonstrate that the geomet- ric priors of scattering representations provide a better trade-oï¬€than data-driven models in the small-training regime, where large-capacity CNNs tend to overï¬t. 8.4 Scattering Representations of Stochastic Processes 363 Even ï¬rst-order scattering coeï¬ƒcients may be used to ease inference and learning within CNN pipelines, as demonstrated in Oyallon et al. (2018a). Also, let us mention models that are hybrids between fully structured scattering networks and fully trainable CNNs. Jacobsen et al. (2016) proposed the learning of convolutional ï¬lters in the wavelet domain, leveraging the beneï¬ts of multiscale decompositions. Cohen and Welling (2016a,b) and Kondor and Trivedi (2018) added group convolution of the joint scattering representation of Â§8.5 to the CNNs, signiï¬cantly improving the sample complexity. Finally, Zarka et al. (2020) achieved Res-Net performance on Imagenet by combining scattering operators with learned channel-wise 1 Ã— 1 convolutions. 8.4 Scattering Representations of Stochastic Processes This section reviews the deï¬nitions and basic properties of the expected scattering of random processes (Mallat, 2012; Bruna et al., 2015a). First, we prove a version of scattering mean-squared consistency for orthogonal Haar scattering in Â§8.4.1. 8.4.1 Expected Scattering If (X(t))t âˆˆR is a stationary process or has stationary increments, meaning that Î´sX(t) = X(t) âˆ’X(t âˆ’s) is stationary for all s, then X â‹†ÏˆÎ» is also stationary, and taking the modulus preserves stationarity. It follows that, for any path p = (Î»1,. . .,Î»m) âˆˆPâˆ, the process U[p]X = | Â· Â· Â· |X â‹†ÏˆÎ»1| â‹†Â· Â· Â· | â‹†ÏˆÎ»m| is stationary, hence its expected value does not depend upon the spatial position t. Deï¬nition 8.12. Let X(t) be a stochastic process with stationary increments. The expected scattering of X is deï¬ned for all p âˆˆPâˆby SX(p) = E(U[p]X) = E(| Â· Â· Â· |X â‹†ÏˆÎ»1| â‹†Â· Â· Â· | â‹†ÏˆÎ»m|) . The expected scattering deï¬nes a representation for the process X(t) which carries information on high-order moments of X(t), as we shall see later. It also deï¬nes a metric between stationary processes, given by âˆ¥SX âˆ’SY âˆ¥2 := Ã• pâˆˆPâˆ |SX(p) âˆ’SY(p)|2 . The scattering representation of X(t) is estimated by computing a windowed scattering transform of a realization x of X(t). If Î›J = {Î» = 2j : 2âˆ’j > 2âˆ’J} denotes the set of scales smaller than J, and PJ is the set of ï¬nite paths p = (Î»1,. . .,Î»m) 364 Bruna: The Scattering Transform with Î»k âˆˆÎ›J for all k, then the windowed scattering at scale J of a realization x(t) is SJ[PJ]x = {U[p]x â‹†Ï†J , p âˆˆPJ} . (8.20) Since âˆ« Ï†J(u) du = 1, we have E(SJ[PJ]X) = E(U[p]X) = SX(p), so SJ is an unbiased estimator of the scattering coeï¬ƒcients contained in PJ. When the wavelet Ïˆ satisï¬es the Littlewoodâ€“Paley condition (8.9), the non-expansive nature of the operators deï¬ning the scattering transform implies that S and SJ[PJ] are also non-expansive, similarly to the deterministic case covered in Proposition 8.3: Proposition 8.13. If X and Y are ï¬nite second-order stationary processes then E(âˆ¥SJ[PJ]X âˆ’SJ[PJ]Y âˆ¥2) â‰¤E(|X âˆ’Y|2), (8.21) âˆ¥SX âˆ’SY âˆ¥2 â‰¤E(|X âˆ’Y|2), (8.22) in particular âˆ¥SXâˆ¥2 â‰¤E(|X|2) . (8.23) The L2(Rd) energy conservation theorem, 8.4, yields an equivalent energy con- servation property for the mean-squared power: Theorem 8.14 (Waldspurger, 2017, Theorem 5.1). Under the same assumptions on scattering wavelets as in Theorem 8.4, and if X is stationary, then E(âˆ¥SJ[PJ]Xâˆ¥2) = E(|X|2) . (8.24) The expected scattering coeï¬ƒcients are estimated with the windowed scattering SJ[p]X = U[p]X â‹†ÏˆJ for each p âˆˆPJ. If U[p]X is ergodic, SJ[p]X converges in probability to SX(p) = E(U[p]X) when J â†’âˆ. A process X(t) with stationary increments is said to have a mean squared consistent scattering if the total variance of SJ[PJ]X converges to zero as J increases: lim Jâ†’âˆE(âˆ¥SJ[PJ]X âˆ’SXâˆ¥2) = Ã• pâˆˆPJ E(|SJ[p]X âˆ’SX(p)|2) = 0 . (8.25) This condition implies that SJ[PJ]X converges to SX with probability 1. Mean- squares consistent scattering is observed numerically on a variety of processes, including Gaussian and non-Gaussian fractal processes. It was conjectured in Mallat (2012) that Gaussian stationary processes X whose autocorrelation RX is in L1 have a mean squared consistent scattering. Consistency of orthogonal Haar scattering. We show a partial aï¬ƒrmative answer of the above conjecture, by considering a speciï¬c scattering representation built from discrete orthogonal real Haar wavelets. Consider (Xn)nâˆˆZ, a stationary process 8.4 Scattering Representations of Stochastic Processes 365 deï¬ned over discrete time steps. The orthogonal Haar scattering transform SH J maps 2J samples of Xn into 2J coeï¬ƒcients, deï¬ned recursively as x0,k = Xk, k = 0,. . .,2J âˆ’1, x j,k = 1 2(x jâˆ’1,2k + x jâˆ’1,2k+1), x j,k+2Jâˆ’j = 1 2|x jâˆ’1,2k âˆ’x jâˆ’1,2k+1| , 0 < j â‰¤J, k = 0,. . .,2Jâˆ’1 âˆ’1, SH J X := (xJ,k; k = 0,. . .,2J âˆ’1) . (8.26) This representation thus follows a multiresolution analysis (MRA) (Mallat, 1999) but also decomposes the details at each scale, after applying the modulus non- linearity. It is easy to verify by induction that (8.26) deï¬nes an orthogonal trans- formation that preserves energy: âˆ¥SH J xâˆ¥= âˆ¥xâˆ¥. However, in contrast with the Littlewoodâ€“Paley wavelet decomposition, orthogonal wavelets are deï¬ned from downsampling operators, and therefore the resulting scattering representation SH J is not translation invariant when J â†’âˆ. We have the following consistency result: Theorem 8.15 (Bruna, 2019). The progressive Haar Scattering operator SH J is consistent in the class of compactly supported linear processes, in the sense that lim Jâ†’âˆE(âˆ¥SH J X âˆ’ESH J Xâˆ¥2) = 0 , (8.27) for stationary processes X which can be represented as X = W â‹†h, where W is a white noise and h is compactly supported. As a consequence of Theorem 8.14, mean-squared consistency implies an ex- pected scattering energy conservation: Corollary 8.16. For admissible wavelets as in Theorem 8.14, SJ[PJ]X is mean- squared consistent if and only if âˆ¥SXâˆ¥2 = E(|X|2) . The expected scattering coeï¬ƒcients depend upon normalized high-order mo- ments of X. If one expresses |U[p]X|2 as |U[p]X(t)|2 = E(|U[p]X|2)(1 + Ïµ(t)) , then, assuming |Ïµ| â‰ª1, a ï¬rst-order approximation U[p]X(t) = q |U[p]X(t)|2 â‰ˆE(|U[p]X|2)1/2(1 + Ïµ/2) yields U[p + Î»]X = |U[p]X â‹†ÏˆÎ»| â‰ˆ||U[p]X|2 â‹†ÏˆÎ»| 2E(|U[p]X|2)1/2 , 366 Bruna: The Scattering Transform thus showing that SX(p) = E(U[p]X) for p = (Î»1,. . .,Î»m) depends upon normal- ized moments of X of order 2m, determined by the cascade of wavelet sub-bands Î»k. As opposed to a direct estimation of high moments, scattering coeï¬ƒcients are computed with a non-expansive operator which allows consistent estimation with few realizations. This is a fundamental property which enables texture recognition and classiï¬cation from scattering representations (Bruna, 2013). The scattering representation is related to the sparsity of the process through the decay of its coeï¬ƒcients SX(p) as the order |p| increases. Indeed, the ratio of the ï¬rst two moments of X, ÏX = E(|X|) E(|X|2)1/2 gives a rough measure of the fatness of the tails of X. For each p, the Littlewoodâ€“Paley unitarity condition satisï¬ed by Ïˆ gives E(|U[p]X|2) = E(U[p]X)2 + Ã• Î» E(|U[p + Î»]X|2) , which yields 1 = ÏU[p]X + 1 E(|U[p]X|2) Ã• Î» E(|U[p + Î»]X|2) . (8.28) Thus, the fraction of energy that is trapped at a given path p is given by the relative sparsity ÏU[p]X. This relationship between sparsity and scattering decay across the orders is of particular importance for the study of point processes, which are sparse in the original spatial domain, and for regular image textures, which are sparse when decomposed in the ï¬rst level UX of the transform. In particular, the scattering transform can easily discriminate between white noises of diï¬€erent sparsity, such as Bernouilli and Gaussian. The autocovariance of a real stationary process X is denoted RX(Ï„) = E  X(x) âˆ’E(X)  X(x âˆ’Ï„) âˆ’E(X) . Its Fourier transform bRX(Ï‰) is the power spectrum of X. Replacing X by X â‹†ÏˆÎ» in the energy conservation formula (8.24) implies that Ã• pâˆˆPJ E(|SJ[p + Î»]X|2) = E(|X â‹†ÏˆÎ»|2) . (8.29) These expected squared wavelet coeï¬ƒcients can also be written as a ï¬ltered inte- gration of the Fourier power spectrum bRX(Ï‰): E(|X â‹†ÏˆÎ»|2) = âˆ« bRX(Ï‰) | Ë†Ïˆ(Î»âˆ’1Ï‰)|2 dÏ‰ . (8.30) 8.4 Scattering Representations of Stochastic Processes 367 These two equations prove that summing the scattering coeï¬ƒcients recovers the power spectrum integral over each waveletâ€™s frequency support, which depends only upon second-order moments of X. However, the scattering coeï¬ƒcients SX(p) depend upon moments of X up to the order 2m if p has length m. Scattering coeï¬ƒcients can thus discriminate textures having same second-order moments but diï¬€erent higher-order moments. 8.4.2 Analysis of Stationary Textures with Scattering In Â§8.4.1 we showed that the scattering representation can be used to describe stationary processes, in such a way that high-order moment information is captured and estimated consistently with few realizations. Image textures can be modeled as realizations of stationary processes X(u). The Fourier spectrum bRX(Ï‰) is the Fourier transform of the autocorrelation RX(Ï„) = E [X(u) âˆ’E(X)][X(u âˆ’Ï„) âˆ’E(X)] . Despite the importance of spectral methods, the Fourier spectrum is often not suï¬ƒcient to discriminate image textures because it does not take into account higher-order moments. (a) (b) (c) (d) Figure 8.9 Two diï¬€erent textures having the same Fourier power spectrum. (a) Textures X(u). Upper, Brodatz texture; lower, Gaussian process. (b) Same estimated power spectrum bRX(Ï‰). (c) Nearly the same scattering coeï¬ƒcients SJ[p]X for m = 1 and 2J equal to the image width. (d) Diï¬€erent scattering coeï¬ƒcients SJ[p]X for m = 2. The discriminative power of scattering representations is illustrated using the two textures in Figure 8.9, which have the same power spectrum and hence the same second-order moments. The scattering coeï¬ƒcients SJ[p]X are shown for m = 1 and 368 Bruna: The Scattering Transform m = 2 with the frequency tiling illustrated in Figure 8.5. The ability to discriminate the upper process X1 from the lower, X2, is measured by the scattering distance normalized by the variance: Ï(m) = âˆ¥SJ X1[Î›m J ] âˆ’E(SJ X2[Î›m J ])âˆ¥2 E(âˆ¥SJ X2[Î›m J ] âˆ’E(SJ X2[Î›m J ])âˆ¥2) . For m = 1, the scattering coeï¬ƒcients mostly depend upon second-order moments and are thus nearly equal for both textures. One can indeed verify numerically that Ï(1) = 1 so the textures cannot be distinguished using ï¬rst-order scattering coeï¬ƒ- cients. In contrast, scattering coeï¬ƒcients of order 2 are highly dissimilar because they depend on moments up to order 4, and Ï(2) = 5. A scattering representation of stationary processes includes second- and higher-order moment descriptors of stationary processes, which discriminates between such textures. The windowed scattering SJ[PJ]X estimates scattering coeï¬ƒcients by averaging wavelet modulus over a support of size proportional to 2J. If X is a stationary process, we saw in Â§8.4.1 that the expected scattering transform SX is estimated using the windowed scattering SJ[PJ]X = {U[p]X â‹†Ï†J , p âˆˆPJ} . This estimate is called mean-squared consistent if its total variance over all paths converges: lim Jâ†’âˆ Ã• pâˆˆPJ E(|SJ[p]X âˆ’SX(p)|2) = 0 . Corollary 8.16 showed that mean-squared consistency is equivalent to E(|X|2) = Ã• pâˆˆPâˆ |SX(p)|2 , which in turn is equivalent to lim mâ†’âˆ Ã• pâˆˆPâˆ, |p|=m E(|U[p]X|2) = 0 . (8.31) If a process X(t) has a mean-squared consistent scattering, then one can recover the scaling law of its second moments with scattering coeï¬ƒcients: Proposition 8.17. Suppose that X(t) is a process with stationary increments such that SJ X is mean-squared consistent. Then E(|X â‹†Ïˆj|2) = Ã• pâˆˆPâˆ |SX(j + p)|2 . (8.32) 8.4 Scattering Representations of Stochastic Processes 369 Table 8.2 Percentage decay of the total scattering variance Ã pâˆˆPJ E(|SJ[p]X âˆ’SX(p)|2)/E(|X|2) as a function of J, averaged over the Brodatz dataset. Results obtained using cubic spline wavelets. J = 1 J = 2 J = 3 J = 4 J = 5 J = 6 J = 7 85 65 45 26 14 7 2.5 Table 8.3 Percentage of expected scattering energy Ã pâˆˆÎ›m âˆ|SX(p)|2, as a function of the scattering order m, computed with cubic spline wavelets, over the Brodatz dataset. m = 0 m = 1 m = 2 m = 3 m = 4 0 74 19 3 0.3 For a large class of ergodic processes, including most image textures, it is ob- served numerically that the total scattering variance Ã pâˆˆPJ E(|SJ[p]X âˆ’SX(p)|2) decreases to zero when 2J increases. Table 8.2 shows the decay of the total scattering variance, computed on average over the Brodatz texture dataset. Corollary 8.16 showed that this variance decay then implies that âˆ¥SXâˆ¥2 = âˆ Ã• m=0 Ã• pâˆˆÎ›m âˆ |SX(p)|2 = E(|X|2) . Table 8.3 gives the percentage of the expected scattering energy Ã pâˆˆÎ›m âˆ|SX(p)|2 carried by paths of length m, for textures in the Brodatz database. Most of the energy is concentrated in paths of length m â‰¤3. 8.4.3 Multifractal Analysis with Scattering Moments Many physical phenomena exhibit irregularities at all scales, as illustrated by the canonical example of turbulent ï¬‚ow or Brownian motion. Fractals are mathematical models of stochastic processes that express such a property through scale-invariance symmetries of the form for all s > 0, {X(st): t âˆˆR} d= As Â· {X(t): t âˆˆR} . (8.33) In other words, the law of stochastic processes is invariant under time dilation, up to a scale factor. Here As denotes a random variable independent of X that controls 370 Bruna: The Scattering Transform the strength of the irregularity of sample trajectories of X(t). Fractional Brownian motions are the only Gaussian processes satisfying (8.33) with As := sH, where H = 0.5 corresponds to the standard Wiener process. Fractals can be studied from wavelet coeï¬ƒcients through the distribution of point-wise HÃ¶lder exponents (Doukhan et al., 2002). Moments of order q deï¬ne a scaling exponent Î¶(q) such that E[|X â‹†Ïˆj|q] â‰ƒ2jÎ¶(q) , as j â†’âˆ’âˆ. This characteristic exponent provides rich information about the process; in par- ticular, the curvature of Î¶(q) measures the presence of diï¬€erent HÃ¶lder exponents within a realization and can be interpreted as a measure of intermittency. Intermit- tency is an ill-deï¬ned mathematical notion that is used in physics to describe those irregular bursts of large amplitude variations appearing for example in turbulent ï¬‚ows (Yoshimatsu et al., 2011). Multiscale intermittency appears in other domains such as network traï¬ƒc, ï¬nancial time series, and geophysical and medical data. Intermittency is created by heavy-tail processes, such as LÃ©vy processes. It pro- duces large, if not inï¬nite, polynomial moments of degree greater than 2, and em- pirical estimations of second-order moments have a large variance. These statistical instabilities can be reduced by calculating the expected values of non-expansive op- erators in mean-squared norm, which reduces the variance of empirical estimation. Scattering moments are computed with such a non-expansive operator. In Bruna et al. (2015a), it was shown that second-order scattering moments pro- vide robust estimation of such intermittency through the following renormalisation scheme. In the univariate case, we consider, for each j, j1, j2 âˆˆZ, ËœSX(j) := E[|X â‹†Ïˆj|] E[|X â‹†Ïˆ0|] , ËœSX(j1, j2) = E[||X â‹†Ïˆj1| â‹†Ïˆj2|] E[|X â‹†Ïˆj1|] . (8.34) This renormalized scattering can be estimated by substituting into both numerator and denominator the windowed scattering estimators (8.20). These renormalized scattering moments capture both self-similarity and intermittence, as illustrated by the following result. Proposition 8.18 (Bruna et al., 2015a, Proposition 3.1). Let X(t) be a self-similar process (8.33) with stationary increments. Then, for all j1 âˆˆZ, ËœSX(j1) = 2j1H , (8.35) and, for all (j1, j2) âˆˆZ2, ËœSX(j1, j2) = S ËœX(j2 âˆ’j1) with ËœX(t) = |X â‹†Ïˆ(t)| E(|X â‹†Ïˆ|) . (8.36) 8.5 Non-Euclidean Scattering 371 Moreover, the discrete curvature Î¶(2) âˆ’2Î¶(1) satisï¬es 2j(Î¶(2)âˆ’2Î¶(1)) â‰ƒE(|X â‹†Ïˆj|2) E(|X â‹†Ïˆj|)2 â‰¥1 + +âˆ Ã• j2=âˆ’âˆ | ËœSX(j, j2)|2 . (8.37) This proposition illustrates that second-order scattering coeï¬ƒcients ËœSX(j1, j2) of self-similar processes are only functions of the diï¬€erence j2 âˆ’j1, which can be interpreted as a stationarity property across scales. Moreover, it follows from (8.37) that if Ã+âˆ j2=âˆ’âˆËœSX(j, j2)2 â‰ƒ2jÎ² as j â†’âˆ’âˆwith Î² < 0, then Î¶(2) âˆ’2Î¶(1) < 0. Therefore, the decay of ËœSX(j, j + l) with l (or absence thereof) captures a rough measure of intermittency. Figure 8.10 illustrates the behavior of normalized scattering coeï¬ƒcients for three representative processes: Poisson point processes, fractional Brownian motion, and a Mandelbrot cascade. The asymptotic decay of the scattering moments clearly distinguishes the diï¬€erent intermittent behaviors. Bruna et al. (2015a) explore the applications of such scattering moments to perform model selection in real-world applications, such as turbulent ï¬‚ows and ï¬nancial time series. 0 2 4 6 8 10 12 x 10 4 âˆ’200 0 200 400 600 0 2 4 6 8 10 12 x 0 10 20 30 40 50 0 2 4 6 8 10 12 x 10 4 âˆ’6 âˆ’4 âˆ’2 0 2x 10 4 0 2 4 6 8 10 12 x 10 4 0 50 100 150 200 0 5 10 0 5 0 0 5 10 15 8 6 4 2 0 2 4 6 8 10 12 4 3 2 1 0 2 4 6 8 10 1 3 5 2 5 1 0.0 0.0 0.1 Figure 8.10 Upper row: Realizations of a Brownian motion, a Poisson point process, a LÃ©vy process and a multifractal random cascade. Lower row: the corresponding normalized second- order coeï¬ƒcients. 8.5 Non-Euclidean Scattering Scattering representations deï¬ned over the translation group are extended to other global transformation groups by deï¬ning Littlewoodâ€“Paley wavelet decompositions on non-Euclidean domains with group convolutions. Wavelet decompositions can also be deï¬ned on domains lacking global symmetries, such as graphs and mani- folds. In this section we present this formalism and discuss several applications. 372 Bruna: The Scattering Transform 8.5.1 Joint versus Separable Scattering Let us consider the question of building a signal representation Î¦(x) that is invariant under the action of a certain transformation group G acting on L2(Rd): G Ã— L2(Rd) â†’L2(Rd), (g,x) 7â†’xg . The signal representation Î¦ is G-invariant if Î¦(xg) = Î¦(x) for all g âˆˆG, and G-equivariant if Î¦(xg) = (Î¦(x))g; that is, G acts on the image of Î¦ respecting the axioms of a group action. Now, suppose that the group G admits a factorization as a semidirect product of two subgroups G1,G2: G = G1 â‹ŠG2 . This means that G1 is a normal subgroup of G and that each element g âˆˆG can be uniquely written as g = g1g2, with gi âˆˆGi. It is thus tempting to leverage group factorizations to build invariants to complex groups by combining simpler invariants and equivariants as building blocks. Suppose that Î¦1 is G1-invariant and G2-equivariant, and Î¦2 is G2-invariant. Then Â¯Î¦ := Î¦2 â—¦Î¦1 satisï¬es, for all (g1,g2) âˆˆG1 â‹ŠG2, Â¯Î¦(xg1g2) = Î¦2((Î¦1(x))g2) = Î¦2(Î¦1(x)) = Â¯Î¦(x) , showing that we can eï¬€ectively build larger invariants by composing simpler in- variants and equivariants. However, such a compositional approach comes with a loss of discriminative power (Sifre and Mallat, 2013). Indeed, whereas a group can be factorized into smaller groups, the group action that acts on the data is seldom separable, as illustrated in Figure 8.11. In the case of images x âˆˆL2(R2), an important example comes from the action of general aï¬ƒne transformations of R2. This motivates the construction of joint scattering representations in the roto-translation group, to be discussed next. 8.5.2 Scattering on Global Symmetry Groups We illustrate the ideas from Â§8.5.1 with the construction of a scattering represen- tation over the roto-translation group for images, as developed in Sifre and Mallat (2013) and Oyallon and Mallat (2015), for the Heisenberg group of frequency trans- positions (AndÃ©n and Mallat, 2014; AndÃ©n et al., 2018), and for SO(3) in quantum chemistry (Hirn et al., 2017; Eickenberg et al., 2017). In essence, these representa- tions adapt the construction in Â§8.3 by deï¬ning appropriate wavelet decompositions over the roto-translation group. 8.5 Non-Euclidean Scattering 373 Figure 8.11 From Sifre and Mallat (2013). The left and right textures are not discriminated by a separable invariant along rotations and translations, but can be discriminated by a joint invariant. Roto-translation group. The roto-translation group is formed by pairs g = (v,Î±) âˆˆ R2 Ã— SO(2) acting on u âˆˆâ„¦as follows: (g,u) 7â†’g Â· u := v + RÎ±u, where RÎ± is a rotation of the plane by an angle Î±. One can easily verify that the set of all pairs (v,Î±) forms a group GRot â‰ƒR2 â‹ŠSO(2), with group multiplication deï¬ned as (v1,Î±1) Â· (v2,Î±2) = (v1 + RÎ±1v2,Î±1 + Î±2) . The group acts on images x(u) by the usual composition: xg := x(gâˆ’1 Â· u). Wavelet decompositions over a compact group are obtained from group convo- lutions, deï¬ned as weighted averages over the group. Speciï¬cally, if Ëœx âˆˆL2(G) and h âˆˆL1(G), the group convolution of x with the ï¬lter h is Ëœx â‹†G h(g) := âˆ« G xgh(gâˆ’1) dÂµ(g) . (8.38) Here Âµ is the uniform Haar measure over G. One can immediately verify that group convolutions are the only linear operators which are equivariant with respect to the group action: Ëœxgâ€² â‹†G h(g) = Ëœx â‹†G h((gâ€²)âˆ’1 Â· g) for all g,gâ€² âˆˆG. Given an input x(u), u âˆˆâ„¦âŠ‚R2, we consider ï¬rst a wavelet decomposition over the translation group W1 = {Ïˆj,Î¸}Î¸ âˆˆSO(2),jâˆˆZ, with the dilations and rotations of a given mother wavelet. The corresponding propagated wavelet modulus coeï¬ƒcients become U1(x)(p1) = |x â‹†Ïˆj1,Î¸1|(u), with p1 := (u, j1,Î¸1) . The vector of the coeï¬ƒcients is equivariant with respect to translations, since it is deï¬ned through spatial convolutions and pointwise non-linearities. We can verify that it is also equivariant with respect to rotations, since U1(rÎ±x)(u, j1,Î¸1) = U1(x)(râˆ’Î±u, j1,Î¸1 âˆ’Î±) . 374 Bruna: The Scattering Transform Î¸ u1 u2 ÏˆÎ¸2,j2(u1,u2) Ïˆk2(Î¸) Figure 8.12 From Sifre and Mallat (2013). A wavelet deï¬ned on the roto-translation group, displayed in the 3D domain deï¬ned by positions u1, u2 and angles Î¸. In summary, the ï¬rst layer U1 is GRot-equivariant, U1(xg) = [U1(x)]g, with group action on the coeï¬ƒcients g Â· p1 = (g Â· u, j1,Î¸1 âˆ’Î±), for g = (v,Î±) âˆˆGRot. While the original scattering operator from Â§8.3 would now propagate each sub- band of U1x independently using the same wavelet decomposition operator, roto- translation scattering now considers a joint wavelet decomposition W2 deï¬ned over functions of GRot. Speciï¬cally, W2 = {Î¨Î³}Î³ is a collection of wavelets deï¬ned in L1(GRot). In Sifre and Mallat (2013) and Oyallon and Mallat (2015) these wavelets are deï¬ned as separable products of spatial wavelets deï¬ned in â„¦âŠ‚R2 with 1D wavelets deï¬ned in SO(2). Figure 8.12 illustrates one such Î¨Î³. Importantly, the geometric stability and energy conservation properties described in Â§Â§8.3.2 and 8.3.3 carry over the roto-translation scattering (Mallat, 2012; Oy- allon and Mallat, 2015). As discussed earlier, addressing the invariants jointly or separately gives diï¬€erent discriminability trade-oï¬€s. Some numerical applications greatly beneï¬t from the joint representatation, in particular texture recognition under large viewpoint variability (Sifre and Mallat, 2013). Timeâ€“frequency scattering. Joint scattering transforms also appear naturally in speech and audio processing, to leverage interactions of the signal energy at dif- ferent timeâ€“frequency scales. The successful recognition of audio signals requires stability to small time-warps as well as frequency transpositions. Similarly to the previous example, where the input x(u) was â€˜liftedâ€™ to a function over the roto- translation group with appropriate equivariance properties, in the case of audio signals this initial lifting is carried out by the so-called scalogram, which com- putes a Littlewoodâ€“Paley wavelet decomposition mapping a time series x(t) to a two-dimensional function z(t,Î») = |x â‹†ÏˆÎ»(t)| (AndÃ©n and Mallat, 2014). The timeâ€“frequency interactions in z can be captured by a joint wavelet decomposition 8.5 Non-Euclidean Scattering 375 Figure 8.13 From Eickenberg et al. (2017). Left: Real parts of 2D solid harmonic wavelets. Cartesian slices of 3D spherical harmonic wavelets yield similar patterns. Right: Solid harmonic wavelet moduli S[j, l, 1](Ïx)(u) = |Ïx â‹†Ïˆj |(u) for a molecule Ïx. The interference patterns at the diï¬€erent scales are reminiscent of molecular orbitals obtained in, e.g., density functional theory. frame, leading to state-of-the-art classiï¬cation and synthesis on several benchmarks (AndÃ©n et al., 2018). Solid harmonic scattering for quantum chemistry Building representations of physical systems with rotational and translational invariance and stability to de- formations is of fundamental importance across many domains, since these sym- metries are present in many physical systems. Speciï¬cally, Hirn et al. (2017) and Eickenberg et al. (2017) studied scattering representations for quantum chemistry, by considering a wavelet decomposition over SO(3). Such a wavelet decomposition is constructed in the spectral domain, and given by spherical harmonics. The re- sulting scattering representation enjoys provable roto-translational invariance and stability under small deformations, and leads to state-of-the-art performance in the regression of molecular energies (Eickenberg et al., 2017). Figure 8.13 illustrates the â€˜harmonicâ€™ wavelets as well as the resulting scattering coeï¬ƒcients for some molecules. 8.5.3 Graph Scattering In Â§8.5.2 we described invariant representations of functions deï¬ned over a ï¬xed do- main with global symmetries. Despite being of fundamental importance in physics, global symmetries are lacking in many systems in other areas of science, such as networks, surface meshes, or proteins. In those areas, one is rather interested in 376 Bruna: The Scattering Transform local symmetries, and often the domain is variable as well as the measurements over that domain. Invariance and Stability in Graphs In this context, graphs are ï¬‚exible data structures that enable general metric struc- tures and the modeling of non-Euclidean domains. The main ingredients of the scattering transform can be generalized using tools from computational harmonic analysis on graphs. As described in Â§8.2.3, the Euclidean treatment of deformations as changes of variables in the signal domain â„¦âŠ‚Rd, u 7â†’Ï•Ï„(u) = u âˆ’Ï„(u), can now be seen more generally as a change of metric, from an original metric domain X to a deformed metric domain XÏ„. We shall thus focus on deformations on the underlying graph domain, while keeping the same function-mapping, i.e., we model deformations as a change in the underlying graph support and analyze how this aï¬€ects the interaction between the function mapping and the graph. Similarly to the group scattering constructions of Â§8.5.2, deï¬ning scattering representations for graphs amounts to deï¬ning wavelet decompositions with appropriate equivariance and stability and with averaging operators providing the invariance. Consider a weighted undirected graph G = (V, E,W) with |V| = n nodes, edge set E and adjacency matrix W âˆˆRnÃ—n, with Wi,j > 0 if and only if (i, j) âˆˆE. In this context, the natural notion of invariance is given by permutations acting simultaneously on nodes and edges. Let us deï¬ne GÏ€ = ( ËœV, ËœE, ËœW) such that there exists a permutation Ï€ âˆˆSn with Ëœvi = vÏ€(i), (Ëœi, Ëœj) âˆˆËœE if and only if (Ï€(i),Ï€(j)) âˆˆE and ËœW = Î WÎ âŠ¤, where Î  âˆˆ{0,1}nÃ—n is the permutation matrix associated with Ï€. Many applications require a representation Î¦ such that Î¦(x; G) = Î¦(xÏ€,GÏ€) = Î¦(x,G) for all Ï€. Previously, Littlewoodâ€“Paley wavelets were designed in connection with a non- expansive operator âˆ¥Wâˆ¥â‰¤1 with small commutation error with respect to defor- mations: âˆ¥[W,LÏ„]âˆ¥â‰²âˆ¥âˆ‡Ï„âˆ¥. The ï¬rst task is to quantify the metric perturbations XÏ„ induced by deforming the graph. Diï¬€usion Metric Distances A weighted undirected graph G = (V, E,W) with |V| = n nodes, edge set E and adjacency matrix W âˆˆRnÃ—n deï¬nes a diï¬€usion process A on its nodes that is given in its symmetric form by the normalized adjacency W := Dâˆ’1/2WDâˆ’1/2, with D = diag(d1,. . ., dn), (8.39) where di = Ã (i,j)âˆˆE Wi,j denotes the degree of node i. Denote by d = W1 the degree vector containing di in the ith element. By construction, W is well localized in space (it is non-zero only where there is an edge connecting nodes), it is self-adjoint and 8.5 Non-Euclidean Scattering 377 it satisï¬es âˆ¥Wâˆ¥â‰¤1, where âˆ¥Wâˆ¥is the operator norm. It is convenient to assume that the spectrum of A (which is real and discrete since W is self-adjoint and ï¬nite dimensional) is non-negative. Since we shall be taking powers of W, this will avoid folding negative eigenvalues into positive ones. For that purpose, we adopt so-called lazy diï¬€usion, given by T := 1 2(I +W). We will use this diï¬€usion operator to deï¬ne both a multiscale wavelet ï¬lter bank and a low-pass average pooling, leading to the diï¬€usion scattering representation. This diï¬€usion operator can be used to construct a metric on G. So-called diï¬€usion maps (Coifman and Lafon, 2006; Nadler et al., 2006) measure distances between two nodes x, xâ€² âˆˆV in terms of their associated diï¬€usion at time s: dG,s(x, xâ€²) = âˆ¥T s GÎ´x âˆ’T s GÎ´xâ€²âˆ¥, where Î´x is a vector with all zeros except for a unit in position x. This diï¬€usion metric can be now used to deï¬ne a distance between two graphs G,Gâ€². Assuming ï¬rst that G and Gâ€² have the same size, the simplest formulation is to compare the diï¬€usion metrics generated by G and Gâ€² up to a node permutation: Deï¬nition 8.19. Let G = (V, E,W), Gâ€² = (V â€², E â€²,W â€²) have the same size |V| = |V â€²| = n. The normalized diï¬€usion distance between graphs G, Gâ€² at time s > 0 is ds(G,Gâ€²) := inf Î âˆˆÎ n âˆ¥(T s G)âˆ—(T s G) âˆ’Î âŠ¤(T s Gâ€²)âˆ—(T s Gâ€²)Î âˆ¥= inf Î âˆˆÎ n âˆ¥T2s G âˆ’Î âŠ¤T2s Gâ€²Î âˆ¥, (8.40) where Î n is the space of n Ã— n permutation matrices. The diï¬€usion distance is deï¬ned at a speciï¬c time s. As s increases, this distance becomes weaker,3 since it compares points at later stages of diï¬€usion. The role of time is thus to select the smoothness of the â€˜graph deformationâ€™, just as âˆ¥âˆ‡Ï„âˆ¥ measures the smoothness of the deformation in the Euclidean case. For convenience, we write d(G,Gâ€²) = d1/2(G,Gâ€²) and use the distance at s = 1/2 as our main deformation measure. The quantity d deï¬nes a distance between graphs (seen as metric spaces) and yields a stronger topology than other alternatives such as the Gromovâ€“Hausdorï¬€distance, deï¬ned as ds GH(G,Gâ€²) = inf Î  sup x,xâ€²âˆˆV ds G(x, xâ€²) âˆ’ds Gâ€²(Ï€(x),Ï€(xâ€²)) with ds G(x, xâ€²) = âˆ¥Tt G(Î´x âˆ’Î´xâ€²)âˆ¥L2(G) . Finally, we are considering for simplicity only the case where the sizes of G and Gâ€² are equal, but Deï¬nition 8.19 can be naturally extended to compare variable-sized graphs by replacing permutations by soft correspondences (see Bronstein et al., 2010). Our goal is to build a stable and rich representation Î¦G(x). The stability property is stated in terms of the diï¬€usion metric above: for a chosen diï¬€usion time s, for all 3 In the sense that it deï¬nes a weaker topology, i.e., limmâ†’âˆds(G, Gm) â†’0 â‡’limmâ†’âˆdsâ€²(G, Gm) = 0 for sâ€² > s, but not vice versa. 378 Bruna: The Scattering Transform x âˆˆRn, G = (V, E,W), Gâ€² = (V â€², E â€²,W â€²) with |V| = |V â€²| = n, we require that âˆ¥Î¦G(x) âˆ’Î¦Gâ€²(x)âˆ¥â‰²âˆ¥xâˆ¥d(G,Gâ€²) . (8.41) This representation can be used to model both signals and domains, or just domains G, by considering a prespeciï¬ed x = f (G) such as the degree, or by marginalizing from an exchangeable distribution Î¦G = Exâˆ¼QÎ¦G(x). The motivation of (8.41) is two-fold: on the one hand, we are interested in appli- cations where the signal of interest may be measured in dynamic environments that modify the domain, e.g., in measuring brain signals across diï¬€erent individuals. On the other hand, in other applications, such as building generative models for graphs, we may be interested in representing the domain G itself. A representation from the adjacency matrix of G needs to build invariance with respect to node per- mutations, while capturing enough discriminative information to separate diï¬€erent graphs. In particular, and similarly to Gromov-Hausdorï¬€distance, the deï¬nition of d(G,Gâ€²) involves a matching problem between two kernel matrices, which deï¬nes an NP-hard combinatorial problem. This further motivates the need for eï¬ƒcient representations Î¦G of graphs that can eï¬ƒciently tell two graphs apart, and such that â„“(Î¸) = âˆ¥Î¦G âˆ’Î¦G(Î¸)âˆ¥can be used as a diï¬€erentiable loss for training generative models. Diï¬€usion Wavelets Diï¬€usion wavelets (Coifman and Lafon, 2006) provide a simple framework to deï¬ne a multi-resolution analysis from powers of a diï¬€usion operator deï¬ned on a graph, and they are stable to diï¬€usion metric changes. Let Î»0 â‰¥Î»1 â‰¥Â· Â· Â· â‰¥Î»nâˆ’1 denote the eigenvalues of an operator A in decreasing order. Deï¬ning d1/2 = (âˆšd1,. . .,âˆšdn), one can easily verify that the normalized square-root degree vector v = d1/2/âˆ¥d1/2âˆ¥2 = d/âˆ¥dâˆ¥1 is the eigenvector with associated eigenvalue Î»0 = 1. Also, note that Î»nâˆ’1 = âˆ’1 if and only if G has a connected component that is non-trivial and bipartite (Chung and Graham, 1997). Following Coifman and Lafon (2006), we construct a family of multiscale ï¬lters by exploiting the powers of the diï¬€usion operator T2j. We deï¬ne Ïˆ0 := I âˆ’T, Ïˆj := T2jâˆ’1(I âˆ’T2jâˆ’1) = T2jâˆ’1 âˆ’T2j, j > 0 . (8.42) This corresponds to a graph wavelet ï¬lter bank with optimal spatial localization. Graph diï¬€usion wavelets are localized both in space and frequency, and favor a spatial localization, since they can be obtained with only two ï¬lter coeï¬ƒcients, namely h0 = 1 for diï¬€usion T2jâˆ’1 and h1 = âˆ’1 for diï¬€usion T2j. The ï¬nest scale Ïˆ0 corresponds to one-half of the normalized Laplacian operator Ïˆ0 = 1 2âˆ†= 1 2(I âˆ’Dâˆ’1/2WDâˆ’1/2), 8.5 Non-Euclidean Scattering 379 here seen as a temporal diï¬€erence in a diï¬€usion process, seeing each diï¬€usion step (each multiplication by âˆ†) as a time step. The coarser scales Ïˆj capture temporal diï¬€erences at increasingly spaced diï¬€usion times. For j = 0,. . ., Jnâˆ’1, we consider the linear operator W : L2(G) â†’ (L2(G))Jn, x 7â†’ (Ïˆjx)j=0,...,Jnâˆ’1 , (8.43) which is the analog of the wavelet ï¬lter bank in the Euclidean domain. Whereas several other options exist for deï¬ning graph wavelet decompositions (Rustamov and Guibas, 2013; Gavish et al., 2010), we consider here wavelets that can be expressed with few diï¬€usion terms, favoring spatial over frequential localization, for stability reasons that will become apparent next. We choose dyadic scales for convenience, but the construction is analogous if one replaces scales 2j by âŒˆÎ³jâŒ‰, for any Î³ > 1, in (8.42). If the graph G exhibits a spectral gap, i.e., Î²G = supi=1,...nâˆ’1 |Î»i| < 1, the linear operator W deï¬nes a stable frame. Proposition 8.20 (Gama et al., 2018, Prop 4.1). For each n, let W deï¬ne the diï¬€usion wavelet decomposition (8.43) and assume that Î²G < 1. Then there exists a constant 0 < C(Î²), depending only on Î², such that for any x âˆˆRn satisfying âŸ¨x,vâŸ©= 0, it holds that C(Î²)âˆ¥xâˆ¥2 â‰¤ Jnâˆ’1 Ã• j=0 âˆ¥Ïˆjxâˆ¥2 â‰¤âˆ¥xâˆ¥2 . (8.44) This proposition thus provides the Littlewoodâ€“Paley bounds of W, which control the ability of the ï¬lter bank to capture and amplify the signal x along each â€˜fre- quencyâ€™. We note that diï¬€usion wavelets are neither unitary nor analytic and there- fore do not preserve energy. However, the frame bounds in Proposition 8.20 provide lower bounds on the energy lost. They also inform us about how the spectral gap Î² determines the appropriate diï¬€usion scale J: the maximum of p(u) = (ur âˆ’u2r)2 is at u = 2âˆ’1/r, thus the cutoï¬€râˆ—should align with Î² as râˆ—= âˆ’1/log2 Î², since larger values of r capture energy in a spectral range where the graph has no information. Therefore, the maximum scale can be adjusted to be J = âŒˆ1 + log2 râˆ—âŒ‰= 1 +  log2  âˆ’1 log2 Î²  . Diï¬€usion Scattering Recall that the Euclidean scattering transform is constructed by cascading three building blocks: a wavelet decomposition operator, a pointwise modulus activation function, and an averaging operator. Following the procedure for Euclidean scat- tering, given a graph G and x âˆˆL2(G) we deï¬ne an analogous diï¬€usion scattering 380 Bruna: The Scattering Transform transform SG(x) by cascading three building blocks: the wavelet decomposition operator W, a pointwise activation function Ï, and an average operator A which extracts the average over the domain. The average over a domain can be interpreted as the diï¬€usion at inï¬nite time, thus Ax = limtâ†’âˆTtx = âŸ¨v,xâŸ©. More speciï¬cally, we consider a ï¬rst-layer transformation given by SG[Î›1]x) = AÏWx = {AÏÏˆjx}0â‰¤j â‰¤Jnâˆ’1 , (8.45) followed by second-order coeï¬ƒcients SG[Î›2]x) = AÏWÏWx = {AÏÏˆj2 ÏÏˆj1x}0â‰¤j1,j2 â‰¤Jnâˆ’1 , (8.46) and so on. The representation obtained from m layers of such a transformation is thus SG,m(x) = {Ax,SG[Î›1](x),. . .,SG[Î›m](x)} = {A(ÏW)kx ; k = 0,. . .,m âˆ’1} . (8.47) Stability and Invariance of Diï¬€usion Scattering The scattering transform coeï¬ƒcients SG(x) obtained after m layers are given by (8.47), for a low-pass operator A such that Ax = âŸ¨v,xâŸ©. The stability of diï¬€usion wavelets with respect to small changes of the diï¬€usion metric can be leveraged to obtain a resulting diï¬€usion scattering representation with prescribed stability, as shown by the following theorem. Theorem 8.21 (Gama et al., 2018, Theorem 5.3). Let Î²âˆ’= min(Î²G, Î²Gâ€²) and Î²+ = max(Î²G, Î²Gâ€²) and assume Î²+ < 1. Then, for each k = 0,. . .,m âˆ’1, the following holds: âˆ¥SG,m(x) âˆ’SGâ€²,m(x)âˆ¥2 â‰¤ mâˆ’1 Ã• k=0 ï£®ï£¯ï£¯ï£¯ï£¯ï£°  2 1 âˆ’Î²âˆ’ d(G,Gâ€²) 1/2 + k s Î²2 +(1 + Î²2 +) (1 âˆ’Î²2 +)3 d(G,Gâ€²) ï£¹ï£ºï£ºï£ºï£ºï£» 2 âˆ¥xâˆ¥2 â‰²m d(G,Gâ€²)âˆ¥xâˆ¥2 if d(G,Gâ€²) â‰ª1. (8.48) This result shows that the closer two graphs are in terms of the diï¬€usion metric, the closer will be their scattering representations. The constant is given by topological properties, the spectral gaps of G and Gâ€², as well as design parameters and the number of layers m. We observe that, as the stability bound grows, the smaller the spectral gap becomes, as is the case when more layers are considered. The spectral gap is tightly linked with diï¬€usion processes on graphs, and thus it does emerge from the choice of a diï¬€usion metric. Graphs with values of Î² closer to 1 exhibit weaker diï¬€usion paths, and thus a small perturbation on the edges of these graphs 8.5 Non-Euclidean Scattering 381 would lead to a larger diï¬€usion distance. We also note that the spectral gap appears in our upper bounds, but it is not necessarily sharp. In particular, the spectral gap is a poor indication of stability in regular graphs, and we believe that our bound can be improved by leveraging the structural properties of regular domains. Finally, we note that the size of the graphs under comparison impacts the stability result inasmuch as it impacts the distance measure d(G,Gâ€²). A similar scattering construction was developed in Zou and Lerman (2020), where the authors estab- lished stability with respect to a graph measure that depends on the spectrum of the graph through both eigenvectors and eigenvalues. More speciï¬cally, it is required that the spectrum becomes more concentrated as the graphs grow. However, in general, it is not straightforward to relate the topological structure of the graph to its spectral properties. As mentioned above, the stability is computed with a metric d(G,Gâ€²) which is stronger than might be expected. This metric is permutation invariant, in analogy with the rigid translation invariance in the Euclidean case, and stable with respect to small perturbations around permutations. Recently, Gama et al. (2019) extended the previous stability analysis to more general wavelet decompositions, using a relative notion of deformation. Figure 8.14 illustrates the performance of graph scattering operators on several graph signal processing tasks. Moreover, Gao et al. (2019) developed a similar scattering representation for graphs, achieving state-of-the-art results on several graph classiï¬cation tasks. The extension of (8.48) to weaker metrics, using e.g. multiscale deformations, is an important open question. Unsupervised Haar Scattering on Graphs A particularly simple wavelet representation on graphs â€“ which avoids any spectral decomposition â€“ is given by Haar wavelets (Gavish et al., 2010). Such wavelets were used in the ï¬rst work, Chen et al. (2014), that extended scattering representations to graphs. Given an undirected graph G = (V, E), an orthogonal Haar scattering transform is obtained from a multiresolution approximation of G. Let G0 := G. In the dyadic case |V| = 2J, this transform is deï¬ned as a hierarchical partition {Vj,n}j,n of G of the form V0,i = {i}, i â‰¤2J; Vj+1,i = Vj,ai âŠ”Vj,bi, i = 1 â‰¤2Jâˆ’jâˆ’1 , where the pairings (ai, bi) are connected in the induced subsampled graph Gj = (Vj, Ej), of size |Vj| = 2Jâˆ’j, whose vertices are precisely Vj := {Vj,i}i and whose edges are inherited recursively from Gjâˆ’1: (i,iâ€²) âˆˆEj if and only if there exists Â¯e = (Â¯i, Â¯iâ€²) âˆˆEjâˆ’1 with Â¯i âˆˆVj,i and Â¯iâ€² âˆˆVj,iâ€². Let x âˆˆl2(G). By rearranging the pairings sequentially, the resulting orthogonal 382 Bruna: The Scattering Transform (a) Small world 0.2 0.4 0.6 0.8 1.0 Îµ 10âˆ’4 10âˆ’3 10âˆ’2 10âˆ’1 100 Representation Error: âˆ¥Î¦(S, x) âˆ’Î¦(Ë†S, x)âˆ¥/âˆ¥Î¦(S, x)âˆ¥ Diï¬€usion Monic Cubic Tight Hann GFT Bound (b) Authorship attribution 400 600 800 1000 1200 1400 Number of training samples 0.70 0.75 0.80 0.85 0.90 Classiï¬cation accuracy Diï¬€usion Monic Cubic Tight Hann GFT (c) Facebook graph 10âˆ’2 10âˆ’1 Probability of edge failure 0.6 0.7 0.8 0.9 1.0 1.1 Classiï¬cation accuracy Diï¬€usion Monic Cubic Tight Hann GFT Figure 8.14 From Gama et al. (2019). (a): Diï¬€erence in representation between signals deï¬ned using the original graph scattering SG and SGâ€² corresponding to the deformed graph, as a function of the perturbation size d(G, Gâ€²). (b), (c): Classiï¬cation accuracy as a function of perturbation for an authorship attribution graph and a Facebook graph, respectively. 8.5 Non-Euclidean Scattering 383 Haar scattering representation SJx is deï¬ned recursively as S0x(i,0) := x(i), i = 1,. . .,2J Sj+1x(i,2q) := Sjx(ai,q) + Sjx(bi,q), Sj+1x(i,2q + 1) := |Sj(ai,q) âˆ’Sj(bi,q)|, i = 1,. . .,2Jâˆ’jâˆ’1, q = 2j. (8.49) One easily veriï¬es (Chen et al., 2014; Cheng et al., 2016) that the resulting transfor- mation preserves the number, 2J, of coeï¬ƒcients, and is contractive and unitary up to a normalization factor 2J/2. However, since the multiresolution approximation deï¬nes an orthogonal transformation, the resulting orthogonal scatering coeï¬ƒcients are not permutation invariant. In order to recover an invariant representation, it is thus necessary to average an ensemble of orthogonal transforms using diï¬€erent multiresolution approximations. Nevertheless, the main motivation in Chen et al. (2014); Cheng et al. (2016) was to perform graph scattering on domains with un- known (but presumed) graph connectivity structure. In that case, the sparsity of scattering coeï¬ƒcients was used as a criterion to ï¬nd the optimal multiresolution approximation, resulting in state-of-the-art performance on several graph classiï¬- cation datasets. 8.5.4 Manifold Scattering In the previous sections we have seen some instances of extending scattering rep- resentations to non-Euclidean domains, including compact Lie groups and graphs. Such extensions (which in fact also apply to the wider class of convolutional neural network architectures; see Bronstein et al. (2017) for an in-depth review) can be understood through the lens of the spectrum of diï¬€erential operators, in particu- lar the Laplacian. Indeed, the Laplacian operator encapsulates the symmetries and stability requirements that we have been manipulating so far, and can be deï¬ned across many diï¬€erent domains. In particular, if M denotes a compact, smooth Riemannian manifold without boundary, one can deï¬ne the Laplaceâ€“Beltrami operator âˆ†in M as the divergence of the manifold gradient. In these conditions, âˆ’âˆ†is self-adjoint and positive semi- deï¬nite; therefore its eigenvectors deï¬ne an orthonormal basis of L2(M, Âµ), where Âµ is the uniform measure on M. Expressing any f âˆˆL2(M) in this basis amounts to computing a â€˜Fourier transformâ€™ on M. Indeed, the Laplacian operator in Rd is precisely diagonal in the standard Euclidean Fourier basis. Convolutions in the Euclidean case can be seen as linear operators that diagonalize in the Fourier basis or equivalently that commute with the Laplacian operator. A natural generalization of convolutions to non-Euclidean domains M thus formally sees them as linear operators that commute with the Laplacian deï¬ned in M (Bruna et al., 2013; 384 Bruna: The Scattering Transform Bronstein et al., 2017). Speciï¬cally, if {Ï•k}k are the eigenvectors of âˆ†and Î› := {Î»k}k its eigenvalues, a function of spectral multipliers Î·: Î› â†’R deï¬nes a kernel in M: KÎ·(u,v) = Ã• k Î·(Î»k)Ï•k(u)Ï•k(v), u,v âˆˆM , and a â€˜convolutionâ€™ from its corresponding integral operator: L2(M) â†’L2(M) x 7â†’(TÎ·x)(u) = âˆ« KÎ·(u,v)x(v)Âµ(dv). (8.50) Perlmutter et al. (2018) used this formalism to build scattering representations on Riemannian manifolds, by deï¬ning Littlewoodâ€“Paley wavelet decompositions from appropriately chosen spectral multipliers (Î·j)j. The resulting scattering rep- resentation is shown to be stable to additive noise and to smooth diï¬€eomorphisms of M. 8.6 Generative Modeling with Scattering In this section we discuss applications of scattering representation to build high- dimensional generative models. Data priors deï¬ned from the scattering representa- tion enjoy geometric stability and may be used as models for stationary processes or to regularize ill-posed inverse problems. 8.6.1 Suï¬ƒcient Statistics Deï¬ning probability distributions of signals x(u) âˆˆL2(Rd) is a challenging task due to the curse of dimensionality and the lack of tractable analytic models of â€˜realâ€™ data. A powerful way for approaching this challenge is via the principle of maximum entropy: construct probability models that are maximally regular while satisfying a number of constraints given by a vector Î¦(x) âˆˆRK of suï¬ƒcient statistics that is ï¬tted to the available data. When Î¦(x) = xxâŠ¤consists of covariance measurements, the resulting maximum entropy model is a Gaussian process, and when Î¦(x) computes local potentials one obtains Markov random ï¬elds instead. In either case, one is quickly confronted with fundamental challenges, either statistical (exponential sample complexity for powerful statistical models, or large bias in small parametric ones) or computational, coming from the intractability of computing partition functions and sampling in high dimensions. The suï¬ƒcient statistics in a maximum entropy model capture our prior infor- mation about â€˜what mattersâ€™ in the input data. In this subsection, we shall explore 8.6 Generative Modeling with Scattering 385 maximum entropy models where the suï¬ƒcient statistics are given by scattering rep- resentations. Depending on the localization scale 2J, two distinct regimes emerge. For ï¬xed and relatively small scales J, windowed scattering representations pro- vide local statistics that are nearly invertible, and help regularize ill-posed inverse problems (Â§8.6.4). As J â†’âˆ, expected scattering moments may be used to deï¬ne models for stationary processes (Â§8.6.5). Thanks to the scattering mean-squared consistency discussed in Â§8.4, we can circumvent the aforementioned challenges of maximum entropy models with the so-called microcanonical models from statistical physics, to be described in Â§8.6.2. In both regimes an important algorithmic component will be solving a problem of the form minx âˆ¥S(x) âˆ’yâˆ¥. We discuss a gradient descent strategy for that purpose in Â§8.6.3. 8.6.2 Microcanonical Scattering Models First, suppose that we wish to characterize a probability distribution Âµ over input signals x âˆˆD = L2(Rd), from the knowledge that SJ(x) â‰ˆy. In this setup, we could think of y as being an empirical average y = 1 n n Ã• i=1 SJ(xi), where xi are training samples that are conditionally independent and identically distributed. Recall that the diï¬€erential entropy of a probability distribution Âµ which admits a density p(x) relative to the Lebesgue measure is H(Âµ) := âˆ’ âˆ« p(x) log p(x) dx . (8.51) In the absence of any other source of information, the classic macrocanonical model from Boltzmann and Gibbs, Âµma, has density pma at maximum entropy, conditioned on Epma(SJ(x)) = y. However, a microcanonical model replaces the expectation constraint with an empirical constraint of the form âˆ¥SJ(x) âˆ’yâˆ¥â‰¤Ïµ for small, appropriately chosen, Ïµ. Despite being similar in appearance, microcanonical and macrocanonical models have profound diï¬€erences. On the one hand, under appropriate conditions, macro- canonical models may be expressed as Gibbs distributions of the form pma(x) = eâŸ¨Î¸,SJ (x)âŸ© ZÎ¸ , where ZÎ¸ is the normalizing constant or partition function and Î¸ is a vector of 386 Bruna: The Scattering Transform Lagrange multipliers enforcing the expectation constraint. Unfortunately, this vector has no closed-form expression in terms of estimable quantities in general, and needs to be adjusted using MCMC (Wainwright et al., 2008). On the other hand, microcanonical models have compact support. However, under mild ergodicity assumptions on the underlying data-generating process, one can show that both models become asymptotically equivalent via the Boltzmann equivalence principle (Dembo and Zeitouni, 1993) as J â†’âˆ, although microcanonical models may exist even when their macrocanonical equivalents do not (Bruna and Mallat, 2018; Chatterjee, 2017). Also, estimating microcanonical models does not require the costly estimation of Lagrange multipliers. Microcanonical models. The microcanonical set of width Ïµ associated with y is â„¦J,Ïµ = {x âˆˆD : âˆ¥SJ(x) âˆ’yâˆ¥â‰¤Ïµ} . A maximum entropy microcanonical model Âµmi(J,Ïµ, y) was deï¬ned by Boltzmann as the maximum entropy distribution supported in â„¦J,Ïµ. If we assume conditions which guarantee that SJ preserves energy (Â§8.3.2), then we can verify that â„¦J,Ïµ is a compact set. It follows that the maximum entropy distribution has a uniform density pd,Ïµ: pd,Ïµ(x) := 1â„¦J ,Ïµ (x) âˆ« â„¦J ,Ïµ dx . (8.52) Its entropy is therefore the logarithm of the volume of â„¦J,Ïµ: H(pd,Ïµ) = âˆ’ âˆ« pd,Ïµ(x) log pd,Ïµ(x) dx = log  âˆ« â„¦J ,Ïµ dx  . (8.53) The scale J corresponds to an important trade-oï¬€in this model, as illustrated in the case where y = SJ(Â¯x) represent measurements coming from a single realization. When J is small, as explained in Â§8.3.4, the number of scattering coeï¬ƒcients is larger than the input dimension, and thus one may expect â„¦J,Ïµ to converge to a single point Â¯x as Ïµ â†’0. As J increases, the system of equations SJ(x) = SJ(Â¯x) becomes under-constrained, and thus â„¦J,Ïµ will be a non-singular set. Figure 8.8 illustrates this on a collection of input images. The entropy of the microcanonical model thus grows with J. It was proved in Bruna and Mallat (2018) that under mild assumptions the entropy is an extensive quantity, meaning that its growth is of the same order as 2J, the support of the representation. The appropriate scale J needs to balance two opposing eï¬€ects. On the one hand, we want SJ to satisfy a concentration property to ensure that typical samples from the unknown data distribution Âµ are included in â„¦J,Ïµ with high probability, and hence are typical for the microcanonical measure Âµmi. On the other hand, the sets â„¦J,Ïµ must 8.6 Generative Modeling with Scattering 387 not be too large in order to avoid having elements of â„¦J,Ïµ â€“ and hencetypical samples of Âµmi â€“ which are not typical for Âµ. To obtain an accurate microcanonical model, the scale J must deï¬ne microcanonical sets of minimum volume, while satisfying a concentration property akin to (8.25); see Bruna and Mallat (2018, Section 2.1) for further details. In particular, this implies that the only data distributions that admit a valid microcanonical model as J increases are ergodic, stationary textures, where spatial averages converge to the expectation. Bruna and Mallat (2018) developed microcanonical models built from scattering representations, showing the ability of such representations to model complex stationary phenomena such as Ising models, point processes and natural textures with tractable sample complexity. We will illustrate scattering microcanonical models for such textures in Â§8.6.5. In essence, these models need to sample from the uniform measure of sets of the form {x; âˆ¥S(x) âˆ’yâˆ¥â‰¤Ïµ}. We describe next how to solve this eï¬ƒciently using gradient descent. 8.6.3 Gradient Descent Scattering Reconstruction Computing samples of a maximum entropy microcanonical model is typically done with MCMC algorithms or Langevin dynamics (Creutz, 1983), which is computa- tionally expensive. Microcanonical models computed with alternative projections and gradient descents have been implemented to sample texture synthesis models (Heeger and Bergen, 1995; Portilla and Simoncelli, 2000; Gatys et al., 2015). We will consider microcanonical gradient descent models obtained by transport- ing an initial measure towards a microcanonical set, using gradient descent with respect to the distance to the microcanoncal ensemble. Although this gradient de- scent sampling algorithm does not in general correspond to the maximum entropy microcanonical model, it preserves many symmetries of the maximum entropy mi- crocanonical measure, and has been shown to converge to the microcanonical set for appropriate choices of energy vector (Bruna and Mallat, 2018). We transport an initial measure Âµ0 towards a measure supported in a microcanon- ical set â„¦J,Ïµ, by iteratively minimizing E(x) = 1 2 âˆ¥SJ(x) âˆ’yâˆ¥2 (8.54) with mappings of the form Ï•n(x) = x âˆ’Îºnâˆ‡E(x) = x âˆ’Îºnâˆ‚SJ(x)âŠ¤(SJ(x) âˆ’y), (8.55) where Îºn is the gradient step at each iteration n. Given an initial measure Âµ0, the measure update is Âµn+1 := Ï•n,#Âµn, (8.56) 388 Bruna: The Scattering Transform with the standard pushforward measure f#(Âµ)[A] = Âµ[ f âˆ’1(A)] for any Âµ-measur- able set A, where f âˆ’1(A) = {x; f (x) âˆˆA}. Samples from Âµn are thus obtained by transforming samples x0 from Âµ0 with the mapping Â¯Ï• = Ï•n â—¦Ï•nâˆ’1 â—¦Â· Â· Â· â—¦Ï•1. It corresponds to n steps of a gradient descent initialized with x0 âˆ¼Âµ0: xl+1 = xl âˆ’Îºlâˆ‚SJ(xl)âŠ¤(SJ(xl) âˆ’y) . Bruna and Mallat (2018) studied the convergence of the gradient descent mea- sures Âµn for general choices of suï¬ƒcient statistics including scattering vectors. Even if the Âµn converge to a measure supported in a microcanonical set â„¦J,Ïµ, in general they do not converge to a maximum entropy measure on this set. However, the next theorem proves that if Âµ0 is a Gaussian measure of i.i.d. Gaussian random variables then they have a large class of common symmetries with the maximum entropy measure. Let us recall that a symmetry of a measure Âµ is a linear invertible operator L such that, for any measurable set A, Âµ[Lâˆ’1(A)] = Âµ[A]. A linear invertible operator L is a symmetry of Î¦d if for all x âˆˆD, SJ(Lâˆ’1x) = SJ(x). It preserves volumes if its determinant satisï¬es |detL| = 1. It is orthogonal if LtL = LLt = I and we say that it preserves a stationary mean if, L1 = 1 for 1 = (1,. . .,1) âˆˆRâ„“. Theorem 8.22 (Bruna and Mallat, 2018, Theorem 3.4). (i) If L is a symmetry of SJ which preserves volumes then it is a symmetry of the maximum entropy microcanonical measure. (ii) If L is a symmetry of SJ and of Âµ0 then it is a symmetry of Âµn for any n â‰¥0. (iii) Suppose that Âµ0 is a Gaussian white noise measure of d i.i.d. Gaussian random variables. Then, if L is a symmetry of Î¦d which is orthogonal and preserves a stationary mean, it is a symmetry of Âµn for any n â‰¥0. The initial measure Âµ0 is chosen so that it has many symmetries in common with Î¦d and hence the gradient descent measures have many symmetries in common with a maximum entropy measure. A Gaussian measure of i.i.d. Gaussian variables of mean m0 and Ïƒ0 is a maximum entropy measure conditioned by a stationary mean and variance. It is uniform over spheres, which guarantees that it has a large group of symmetries. Observe that periodic shifts are linear orthogonal operators and preserve a sta- tionary mean. The following corollary applies property (iii) of Theorem 8.22 to prove that the Âµn are circular-stationary. Corollary 8.23 (Bruna and Mallat, 2018, Corollary 3.5). When J â†’âˆthen SJ is invariant with respect to periodic shift. Therefore if Âµ0 is a Gaussian white noise then Âµn is circular-stationary for n â‰¥0. 8.6 Generative Modeling with Scattering 389 8.6.4 Regularising Inverse Problems with Scattering Ill-posed inverse problems attempt to estimate an unknown signal x from noisy, pos- sibly non-linear and under-determined measurements y = Gx+ w, where w models the additive noise. A natural Bayesian perspective is to consider the maximum-a- posteriori (MAP) estimate, given by Ë†x âˆˆarg max p(x|y) = arg max p(x) Â· p(y|x) = arg max log p(x) + log p(y|x) . Under a Gaussian noise assumption, âˆ’log p(y|x) takes the familiar formCâˆ¥yâˆ’Gxâˆ¥2. Regularizing inverse problems using microcanonical scattering generative models thus amounts to choosing a prior log p(x) of the form âˆ¥SJ(x) âˆ’zâˆ¥2 , where z can be adjusted using a training set. If Âµ denotes the underlying data-generating distribution of signals x, such a prior implicitly assumes that scattering coeï¬ƒcients SJ(x), are concentrated around x âˆ¼Âµ. In some applications, however, Âµ may not enjoy such ergodocity properties, in which case one can also consider a microcanonical â€˜amortisedâ€™ prior that is allowed to depend on the scattering coeï¬ƒcients of the measurements. The resulting estimator thus becomes Ë†x âˆˆarg min x âˆ¥Gx âˆ’yâˆ¥2 + Î²âˆ¥SJx âˆ’MSJyâˆ¥2a, (8.57) M being a linear operator learned by solving a linear regression of pairs (SJxi,SJyi)i in the scattering domain and where {xi,yi}i is a training set of inputâ€“output pairs. This estimator diï¬€ers from typical data-driven estimators that leverage supervised training in inverse problems using CNNs. More speciï¬cally, given a trainable model xÎ¸ = Î¦(y; Î¸), one considers Ë†xCNN = xÎ¸âˆ—, where Î¸âˆ—âˆˆarg min Î¸ Ã• i âˆ¥xi âˆ’Î¦(yi; Î¸)âˆ¥2 . (8.58) See Adler and Ã–ktem (2018); Zhang et al. (2017); Jin et al. (2017) for recent surveys on data-driven models for imaging inverse problems. Despite their phe- nomenal success across many inverse problems, such estimators suï¬€er from the so-called â€˜regression-to-the-meanâ€™ phenomenon, in which the model is asked to predict a speciï¬c input xi from potentially many plausible signals explaining the same observations yi; this leads to an estimator that averages all such plausible so- lutions, thus losing high-frequency and texture information. Instead, the scattering mircocanonical estimator (8.57) learns a linear operator using the scattering metric, which leverages the stability of the scattering transform to small deformations in order to avoid the regression-to-the-mean of baseline estimators. 390 Bruna: The Scattering Transform Figure 8.15 Comparison of single-image super-resolution using scattering microcanonical prior and pure data-driven models, for a linear model (leading to spline interpolation) and for a CNN model from Dong et al. (2014). From left to right: original iamge, linear model, CNN model, and scattering model. The estimator (8.57) was studied in Bruna et al. (2015b) using localized scatter- ing, in the context of single-image super-resolution, and in DokmaniÄ‡ et al. (2016) for other imaging inverse problems from, e.g., tomography. In all cases, the gradient descent algorithm from Â§8.6.3 was employed. Figure 8.15 compares the resulting estimates with spline interpolation and with estimators of the form (8.58). Generative networks as inverse problems with scattering transforms. Angles and Mallat (2018) considered a variant of the microcanonical scattering model, by replacing the gradient descent sampling scheme of Â§8.6.3 with a learned deep convolutional network generator, which learns to map a vector of scattering coeï¬ƒ- cients z = SJ(x) back to x. Deep generative models such as variational autoencoders (Kingma and Welling, 2014; Rezende et al., 2014) or GANs (Goodfellow et al., 2014) consider two networks, an encoder and a decoder. The encoder maps the data to a latent space with prescribed probability density, e.g. a standard Gaussian distribution, and the decoder maps it back to reconstruct the input. In this context, 8.6 Generative Modeling with Scattering 391 the scattering transform SJ may be used as an encoder on appropriate data distri- butions, thanks to its ability to linearize small deformations and â€˜Gaussianizeâ€™ the input distribution (Angles and Mallat, 2018). Finally, in Andreux and Mallat (2018) the authors used the timeâ€“frequency joint scattering transform of Â§8.5.2 and the learned decoder from Angles and Mallat (2018) for the generation and transformation of musical sounds. 8.6.5 Texture Synthesis with Microcanonical Scattering An image or an audio texture is usually modeled as the realization of a stationary process. A texture model computes an approximation of this stationary process given a single realization, and texture synthesis then consists in calculating new realizations from this stochastic model. Since in general the original stochastic process is not known, perceptual compar- isons are the only criteria that can be used to evaluate a texture synthesis algorithm. Microcanonical models can be considered as texture models computed from an energy function SJ(x) which concentrates close to its mean. Geman and Geman (1984) introduced macrocanonical models based on Markov random ï¬elds. These provide good texture models as long as these textures are realizations of random processes having no long-range correlations. Several ap- proaches were then introduced to incorporate long-range correlations. Heeger and Bergen (1995) captured texture statistics through the marginal distributions ob- tained by ï¬ltering images with oriented wavelets. This approach wasgeneralized by the macrocanonical frame model of Zhu et al. (1998), based on marginal distri- butions of ï¬ltered images. The ï¬lters are optimized by minimizing the maximum entropy conditioned by the marginal distributions. Although the Cramerâ€“Wold the- orem proves that enough marginal probability distributions characterize any random vector deï¬ned over Rd, the number of such marginals is typically intractable which limits this approach. Portilla and Simoncelli (2000) made important improvements to these texture models using wavelet transforms. They captured the correlation of the modulus of wavelet coeï¬ƒcients with a covariance matrix which deï¬nes an energy vector Î¦d(x). Although they used a macrocanonical maximum entropy formalism, their algorithm computes a microcanonical estimation from a single realization with alternate projections as opposed to a gradient descent. Excellent texture syntheses have recently been obtained with deep convolutional neural networks. In Gatys et al. (2015), the authors considered a deep VGG con- volutional network, trained on a large-scale image classiï¬cation task. The energy vector is deï¬ned as the spatial cross-correlation values of feature maps at every layer of the VGG networks. This energy vector is calculated for a particular texture image. Texture syntheses of very good perceptual quality are calculated with a 392 Bruna: The Scattering Transform Figure 8.16 Examples of microcanonical texture synthesis using diï¬€erent vectors of suï¬ƒcient statistics. From top to bottom: original samples, Gaussian model, ï¬rst-order scattering and second-order scattering. gradient descent microcanonical algorithm initialized on random noise. However, the dimension of this energy vector is larger than the dimension of the input image. These estimators are therefore not statistically consistent and have no asymptotic limit. Figure 8.16 displays examples of textures from the Brodatz dataset synthesized using the scattering microcanonical model from Bruna and Mallat (2018), and gives a comparison of the eï¬€ects of using either only ï¬rst-order scattering coeï¬ƒcients or only covariance information. Although qualitatively better than these alternatives, deep convolutional networks reproduce image and audio textures with even better perceptual quality than that of scattering coeï¬ƒcients (Gatys et al., 2015), but use over 100 times more parameters. Much smaller models providing similar perceptual quality can be constructed with wavelet phase harmonics for audio signals (Mallat et al., 2018) or images (Zhang and Mallat, 2021); these capture alignment of phases across scales. However, understanding how to construct low-dimensional multiscale energy vectors for approximating random processes remains mostly an open problem. 8.7 Final Remarks 393 8.7 Final Remarks The aim of this was to provide chapter a comprehensive overview of scattering rep- resentations, more speciï¬cally to motivate their role in the puzzle of understanding the eï¬€ectiveness of deep learning. In the context of high-dimensional learning problems involving geometric data, beating the curse of dimensionality requires exploiting as many geometric priors as possible. In particular, good signal representations should be stable with respect to small metric perturbations of the domain, expressed as deformations in the case of natural images. Scattering representations, through their constructive approach to building such stability, reveal the role of convolutions, depth and scale that underpins the success of CNN architectures. We have focused mostly on the theoretical aspects of the scattering representation, and some of its ramiï¬cations beyond the context of computer vision and learning. That said, logically we could not cover all application areas nor some of the recent advances, especially the links with turbulence analysis and other non-linear PDEs in physics, or applications to ï¬nancial time series (Leonarduzzi et al., 2019), or video. Another important aspect that we did not address is the role of the non- linear activation function. All our discussion has focused on the complex modulus, but recent related work (Mallat et al., 2018) considered the half-rectiï¬cation case through the notion of â€˜phase harmonicsâ€™, of which the modulus can be seen as the â€˜fundamentalâ€™, complemented by higher harmonics. Despite the above points, the inherent limitation of a scattering theory in explain- ing deep learning is precisely that it does not consider the dynamical aspects of learning. Throughout numerous computer vision benchmarks, one systematically ï¬nds a performance gap between hand-designed scattering architectures and their fully trained counterparts, as soon as datasets become suï¬ƒciently large. The ability of CNNs to interpolate high-dimensional data while seemingly avoiding the curse of dimensionality remains an essential ability that scattering-based models currently lack. Hybrid approaches such as those outlined in Oyallon et al. (2018b) hold the promise of combining the interpretability and robustness of scattering models with the data-ï¬tting power of large neural networks. References Adler, Jonas, and Ã–ktem, Ozan. 2018. Learned primalâ€“dual reconstruction. IEEE Transactions on Medical Imaging, 37(6), 1322â€“1332. AndÃ©n, Joakim, and Mallat, StÃ©phane. 2014. Deep scattering spectrum. IEEE Transactions on Signal Processing, 62(16), 4114â€“4128. AndÃ©n, Joakim, Lostanlen, Vincent, and Mallat, StÃ©phane. 2018. Classiï¬cation with joint timeâ€“frequency scattering. ArXiv preprint arXiv:1807.08869. 394 Bruna: The Scattering Transform Andreux, Mathieu, and Mallat, StÃ©phane. 2018. Music generation and transforma- tion with moment matchingâ€“scattering inverse networks. Pages 327â€“333 of: Proc. International Society for Music Retrieval Conference. Andreux, Mathieu, Angles, TomÃ¡s, Exarchakis, Georgios, Leonarduzzi, Roberto, Rochette, Gaspar, Thiry, Louis, Zarka, John, Mallat, StÃ©phane, Belilovsky, Eugene, Bruna, Joan, et al. 2018. Kymatio: Scattering transforms in Python. J. Machine Learning Research, 21(60), 1â€“6. Angles, TomÃ¡s, and Mallat, StÃ©phane. 2018. Generative networks as inverse prob- lems with scattering transforms. In Proc. International Conference on Learn- ing Representations. Bietti, Alberto, and Mairal, Julien. 2019a. Group invariance, stability to defor- mations, and complexity of deep convolutional representations. J. Machine Learning Research, 20(1), 876â€“924. Bietti, Alberto, and Mairal, Julien. 2019b. On the inductive bias of neural tangent kernels. In Proc. 33rd Neural Information Processing Systems, pp. 12873â€“ 12884. Bronstein, Alexander M., Bronstein, Michael M., Kimmel, Ron, Mahmoudi, Mona, and Sapiro, Guillermo. 2010. A Gromovâ€“Hausdorï¬€framework with diï¬€usion geometry for topologically-robust non-rigid shape matching. International Journal of Computer Vision, 89(2â€“3), 266â€“286. Bronstein, Michael M., Bruna, Joan, LeCun, Yann, Szlam, Arthur, and Van- dergheynst, Pierre. 2017. Geometric deep learning: Going beyond Euclidean data. IEEE Signal Processing Magazine, 34(4), 18â€“42. Bruna, J., and Mallat, S. 2013. Invariant scattering convolution networks. Trans. Pattern Analysis and Machine Intelligence, 35(8), 1872â€“1886. Bruna, Joan. 2013. Scattering Representations for Recognition. Ph.D. thesis, Ecole Polytechnique. Bruna, Joan. 2019. Consistency of Haar scattering. Preprint. Bruna, Joan, and Mallat, StÃ©phane. 2018. Multiscale sparse microcanonical models. ArXiv preprint arXiv:1801.02013. Bruna, Joan, Zaremba, Wojciech, Szlam, Arthur, and LeCun, Yann. 2013. Spec- tral networks and locally connected networks on graphs. Proc. International Conference on Learning Representations. Bruna, Joan, Mallat, StÃ©phane, Bacry, Emmanuel, Muzy, Jean-FranÃ§ois, et al. 2015a. Intermittent process analysis with scattering moments. Annals of Statistics, 43(1), 323â€“351. Bruna, Joan, Sprechmann, Pablo, and LeCun, Yann. 2015b. Super-resolution with deep convolutional suï¬ƒcient statistics. In Proc. International Conference on Learning Representations. Chatterjee, Sourav. 2017. A note about the uniform distribution on the intersection of a simplex and a sphere. Journal of Topology and Analysis, 9(4), 717â€“738. Chen, Xu, Cheng, Xiuyuan, and Mallat, StÃ©phane. 2014. Unsupervised deep Haar References 395 scattering on graphs. Pages 1709â€“1717 of: Advances in Neural Information Processing Systems. Cheng, Xiuyuan, Chen, Xu, and Mallat, StÃ©phane. 2016. Deep Haar scattering networks. Information and Inference, 5, 105â€“133. Chung, Fan, and Graham, R. K. 1997. Spectral Graph Theory. American Mathe- matical Society. Cohen, Taco, and Welling, Max. 2016a. Group equivariant convolutional networks. Pages 2990â€“2999 of: Proc. International Conference on Machine Learning. Cohen, Taco S., and Welling, Max. 2016b. Steerable CNNs. In: Proc. International Conference on Learning Representations. Coifman, Ronald R., and Lafon, StÃ©phane. 2006. Diï¬€usion maps. Applied and Computational Harmonic Analysis, 21(1), 5â€“30. Creutz, Michael. 1983. Microcanonical Monte Carlo simulation. Physical Review Letters, 50(19), 1411. Czaja, Wojciech, and Li, Weilin. 2017. Analysis of timeâ€“frequency scattering transforms. Applied and Computational Harmonic Analysis, 47(1), 149â€“171. Dembo, A., and Zeitouni, O. 1993. Large Deviations Techniques and Applications. Jones and Bartlett Publishers. DokmaniÄ‡, Ivan, Bruna, Joan, Mallat, StÃ©phane, and de Hoop, Maarten. 2016. Inverse problems with invariant multiscale statistics. ArXiv preprint arXiv:1609.05502. Dong, Chao, Loy, Chen Change, He, Kaiming, and Tang, Xiaoou. 2014. Learning a deep convolutional network for image super-resolution. Pages 184â€“199 of: Proc. European Conference on Computer Vision. Springer. Doukhan, P., Oppenheim, G., and Taqqu, M. (eds.) 2002. Theory and Applications of Long-Range Dependence. Birkhauser. Eickenberg, Michael, Exarchakis, Georgios, Hirn, Matthew, and Mallat, StÃ©phane. 2017. Solid harmonic wavelet scattering: Predicting quantum molecular en- ergy from invariant descriptors of 3D electronic densities. Pages 6540â€“6549 of: Advances in Neural Information Processing Systems. Fei-Fei, L., Fergus, R., and Perona, P. 2004. Learning generative visual models from few training examples: An incremental Bayesian approach tested on 101 object categories. Proc. IEEE Conference on Computer Vision and Pattern Recognition. Felzenszwalb, Pedro F., Girshick, Ross B., McAllester, David, and Ramanan, Deva. 2010. Object detection with discriminatively trained part-based models. Trans. Pattern Analysis and Machine Intelligence, 32(9), 1627â€“1645. Gama, Fernando, Ribeiro, Alejandro, and Bruna, Joan. 2018. Diï¬€usion scattering transforms on graphs. In Proc. International Conference on Learning Repre- sentations. Gama, Fernando, Bruna, Joan, and Ribeiro, Alejandro. 2019. Stability of graph scattering transforms. Pages 8038â€“8048 of: Advances in Neural Information Processing Systems 32. 396 Bruna: The Scattering Transform Gao, Feng, Wolf, Guy, and Hirn, Matthew. 2019. Geometric scattering for graph data analysis. Pages 2122â€“2131 of: Proc. International Conference on Machine Learning. Gatys, Leon, Ecker, Alexander S, and Bethge, Matthias. 2015. Texture synthesis using convolutional neural networks. Pages 262â€“270 of: Advances in Neural Information Processing Systems. Gavish, Matan, Nadler, Boaz, and Coifman, Ronald R. 2010. Multiscale wavelets on trees, graphs and high dimensional data: Theory and applications to semi supervised learning. In: Proc. International Conference on Machine Learning. Geman, Stuart, and Geman, Donald. 1984. Stochastic relaxation, Gibbs distribu- tions, and the Bayesian restoration of images. IEEE Transactions on Pattern Analysis and Machine Intelligence, 721â€“741. Girshick, Ross, Donahue, Jeï¬€, Darrell, Trevor, and Malik, Jitendra. 2014. Rich feature hierarchies for accurate object detection and semantic segmentation. Pages 580â€“587 of: Proc. IEEE Conference on Computer Vision and Pattern Recognition. Goodfellow, Ian, Pouget-Abadie, Jean, Mirza, Mehdi, Xu, Bing, Warde-Farley, David, Ozair, Sherjil, Courville, Aaron, and Bengio, Yoshua. 2014. Genera- tive adversarial nets. Pages 2672â€“2680 of: Advances in Neural Information Processing Systems. Heeger, David J., and Bergen, James R. 1995. Pyramid-based texture analy- sis/synthesis. Pages 229â€“238 of: Proc. 22nd ACM Conference on Computer Graphics and Interactive Techniques. Hirn, Matthew, Mallat, StÃ©phane, and Poilvert, Nicolas. 2017. Wavelet scattering regression of quantum chemical energies. Multiscale Modeling & Simulation, 15(2), 827â€“863. Jacobsen, Jorn-Henrik, van Gemert, Jan, Lou, Zhongyu, and Smeulders, Arnold W. M. 2016. Structured receptive ï¬elds in CNNs. Pages 2610â€“2619 of: Proc. IEEE Conference on Computer Vision and Pattern Recognition. Jacot, Arthur, Gabriel, Franck, and Hongler, ClÃ©ment. 2018. Neural tangent kernel: Convergence and generalization in neural networks. Pages 8571â€“8580 of: Advances in Neural Information Processing Systems. Jain, Anil K., Zhong, Yu, and Lakshmanan, Sridhar. 1996. Object matching using deformable templates. IEEE Transactions on Pattern Analysis and Machine Intelligence, 18(3), 267â€“278. Jin, Kyong Hwan, McCann, Michael T., Froustey, Emmanuel, and Unser, Michael. 2017. Deep convolutional neural network for inverse problems in imaging. IEEE Transactions on Image Processing, 26(9), 4509â€“4522. Keysers, D., Deselaers, T., Gollan, C., and Hey, N. 2007. Deformation models for image recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence. Kingma, Diederik P., and Welling, Max. 2014. Auto-encoding variational Bayes. In: Proc. International Conference on Learning Representations. References 397 Kondor, Risi, and Trivedi, Shubhendu. 2018. On the generalization of equivariance and convolution in neural networks to the action of compact groups. Pages 2747â€“2755 of: Proc. International Conference on Machine Learning. Kostrikov, Ilya, Bruna, Joan, Panozzo, Daniele, and Zorin, Denis. 2018. Surface networks. Pages 2540â€“2548 of: Proc. IEEE Conference on Computer Vision and Pattern Recognition. Leonarduzzi, Roberto, Rochette, Gaspar, Bouchaud, Jean-Phillipe, and Mallat, StÃ©phane. 2019. Maximum-entropy scattering models for ï¬nancial time se- ries. Pages 5496â€“5500 of: proc. IEEE International Conference on Acoustics, Speech and Signal Processing. Mairal, Julien, Koniusz, Piotr, Harchaoui, Zaid, and Schmid, Cordelia. 2014. Con- volutional kernel networks. Pages 2627â€“2635 of: Advances in Neural Infor- mation Processing Systems. Mallat, StÃ©phane. 1999. A Wavelet Tour of Signal Processing, ï¬rst edition. Aca- demic Press. Mallat, StÃ©phane. 2008. A Wavelet Tour of Signal Processing, second edition. Academic Press. Mallat, StÃ©phane. 2012. Group invariant scattering. Communications in Pure and Applied Mathematics, 65(10), 1331â€“1398. Mallat, StÃ©phane. 2016. Understanding deep convolutional networks. Philosophical Transactions of the Royal Society A, 374(2065). Mallat, StÃ©phane, Zhang, Sixin, and Rochette, Gaspar. 2018. Phase harmonics and correlation invariants in convolutional neural networks. ArXiv preprint arXiv:1810.12136. Minh, H., Niyogi, P., and Yao, Y. 2006. Mercerâ€™s Theorem, Feature Maps and Smoothing. In Proc. Computational Learning Theory Conference. Nadler, Boaz, Lafon, StÃ©phane, Coifman, Ronald R., and Kevrekidis, Ioannis G. 2006. Diï¬€usion maps, spectral clustering and reaction coordinates of dynami- cal systems. Applied and Computational Harmonic Analysis, 21(1), 113â€“127. Oyallon, Edouard, and Mallat, StÃ©phane. 2015. Deep roto-translation scattering for object classiï¬cation. Pages 2865â€“2873 of: Proc. IEEE Conference on Computer Vision and Pattern Recognition. Oyallon, Edouard, Belilovsky, Eugene, and Zagoruyko, Sergey. 2017. Scaling the scattering transform: Deep hybrid networks. Pages 5618â€“5627 of: Proc. IEEE International Conference on Computer Vision. Oyallon, Edouard, Belilovsky, Eugene, Zagoruyko, Sergey, and Valko, Michal. 2018a. Compressing the Input for CNNs with the ï¬rst-order scattering trans- form. Pages 301â€“316 of: Proc. European Conference on Computer Vision. Oyallon, Edouard, Zagoruyko, Sergey, Huang, Gabriel, Komodakis, Nikos, Lacoste- Julien, Simon, Blaschko, Matthew B., and Belilovsky, Eugene. 2018b. Scatter- ing networks for hybrid representation learning. IEEE Transactions on Pattern Analysis and Machine Intelligence, 41(9), 2208â€“2221. 398 Bruna: The Scattering Transform Perlmutter, Michael, Wolf, Guy, and Hirn, Matthew. 2018. Geometric scattering on manifolds. ArXiv preprint arXiv:1812.06968. Portilla, Javier, and Simoncelli, Eero P. 2000. A parametric texture model based on joint statistics of complex wavelet coeï¬ƒcients. International Journal of Computer Vision, 40(1), 49â€“70. Rezende, Danilo Jimenez, Mohamed, Shakir, and Wierstra, Daan. 2014. Stochastic backpropagation and variational inference in deep latent Gaussian models. Pages 1278â€“1286 of: Proc. International Conference on Machine Learning. Rustamov, Raif, and Guibas, Leonidas J. 2013. Wavelets on graphs via deep learning. Pages 998â€“1006 of: Advances in Neural Processing Systems 26. Shawe-Taylor, J., and Cristianini, N. 2004. Kernel Methods for Pattern Analysis. Cambridge University Press. Sifre, L., and Mallat, S. 2013. Rotation, scaling and deformation invariant scattering for texture discrimination. In: Proc. IEEE Conference on Computer Vision and Pattern Recognition. Soatto, S. 2009. Actionable information in vision. Proc. Proc. International Conference on Computer Vision. Szegedy, Christian, Zaremba, Wojciech, Sutskever, Ilya, Bruna, Joan, Erhan, Du- mitru, Goodfellow, Ian, and Fergus, Rob. 2014. Intriguing properties of neural networks. In: Proc. International Conference on Learning Representations. Wainwright, Martin J., Jordan, Michael I., et al. 2008. Graphical models, exponen- tial families, and variational inference. Foundations and Trends in Machine Learning, 1(1â€“2), 1â€“305. Waldspurger, I. 2012. Recovering the phase of a complex wavelet transform. CMAP, Ecole Polytechnique, Technical Report. Waldspurger, IrÃ¨ne. 2017. Exponential decay of scattering coeï¬ƒcients. Pages 143â€“ 146 of: Proc. International Conference on Sampling Theory and Applications. Wiatowski, Thomas, and BÃ¶lcskei, Helmut. 2017. A mathematical theory of deep convolutional neural networks for feature extraction. IEEE Transactions on Information Theory, 64(3), 1845â€“1866. Wiatowski, Thomas, Grohs, Philipp, and BÃ¶lcskei, Helmut. 2017. Energy propaga- tion in deep convolutional neural networks. IEEE Transactions on Information Theory, 64(7), 4819â€“4842. Yoshimatsu, K., Schneider, K., Okamoto, N., Kawahura, Y., and Farge, M. 2011. Intermittency and geometrical statistics of three-dimensional homogeneous magnetohydrodynamic turbulence: A wavelet viewpoint. Physics of Plasmas, 18, 092304. Zarka, John, Guth, Florentin, and Mallat, StÃ©phane. 2020. Separation and con- centration in deep networks. In: proc. International Conference on Learning Representations. Zhang, Kai, Zuo, Wangmeng, Gu, Shuhang, and Zhang, Lei. 2017. Learning deep CNN denoiser prior for image restoration. Pages 3929â€“3938 of: Proc. IEEE Conference on Computer Vision and Pattern Recognition. References 399 Zhang, Sixin, and Mallat, Stephane. 2021. Wavelet phase harmonic covariance models of stationary processes. Applied Computational and Harmonic Anal- ysis, 53, 199â€“230. Zhu, Song Chun, Wu, Yingnian, and Mumford, David. 1998. Filters, random ï¬elds and maximum entropy (FRAME): Towards a uniï¬ed theory for texture modeling. International Journal of Computer Vision, 27(2), 107â€“126. Zou, D., and Lerman, G. 2020. Graph convolutional neural networks via scattering. Applied and Computational Harmonic Analysis, 49(3), 1046â€“1074. 9 Deep Generative Models and Inverse Problems Alexandros G. Dimakis Abstract: Deep generative models have been recently proposed as modular data- driven priors to solve inverse problems. Linear inverse problems involve the recon- struction of an unknown signal (e.g. a tomographic image) from an underdetermined system of noisy linear measurements. Most results in the literature require that the reconstructed signal has some known structure, e.g. it is sparse in some known basis (usually Fourier or wavelet). Such prior assumptions can be replaced with pre- trained deep generative models (e.g. generative adversarial networks (GANs) and variational autoencoders (VAEs)) with signiï¬cant performance gains. This chapter surveys this rapidly evolving research area and includes empirical and theoretical results in compressed sensing for deep generative models. 9.1 Introduction In his seminal paper, Claude Shannon (1948) laid out the foundations of information theory by establishing two fundamental theorems: one for the ultimate limits of compressing a source of information and one for communicating reliably over a noisy channel. The implications of these results have since developed into two sub-areas of information theory: the study of compression (called source coding) and the study of protection from noise (called channel coding). Both settings rely critically on probabilistic models: the former for modeling information sources (e.g. images, text, video) of what we want to compress, the latter for modeling noise (e.g. random bit ï¬‚ips, Gaussian noise) or whatever we want to protect against. Channel coding has had a tremendous impact on real-world applications, since all wired and wireless communication systems rely on innovations such as Reedâ€“Solomon codes, turbo codes, LDPC codes and more recently polar codes. In contrast, source coding has inï¬‚uenced the practice of data compression somewhat less, with a few notable exceptions (e.g. for lossless compression). Image, audio and video compression 400 9.2 How to Tame High Dimensions 401 standards have been informed by source coding principles, but the impact of the theory has been arguably smaller than that of channel coding. I attribute this discrepancy to the degree of simplicity of the involved models relative to the physical processes they try to capture. For channel coding, toy mathematical models (e.g. additive white Gaussian noise or the random independent dropping of packets) capture real noisy systems well enough to inform the design of channel codes. For sources, however, modeling images as independent pixels or local Markov chains seems to be insuï¬ƒcient to capture critical aspects of the problem. The central challenge is to invent more expressive models for representing high-dimensional distributions. The diï¬ƒculty, simply put, is that to represent a joint probability distribution over n variables (say pixels of an image), even if each pixel is binary, requires 2n âˆ’1 parameters. Therefore, to eï¬ƒciently represent n dependent variables, some type of structure must be postulated to reduce this exponential complexity. A tremendous volume of research in probability theory, statistical physics, information theory, signal processing and machine learning has been addressing this very question. 9.2 How to Tame High Dimensions Three types of structure are described here. The ï¬rst two are classical and well studied but still many fundamental questions remain open. The third is newer, known empirically to be very powerful, and signiï¬cantly less studied. 9.2.1 Sparsity For the ï¬rst prominent family of models for high-dimensional data it is assumed that the modeled vectors are sparse in a known basis. The well-studied ï¬eld of compressed sensing considers linear inverse problems under sparsity assumptions. The goal is to recover a high-dimensional vector from a small number of linear projections (measurements). If there is no noise in the observed projections, the inverse problem then is to ï¬nd the sparsest vector that satisï¬es a set of given linear equations, a problem that is computationally intractable (NP-complete). If the measurements are noisy, one has to solve a linear regression problem where the number of samples is smaller than the number of parameters but the unknown model is sparse. This is, again, an NP-hard problem and the natural approximation algorithm is to relax the â„“0 sparsity constraint to an â„“1-regularized linear regression called Lasso (Tibshirani, 1996). Remarkably, under natural assumptions for the projections (the measurement matrix, in compressed sensing nomenclature) this â„“1 relaxation provably yields the required sparsest solution. Such conditions include the restricted isometry property 402 Dimakis: Deep Generative Models and Inverse Problems (RIP) or the related restricted eigenvalue condition (REC)(see e.g. Tibshirani, 1996; Candes et al., 2006; Donoho, 2006; Bickel et al., 2009). There is a vast literature on sparsity and high-dimensional problems, including various recovery algorithms, as well as on generalizations of RIP for other vector structures; see e.g. Bickel et al. (2009), Negahban et al. (2009), Agarwal et al. (2010), Loh and Wainwright (2011), Bach et al. (2012), and Baraniuk et al. (2010). Wainwright (2019) provides a comprehensive and rigorous exposition of this area. It should be emphasized that models built on sparsity have been tremendously successful for modeling high-dimensional data such audio, images and video. Spar- sity (e.g. in the discrete cosine transform (DCT) or wavelet domains) plays the central role in most real-world lossy compression standards such as JPEG (Wal- lace, 1992), JPEG-2000 (Marcellin et al., 2000) and MPEG (Manjunath et al., 2002). 9.2.2 Conditional Independence The second prominent family of models postulates that the joint probability distri- bution satisï¬es some conditional independence conditions. This leads to graphical models, factor graphs, Bayesian networks and Markov random ï¬elds. Distributions in these frameworks factorize according to an undirected or directed graph, which represents these conditional independences. For example, fully independent random variables correspond to an empty graph, while Markov chains correspond to path graphs. Markov random ï¬elds and Bayesnets (also called undirected and directed graphical models, respectively) are well-studied families of high-dimensional dis- tributions, and there is an array of tools for performing learning and statistical inference for them; see e.g., Koller et al. (2009), Wainwright et al. (2008), and Jordan (2003). These models are not very frequently used for modeling sources such as images or video, but they have played a central role in communications and channel coding. Low-density parity-check (LDPC) codes are distributions modeled by sparse factor graphs, a type of graphical model (Gallager, 1962; Richardson and Urbanke, 2008). Here, graphical models are used not to describe noise, but rather to design high- dimensional distributions of the codewords used. Belief propagation, the inference algorithm used for decoding LDPC codes, is central for graphical model inference and has been independently discovered in various forms in coding theory (Gallager, 1962) and artiï¬cial intelligence (Pearl, 1982). See also Kschischang et al. (2001) for the remarkable connections between belief propagation, sum-product, the Viterbi algorithm and the fast Fourier transform (FFT). Also, belief propagation can be seen as a form of approximate variational inference; see Wainwright et al. (2008), Blei et al. (2017). 9.2 How to Tame High Dimensions 403 z âˆˆRk G(z) âˆˆRn feed-forward neural net with parameters w Figure 9.1 A deep generative model is a function G(z) that takes a low-dimensional random vector z in Rk and produces a high-dimensional sample G(z) âˆˆRn. The function is usually a convolutional neural network and is therefore continuous and diï¬€erentiable almost everywhere. For invertible generative models, the dimension of the latent code is the same as the ambient dimension, i.e. k = n and an explicit likelihood can be eï¬ƒciently computed for each given output. 9.2.3 Deep Generative Models The central tool considered in this chapter is the third family of models for high- dimensional distributions, which we call deep generative models. These models do not rely on conditional independence or sparsity, but rather are described by a deterministic function G(z) from a low-dimensional space Rk to Rn. This function is called a generator or decoder and is in practice represented by a feed-forward neural network with weights w that are trained from data. To obtain a probability distribution over the high-dimensional ambient space Rn, one can seed the generator with a random latent vector z, usually drawn from a zero-mean isotropic Gaussian, N(0, IkÃ—k). Remarkably, these generator functions can be trained to transform simple randomness into highly structured randomness and to capture natural signals with unprecedented quality. We group deep generative models into the following categories: (i) Generative adversarial networks (GANs); (ii) Variational auto-encoders (VAEs); (iii) Invertible generative models (also called normalizing ï¬‚ows); (iv) Untrained generative models. We emphasize that compressed sensing and inverse problems on general man- ifolds have been studied in great depth previously; see e.g. Hegde et al. (2008), Baraniuk and Wakin (2009), and Eftekhari and Wakin (2015). Also, reconstruction algorithms that exploit structure beyond sparsity have been investigated as model- based compressed sensing (Baraniuk et al., 2010). What is new here is the speciï¬c 404 Dimakis: Deep Generative Models and Inverse Problems parametrization of these models using feed-forward neural networks and the novel methods used for training these generators. 9.2.4 GANs and VAEs To train a deep generative model, one optimizes its parameters w in an unsuper- vised way, given samples from the true data distribution. The goal is that the output distribution produced by the neural network is close to the observed data distribu- tion. Depending on how one measures distance between distributions and how one approximates the solution of this optimization problem, we are led into diï¬€erent types of deep generative models. Two primary examples are variational auto-encoders (VAEs) (Kingma and Well- ing, 2013), which are trained using variational inference techniques, and generative adversarial networks (GANs) (Goodfellow et al., 2014), which are trained by deï¬n- ing a zero-sum game between a player who is setting the parameters w of Gw, and another player who is setting the parameters of another neural network, called the discriminator. The goal of the latter is to set the parameters of the discriminator network in such a way that this network distinguishes as well as possible between samples generated by the generator and samples generated by the true distribution. The goal of the former is to fool the discriminator. Empirically, deep generative mod- els have shown impressive performance in generating complex high-dimensional signals; e.g. BigGAN can produce near-photorealistic images from Imagenet classes (Brock et al., 2019). Understanding deep generative models and improving their performance has received a great deal of attention across many disciplines including theoretical computer science, machine learning, statistics and information theory. One of the major challenges in training GANs, is a phenomenon called mode collapse, which refers to a lack of diversity of the generated samples: the generative model essentially overï¬ts to a subset of the training data; see e.g.Metz et al. (2017), and Arora et al. (2018). Many approaches have been proposed to prevent mode collapse that include diï¬€erent ways of training GANs, such as BiGAN (Donahue et al., 2016) and adversarially learned nference, ALI, (Dumoulin et al., 2016), Wasserstein loss training (Arjovsky et al., 2017; Gulrajani et al., 2017; Sanjabi et al., 2018), Blackwell-inspired packing (Lin et al., 2017) and ideas from linear quadratic control (Feizi et al., 2017). Signiï¬cant challenges remain in understanding theoretically what distributions GANs actually learn; see e.g. Arora et al. (2017a,b, 2018) and Liu et al. (2017). These deep generative models (including VAEs and GANs) are called implicit since they do not provide tractable access to likelihoods (Mohamed and Laksh- minarayanan, 2016; HuszÃ¡r, 2017). Instead, implicit generative models are easy 9.2 How to Tame High Dimensions 405 to sample from, and more importantly to compute subgradients of samples with respect to model parameters. The optimization algorithms used for training and inference are crucially using subgradients with respect to these model parameters. 9.2.5 Invertible Generative Models Another family of deep generative models that is attracting signiï¬cant interest is invertible generative models or normalizing ï¬‚ows; see, e.g., Dinh et al. (2016), Gomez et al. (2017), and Kingma and Dhariwal (2018). In these models, the dimension of the latent code z is the same as the ambient dimension, i.e. k = n. As before, the high-dimensional distribution is created by transforming a simple Gaussian z by means of a complex transformation parametrized by a diï¬€erentiable neural network. The additional constraint is that the transformation G(z) must be invertible. Further, for some of these models, called normalizing ï¬‚ows, it is tractable to compute the Jacobian of the transformation and hence easily to compute likelihoods, for any given point G(z) âˆˆRn. These models do not represent data in a lower-dimensional space and hence do not oï¬€er dimensionality reduction. However, they have the advantage that an explicit likelihood can be easily computed for any given data point. Furthermore, every generated high-dimensional sample G(z) corresponds to a unique z which can be eï¬ƒciently computed, since the generative mapping is invertible by construc- tion. Invertible generative models oï¬€er several advantages: likelihoods make these models easy to use for inverse problems, circumventing many of the diï¬ƒculties we will be discussing for other models. Furthermore, there are recent invertible models such as Glow (Kingma and Dhariwal, 2018) that achieve excellent image quality for human faces and also generalize very well into other natural images. Asim et al. (2019) proposed the use of invertible generative models for solving inverse problems and showed excellent performance using Glow that outperformed GANs that were introduced for inverse problems by Bora et al. (2017). One shortcoming of current invertible generative models is the high training eï¬€ort required. The Glow model (Kingma and Dhariwal, 2018) was trained on 256Ã—256 human faces using 40 GPUs for two weeks, which is signiï¬cantly more compared with GANs that can now achieve higher resolution generation for more diverse datasets; see Odena (2019). Signiï¬cant ongoing work is currently progressing the state of the art for both GANs and Invertible models, so this landscape is changing. 9.2.6 Untrained Generative Models The ï¬nal family that we discuss involves what are called untrained generative models. These are deep neural prior distributions that can be used to solve inverse 406 Dimakis: Deep Generative Models and Inverse Problems problems even without a large dataset and a pre-trained generative model. Deep image prior (DIP) (Ulyanov et al., 2017) was the ï¬rst such model; it was initially proposed for denoising and inpainting but it can be easily extended to solve gen- eral inverse problems (Van Veen et al., 2018). We also mention a closely related technique called the deep decoder, which was proposed in Heckel and Hand (2018). In DIP-based schemes, a convolutional deep generative model (e.g. DCGAN) is initialized with random weights; these weights are subsequently optimized to make the network produce an output as close to the target image as possible. In the general case any inverse problem with a diï¬€erentiable forward operator (Van Veen et al., 2018), the weights are updated through gradient descent to explain the measurements. This procedure is applied directly into one image, using no prior information. The prior is enforced only by the ï¬xed convolutional structure of the generator neural network. Generators used in DIP are typically1 over-parameterized , i.e. the number of network weights is much larger compared to the output dimension. For this reason DIP can overï¬t to noise if run for too many iterations or not carefully regularized (see Ulyanov et al., 2017; Van Veen et al., 2018). Therefore, the design eï¬€ort lies in designing the architecture and also using early stopping and other regularization techniques as we will discuss. We continue with showing how all these generative models can be used to solve inverse problems. 9.3 Linear Inverse Problems Using Deep Generative Models We will start with a linear inverse problem, i.e. reconstructing an unknown vector xâˆ—âˆˆRn after observing m < n linear measurements, y of its entries, possibly with added noise: y = Axâˆ—+ e. Here A âˆˆRmÃ—n is the measurement matrix and e âˆˆRm is the noise. Even with- out the noise term, this is an underdetermined system of linear equations with inï¬nitely many solutions that explain the measurements. We must therefore make a modeling assumption for the unknown vector, and so the problem of modeling high-dimensional data appears. The ï¬eld of compressed sensing started with the structural assumption that the vector xâˆ—is k-sparse in some known basis (or approximately k-sparse). Finding the sparsest solution to an underdetermined system of linear equations is NP-hard, but nevertheless, convex optimization can provably recover the true sparse vector xâˆ—if the matrix A satisï¬es conditions such as the restricted isometry property (RIP) or 1 For a non-overparametrized untrained generative model, see Heckel and Hand (2018). 9.3 Linear Inverse Problems Using Deep Generative Models 407 the related restricted eigenvalue condition (REC) (Tibshirani, 1996; Candes et al., 2006; Donoho, 2006; Bickel et al., 2009). The key idea is to change this structural assumption and model the unknown vector xâˆ—as being in (or near) the range of a pre-trained deep generative model. This is called the CSGM (compressed sensing with generative models) framework proposed in Bora et al. (2017), and assumes that there exists a hidden latent vector zâˆ—such that: xâˆ—= G(zâˆ—). We know the measurement matrix A, the measurement vector y = A G(zâˆ—) and want to reconstruct zâˆ—or equivalently xâˆ—= G(zâˆ—). The deep generative model G(z) is assumed to be known and ï¬xed. The CSGM algorithm searches the latent space for explaining the measurements, i.e. solves the following minimization problem with gradient descent: min z âˆ¥A G(z) âˆ’yâˆ¥2. (9.1) In words, we are searching for a (low-dimensional) vector z such that the gener- ated image G(z), after it has been measured using A, agrees with the observed measurements y. The optimization problem (9.1) is non-convex and further, for generative models with four or more layers, is known to be NP-hard (Lei et al., 2019). One natural algo- rithm starts with a random initial vector z0, evaluates the gradient of the loss function with respect to z using back-propagation, and uses standard gradient-descent-based optimizers in the latent z space. If the optimization procedure terminates at Ë†z, our reconstruction for xâˆ—is G(Ë†z). We deï¬ne the measurement error to be âˆ¥AG(Ë†z) âˆ’yâˆ¥2 and the reconstruction error to be âˆ¥G(Ë†z) âˆ’xâˆ—âˆ¥2. Empirically, gradient descent performs very well in solving problem (9.1), even from random initializations. In simple experiments with a known ground truth, gradient descent very frequently found the global optimum of this non-convex problem in a few hundred iterations. Furthermore, Bora et al. (2017) showed that this method can signiï¬cantly outperform the standard reconstruction algorithms that rely on sparsity (in DCT or a wavelet domain) for natural images. An example of reconstructions using CSGM versus Lasso is shown in Figure 9.3. 9.3.1 Reconstruction from Gaussian Measurements Following a standard benchmark in compressed sensing, we create an linear inverse problem that involves making A to be a matrix with random independent Gaussian entries with zero mean and standard deviation 1/m. Each entry of the noise vector 408 Dimakis: Deep Generative Models and Inverse Problems 20 50 100 200 500 1000 2500 5000 7500 10000 Number of measurements 0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35 Reconstruction error (per pixel) Lasso (DCT) Lasso (Wavelet) DCGAN DCGAN+Reg Figure 9.2 A comparison of the performance of CSGM using a DCGAN prior with baselines that rely on sparsity. The plot shows the per-pixel reconstruction error as the number of measurements is varied. The vertical bars indicate 95% conï¬dence intervals. Figure taken from Bora et al. (2017). Î· is also i.i.d. Gaussian. We compared the performance of diï¬€erent sensing algo- rithms qualitatively and quantitatively. For quantitative comparison, we used the reconstruction error âˆ¥Ë†x âˆ’xâˆ—âˆ¥2, where Ë†x is the reconstructed signal estimate of xâˆ—. In all cases the results were shown on a held-out test set, unseen by the generative model at training time. In these experiments, the optimization problem (9.1) was solved using the Adam optimizer (Kingma and Ba, 2014), with a learning rate of 0.1 and a few random restarts. In Figure 9.2, we show the reconstruction error as the number of mea- surements both for Lasso and for CSGM is varied. As can be seen, CSGM with a DCGAN prior (for k = 100) produces reasonable reconstructions with as few as 500 measurements, while the output of the baseline algorithms is quite blurry. If the number of measurements increases, the beneï¬ts of the stronger prior diminish and CSGM performance saturates. For more than 5000 measurements, Lasso gives a better reconstruction since the generative model has produced the best approxi- mation in its range and further measurements no longer improve performance. In this regime, a more powerful generative model is needed, possibly increasing the latent code dimension k > 100 to obtain better performance in a high-measurement regime. 9.3 Linear Inverse Problems Using Deep Generative Models 409 Figure 9.3 Reconstruction results using m = 500 measurements (of n = 12, 288 dimensional human face images). We show original images (top row), reconstructions by Lasso with DCT basis (second row), Lasso with wavelet basis (third row), and our algorithm (last row). Here the measurement process was performed by taking A to be a random matrix with IID Gaussian entries with zero mean and standard deviation of 1/m, a standard compressed sensing measurement process. The generative model used in our algorithm was trained on CelebA, a human face dataset, but all the reported results are on images from a hold-out test set and hence were unseen by the generative model at training time. Figure from Bora et al. (2017). 9.3.2 Optimization Challenges From a theoretical standpoint, a key complication is the loss of information due to projection onto the rows of A. The work of Bora et al. (2017) obtained conditions for the measurement matrix under which a minimizer Ë†z of (9.1) guarantees that G(Ë†z) is itself as close as possible to the true unknown signal xâˆ—. The obtained condition generalizes the restricted egenvalue condition (REC) (Bickel et al., 2009; Negahban et al., 2009; Agarwal et al., 2010) developed for classical compressed sensing. Speciï¬cally, Bora et al. (2017) deï¬ned an REC for vectors generated by a generative model and showed that a matrix with random Gaussian entries will satisfy this property if the number of measurements scales as O(k log L). Here, L is the Lipschitz constant of the generative model, which can be easily bounded by a quantity that scales exponentially in the depth of the network and in the maximum neuron weight values. This framework analyzes the exact solution to the minimization of (9.1), while in practice this was approximated using gradient descent. Subsequently, Hand and Voroninski (2017) obtained theoretical guarantees for the performance of gradient descent for non-convex problem under the assumption that the generative model has weights w that are random and independent. The surprising result is that gradient descent will provably solve this non-convex optimization problem if the generative model has suï¬ƒciently expansion, i.e. every layer is approximately a log k factor larger than the previous layer. Daskalakis et al. (2020) improved this result and 410 Dimakis: Deep Generative Models and Inverse Problems demonstrated similar guarantees with only a constant factor expansion in the layer sizes. We also note that some recent works have considered diï¬€erent reconstruction algorithms, beyond gradient descent, for projecting onto the range of a generative model. We mention Lei et al. (2019), who invert a generative model one layer after another, and also some other work (Fletcher et al., 2018; Pandit et al., 2019) in which was proposed an approximate message-passing (AMP) inference algorithm for these inference problems. Most real generators do not have expansive layers (especially in the ï¬nal layers), so theoretically extending the result (Hand and Voroninski, 2017) for non-expansive architectures remains as an important mathematical open prob- lem. In addition, relaxing the random generator weight assumption and analyzing diï¬€erent measurement matrices remain to be investigated. 9.3.3 Extending the Range of the Generator Perhaps the central limitation of using low-dimensional deep priors (e.g. GANs and VAEs) for inverse problems is that the low dimensionality of their latent space impedes the reconstruction of images that lie outside their generated manifold. To mitigate this issue, one natural idea is to model the ï¬nal image as the sum of an image in the range of a generator plus a sparse deviation (Dhar et al., 2018). This idea showed signiï¬cant performance gains over CSGM and has subsequently been generalized to internal representations of the generator which were allowed to deviate from their realizable values through a technique called intermediate layer optimization (ILO) (Daras et al., 2021); see Figures 9.4 and 9.5. This technqiue extends the set of signals that can be reconstructed by allowing sparse deviations from the range of intermediate layers of the generator. Correctly regularizing inter- mediate layers is crucial when solving inverse problems, to avoid overï¬tting to the measurements (Daras et al., 2021). 9.3.4 Non-Linear Inverse Problems The ï¬nal research direction that I would like to mention involves extending the use of deep generative priors to non-linear measurements. In this case, the unknown true vector xâˆ—âˆˆRn is passed through a non-linear measurement operator A: Rn â†’Rm (also called the forward operator). The measurements y âˆˆRm are then observed, possibly with added noise: y = A[xâˆ—] + e. If the forward operator A is diï¬€erentiable then the previous method can be applied: deï¬ne the loss function loss(z) = âˆ¥A[G(z)]âˆ’yâˆ¥2 and minimize it iteratively 9.3 Linear Inverse Problems Using Deep Generative Models 411 Original Observation CSGM MSE (PULSE) CSGM LPIPS CSGM LPIPS + MSE Ours Figure 9.4 Results on the inpainting task. Rows 1, 2, 3 and 5 are real images (outside of the test set, collected from the web) while rows 4 and 6 are StyleGAN-2 generated images. Column 2: the ï¬rst ï¬ve images have masks that were chosen to remove important facial features. The last row is an example of randomized inpainting, i.e. a random 1% of the total pixels is observed. Columns 3â€“5: reconstructions using the CSGM (Bora et al., 2017) algorithm with the StyleGAN-2 generator and the optimization setting described in PULSE (Menon et al., 2020). While PULSE only applies to super-resolution, it was extended using MSE, LPIPS and a joint MSE + LPIPS loss.. The experiments of Columns 3â€“5 form an ablation study of the beneï¬ts of each loss function. Column 6: reconstructions with ILO (ours). As shown, ILO consistently gives better reconstructions of the original image. Also, many biased reconstructions can be corrected with ILO. In the last two rows, recovery of the image is still possible from very few pixel observations using our method. Figure from Daras et al. (2021). using gradient descent or some other optimization method. A useful non-linear inverse problem is phase retrieval (Candes et al., 2015a,b) and the generative prior framework has been extended to this problem (Hand et al., 2018). 412 Dimakis: Deep Generative Models and Inverse Problems Figure 9.5 Results on super-resolution using intermediate layer Optimization (ILO) (with a StyleGAN2 prior) versus the highly inï¬‚uential PULSE baseline (also using the StyleGAN2 prior and the CSGM framework). As shown, ILO can signiï¬cantly mitigate biases in reconstructions. It should be emphasized that both PULSE and ILO use the same training prior (i.e. the same training data) and there is no need for a characterization involving biases that must be corrected. ILO simply makes the reconstruction algorithm more ï¬‚exible and less reliant on its prior. Figure from Daras et al. (2021). 9.3.5 Inverse Problems with Untrained Generative Priors As discussed above, deep generative models are very powerful priors since they can capture various nuances of real signals. However, training a deep generative model for some types of data is diï¬ƒcult since it requires a large number of examples and careful neural network design. While this has been achieved for various types of im- ages, e.g. the human faces of CelebA (Liu et al., 2015) via DCGAN (Radford et al., 2015), it remains challenging for other datasets, e.g. medical images (Wolterink et al., 2017; Schlegl et al., 2017; Nie et al., 2017; Schlemper et al., 2017). Untrained generative models provide a way of circumventing this problem. 9.3 Linear Inverse Problems Using Deep Generative Models 413 Ulyanov et al. (2017) proposed a deep image prior (DIP), which uses untrained convolutional neural networks to solve various imaging problems such as denoising and inpainting. Further, Van Veen et al. (2018) generalized this to any inverse problem with a diï¬€erentiable forward operator. Assume, as before, that some measurements of an unknown vector xâˆ—âˆˆRn are observed through a diï¬€erentiable forward operator A: Rn â†’Rm to obtain measurements y âˆˆRm: y = A[xâˆ—] + e. In contrast with the previous discussion, we do not assume that we have some good generative model for xâˆ—. Instead, we start with a deep generative convolutional network (e.g. DCGAN topology) and initialize it with random weights w0. We also select some random input noise z0 âˆˆRk. Clearly, the signal generated with this random input and random weights Gw0(z0) will have nothing to do with xâˆ—. The DIP approach is to minimize over weights w, to ï¬nd some that explain the measurements while keeping the input ï¬xed at z0. In other words, we aim to optimize over w so that A[Gwâˆ—(z0)] matches the measurements y. Hence, CS-DIP (Van Veen et al., 2018) solves the following optimization problem: wâˆ—= argmin w âˆ¥y âˆ’AGw(z0)âˆ¥2. (9.2) This is, of course, a non-convex problem because Gw(z0) is a non-convex function of w. Still, we can use gradient-based optimizers for any generative model and measurement process that is diï¬€erentiable. Remarkably, this procedure, if stopped early produces natural signals, due to the convolutional structure of the generator G. The convolutional network structure alone, provides a good prior for reconstructing images (Ulyanov et al., 2017). In CS-DIP (Van Veen et al., 2018) various simple regularization techniques were also deployed, including total variation (Rudin et al., 1992; Liu et al., 2018) and hence the ï¬nal optimization problem becomes: wâˆ—= argmin w âˆ¥y âˆ’A Gw(z0)âˆ¥2 + R(w). (9.3) The speciï¬c design of the regularizer can range from total variation (Liu et al., 2018) to a combination of data-driven regularizers (Van Veen et al., 2018) and early stopping (see also Gilton et al., 2019). These techniques improve the performance, but even without any of them, DIP priors are surprisingly good models for natural images. Van Veen et al. (2018) established theoretically that DIP can ï¬t any measurements to zero error with gradient descent, in the over-parametrized regime for a single layer under technical conditions. It is expected that over-parametrized neural networks can ï¬t any signal, but the fact that gradient descent can provably solve this non-convex 414 Dimakis: Deep Generative Models and Inverse Problems problem to a global optimum is interesting and provides theoretical justiï¬cation for early stopping. Empirically, CS-DIP outperforms other untrained methods in many cases. Figure 9.6 shows that CS-DIP achieves good reconstruction results for a retinopathy inverse problem where no known generative models are available. It should be emphasized that methods that leverage pre-trained deep generative models perform better (Bora et al., 2017; Asim et al., 2019), but require large datasets of ground truth images. 512x512 - full 31.99 16.70 12.39 Original 128x128 - crop Ours 37.27 Lasso - DCT 21.91 Lasso - DB4 13.02 Figure 9.6 Reconstruction of retinopathy medical images from random Gaussian measurements using CS-DIP. This was done by performing compressed sensing using untrained convolutional neural networks. In CS-DIP, a convolutional neural network generator (DCGAN) is initialized with random weights which are subsequently optimized using gradient descent to make the network produce an output that explains the measurements as well as possible. The values in the lower right of the reconstructed images denote the PSNR (dB). Figure from Van Veen et al. (2018). 9.4 Supervised Methods for Inverse Problems Finally, I mention brieï¬‚y another large family of deep learning techniques for solving inverse problems: supervised methods. If a large dataset of paired measurements and reconstructions is available, a supervised method simply trains an inversion network to go from measurements to reconstructions. Numerous optimization and regularization ideas have been proposed that build on this fundamental framework. With enough training data, end-to-end supervised methods frequently outperform the unsupervised methods discussed in this chapter; see Tian et al. (2020), Sun et al. (2020), and Tripathi et al. (2018) for denoising; Sun and Chen (2020) and 9.4 Supervised Methods for Inverse Problems 415 Yang et al. (2019) for super-resolution; and Yu et al. (2019), Liu et al. (2019) for inpainting. The main disadvantages of solving inverse problems with supervised methods are: (i) separate training is required for each forward problem; (ii) there is sig- niï¬cant fragility to forward operator changes (robustness issues) (Darestani et al., 2021; Ongie et al., 2020); and (iii) it is much easier to modify the loss function or the forward operator in an interactive way for unsupervised techniques. This makes it possible for experts to adaptively modify reconstructed imaging parameters as desired, something frequently done in practice. I would also like to point out the re- cent taxonomy of deep learning techniques for inverse problems (Ongie et al., 2020) that organizes the signiï¬cant volume of deep learning methods into supervised and unsupervised methods, depending on the setting. Conclusions and Outlook I have argued that deep generative models are a third important family of high- dimensional priors, in addition to sparsity and graphical models. The data-driven nature of these priors allows them to leverage massive training datasets and hence to outperform simpler models. This also leads to numerous theoretical problems to do with learning and inference which take new mathematical forms for feed-forward neural networks priors. Further, inverse problems can be solved even with untrained neural networks (e.g. with deep image prior or the deep decoder) which are suitable for inverse problems when very little training data are available. As a ï¬nal note, I would like to mention the rapid progress in designing and training deep generative models. The new family of score-based generative models (Song et al., 2021) and the probabilistic models closely related to diï¬€usion (Sohl- Dickstein et al., 2015; Ho et al., 2020) are demonstrating remarkable performance in terms of generation, ease of training and use in inverse problems even as I ï¬nalize this chapter. I would like to emphasize one signiï¬cant beneï¬t of the presented unsupervised framework for solving inverse problems: that of modularity. One can download pre-trained generators (score-based generators, GANs or any other feed-forward generator), apply the techniques discussed in this chapter, and get increasingly better results. The massive dataset and computational resources used to train the generators are easily leveraged for solving all types of inverse problems by separating the forward operator from the natural signal modeling problem. Owing to this modularity, I expect that deep generative models will have a signiï¬cant role for numerous inverse problems, as powerful data-driven priors of natural signals. 416 Dimakis: Deep Generative Models and Inverse Problems Acknowledgements and Funding Information This research has been supported by NSF Grants CCF 1934932, AF 1901292, 2008710, 2019844, and the NSF IFML 2019844 award, as well as research gifts from Western Digital, Interdigital, WNCG and MLL, and computing resources from TACC and the Archie Straiton Fellowship. References Agarwal, Alekh, Negahban, Sahand, and Wainwright, Martin J. 2010. Fast global convergence rates of gradient methods for high-dimensional statistical recov- ery. Pages 37â€“45 of: Advances in Neural Information Processing Systems. Arjovsky, Martin, Chintala, Soumith, and Bottou, LÃ©on. 2017. Wasserstein GAN. ArXiv preprint arXiv:1701.07875. Arora, Sanjeev, Ge, Rong, Liang, Yingyu, Ma, Tengyu, and Zhang, Yi. 2017a. Generalization and equilibrium in generative adversarial nets (GANs). In: Proc. 34th International Conference on Machine Learning. Arora, Sanjeev, Risteski, Andrej, and Zhang, Yi. 2017b. Theoretical limitations of encoderâ€“decoder GAN architectures. ArXiv preprint arXiv:1711.02651. Arora, Sanjeev, Risteski, Andrej, and Zhang, Yi. 2018. Do GANs learn the dis- tribution? Some theory and empirics. In: Proc. International Conference on Learning Representations. Asim, Muhammad, Ahmed, Ali, and Hand, Paul. 2019. Invertible generative models for inverse problems: Mitigating representation error and dataset bias. ArXiv preprint arXiv:1905.11672. Bach, Francis, Jenatton, Rodolphe, Mairal, Julien, Obozinski, Guillaume, et al. 2012. Optimization with sparsity-inducing penalties. Foundations and Trends in Machine Learning, 4(1), 1â€“106. Baraniuk, Richard G., and Wakin, Michael B. 2009. Random projections of smooth manifolds. Foundations of Computational Mathematics, 9(1), 51â€“77. Baraniuk, Richard G., Cevher, Volkan, Duarte, Marco F., and Hegde, Chinmay. 2010. Model-based compressive sensing. IEEE Transactions on Information Theory, 56(4), 1982â€“2001. Bickel, Peter J., Ritov, Yaâ€™cov, Tsybakov, Alexandre B., et al. 2009. Simultaneous analysis of Lasso and Dantzig selector. Annals of Statistics, 37(4), 1705â€“1732. Blei, David M., Kucukelbir, Alp, and McAuliï¬€e, Jon D. 2017. Variational inference: A review for statisticians. Journal of the American Statistical Association, 112(518), 859â€“877. Bora, Ashish, Jalal, Ajil, Price, Eric, and Dimakis, Alexandros G. 2017. Com- pressed sensing using generative models. In: Proc. International Conference on Machine Learning. Brock, Andrew, Donahue, Jeï¬€, and Simonyan, Karen. 2019. Large scale GAN train- ing for high ï¬delity natural image synthesis. ArXiv preprint arXiv:1809.11096 References 417 Candes, Emmanuel J., Romberg, Justin K., and Tao, Terence. 2006. Stable signal recovery from incomplete and inaccurate measurements. Communications on Pure and Applied Mathematics, 59(8), 1207â€“1223. Candes, Emmanuel J., Eldar, Yonina C., Strohmer, Thomas, and Voroninski, Vladislav. 2015a. Phase retrieval via matrix completion. SIAM Review, 57(2), 225â€“251. Candes, Emmanuel J., Li, Xiaodong, and Soltanolkotabi, Mahdi. 2015b. Phase retrieval via Wirtinger ï¬‚ow: Theory and algorithms. IEEE Transactions on Information Theory, 61(4), 1985â€“2007. Daras, Giannis, Dean, Joseph, Jalal, Ajil, and Dimakis, Alexandros G. 2021. Inter- mediate layer optimization for inverse problems using deep generative models. In: Proc. International Conference on Machine Learning. Darestani, Mohammad Zalbagi, Chaudhari, Akshay, and Heckel, Reinhard. 2021. Measuring robustness in deep learning based compressive sensing. ArXiv preprint arXiv:2102.06103. Daskalakis, Constantinos, Rohatgi, Dhruv, and Zampetakis, Emmanouil. 2020. Constant-expansion suï¬ƒces for compressed sensing with generative priors. Pages 13917â€“13926 of: Advances in Neural Information Processing Systems, 33. Dhar, Manik, Grover, Aditya, and Ermon, Stefano. 2018. Modeling sparse devia- tions for compressed sensing using generative models. Pages 1214â€“1223 of: Proc. International Conference on Machine Learning. Dinh, Laurent, Sohl-Dickstein, Jascha, and Bengio, Samy. 2016. Density estimation using real NVP. ArXiv preprint arXiv:1605.08803. Donahue, Jeï¬€, KrÃ¤henbÃ¼hl, Philipp, and Darrell, Trevor. 2016. Adversarial feature learning. ArXiv preprint arXiv:1605.09782. Donoho, David L. 2006. Compressed sensing. IEEE Transactions on Information Theory, 52(4), 1289â€“1306. Dumoulin, Vincent, Belghazi, Ishmael, Poole, Ben, Lamb, Alex, Arjovsky, Mar- tin, Mastropietro, Olivier, and Courville, Aaron. 2016. Adversarially learned inference. ArXiv preprint arXiv:1606.00704. Eftekhari, Armin, and Wakin, Michael B. 2015. New analysis of manifold em- beddings and signal recovery from compressive measurements. Applied and Computational Harmonic Analysis, 39(1), 67â€“109. Feizi, Soheil, Suh, Changho, Xia, Fei, and Tse, David. 2017. Understanding GANs: The LQG setting. ArXiv preprint arXiv:1710.10793. Fletcher, Alyson K., Rangan, Sundeep, and Schniter, Philip. 2018. Inference in deep networks in high dimensions. Pages 1884â€“1888 of: Proc. IEEE International Symposium on Information Theory. IEEE. Gallager, Robert. 1962. Low-density parity-check codes. IRE Transactions on Information Theory, 8(1), 21â€“28. Gilton, Davis, Ongie, Greg, and Willett, Rebecca. 2019. Neumann networks for inverse problems in imaging. ArXiv preprint arXiv:1901.03707. 418 Dimakis: Deep Generative Models and Inverse Problems Gomez, Aidan N., Ren, Mengye, Urtasun, Raquel, and Grosse, Roger B. 2017. The reversible residual network: Backpropagation without storing activations. Pages 2214â€“2224 of: Advances in Neural Information Processing Systems. Goodfellow, Ian, Pouget-Abadie, Jean, Mirza, Mehdi, Xu, Bing, Warde-Farley, David, Ozair, Sherjil, Courville, Aaron, and Bengio, Yoshua. 2014. Genera- tive adversarial nets. Pages 2672â€“2680 of: Advances in Neural Information Processing Systems. Gulrajani, Ishaan, Ahmed, Faruk, Arjovsky, Martin, Dumoulin, Vincent, and Courville, Aaron C. 2017. Improved training of Wasserstein GANs. Pages 5769â€“5779 of: Advances in Neural Information Processing Systems. Hand, Paul, and Voroninski, Vladislav. 2017. Global guarantees for enforcing deep generative priors by empirical risk. ArXiv preprint arXiv:1705.07576. Hand, Paul, Leong, Oscar, and Voroninski, Vlad. 2018. Phase retrieval under a generative prior. Pages 9136â€“9146 of: Advances in Neural Information Processing Systems. Heckel, Reinhard, and Hand, Paul. 2018. Deep decoder: Concise image rep- resentations from untrained non-convolutional networks. ArXiv preprint arXiv:1810.03982. Hegde, Chinmay, Wakin, Michael, and Baraniuk, Richard. 2008. Random projec- tions for manifold learning. Pages 641â€“648 of: Advances in Neural Information Processing Systems. Ho, Jonathan, Jain, Ajay, and Abbeel, Pieter. 2020. Denoising diï¬€usion probabilistic models. ArXiv preprint arXiv:2006.11239. HuszÃ¡r, Ferenc. 2017. Variational inference using implicit distributions. ArXiv preprint arXiv:1702.08235. Jordan, Michael I. 2003. An Introduction to Probabilistic Graphical Models. https://people.eecs.berkeley.edu/~jordan/prelims/ Kingma, Diederik, and Ba, Jimmy. 2014. Adam: A method for stochastic optimiza- tion. ArXiv preprint arXiv:1412.6980. Kingma, Diederik P., and Welling, Max. 2013. Auto-encoding variational bayes. ArXiv preprint arXiv:1312.6114. Kingma, Durk P., and Dhariwal, Prafulla. 2018. Glow: Generative ï¬‚ow with in- vertible 1 Ã— 1 convolutions. Pages 10215â€“10224 of: Advances in Neural Information Processing Systems. Koller, Daphne, Friedman, Nir, and Bach, Francis. 2009. Probabilistic Graphical Models: Principles and Techniques. MIT Press. Kschischang, Frank R., Frey, Brendan J., Loeliger, Hans-Andrea, et al. 2001. Factor graphs and the sumâ€“product algorithm. IEEE Transactions on Information Theory, 47(2), 498â€“519. Lei, Qi, Jalal, Ajil, Dhillon, Inderjit S., and Dimakis, Alexandros G. 2019. Inverting deep generative models, one layer at a time. ArXiv preprint arXiv:1906.07437. References 419 Lin, Zinan, Khetan, Ashish, Fanti, Giulia, and Oh, Sewoong. 2017. PacGAN: The power of two samples in generative adversarial networks. ArXiv preprint arXiv:1712.04086. Liu, Hongyu, Jiang, Bin, Xiao, Yi, and Yang, Chao. 2019. Coherent semantic attention for image inpainting. Proc. International Conference on Computer Vision. Liu, Jiaming, Sun, Yu, Xu, Xiaojian, and Kamilov, Ulugbek S. 2018. Image restoration using total variation regularized deep image prior. ArXiv preprint arXiv:1810.12864. Liu, Shuang, Bousquet, Olivier, and Chaudhuri, Kamalika. 2017. Approximation and convergence properties of generative adversarial learning. Pages 5551â€“ 5559 of: Advances in Neural Information Processing Systems. Liu, Ziwei, Luo, Ping, Wang, Xiaogang, and Tang, Xiaoou. 2015 (December). Deep learning face attributes in the wild. In: Proc. International Conference on Computer Vision. Loh, Po-Ling, and Wainwright, Martin J. 2011. High-dimensional regression with noisy and missing data: Provable guarantees with non-convexity. Pages 2726â€“ 2734 of: Advances in Neural Information Processing Systems. Manjunath, Bangalore S., Salembier, Philippe, and Sikora, Thomas. 2002. Intro- duction to MPEG-7: Multimedia Content Description Interface. Vol. 1. John Wiley & Sons. Marcellin, Michael W., Gormish, Michael J., Bilgin, Ali, and Boliek, Martin P. 2000. An overview of JPEG-2000. Pages 523â€“541 of: Proc. Data Compression Conference. Menon, Sachit, Damian, Alexandru, Hu, Shijia, Ravi, Nikhil, and Rudin, Cynthia. 2020. PULSE: Self-supervised photo upsampling via latent space exploration of generative models. Proc. Conference on Computer Vision and Pattern Recognition. Metz, Luke, Poole, Ben, Pfau, David, and Sohl-Dickstein, Jascha. 2017. Unrolled generative adversarial networks. In: Proc. International Conference on Learn- ing Representations. Mohamed, Shakir, and Lakshminarayanan, Balaji. 2016. Learning in implicit gen- erative models. ArXiv preprint arXiv:1610.03483. Negahban, Sahand, Yu, Bin, Wainwright, Martin J., and Ravikumar, Pradeep K. 2009. A uniï¬ed framework for high-dimensional analysis of m-estimators with decomposable regularizers. Pages 1348â€“1356 of: Advances in Neural Information Processing Systems. Nie, Dong, Trullo, Roger, Lian, Jun, Petitjean, Caroline, Ruan, Su, Wang, Qian, and Shen, Dinggang. 2017. Medical image synthesis with context-aware generative adversarial networks. Pages 417â€“425 of: Proc. International Conference on Medical Image Computing and Computer-Assisted Intervention. Odena, Augustus. 2019. Open questions about generative adversarial networks. Distill, 4(4), e18. 420 Dimakis: Deep Generative Models and Inverse Problems Ongie, Gregory, Jalal, Ajil, Metzler, Christopher A., Baraniuk, Richard G., Dimakis, Alexandros G, and Willett, Rebecca. 2020. Deep learning techniques for inverse problems in imaging. IEEE Journal on Selected Areas in Information Theory, 1(1), 39â€“56. Pandit, Parthe, Sahraee, Mojtaba, Rangan, Sundeep, and Fletcher, Alyson K. 2019. Asymptotics of map inference in deep networks. ArXiv preprint arXiv:1903.01293. Pearl, Judea. 1982. Reverend Bayes on inference engines: A distributed hierarchical approach. Pages 133â€“136 of: Proc. AAAI Conference on Artiï¬cal Intelligence. Radford, Alec, Metz, Luke, and Chintala, Soumith. 2015. Unsupervised representa- tion learning with deep convolutional generative adversarial networks. ArXiv preprint arXiv:1511.06434. Richardson, Tom, and Urbanke, Ruediger. 2008. Modern Coding Theory. Cam- bridge University Press. Rudin, Leonid I., Osher, Stanley, and Fatemi, Emad. 1992. Nonlinear total variation based noise removal algorithms. Physica D: Nonlinear Phenomena, 60(1-4), 259â€“268. Sanjabi, Maziar, Ba, Jimmy, Razaviyayn, Meisam, and Lee, Jason D. 2018. Solving approximate Wasserstein GANs to stationarity. ArXiv preprint arXiv:1802.08249. Schlegl, Thomas, SeebÃ¶ck, Philipp, Waldstein, Sebastian M., Schmidt-Erfurth, Ur- sula, and Langs, Georg. 2017. Unsupervised anomaly detection with genera- tive adversarial networks to guide marker discovery. Pages 146â€“157 of: Proc. International Conference on Information Processing in Medical Imaging. Schlemper, Jo, Caballero, Jose, Hajnal, Joseph V., Price, Anthony N, and Rueckert, Daniel. 2017. A deep cascade of convolutional neural networks for dynamic MR image reconstruction. IEEE Transactions on Medical Imaging, 37(2), 491â€“503. Shannon, Claude E. 1948. A mathematical theory of communication. The Bell system Technical Journal, 27(3), 379â€“423. Sohl-Dickstein, Jascha, Weiss, Eric, Maheswaranathan, Niru, and Ganguli, Surya. 2015. Deep unsupervised learning using nonequilibrium thermodynamics. Pages 2256â€“2265 of: Proc. International Conference on Machine Learning. Song, Yang, Sohl-Dickstein, Jascha, Kingma, Diederik P., Kumar, Abhishek, Er- mon, Stefano, and Poole, Ben. 2021. Score-based generative modeling through stochastic diï¬€erential equations. In: Proc. International Conference on Learn- ing Representations. Sun, Wanjie, and Chen, Zhenzhong. 2020. Learned image downscaling for upscal- ing using content adaptive resampler. IEEE Transactions on Image Processing, 29, 4027â€“4040. Sun, Yu, Liu, Jiaming, and Kamilov, Ulugbek S. 2020. Block coordinate regu- larization by denoising. IEEE Transactions on Computational Imaging, 6, 908â€“921. References 421 Tian, Chunwei, Fei, Lunke, Zheng, Wenxian, Xu, Yong, Zuo, Wangmeng, and Lin, Chia-Wen. 2020. Deep learning on image denoising: An overview. Neural Networks, 131(November), 251â€“275. Tibshirani, Robert. 1996. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society. Series B (Methodological), 267â€“288. Tripathi, Subarna, Lipton, Zachary C., and Nguyen, Truong Q. 2018. Correction by projection: Denoising images with generative adversarial networks. ArXiv preprint arXiv:1803.04477. Ulyanov, Dmitry, Vedaldi, Andrea, and Lempitsky, Victor. 2017. Deep image prior. ArXiv preprint arXiv:1711.10925. Van Veen, David, Jalal, Ajil, Price, Eric, Vishwanath, Sriram, and Dimakis, Alexan- dros G. 2018. Compressed sensing with deep image prior and learned regu- larization. ArXiv preprint arXiv:1806.06438. Wainwright, Martin J. 2019. High-Dimensional Statistics: A Non-Asymptotic View- point. Vol. 48. Cambridge University Press. Wainwright, Martin J., Jordan, Michael I., et al. 2008. Graphical models, exponen- tial families, and variational inference. Foundations and Trends in Machine Learning, 1(1â€“2), 1â€“305. Wallace, Gregory K. 1992. The JPEG still picture compression standard. IEEE Transactions on Consumer Electronics, 38(1), xviiiâ€“xxxiv. Wolterink, Jelmer M., Leiner, Tim, Viergever, Max A., and IÅ¡gum, Ivana. 2017. Generative adversarial networks for noise reduction in low-dose CT. IEEE Transactions on Medical Imaging, 36(12), 2536â€“2545. Yang, Wenming, Zhang, Xuechen, Tian, Yapeng, Wang, Wei, Xue, Jing-Hao, and Liao, Qingmin. 2019. Deep learning for single image super-resolution: A brief review. IEEE Transactions on Multimedia, 21(12), 3106â€“3121. Yu, Jiahui, Lin, Zhe, Yang, Jimei, Shen, Xiaohui, Lu, Xin, and Huang, Thomas. 2019. Free-form image inpainting with gated convolution. Proc. International Conference on Computer Vision. 10 Dynamical Systems and Optimal Control Approach to Deep Learning Weinan E, Jiequn Han, Qianxiao Li Abstract: We give a short and concise review of the dynamical system and control theory approach to deep learning. From the viewpoint of dynamical systems, the back-propagation algorithm in deep learning becomes a simple consequence of the variational equations in ODEs. From the viewpoint of control theory, deep learning is a case of mean-ï¬eld control in that all the agents share the same control. As an application, we discuss a new class of algorithms for deep learning based on Pontryaginâ€™s maximum principle in control theory. 10.1 Introduction Deep learning is at present the most successful machine learning tool for a wide variety of tasks ranging from computer vision (Deng et al., 2009), to scientiï¬c computing (Han et al., 2018a; E et al., 2017) to molecular modeling (Han et al., 2018b; Zhang et al., 2018a,b). From a mathematical viewpoint, its multilayer compositional structure puts deep learning into a quite diï¬€erent category from other machine learning models, and for this reason there is a need for diï¬€erent kinds of algorithms as well as a diï¬€erent mathematical framework for understanding deep learning. One such example is the back-propagation algorithm, which plays a key role in deep learning. Another, related, example is the control theory approach to deep learning, put forward already in LeCun (1989). Both are quite unique to deep learning. In this chapter, we examine the mathematical structure behind these ideas and we discuss recent eï¬€orts to design new training algorithms using them. The ï¬rst point we will discuss is that one can think about the continuous (in time) analogs of deep neural networks, and this will allow us to make use of the theory and numerical algorithms for ODEs. This simple idea was ï¬rst put forward in E (2017) and has become popular under the name of â€œneural ODEsâ€ (Chen et al., 2018) (see also Haber and Ruthotto, 2017; Ruthotto and Haber, 2018; Sonoda 422 10.1 Introduction 423 and Murate, 2019; Lu et al., 2017). In light of this, the back-propagation algorithm becomes a natural development of the well-known variational equation for ODEs. The second issue we will examine is the control theory viewpoint of deep learning. It turns out that, though intuitively very clear, making this approach rigorous is non- trivial since the problem falls into the category of â€œmean-ï¬eld controlâ€ in that all agents share the same control. This forms a loose parallel with mean-ï¬eld games (E et al., 2019a), in which the input-label samples play the roles of agents and the control (consisting of trained weights) is determined by some collective behavior of these samples or agents. As such, the state space becomes inï¬nite dimensional and the associated Hamiltonâ€“Jacobiâ€“Bellman (HJB) equation becomes highly non- trivial. For this reason, we will not dwell on the HJB approach to this mean-ï¬eld control problem. Instead, we will focus on the maximum principle approach initiated by E et al. (2019a). As an application of these ideas, we will discuss a new class of training algorithm motivated by the maximum principle. In this algorithm, one solves a coupled forwardâ€“backward problem, but the optimization, in the form of the maximization of Hamiltonian functions, is done layer-wise. Note that the standard back-propagation algorithm is a special case where the Hamiltonian maximizations are performed by one step of gradient ascent. One advantage of considering this more general class of algorithms is that other methods, such as L-BFGS (Liu and Nocedal, 1989), can be used to perform this layer-wise optimization, leading to novel algorithms with diï¬€erent properties (Li et al., 2017). Another advantage of this approach is that it applies equally well to the case when the weights live on discrete spaces. This is useful for developing principled training algorithms for quantized neural networks, in which the weights are restricted to take values in a ï¬nite set, e.g. binary and ternary networks (Li and Hao, 2018). 10.1.1 The Problem of Supervised Learning In supervised learning, the object of interest is a target function f âˆ—, where f : Rd â†’ R1; f âˆ—is deï¬ned by f together with a probability distribution Âµ0 on Rd. We are given a data set S = {(xj, yj)}n j=1, where the xj are sampled from a probability distribution Âµ0, yj = f âˆ—(xj). In principle one should also allow the presence of measurement noise in yj, but here we will focus on the case without measurement noise. Our task is to approximate f âˆ—on the basis of the information in S. Generally speaking, supervised learning strategies consist of the following components: â€¢ The construction of some â€œhypothesis spaceâ€ (a space of functions) Hm. Here m is roughly the dimension of Hm or the number of free parameters that describe a function in the hypothesis space. 424 E et al: Dynamical Systems, Optimal Control and Deep Learning â€¢ Minimization of the â€œempirical riskâ€, Ë†Rn = 1 n n Ã• j=1 Î¦( f (xj), yj) = 1 n n Ã• j=1 Î¦( f (xj), f âˆ—(xj)), where f âˆˆHm. Here, Î¦ denotes the loss function, e.g., mean square loss could correspond to Î¦(y, yâ€²) := (y âˆ’yâ€²)2. Notice immediately that what we really want to minimize is the â€œpopulation riskâ€, R = EÎ¦( f (x), f âˆ—(x)) = âˆ« Rd Î¦( f (x), f âˆ—(x))dÂµ0(x). Whether the minimizers of these two risk functions are close to each other is a central problem in machine learning. An important issue is how to choose the hypothesis space. The simplest choice is the space of linear functions, Hm = { f (x) = Î² Â· x + Î²0, Î² âˆˆRd, Î²0 âˆˆR}. This is the case in linear regression. A slight extension is the generalized linear model, Hm = { f (x) = Ãm k=1 ckÏ†k(x), ck âˆˆR}. Here {Ï†k} is a set of linearly independent functions. A shallow neural network with one hidden layer corresponds to the case when Hm = { f (x) = Ãm k=1 akÏƒ(bk Â· x + ck), ak,ck âˆˆR, bk âˆˆRd}, where Ïƒ is some non-linear function, called the activation function, e.g. Ïƒ(z) = max(z,0), the ReLU (rectiï¬ed linear unit) activation function . In this view, deep neural networks (DNN) correspond to the case when the hypothesis space consists of compositions of functions of the form above: f (x,Î¸) = WLÏƒ â—¦(WLâˆ’1Ïƒ â—¦(Â· Â· Â· Ïƒ â—¦(W0x))), Î¸ = (W0,W1,. . .,WL). Here â€œâ—¦â€ means that the non-linear function Ïƒ is applied to each component of the argument vector, the Wi are matrices (note that we have adopted the standard abuse of notation and have omitted the constant term; W0x should really be an aï¬ƒne function instead of a linear function). The trick is to ï¬nd the right set of functions that are rich enough to approximate accurately the target function but simple enough that the task of ï¬nding the best approximation is manageable. 10.2 ODE Formulation An interesting way of constructing functions is through the ï¬‚ow map of an ODE system. This viewpoint was ï¬rst proposed in E (2017) and later in Chen et al. (2018). Consider the following ODEs in Rd: dzt dt = f (zt Â· Î¸t), z0(x) = x. (10.1) 10.3 Mean-Field Optimal Control and Pontryaginâ€™s Maximum Principle 425 The solution at time t is denoted by zt(x). We say that x â†’zt(x) is the ï¬‚ow map at time t following the dynamics (10.1), and so the ï¬‚ow map implicitly depends on Î¸ â‰¡{Î¸t : t âˆˆ[0,T]}. Fixing a time horizon T, we can deï¬ne the hypothesis space through the diï¬€erent ï¬‚ow maps obtained by choosing diï¬€erent values for Î¸ and Î±: Hm = {Î± Â· zT(Â·), Î± âˆˆRd,Î¸ âˆˆLâˆ}. (10.2) How should we choose the functional form of f ? One reasonable strategy is to choose the simplest non-linear function in some sense and f (z,Î¸) = Ïƒ â—¦(Î¸z) is arguably such a choice. Here only a ï¬xed scalar non-linear function enters into the construction; all the parameters are in the linear transformation Î¸. A DNN can be understood as the discrete form of the formalism discussed above. The structures before the output layer can be viewed as a discrete dynamical system in the feature space. In this case, the matrices Î¸ do not have to be square matrices and a change of dimensionality is allowed in the dynamical system. A byproduct of the ODE formulation is that the back-propagation algorithm in deep learning becomes the direct analog of the variational equation for ODEs. To see this, note that if we deï¬ne the matrix Vt := âˆ‡ztzT then we can derive a variational equation for the evolution of Vt in the form Ã›Vt = âˆ’âˆ‡z f (zt,Î¸t)Vt, VT = I. (10.3) This equation describes the eï¬€ects of perturbations in zt on the ï¬nal state zT, from which one can then use the chain rule to obtain gradients with respect to each Î¸t. In fact, the variational equation (10.3) is simply the back-propagation of the derivatives with respect to the hidden states. See Li et al. (2017) and E et al. (2019a) for further discussions of this connection and the role of variational equations in the optimality conditions. 10.3 Mean-Field Optimal Control and Pontryaginâ€™s Maximum Principle It is natural to think of deep learning as an optimal control problem: choosing the best control Î¸ to minimize the risk. Although this viewpoint was formulated by LeCun (1989), the full mathematical theory was only established much later in E et al. (2019a). The subtlety lies in the fact that this is not an ordinary control problem, but a mean-ï¬eld control problem. As was shown in E et al. (2019a), the associated HJB equation should be formulated on the Wasserstein space, which is an inï¬nite-dimensional space of probability measures. We will not discuss the HJB equation here since it is quite technical and requires a considerable amount of new notations. Instead, we will focus on the maximum principle viewpoint in control theory and discuss algorithms that are based on the maximum principle. Consider a set of admissible controls or training weights Î˜ âŠ†Rm. In typical deep 426 E et al: Dynamical Systems, Optimal Control and Deep Learning learning, Î˜ is taken as the whole space Rm, but here we consider the more general case where Î˜ can be constrained. To cast the deep learning problem as a control problem, it is helpful to consider the more general setting where a regularization term is added to the objective function. The population risk minimization problem in deep learning can hence be posed as the following mean-ï¬eld optimal control problem inf Î¸âˆˆLâˆ([0,T],Î˜) J(Î¸) := EÂµ0  Î¦(zT, y) + âˆ«T 0 L(zt,Î¸t)dt  , subject to (10.1). (10.4) Note that here we have simpliï¬ed the set-up slightly by absorbing Î±, assuming it is constant, into Î¦, so that the latter is now a function from Rd Ã— R1 to R1. Also, we now denote by Âµ0 the joint distribution of the input-label pair (x, y). The term â€œmean-ï¬eldâ€ highlights the fact that Î¸ is shared by a whole population, and the optimal control must depend on the law of the input-target random variables. Note that the law of zt does not enter the forward equations explicitly (unlike e.g. McKeanâ€“Vlasov control Carmona and Delarue, 2015), and hence our forward dynamics are not explicitly in mean-ï¬eld form. We also refer to E et al. (2019a) for a discussion on batch-normalized dynamical systems which possess explicit mean-ï¬eld dynamics. The empirical risk minimization problem can be posed as a sampled optimal control problem inf Î¸âˆˆLâˆ([0,T],Î˜) J(Î¸) :=1 n n Ã• i=1  Î¦(zi T,yi) + âˆ«T 0 L(zi t,Î¸t)dt  , subject to Ã›zi t = f (zi t,Î¸t), 1 = 1,. . .,n, (10.5) where {xi,yi} are i.i.d. samples from Âµ0. The solutions of sampled optimal control problems are typically random variables. Note that this is a special case of (10.4) with Âµ0 replaced by the empirical measure {xi,yi}. It is likely that, to obtain optimal error control, one should add an n-dependent coeï¬ƒcient in front of the regularization term (see E et al., 2019b). At the moment, this theory is not yet complete. Therefore, here, we will neglect such a dependence. 10.3.1 Pontryaginâ€™s Maximum Principle One of the most powerful tools in control theory is the maximum principle dis- covered by Pontryagin (PMP); see Boltyanskii et al. (1960) and Pontryagin (1987). This maximum principle is amazingly general: for example it applies to cases when the set of controls is discrete. 10.3 Mean-Field Optimal Control and Pontryaginâ€™s Maximum Principle 427 To begin with, we deï¬ne the Hamiltonian H : [0,T] Ã— Rd Ã— Rd Ã— Î˜ â†’R given by H(z,p,Î¸) := p Â· f (z,Î¸) âˆ’L(z,Î¸). Theorem 10.1 (Mean-ï¬eld PMP). Suppose that f is bounded, f , L are continuous in Î¸ and f , L, Î¦ are continuously diï¬€erentiable with respect to z. Suppose further that the distribution Âµ0 has bounded support. Let Î¸âˆ—âˆˆLâˆ([0,T],Î˜) be a solution of (10.4) in the sense that J(Î¸âˆ—) attains the inï¬mum. Then there exist absolutely continuous stochastic processes zâˆ—, pâˆ—such that Ã›zâˆ— t = f (zâˆ— t ,Î¸âˆ— t ), zâˆ— t = x, (10.6) Ã›pâˆ— t = âˆ’âˆ‡zH(zâˆ— t ,pâˆ— t,Î¸âˆ— t ), pâˆ— T = âˆ’âˆ‡zÎ¦(zâˆ— T,y), (10.7) EÂµ0H(zâˆ— t ,pâˆ— t,Î¸âˆ— t ) â‰¥EÂµH(zâˆ— t ,pâˆ— t,Î¸), for all Î¸ âˆˆÎ˜ a.e. t âˆˆ[0,T], (10.8) where (x,y) âˆ¼Âµ0 are random variables. This is a simple generalization of the usual Pontryagin maximum principle (for which the expectation is absent; see e.g. Athans and Falb, 2013, Bertsekas, 1995, and Liberzon, 2012) to the learning setting, and the proof can be found in E et al. (2019a). The PMP can be regarded as a generalization of the Karushâ€“Kuhnâ€“Tucker (KKT) conditions for non-linear constrained optimization to non-smooth (in Î¸) and mean-ï¬eld settings. Indeed, we can view (10.4) as a non-linear program over the function space Lâˆ([0,T],Î˜) where the constraint is the ODE (10.1). In this sense, the co-state process pâˆ—plays the role of a continuous-time analogue of Lagrange multipliers. The key diï¬€erence between the PMP and the KKT conditions (besides the lack of inequality constraints on the state) is the Hamiltonian maximization condition (10.19), which is stronger than a typical ï¬rst-order condition that assumes smoothness with respect to Î¸ (e.g. âˆ‡Î¸EÂµ0H = 0). In particular, the PMP says that EÂµ0H is not only stationary, but globally maximized at an optimal control â€“ which is a much stronger statement if the averaged Hamiltonian is not concave. Moreover, the PMP makes minimal assumptions on the parameter space Î˜; it holds even when f is non-smooth with respect to Î¸, or worse, when Î˜ is a discrete subset of Rm. The three simultaneous equations in Theorem 10.1 allow us to solve for the unknowns zâˆ—, pâˆ—, Î¸âˆ—simultaneously as a function of t. In this sense, the resulting optimal control Î¸âˆ—is open-loop and is not in a feedback form Î¸âˆ— t = g(t,zâˆ— t ) for some control function g. The latter is of closed-loop type and is typically obtained from dynamic programming and the Hamiltonâ€“Jacobiâ€“Bellman formalism (Bellman, 2013). In this sense, the PMP gives a weaker control. However, open-loop solutions are suï¬ƒcient for neural network applications, where the trained weights and biases are ï¬xed and depend only on the layer number and not explicitly on the inputs and hidden states at the times when inferences are made. 428 E et al: Dynamical Systems, Optimal Control and Deep Learning 10.4 Method of Successive Approximations 10.4.1 Extended Pontryagin Maximum Principle We now discuss the development of novel training algorithms based on the PMP. For practical purposes we are always dealing with empirical risk minimization, where Âµ0 is an empirical measure induced by samples. Thus, we can simplify the notation by concatenating all the xi and yi into a pair of long vectors (x,y) and redeï¬ning ( f,. . ., f ) as f. Consequently, the expectations in (10.4) (or the sum in (10.5)) can now be ignored and x, y can be considered deterministic. It turns out that for the purpose of designing eï¬ƒcient algorithms with error control, it is useful to introduce an extended version of the maximum principle, by applying similar ideas to augmented Lagrangians (Hestenes, 1969). This extended maximum principle was introduced in Li et al. (2017). Fix some Ï > 0 and introduce the augmented Hamiltonian ËœH(z,p,Î¸,v,q) := H(z,p,Î¸) âˆ’1 2 Ïâˆ¥v âˆ’f(z,Î¸)âˆ¥2 âˆ’1 2 Ïâˆ¥q + âˆ‡zH(z,p,Î¸âˆ¥2. (10.9) Then we have the following set of alternative necessary conditions for optimality: Proposition 10.2 (Extended PMP). Suppose that Î¸âˆ—âˆˆLâˆ([0,T],Î˜) is a solution to the optimal control problem (10.5) (with the concatenated notation described above). Then there exists an absolutely continuous co-state process pâˆ—such that the tuple (zâˆ— t ,pâˆ— t,Î¸âˆ— t ) satisï¬es the necessary conditions Ã›zâˆ— t = âˆ‡p ËœH(zâˆ— t ,pâˆ— t,Î¸âˆ— t, Ã›zâˆ— t , Ã›pâˆ— t ), zâˆ— 0 = x, (10.10) Ã›pâˆ— t = âˆ’âˆ‡z ËœH(zâˆ— t ,pâˆ— t,Î¸âˆ— t, Ã›zâˆ— t , Ã›pâˆ— t ), pâˆ— T = âˆ’âˆ‡zÎ¦(zâˆ— T,y), (10.11) ËœH(zâˆ— t ,pâˆ— t,Î¸âˆ— t, Ã›zâˆ— t , Ã›pâˆ— t ) â‰¥ËœH(zâˆ— t ,pâˆ— t,Î¸, Ã›zâˆ— t , Ã›pâˆ— t ), Î¸ âˆˆÎ˜, t âˆˆ[0,T]. (10.12) The proof can be found in Li et al. (2017). Compared with the usual PMP, the extended PMP is a weaker necessary condition. However, the advantage is that maximizing ËœH naturally penalizes errors in the Hamiltonian dynamical equations, which is useful for ensuring convergence of the successive approximation type of algorithms, which we now discuss. 10.4.2 The Basic Method of Successive Approximation Here again we will focus on the continuous-in-time setting, but one should note that the algorithm can also be formulated in a discrete setting, which is the case for the deep neural networks used in practice. In the optimal control literature, there are many methods for the numerical solution of the PMP, including two-point boundary value problem methods (Bryson, 1975; Roberts and Shipman, 1972), and collocation methods (Betts, 1988) coupled with general non-linear programming 10.4 Method of Successive Approximations 429 techniques (Bertsekas, 1999; Bazaraa et al., 2013). See Rao (2009) for a review. Most of these methods concern the small-scale problems typically encountered in control applications (e.g. trajectory optimization of spacecraft) and do not scale well to modern machine learning problems with a large number of state and con- trol variables. One exception is the method of successive approximations (MSA) (Chernousko and Lyubushin, 1982), which is an iterative method based on alternat- ing propagation and optimization steps. Observe that (10.6) is simply the equation Ã›zâˆ— t = f (zâˆ— t ,Î¸âˆ— t ) and is independent of the co-state pâˆ—. Therefore, we may proceed in the following manner. First, we make an initial guess of the optimal control Î¸0 âˆˆLâˆ([0,T],Î˜). For each k = 0,1,2,. . ., we ï¬rst solve (10.6), Ã›zt = f (zt,Î¸k t ), z0 = x, (10.13) for {zt}, which then allows us to solve (10.7), Ã›pt = âˆ’âˆ‡zH(zt,pt,Î¸k t ), pT = âˆ’âˆ‡Î¦(zT,y) (10.14) to get {pt}. Finally, we use the maximization condition (10.8) to set Î¸k+1 t = argmax Î¸âˆˆÎ˜ H(zt,pt,Î¸) for t âˆˆ[0,T]. The algorithm is summarized in Algorithm 10.1. Now, MSA consists of two major components: the forwardâ€“backward Hamilto- nian dynamics (Steps 4, 5) and the maximization for the optimal parameters at each time (Step 6). An important feature of MSA is that the Hamiltonian maximization step is decoupled for each t âˆˆ[0,T]. In the language of deep learning, the optimiza- tion step is decoupled for diï¬€erent layers and only the Hamiltonian ODEs (Steps 3, 4 of Algorithm 10.1) involve propagation through the layers. This allows the par- allelization of the maximization step, which is typically the most time-consuming step. Although it can been shown that the basic MSA converges for a restricted class of linear quadratic regulators (Aleksandrov, 1968), in general it tends to diverge, espe- cially if a bad initial Î¸0 is chosen (Aleksandrov, 1968; Chernousko and Lyubushin, 1982). Our goal is to modify the basic MSA to control its divergent behavior. Before we do so, it is important to understand why the MSA diverges and, in particular, the relationship between the maximization step in Algorithm 10.1 and the empirical risk minimization problem (10.5). 430 E et al: Dynamical Systems, Optimal Control and Deep Learning Algorithm 10.1 Basic method of successive approximations 1: procedure Basic MSA 2: Initialize k = 0, Î¸0 âˆˆLâˆ([0,T],Î˜) 3: while Stopping criterion not satisï¬ed do 4: {zt : t âˆˆ[0,T]} â†solution of Ã›zt = f (zt,Î¸k t ), z0 = x â–·Solve state equation 5: {pt : t âˆˆ[0,T]} â†solution of Ã›pt = âˆ’âˆ‡zH(zt,pt,Î¸k t ), pT = âˆ’âˆ‡zÎ¦(zT,y) â–·Solve co-state equation 6: Î¸k+1 t â†argmaxÎ¸âˆˆÎ˜ H(zt,pt,Î¸), t âˆˆ[0,T] â–·Maximize Hamiltonian 7: k â†k + 1 8: return Î¸k For each Î¸ âˆˆLâˆ([0,T],Î˜), recall that we are minimizing J(Î¸) := Î¦(zÎ¸ T, y) + âˆ«T 0 L(zÎ¸ t ,Î¸t)dt, where {zÎ¸ t }, satisfying (10.13) with parameters Î¸, Î¦, is the empirical risk function. We show in the following lemma the relationship between the values of J and the Hamiltonian maximization step. We start by making the following assumptions. (A1) Î¦ is twice continuously diï¬€erentiable in z, with Î¦ and âˆ‡Î¦ satisfying a Lipschitz condition in z, i.e. there exists K > 0 such that |Î¦(z, y) âˆ’Î¦(zâ€², y)| + âˆ¥âˆ‡Î¦(z, y) âˆ’âˆ‡Î¦(zâ€², y)âˆ¥â‰¤Kâˆ¥z âˆ’zâ€²âˆ¥ for all z, zâ€² âˆˆRd and y âˆˆR. (A2) f(Â·,Î¸) is twice continuously diï¬€erentiable in z, with f , âˆ‡z f satisfying a Lipschitz condition in z uniformly in Î¸ and t, i.e. there exists K > 0 such that âˆ¥f (z,Î¸) âˆ’f (zâ€²,Î¸)âˆ¥+ âˆ¥âˆ‡z f (z,Î¸) âˆ’âˆ‡z f (zâ€²,Î¸)âˆ¥2 â‰¤Kâˆ¥z âˆ’zâ€²âˆ¥ for all z,zâ€² âˆˆRd and t âˆˆ[0,T], where âˆ¥Â· âˆ¥2 denotes the induced 2-norm. With these assumptions, we have the following estimate. Lemma 10.3. Suppose that (A1)â€“(A2) hold. Then there exists a constant C > 0 such that for any Î¸,Ï† âˆˆLâˆ([0,T],Î˜), J(Ï†) â‰¤J(Î¸) âˆ’ âˆ«T 0 âˆ†Ï†,Î¸H(t) dt + C âˆ«T 0 âˆ¥f (zÎ¸ t ,Ï†t) âˆ’f (zÎ¸ t ,Î¸t âˆ¥dt + C âˆ«T 0 âˆ¥âˆ‡z(zÎ¸ t ,pÎ¸ t ,Ï†t) âˆ’âˆ‡zH(zÎ¸ T,pÎ¸ t ,Î¸t)âˆ¥2 dt, 10.4 Method of Successive Approximations 431 Algorithm 10.2 Extended method of successive approximations 1: procedure Extended MSA (Hyper-parameter: Ï) 2: Initialize k = 0, Î¸0 âˆˆLâˆ([0,T],Î˜) 3: while Stopping criterion not satisï¬ed do 4: {zt : t âˆˆ[0,T]} â†solution of Ã›zt = f (zt,Î¸k t ), z0 = x â–·Solve state equation 5: {pt : t âˆˆ[0,T]} â†solution of Ã›pt = âˆ’âˆ‡zH(zt,pt,Î¸k t ), pT = âˆ’âˆ‡zÎ¦(zT, y) â–·Solve co-state equation 6: Î¸k+1 t â†argmaxÎ¸ âˆˆÎ˜ ËœH(zt,pt,Î¸, Ã›zt, Ã›pt), t âˆˆ[0,T] â–·Maximize extended Hamiltonian 7: k â†k + 1 8: return Î¸k where zÎ¸,pÎ¸ satisfy (10.13), (10.14) respectively and âˆ†HÏ†,Î¸ denotes the change in Hamiltonian: âˆ†HÏ†,Î¸ := H(zÎ¸ t ,pÎ¸ t ,Ï†t) âˆ’H(zÎ¸ t ,pÎ¸ t ,Î¸t). See Li et al. (2017) for the proof of this lemma. Lemma 10.3 says that the Hamiltonian maximization step in MSA (Step 6 in Algorithm 10.1) is in some sense the optimal descent direction for J. However, the last two terms on the right- hand side indicates that this descent can be nulliï¬ed if substituting Ï† for Î¸ incurs too much error in the Hamiltonian dynamics (Steps 4, 5 in Algorithm 10.1). In other words, the last two integrals measure the degree to which the Hamiltonian dynamics (10.6), (10.18) are satisï¬ed, and can be viewed as feasibility conditions, when one replaces Î¸ by Ï†. Thus, we may call these feasibility errors. The divergence of the basic MSA happens when these feasibility errors dominate the descent due to Hamiltonian maximization. The controlling of feasibility errors is the motivation for developing the extended PMP and the extended MSA. 10.4.3 Extended Method of Successive Approximation The extended MSA can be understood as an application of the basic MSA not to the original PMP but to the extended PMP introduced in Li et al. (2017), where the terms with Ï regularize the feasibility errors. Hence, we have Algorithm 10.2. Deï¬ne Âµk := âˆ«T 0 âˆ†HÎ¸k+1,Î¸k(t) â‰¥0. 432 E et al: Dynamical Systems, Optimal Control and Deep Learning If Âµk = 0, then from the Hamiltonian maximization step (10.12) we must have 0 = âˆ’Âµk â‰¤1 2 Ï âˆ«T 0 âˆ¥f (zÎ¸k t ,Î¸k+1 t ) âˆ’f (zÎ¸k t ,Î¸k t )âˆ¥2 dt âˆ’1 2 Ï âˆ«T 0 âˆ¥âˆ‡zH(zÎ¸k t ,pÎ¸k t ,Î¸k+1 t ) âˆ’âˆ‡zH(zÎ¸k t ,pÎ¸k t ,Î¸k t )âˆ¥2 dt â‰¤0 and so max Î¸ ËœH(zÎ¸k t ,pÎ¸k t ,Î¸, Ã›zÎ¸k t , Ã›pÎ¸k t ) = ËœH(zÎ¸k t ,pÎ¸k t ,Î¸k, Ã›zÎ¸k t , Ã›pÎ¸k t ), i.e., (zÎ¸k,pÎ¸k,Î¸k) satisï¬es the extended PMP. In other words, the quantity Âµk â‰¥0 measures the distance from a solution of the extended PMP, and if it equals 0, then we have a solution. The following result gives a suï¬ƒcient condition for the convergence of Âµk. Theorem 10.4. Assume that the conditions in Lemma 10.3 hold and let Î¸0 âˆˆ Lâˆ([0,T],Î˜) be any initial measurable control with J(Î¸0) < +âˆ. Assume also that infÎ¸âˆˆLâˆ([0,T],Î˜) J(Î¸) > âˆ’âˆ. Then, for Ï large enough, we have from Algorithm 10.2, J(Î¸k+1) âˆ’J(Î¸k) â‰¤âˆ’DÂµk. for some constant D > 0 and lim kâ†’0 Âµk = 0; i.e., the extended MSA algorithm converges to the set of solutions of the extended PMP. Since the proof is quite simple and informative, we reproduce it here from Li et al. (2017). Proof Using Lemma 10.3 with Î¸ â‰¡Î¸k, bÏ† â‰¡Î¸k+1, we have J(Î¸k+1) âˆ’J(Î¸k) â‰¤Âµk + C âˆ«T 0 âˆ¥f (zÎ¸k t ,Î¸k+1 t ) âˆ’f (zÎ¸k t ,Î¸k t )âˆ¥2 dt + C âˆ«T 0 âˆ¥âˆ‡zH(zÎ¸k t ,pÎ¸k t ,Î¸k+1 t ) âˆ’âˆ‡zH(zÎ¸k t ,pÎ¸k t ,Î¸k t )âˆ¥2 dt. From the Hamiltonian maximization step in Algorithm 10.2, we know that H(zÎ¸k t ,pÎ¸k t ,Î¸k t ) â‰¤H(zÎ¸k t ,pÎ¸k t ,Î¸k+1 t ) + 1 2 Ïâˆ¥f(zÎ¸k t ,Î¸k+1 t ) âˆ’f(zÎ¸k t ,Î¸k t )âˆ¥2 âˆ’1 2 Ïâˆ¥âˆ‡zH(zÎ¸k t ,pÎ¸k t ,Î¸k+1 t ) âˆ’âˆ‡zH(zÎ¸k t ,pÎ¸k t ,Î¸k t )âˆ¥2. 10.4 Method of Successive Approximations 433 Hence we have J(Î¸k+1) âˆ’J(Î¸k) â‰¤âˆ’  1 âˆ’2C Ï  Âµk. Pick Ï > 2C; then we indeed have J(Î¸k+1)âˆ’J(Î¸k) â‰¤âˆ’DÂµk with D =  1 âˆ’2C Ï  > 0. Moreover, we can rearrange and sum the above expression to get M Ã• k=0 Âµk â‰¤Dâˆ’1(J(Î¸0) âˆ’J(Î¸M+1)) â‰¤Dâˆ’1  J(Î¸0) âˆ’ inf Î¸âˆˆLâˆ([0,T],Î˜) J(Î¸)  and hence Ãâˆ k=0 Âµk < +âˆ, which implies Âµk â†’0. â–¡ In an actual implementation of Algorithm 10.2, one can solve the forward and backward equations using standard numerical integration methods (e.g. the forward Euler method, as in Li et al., 2017). On the other hand, the Hamiltonian maximiza- tion step deserves some discussion. Note that if we replace this step by a gradient ascent step in the Hamiltonian, in essence we recover (at least for the case of Ï = 0) classical back-propagation with a gradient descent algorithm. In Li et al. (2017), a diï¬€erent option was proposed where the Hamiltonian maximization is performed using the L-BFGS method; it was shown that the resulting algorithm enjoys certain advantages over the usual back-propagation with gradient descent method, such as faster initial descent and a lesser likelihood of being trapped near ï¬‚at regions of the loss function. Another instance for which the MSA occurs useful is when the Hamiltonian max- imization step admits a closed-form solution. This is the case for quantized neural networks, where the weight space Î˜ is ï¬nite, e.g. Î˜ = {+1,âˆ’1}m (a binary network) or Î˜ = {+1,0,âˆ’1}m (a ternary network). In this case, one can exploit explicit solu- tions of the Hamiltonian maximization to design eï¬ƒcient optimization algorithms. The reader is referred to Li and Hao (2018) for details. Novel algorithms based on the maximum principles have also found applications in distributed optimization (Parpas and Muir, 2019) and adversarial training (Zhang et al., 2019). 10.4.4 Discrete PMP and Discrete MSA So far, we have focused on a continuous-time idealization of the deep learning problem and its theoretical and algorithmic consequences. It is thus a natural question to ask how much of this holds in the discrete setting. Below, we brieï¬‚y discuss the optimal control viewpoint in the discrete-time setting and some known theoretical results in this direction. For simplicity, we shall stick to the empirical risk minimization setting, but these statements also hold for a general setting with explicit consideration of stochasticity. 434 E et al: Dynamical Systems, Optimal Control and Deep Learning In the discrete setting, T âˆˆZ+ now denotes the number of layers and, in place of the ODE (10.1), we have the feed-forward dynamical system zt+1 = ft(zt,Î¸t), t = 0,1,. . .,T âˆ’1, z0 = x. (10.15) We will now consider the case when the dimensions of zt âˆˆRdt and Î¸t âˆˆÎ˜t depend on and can vary with layers t. Also, we allow the feed-forward function ft to vary with t. This framework now encompasses most deep neural networks employed in practice. The empirical learning problem then takes the form min Î¸ J(Î¸) := Î¦(zT, y) + Tâˆ’1 Ã• t=0 Lt(zt,Î¸t) subject to: zt+1 = ft(zt,Î¸t), t = 0,. . .,T âˆ’1. (10.16) It turns out that, even in the discrete case, one indeed has a Pontryaginâ€™s maximum principle, albeit with some additional caveats. This was ï¬rst proved in Halkin (1966) and we reproduce a simpliï¬ed statement below. Theorem 10.5 (Discrete PMP). Let ft and Î¦ be suï¬ƒciently smooth in z. Assume further that for each t and z âˆˆRdt, the sets {ft(z,Î¸): Î¸ âˆˆÎ˜t} and {Lt(z,Î¸): Î¸ âˆˆ Î˜t} are convex. Then there exist co-state processes, pâˆ—:= {pâˆ— t : t = 0,. . .,T}, such that the following hold for t = 0,. . .,T âˆ’1: zâˆ— t+1 = f(zâˆ— t ,Î¸âˆ— t ), z0 = x, (10.17) pâˆ— t = âˆ‡zHt(zâˆ— t ,pâˆ— t+1,Î¸âˆ— t ), pâˆ— T = âˆ’âˆ‡Î¦(xâˆ— T, y), (10.18) Ht(zâˆ— t ,pâˆ— t+1,Î¸t), â‰¥Ht(zâˆ— t ,pâˆ— t+1,Î¸), for all Î¸ âˆˆÎ˜t, t = 0,. . .,T âˆ’1, (10.19) where the Hamiltonian function is given by Ht(z,p,Î¸) := p Â· ft(z,Î¸) âˆ’1 S Lt(z,Î¸). (10.20) The precise statement of Theorem 10.5 involves explicit smoothness assumptions and additional technicalities (such as the inclusion of an abnormal multiplier). We refer the reader to Li et al. (2017) and the original proof (Halkin, 1966) for a thorough discussion of these issues. Compared with the continuous-time PMP, the striking additional assumption in Theorem 10.5 is the convexity of the sets {ft(z,Î¸): Î¸ âˆˆÎ˜t} and {Lt(z,Î¸): Î¸ âˆˆÎ˜t} for each ï¬xed z. Note that this is in general unrelated to the convexity, in the sense of functions of Lt with respect to either z or Î¸. For example, the scalar function f(z,Î¸) = Î¸3 sin(z) is evidently non-convex in both arguments, but {f(z,Î¸): Î¸ âˆˆR} 10.5 Future Work 435 is convex for each z. On the other hand {Î¸z : Î¸ âˆˆ{âˆ’1,1}} is non-convex because here we have a non-convex admissible set. The convexity assumptions place some mild restrictions on the types of neural network structures to which this result can apply. Let us ï¬rst assume that the admissible sets Î˜t are convex. Then, the assumption with respect to Lt is not restrictive since most regularizers (e.g. â„“1, â„“2) satisfy it. Let us consider the convexity of {ft(z,Î¸): Î¸ âˆˆÎ˜t}. In classical feed-forward neural networks, there are two types of layers: trainable and non-trainable. Suppose layer t is non-trainable (e.g. f(zt,Î¸t) = Ïƒ(zt) where Ïƒ is a non-linear activation function); then for each z the set {ft(z,Î¸): Î¸ âˆˆÎ˜t} is a singleton, and hence trivially convex. On the other hand, in trainable layers, ft is usually aï¬ƒne in Î¸. This includes fully connected layers, convolution layers and batch normalization layers (Ioï¬€e and Szegedy, 2015). In these cases, as long as the admissible set Î˜t is convex, the convexity assumption is again satisï¬ed. Residual networks also satisfy the convexity constraint if one introduces auxiliary variables (see Li and Hao, 2018, Appendix A.1). When the set Î˜t is not convex then it is not generally true that the PMP constitutes necessary conditions. Nevertheless, even in cases where the PMP does not hold, an error estimate similar to Lemma 10.3 can be derived, on the basis of which eï¬ƒcient training algorithms can be developed (Li and Hao, 2018). Finally, we remark that in the continuous-time case, the convexity condition can be removed due to the â€œconvexifyingâ€ eï¬€ect of integration with respect to time (Halkin, 1966; Warga, 1962). Hence, the convexity condition is purely an artifact of discrete-time dynamical systems. 10.5 Future Work A very important issue is to analyze the generalization error in deep neural network models. To do so, one has to be careful about choosing the right regularization term, as was suggested in the work of E et al. (2019b). It is also of great interest to formulate similar continuous-in-time, control theory, or game theory models for other networks such as GANs (generative adversarial network) or autoencoders. References Vladimir V. Aleksandrov (1968). On the accumulation of perturbations in the linear systems with two coordinates. Vestnik MGU, 3, 67â€“76. Michael Athans and Peter L. Falb (2013). Optimal Control: An Introduction to the Theory and its Applications. Courier Corporation. Mokhtar S. Bazaraa, Hanif D. Sherali, and Chitharanjan M. Shetty (2013). Nonlin- ear Programming: Theory and Algorithms. John Wiley & Sons. 436 E et al: Dynamical Systems, Optimal Control and Deep Learning Richard Bellman (2013). Dynamic Programming. Courier Corporation. Dimitri P. Bertsekas (1995). Dynamic Programming and Optimal Control, volume 1. Athena Scientiï¬c. Dimitri P. Bertsekas (1999). Nonlinear Programming. Athena Scientiï¬c. John T. Betts (1988). Survey of numerical methods for trajectory optimization. Journal of Guidance Control and Dynamics, 21(2), 193â€“207. Vladimir Grigorâ€™evich Boltyanskii, Revaz Valerâ€™yanovich Gamkrelidze and Lev Semenovich Pontryagin (1960). The theory of optimal processes. I. The max- imum principle. Izv. Akad. Nauk SSSR. Ser. Mat., 24, 3â€“42. Arthur Earl Bryson (1975). Applied Optimal Control: Optimization, Estimation and Control. CRC Press. RenÃ© Carmona and FranÃ§ois Delarue (2015). Forwardâ€“backward stochastic diï¬€er- ential equations and controlled McKeanâ€“Vlasov dynamics. Annals of Proba- bility, 43(5), 2647â€“2700. Tian Qi Chen, Yulia Rubanova, Jesse Bettencourt, and David K. Duvenaud (2018). Neural ordinary diï¬€erential equations. In Advances in Neural Information Processing Systems 31, 6572â€“6583. Felix L. Chernousko and Alexey A. Lyubushin (1982). Method of successive ap- proximations for solution of optimal control problems. Optimal Control Ap- plications and Methods, 3(2), 101â€“114. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei (2009). ImageNet: A large-scale hierarchical image database. In: Proc. Computer Vision and Pattern Recognition. Weinan E (2017). A proposal on machine learning via dynamical systems. Com- munications in Mathematics and Statistics, 5(1), 1â€“11. Weinan E, Jiequn Han, and Arnulf Jentzen (2017). Deep learning-based numeri- cal methods for high-dimensional parabolic partial diï¬€erential equations and backward stochastic diï¬€erential equations. Communications in Mathematics and Statistics, 5(4), 349â€“380. Weinan E, Jiequn Han, and Qianxiao Li (2019a). A mean-ï¬eld optimal control formulation of deep learning. Research in the Mathematical Sciences, 6(1). 10. Weinan E, Chao Ma, and Qingcan Wang (2019b). A priori estimates for the popu- lation risk for residual networks. Submitted. Eldad Haber and Lars Ruthotto (2017). Stable architectures for deep neural net- works. Inverse Problems, 34(1), 014004. Hubert Halkin (1966). A maximum principle of the Pontryagin type for systems described by nonlinear diï¬€erence equations. SIAM Journal on Control, 4(1), 90â€“111. Jiequn Han, Arnulf Jentzen, and Weinan E (2018a). Solving high-dimensional partial diï¬€erential equations using deep learning. Proceedings of the National Academy of Sciences, 115(34), 8505â€“8510. References 437 Jiequn Han, Linfeng Zhang, Roberto Car, and Weinan E (2018b). Deep potential: A general representation of a many-body potential energy surface. Communi- cations in Computational Physics, 23 (3), 629â€“639. Magnus R. Hestenes. Multiplier and gradient methods (1969). Journal of Optimiza- tion Theory and Applications, 4(5), 303â€“320. Sergey Ioï¬€e and Christian Szegedy (2015). Batch normalization: accelerating deep network training by reducing internal covariate shift. Pages 448â€“456 of: Proc. International Conference on Machine Learning. Yann LeCun (1989). A theoretical framework for back-propagation. In: Proc. 1988 Connectionist Models Summer School, vol. 1 D. Touretzky, G. Hinton and T. Sejnowski (eds). Morgan Kauï¬€man, pp. 21â€“28. Qianxiao Li and Shuji Hao (2018). An optimal control approach to deep learning and applications to discrete-weight neural networks. Pages 2985â€“2994 of: Proc. International Conference on Machine Learning. Qianxiao Li, Long Chen, Cheng Tai, and Weinan E (2017). Maximum principle based algorithms for deep learning. Journal of Machine Learning Research, 18(1), 5998â€“6026. Daniel Liberzon (2012). Calculus of Variations and Optimal Control Theory: A Concise Introduction. Princeton University Press. Dong C. Liu and Jorge Nocedal (1989). On the limited memory BFGS method for large scale optimization. Mathematical Programming, 45(1), 503â€“528. Yiping Lu, Aoxiao Zhong, Quanzheng Li, and Bin Dong (2017). Beyond ï¬nite layer neural networks: Bridging deep architectures and numerical diï¬€erential equations. ArXiv 1710.10121. Panos Parpas and Corey Muir (2019). Predict globally, correct locally: Parallel-in- time optimal control of neural networks. ArXiv 1902.02542. Lev S. Pontryagin (1987). Mathematical Theory of Optimal Processes. CRC Press. Lars Ruthotto and Eldad Haber (2018). Deep neural networks motivated by partial diï¬€erential equations. ArXiv 1804.04272. Anil V. Rao (2009). A survey of numerical methods for optimal control. Advances in the Astronautical Sciences, 135(1), 497â€“528. Sanford M. Roberts and Jerome S. Shipman (1972). Two-point boundary value problems: Shooting methods. SIAM Rev., 16(2), 265â€“266. Sho Sonoda and Noboru Murata (2019). Transport analysis of inï¬nitely deep neural network. Journal of Machine Learning Research, 20(1), 31â€“82. Jack Warga (1962). Relaxed variational problems. Journal of Mathematical Analysis and Applications, 4(1), 111â€“128. Dinghuai Zhang, Tianyuan Zhang, Yiping Lu, Zhanxing Zhu, and Bin Dong (2019). You only propagate once: Accelerating adversarial training via maximal prin- ciple. ArXiv 1905.00877. Linfeng Zhang, Jiequn Han, Han Wang, Roberto Car, and Weinan E (2018a). Deep potential molecular dynamics: A scalable model with the accuracy of quantum mechanics. Physical Review Letters, 120(14), 143001. 438 E et al: Dynamical Systems, Optimal Control and Deep Learning Linfeng Zhang, Jiequn Han, Han Wang, Wissam Saidi, Roberto Car, and Weinan E (2018b). End-to-end symmetry preserving inter-atomic potential energy model for ï¬nite and extended systems. In: Advances in Neural Information Processing Systems 31, 4441â€“4451. 11 Bridging Many-Body Quantum Physics and Deep Learning via Tensor Networks Yoav Levine, Or Sharir, Nadav Cohen and Amnon Shashua Abstract: Deep network architectures have exhibited an unprecedented ability to encompass the convoluted dependencies which characterize hard learning tasks such as image classiï¬cation or speech recognition. However, some key questions regarding deep learning architecture design have no adequate theoretical answers. In the seemingly unrelated ï¬eld of many-body physics, there is a growing need for highly expressive computational schemes that are able to eï¬ƒciently represent highly entangled many-particle quantum systems. In this chapter, we describe a tensor network (TN) based common language that has been established between the two disciplines, which allows for bidirectional contributions. By showing that many-body wave functions are structurally equivalent to mappings of convolutional and recurrent networks, we can construct their TN descriptions and bring forth quantum entanglement measures as natural quantiï¬ers of dependencies modeled by such networks. Accordingly, we propose a novel entanglement-based deep learning design scheme that sheds light on the success of popular architectural choices made by deep learning practitioners and suggests new practical prescriptions. In the other direction, we identify the fact that an inherent re-use of information in prominent deep learning architectures is a key trait distinguishing them from standard TN- based wave function representations. Therefore, we employ a TN manifestation of information re-use and construct TNs corresponding to deep recurrent networks and overlapping convolutional networks. This allows us to demonstrate theoreti- cally that these architectures are powerful enough to represent highly entangled quantum systems polynomially more eï¬ƒciently than the previously employed re- stricted Boltzmann machines. We thus provide theoretical motivation to shift trend- ing neural-network-based wave function representations closer to state-of-the-art deep learning architectures. 439 440 Levine et al: Quantum Physics and Deep Learning via TNs 11.1 Introduction Machine learning and many-body physics are distinct scientiï¬c disciplines; however, they share a common need for the eï¬ƒcient representation of highly expressive multivariate function classes. In the former, the function class of interest describes the dependencies required for performing a modern machine learning task and in the latter it captures the entanglement of the many-body quantum system under examination. In the physics domain, a prominent approach for simulating many-body quantum systems makes use of their entanglement properties in order to construct tensor network (TN) architectures that aptly model them. Though this method is successful in modeling one-dimensional (1D) systems through the matrix product state (MPS) TN (Fannes et al., 1992; Perez-GarcÃ­a et al., 2007), it still faces diï¬ƒculties in modeling two-dimensional (2D) systems due to their intractability (Verstraete and Cirac, 2004; OrÃºs, 2014). In the machine learning domain, deep network architectures have enabled un- precedented results in recent years (Krizhevsky et al., 2012; Simonyan and Zisser- man, 2014; Szegedy et al., 2015; He et al., 2016; Sutskever et al., 2011; Graves et al., 2013; Bahdanau et al., 2014; Amodei et al., 2016), owing to their ability to eï¬ƒciently capture intricate dependencies in complex data sets. However, despite their popularity in science and industry, formal understanding of these architec- tures is limited. Speciï¬cally, the question of why the multivariate function families induced by common deep learning architectures successfully capture the dependen- cies brought forth by challenging machine learning tasks, is largely open. The applications of TN in machine learning include optimizations of an MPS to perform learning tasks (Stoudenmire and Schwab, 2016; Han et al., 2017) and un- supervised preprocessing of the data set via tree TNs (Stoudenmire, 2017). Inspired by recent achievements in machine learning, quantum wave function representa- tions based on fully connected neural networks and restricted Boltzmann machines (RBMs), which represent relatively mature deep learning constructs, have recently been suggested (Carleo and Troyer, 2017; Saito, 2017; Deng et al., 2017a; Gao and Duan, 2017; Carleo et al., 2018; Cai and Liu, 2018). Consequently, RBMs have been shown as being able to support high entanglement with a number of parame- ters that is linear in the number of quantum particles in 2D (Deng et al., 2017b), as opposed to thequadratic dependence required in 2D fully connected networks. In this chapter, we describe a TN-based approach for modeling the deep learn- ing architectures that are at the forefront of recent empirical successes. Thus, we establish a bridge that facilitates an interdisciplinary transfer of results and tools, and allows us to address the abovementioned needs of both ï¬elds. First, we import concepts from quantum physics that enable us to obtain new results in the rapidly 11.1 Introduction 441 evolving ï¬eld of deep learning theory. In the opposite direction, we obtain TN man- ifestations of provably powerful deep learning principles, which help us to establish the beneï¬ts of employing such principles for the representation of highly entangled quantum systems. We begin in Â§11.2 by providing a brief introduction to the computational chal- lenges which many-body quantum physicists face. We present the relevant concepts and tools, namely, many-body wave-functions, quantum entanglement, and ten- sor networks. Next, in Â§11.3 we identify an equivalence between the tensor-based form of a many-body wave function and the function realized by convolutional arithmetic circuits (ConvACs) (Cohen et al., 2016b; Cohen and Shashua, 2016, 2017) and single-layered recurrent arithmetic circuits (RACs) (Levine et al., 2017; Khrulkov et al., 2018). These are representatives of two prominent deep learning ar- chitecture classes: convolutional networks, commonly operating over spatial inputs and used for tasks such as image classiï¬cation; and recurrent networks, commonly operating over temporal inputs and used for tasks such as speech recognition.1 Given the above equivalence, we construct a tree TN representation of the ConvAC architecture and an MPS representation of the RAC architecture (Figure 11.4), and show how entanglement measures (Plenio and Virmani, 2007) quantify, in an natu- ral way, the ability of the multivariate function realized by such networks to model dependencies. Consequently, we are able to demonstrate in Â§11.4 how the common practice, in many-body physics, of entanglement-based TN architecture selection can be readily converted into a methodological approach for matching the architecture of a deep network to a given task. Speciï¬cally, our construction allows the translation of derived bounds on the maximal entanglement represented by an arbitrary TN (Cui et al., 2016) into machine learning terms. We thus obtain novel quantum- physics inspired practical guidelines for the task-tailored architecture design of deep convolutional networks. The above analysis highlights a key principle separating powerful deep learning architectures from common TN-based representations, namely, the re-use of in- formation. Speciï¬cally, in the ConvAC architecture, which can be shown to be de- scribed by a tree TN, the convolutional windows have 1Ã—1 receptive ï¬elds and there- fore do not spatially overlap when slid across the feature maps. In contrast, state-of- the-art convolutional networks involve larger convolution kernels (e.g. 3Ã—3), which therefore overlap during calculation of the convolution (Krizhevsky et al., 2012; Simonyan and Zisserman, 2014). Such overlapping architectures inherently involve the re-use of information, since the same activation is used for the calculation of several adjacent activations in the subsequent layer. Similarly, unlike the shallow 1 See a similar analysis of the more recent self-attention architecture class, which has advanced the ï¬eld of natural language processing, in Levine et al. (2020). 442 Levine et al: Quantum Physics and Deep Learning via TNs recurrent network, which can be shown to be described by an MPS TN, state-of- the-art deep recurrent networks inherently involve information re-use (Graves et al., 2013). Relying on the above observation, we employ in Â§11.5 a duplication method for representing information re-use within the standard TN framework, reminiscent of that introduced in the context of categorical TN states (Biamonte et al., 2011). Accordingly, we are able to present new TN constructs that correspond to deep recurrent and overlapping convolutional architectures (Figures 11.8 and 11.9). We thus obtain the tools to examine the entanglement scaling of powerful deep networks. Speciï¬cally, we prove that deep recurrent networks support entanglement which corresponds to critical quantum systems in 1D, and that overlapping convolutional networks can support arbitrary entanglement scaling in 1D and in 2D (see Â§11.2.2 for the deï¬nition of entanglement scaling). Our analysis shows that the number of parameters required for supporting highly entangled systems scales as the square root of the number of particles in 2D overlap- ping convolutional networks. Therefore, we can demonstrate that these networks not only allow tractable access to 2D quantum systems of sizes unattainable by current TN-based approaches but also are polynomially more eï¬ƒcient in representing highly entangled 2D systems in comparison with previously suggested neural-network wave function representations. We thus establish the formal beneï¬ts of employing state-of-the-art deep learning principles for many-body wave function represen- tation (which have recently inspired corresponding empirical achievements; see Sharir et al., 2020), and we suggest a practical framework for implementing and investigating these architectures within the standard TN platform. 11.2 Preliminaries â€“ Many-Body Quantum Physics In this section we succinctly outline relevant eï¬€orts in the numerical investigation of many-body quantum phenomena that have been carried out in the condensed matter physics community over the past few decades. This ï¬eld addresses physical systems composed of many interacting quantum particles, as is the case in nearly all materials in nature (a few grams of any material contain of the order of 1023 atoms). The quantum properties governing the interactions between the particles give rise to to highly non-intuitive phenomena, such as magnetism, superconductivity, and more, that are observed on a macroscopic scale. The analytical and computational methods that have been developed to solve problems in atomic and nuclear physics do not scale well for a large number of particles, and are unable to account for the observed many-body phenomena or to predict emergent properties. Therefore, a numerical toolbox has been developed with the intention of harnessing modern computational power for probing these, until recently, unattainable natural properties. 11.2 Preliminaries â€“ Many-Body Quantum Physics 443 Our analysis, presented subsequently in Â§Â§11.3â€“11.4, draws inspiration from concepts and tools developed in the ï¬eld of many-body quantum physics, which is considerably more mature than the ï¬eld of deep learning theory. With that said, the established connections also contribute in the other direction: in Â§11.5 we rely on recent theoretical advances in deep learning theory and provide novel contributions to the eï¬€ort of investigating interacting many-body quantum systems. A secondary goal of this section is to provide a ramp-up regarding basic concepts in tensor analysis and many-body quantum physics that are necessary for following our analyses. We begin in Â§11.2.1 by describing the concept of many-body quantum wavevfunctions, and then move on to present the notion of quantum entanglement and its measures in Â§11.2.2. Finally, in Â§11.2.3 we give a short introduction to TNs â€“ a numerical tool employed for modeling many-body quantum systems â€“ and discuss its strengths and limitations. 11.2.1 The Many-Body Quantum Wave Function We provide below a short introduction to the notation used by physicists when describing quantum mechanical properties of a many-body system. We follow relevant derivations in Preskill (1998) and Hall (2013), referring the interested reader to these sources for a more comprehensive mathematical introduction to quantum mechanics. A complete description of a quantum system is given in quantum mechanics by its wave function, alternatively referred to as its state. We limit our discussion to states which reside in ï¬nite-dimensional Hilbert spaces, as these are at the heart of the analogies that we draw with deep learning architectures. The quantum state is simply a vector in such a space. Besides being of interest to us, these spaces have also been extensively investigated in the physics community. For example, the spin component of a spinful particleâ€™s wave function resides in a ï¬nite-dimensional Hilbert space. Physicists employ the â€˜ketâ€™ notation, in which a vector Ïˆ is denoted by |ÏˆâŸ©âˆˆH. The Hilbert space H has an inner product denoted by âŸ¨Ï†|ÏˆâŸ©, which maps a pair of vectors in H to a scalar. A â€˜braâ€™ notation, âŸ¨Ï†|, is used for the â€˜dual vectorâ€™, which formally is a linear mapping between vectors to scalars, deï¬ned as |ÏˆâŸ©7â†’âŸ¨Ï†|ÏˆâŸ©. We can intuitively think of a â€˜ketâ€™ as a column vector and â€˜braâ€™ as a row vector. One can represent a general single-particle state |ÏˆâŸ©âˆˆH1, where dim(H1) = M, as a linear combination of some orthonormal basis vectors: |ÏˆâŸ©= M Ã• d=1 vd| Ë†ÏˆdâŸ©, (11.1) 444 Levine et al: Quantum Physics and Deep Learning via TNs where v âˆˆCM is the vector of coeï¬ƒcients compatible with the basis  | Ë†ÏˆdâŸ© M d=1 of H1, each entry of which can be calculated by the projection: vd = âŸ¨Ë†Ïˆd|ÏˆâŸ©. We extend the discussion to the many-body case of N particles, each correspond- ing to a local Hilbert space Hj for j âˆˆ[N] such that, for all j, dim(Hj) = M. Denoting an orthonormal basis of the local Hilbert space by  | Ë†ÏˆdâŸ© M d=1, the many- body wave function |ÏˆâŸ©âˆˆH = âŠ—N j=1Hj can be written as |ÏˆâŸ©= Ã•M d1,...,dN =1 Ad1,...,dN Ë†Ïˆd1,...,dN , (11.2) where Ë†Ïˆd1,...,dN := Ë†Ïˆd1 âŠ—Â· Â· Â· âŠ— Ë†ÏˆdN is a basis vector of the M N-dimensional many-body Hilbert space H, and the coeï¬ƒcient tensor A is the generalization of the above coeï¬ƒcients vector to the many-body case. A tensor may be thought of as a multi-dimensional array. The order of a tensor is deï¬ned to be the number of indexing entries in the array, which are referred to as modes. The dimension of a tensor in a particular mode is deï¬ned as the number of values that may be taken by the index in that mode. The coeï¬ƒcient tensor A is a tensor of order N and dimension M in each mode j âˆˆ[N] := {1,. . ., N}; its entries are denoted Ad1,...,dN , where the index in each mode takes values between 1 and the dimension, dj âˆˆ[M]. As per its deï¬nition, the number of entries in the coeï¬ƒcient tensor is exponential in the number of quantum particles. This is in fact the reason why many-body quantum systems cannot be handled by the analytical and computational tools developed for other ï¬elds in quantum physics that involve only a small number of particles. However, even when attempting to extract relevant information by an arbitrarily powerful computer program, solutions for systems of over 100 particles would be unattainable as they require more coeï¬ƒcients than the number of atoms in the universe. In what appears to be an unsurpassable barrier, interesting quantum phenomena can be observed only for systems of sizes much larger than 100. In the following subsection, we obtain tools for discussing â€˜luckyâ€™ natural traits that, despite the above, allow physicists to escape this exponential curse of dimensionality and achieve meaningful insights regarding many-body quantum systems. 11.2.2 Quantum Entanglement Measures We present below the concept of quantum entanglement that is widely used by physicists as a quantiï¬er of dependencies in a many-body quantum system. One of the fathers of quantum mechanics, Erwin SchrÃ¶dinger, deï¬ned entanglement as â€œthe characteristic trait of quantum mechanics, the one that enforces its entire departure from classical lines of thoughtâ€ (SchrÃ¶dinger, 1935). In the last few decades, en- tanglement has played a key role in understanding emergent quantum many-body 11.2 Preliminaries â€“ Many-Body Quantum Physics 445 phases of matter. Following the formal deï¬nition, which directly relates to the many-body wave function above discussed, we will elaborate on the entanglement properties that have been examined by many-body physicists. Consider a partition of a system of N particles, labeled by integers [N] := {1,. . ., N}, which splits it into two disjoint subsystems A Â·âˆªB = [N]. Let H A and H B be the Hilbert spaces in which the many body wave functions of the particles in subsystems A and B reside, respectively, with H  H A âŠ—H B. The many-body wave function in (11.2) can be now written as: |ÏˆâŸ©= dim(H A) Ã• Î±=1 dim(HB) Ã• Î²=1 (âŸ¦AâŸ§A,B)Î±,Î²|Ïˆ A Î± âŸ©âŠ—|ÏˆB Î² âŸ©, (11.3) where {|Ïˆ A Î± âŸ©}dim(H A) Î±=1 and {|ÏˆB Î² âŸ©}dim(HB) Î²=1 are bases for H A and H B, respectively,2 and âŸ¦AâŸ§A,B is the matricization of A with respect to the partition (A, B), which is essentially the rearrangement of the tensor elements as a matrix whose rows correspond to A and columns to B.3 Let us denote the maximal rank of âŸ¦AâŸ§A,B by r := min  dim(H A),dim(H B). A singular value decomposition of âŸ¦AâŸ§A,B results in the following form (also referred to as the Schmidt decomposition): |ÏˆâŸ©= rÃ• Î±=1 Î»Î±|Ï†A Î±âŸ©âŠ—|Ï†B Î±âŸ©, (11.4) where Î»1 â‰¥Â· Â· Â· â‰¥Î»r are the singular values of âŸ¦AâŸ§A,B, and {|Ï†A Î±âŸ©}r Î±=1, {|Ï†B Î±âŸ©}r Î±=1 are r vectors in new bases for H A and H B, respectively, obtained by the decompo- sition. Equation (11.4) represents the N-particle wave function in terms of a sum of tensor products between its two disjoint parts. Each summand in (11.4) is a separable state with respect to the partition (A, B), which represents the fact that there is no quantum dependency between these two subsystems. Essentially, the measure of entanglement with respect to the partition (A, B) is a quantity that represents the diï¬€erence between the state in question and a state that is separable with respect to this partition. There are several diï¬€erent quantum entanglement measures. The entanglement entropy (Vedral and Plenio, 1998) is one such measure; it is deï¬ned as:4 SA,B = âˆ’Ã Î± |Î»Î±|2 ln |Î»Î±|2. The minimal entanglement entropy, S = 0, is achieved when 2 It is possible to write |Ïˆ A Î± âŸ©= Ïˆda1 E âŠ—Â· Â· Â· âŠ—|Ïˆda| A| âŸ©and |ÏˆB Î² âŸ©= |Ïˆdb1 âŸ©âŠ—Â· Â· Â· âŠ—|Ïˆdb|B| âŸ©with the matricization mapping from A := {a1, . . . , a| A| } to Î± and from B := {b1, . . . , b|B| } to Î². 3 Speciï¬cally, âŸ¦AâŸ§A,B, is the M | A| Ã— M |B| matrix containing the entries of A such that Ad1,...,dN is placed in the row whose index is 1+Ã| A| t=1(dit âˆ’1)M | A|âˆ’t and the column whose index is 1+Ã|B| t=1(djt âˆ’1)M |B|âˆ’t. 4 |ÏˆâŸ©is conventionally chosen to be normalized so that the singular values satisfy Ã Î± |Î»Î± |2 = 1. This can be relaxed and the entropy may be deï¬ned on the normalized singular values. 446 Levine et al: Quantum Physics and Deep Learning via TNs the rank of âŸ¦AâŸ§A,B is 1. When âŸ¦AâŸ§A,B is fully ranked, the entanglement entropy obtains its maximal value of ln(r) (upon normalization of the singular values). Another measure of entanglement is the geometric measure, deï¬ned as the L2 distance of |ÏˆâŸ©from the set of separable states, min|Ïˆsp(A,B)âŸ©|âŸ¨Ïˆsp(A,B)|ÏˆâŸ©|2; it can be shown (e.g., Cohen and Shashua, 2017) to be D = s 1 âˆ’ |Î»1|2 Ãr Î±=1 |Î»Î±|2 . A ï¬nal measure of entanglement we mention is the Schmidt number, which is simply the rank of âŸ¦AâŸ§A,B. i.e., the number of its non-zero singular values. All these measures are minimal for states which are separable with respect to the partition (A, B) and increase when the quantum dependency between sub-systems A and B is more complicated. To connect the abstract notion of entanglement with condensed matter problems currently being investigated, we describe a partition commonly employed by many- body physicists to examine the entanglement properties of an N-particle system in d dimensions. For this partition, A is taken to be a small contiguous subsystem of size |A| â‰ªN/2, and B is taken to be its complement (see Figure 11.1a). In this setting, one asks how the entanglement measures scale as the size of subsystem A is gradually varied. We denote by Alin the typical size of subsystem A along each dimension (also referred to as its linear dimension). If the entanglement entropy with respect to the partition A, B scales as subsystem Aâ€™s d-dimensional volume, i.e., SA,B âˆ(Alin)d, then the entire N-particle system is said to exhibit volume-law entanglement scaling. This is referred to as a highly entangled scenario, since it implies that each particle in subsystem A exhibits a quantum dependency on particles in subsystem B (see Figure 11.1b). If, in contrast, the entanglement entropy scales as subsystem Aâ€™s d- dimensional area, i.e., SA,B âˆ(Alin)dâˆ’1, then the N-particle system is said to exhibit area-law entanglement scaling. In this scenario, qualitatively, the entanglement is short range and therefore only particles at the boundary between subsystems A and B are entangled (see Figure 11.1c). If one were to randomize a many-body wave function (i.e., choose a random coef- ï¬cients tensor A), a state obeying volume-law entanglement scaling will be obtained with probability 1. Stated diï¬€erently, besides a set of states of measure 0 within the many-body Hilbert space, all states exhibit volume-law entanglement scaling. Luck- ily â€“ or owing to some yet-to-be explained fundamental low-dimensional quality of nature â€“ some actual many-body systems of interest are members of this negligible set and follow sub-volume law entanglement scaling. In the following subsection we present a leading numerical approach for simulating such systems. 11.2 Preliminaries â€“ Many-Body Quantum Physics 447 Figure 11.1 (a) A typical partition considered in entanglement scaling â€“ A is a small contiguous subsystem of linear dimension Alin and B is its complement. (b) Volume-law entanglement scaling implies that all quantum particles in subsystem A (again deï¬ned by the square outline) are entangled with those of B (the red area marks the interacting particles). (c) Area-law entanglement scaling implies that only particles in the boundary between the subsystems are entangled. 11.2.3 Tensor Networks Directly storing the entries of a general order-N tensor, though very eï¬ƒcient in lookup time (all the entries are stored â€˜waitingâ€™ to be called upon), is very costly in storage, being exponential in N, which, as discussed above, is not feasible. A tradeoï¬€can be employed in which only a polynomial amount of parameters is kept while the lookup time increases. Namely, some calculation has to be performed in order to obtain the entries of Ad1,...,dN .5 The TN tool is a graphical representation of such a calculation; it amounts to a compact representation of a high-order tensor in terms of inner products of lower-order tensors. Its graphical description allows physicists to construct TN architectures which are straightforwardly compliant with the entanglement characterizing the state to be represented. We now provide a brief introduction to TNs, followed by some remarks on the expressive abilities and shortcomings of common TNs. Introduction to Tensor Networks A TN is essentially a weighted graph, where each node corresponds to a low-order tensor, whose order is equal to the degree of the node in the graph. Accordingly, the edges emanating out of a node, also referred to as its legs, represent the diï¬€erent modes of the corresponding low-order tensor. The weight of each edge in the graph, also referred to as its bond dimension, is equal to the dimension of the appropriate tensor mode. In accordance with the relation between the mode, dimension, and index of a tensor presented in Â§11.2.1, each edge in a TN is represented by an index that runs between 1 and its bond dimension. Figure 11.2a shows three examples: (1) a vector, which is a tensor of order 1, is represented by a node with one leg; (2) 5 It is noteworthy that, for a given tensor, there is no guarantee that the number of parameters can be actually reduced. This is dependent on its rank and on how well the decomposition ï¬ts the tensorâ€™s dependencies. 448 Levine et al: Quantum Physics and Deep Learning via TNs Figure 11.2 A quick introduction to tensor Nnetworks (TNs). (a) Tensors in the TN are repre- sented by nodes. The degree of the node corresponds to the order of the tensor represented by it. (b) A matrix multiplying a vector in TN notation. The contracted index k, which connects two nodes, is summed over, which is not the case for the open index d. The number of open indices equals the order of the tensor represented by the entire network. All the indices receive values that range between 1 and their bond dimension. The contraction is indicated by the dashed line. a matrix, which is a tensor of order 2, is represented by a node with two legs; (3) accordingly, a tensor of order N is represented in the TN as a node with N legs. We move on to present the connectivity properties of a TN. Edges which connect two nodes in the TN represent an operation between the two corresponding tensors. An index which represents such an edge is called a contracted index, and the operation of contracting that index is in fact a summation over all the values it can take. An index representing an edge with one loose end is called an open index. The tensor represented by the entire TN, whose order is equal to the number of open indices, can be calculated by summing over all the contracted indices in the network. An example of the contraction of a simple TN is depicted in Figure 11.2b. There, a TN corresponding to the operation of multiplying a vector v âˆˆRr1 by a matrix M âˆˆRr2Ã—r1 is obtained by summing over the only contracted index, k. As there is only one open index, d, the result of contracting the network is an order-1 tensor (a vector), u âˆˆRr2, which satisï¬es u = Mv. Though below we use the contraction of indices in more elaborate TNs, this operation can be essentially viewed as a generalization of matrix multiplication. Entanglement in Common Tensor Networks A quantitative connection exists between a TN representing a quantum state and that stateâ€™s entanglement scaling (Cui et al., 2016). Essentially, for ï¬xed bond dimensions (that are independent of subsystem Aâ€™s size), the weight of the minimal cut in the TN weighted graph which separates subsystems A and B is an upper bound on the entanglement scaling of any state representable by this TN. Tensor networks in current use include the matrix product state (MPS), tree TN, and multiscale 11.2 Preliminaries â€“ Many-Body Quantum Physics 449 Figure 11.3 Common TNs. In 1D: (a) MPS; (b) tree; (c) MERA. In 2D: (d) PEPS. entanglement renormalization ansatz (MERA) in 1D; and the projected entangled pair state (PEPS) in 2D (see Figure 11.3). In these TNs, the decomposition of a high-order tensor into a set of sparsely interconnected lower-order tensors, allows the representation of an order-N tensor by a number of parameters that is linear in N. Therefore, systems containing thousands of interacting quantum particles can be simulated and many-body quantum phenomena can be investigated. As can be easily be seen by applying the above minimal-cut considerations, MPS and PEPS TNs are able to eï¬ƒciently represent area-law entanglement scaling in 1D and 2D, respectively. The MERA TN is more powerful, as it is able to eï¬ƒciently represent a logarithmic dependence on subsystem Aâ€™s volume, which in 1D is referred to as a logarithmic correction to the area-law entanglement scaling. Given the bounds on their entanglement scaling, the above TNs are far from being powerful enough to represent most states in the Hilbert space, which obey volume-law scaling. In fact, in order to represent such highly entangled states, one would need to employ TNs with exponential bond dimensions, rendering them impractical. Despite this, the unlikely existence of natural many-body systems which exhibit low entanglement has rendered the tool of TNs very fruitful in the past decade. Matrix product state TNs are employed for investigating 1D systems deep within a gapped quantum phase, such as Ising spin-chains in their ferromagnetic phase (a proxy for magnetism), as these exhibit area-law entanglement scaling (Hastings, 2007). In contrast, MERA TNs are used in order to investigate phase transition properties, such as on the critical point of Ising spin-chains between the ferromagnetic and normal phases, since at this point the systemâ€™s entanglement scaling has a logarithmic correction. In 2D the situation is more diï¬ƒcult, as PEPS TNs are restricted to area-law entanglement scaling while at the same time they suï¬€er from intractability, which gravely harms their performence. Despite the relative success of TNs in addressing many-body systems (mainly in 1D), researchers are seeking practical means to go beyond sub-volume-law limita- tions and to have numerical access to highly entangled systems in 1D, 2D, and even 3D. Restricted Boltzmann machine (RBM) based wave function representations have recently been shown to demonstrate such abilities (Deng et al., 2017b); however 450 Levine et al: Quantum Physics and Deep Learning via TNs the number of parameters they require is incompatible with current optimization approaches. In contrast, our analysis below (Â§11.5) shows that deep convolutional networks have the potential to decrease signiï¬cantly the number of parameters re- quired for modeling highly entangled systems, connecting state-of-the-art methods in deep learning with the diï¬ƒcult task of probing quantum many-body phenomena. 11.3 Quantum Wave Functions and Deep Learning Architectures In the previous section we introduced the relevant tools and described some of the remaining challenges in modern many-body quantum physics research. In the following, we tie these to the emerging ï¬eld of deep learning theory, in Â§Â§11.3â€“11.4 harnessing the tools that have been developed, and in Â§11.5 addressing contempo- rary needs. The presented analyses are further detailed in Levine et al. (2017, 2018, 2019). We begin by showing the structural equivalence between a many-body wave function and the function which a deep learning architecture implements over its inputs. The convolutional and recurrent networks described below have been ana- lyzed to date via tensor decompositions (Kolda and Bader, 2009; Hackbusch, 2012), which are compact high-order tensor representations based on linear combinations of outer products between low-order tensors. The presented equivalence to wave functions, suggests the slightly diï¬€erent algebraic approach manifested by TNs (see Â§11.2.3); ths approach uses a compact representation of a high-order tensor through contractions (or inner products) among lower-order tensors. Accordingly, we provide TN constructions of the deep learning architectures that we examine. 11.3.1 Convolutional and Recurrent Networks as Wave Functions We consider a convolutional network referred to as a convolutional arithmetic circuit (ConvAC) (Cohen et al., 2016b; Cohen and Shashua, 2016, 2017). This deep convolutional network operates similarly to the ConvNets that have typically been employed in practice (Krizhevsky et al., 2012; Simonyan and Zisserman, 2014), only with linear activations and product pooling instead of the more common non- linear activations and max out pooling. In a ConvAC, depicted in Figure 11.4a, each layer consists of a linear convolution operation followed by a spatial decimation (pooling) operation, which is carried out by multiplying neighboring activations. Proof methodologies related to ConvACs have been extended to common ConvNets (Cohen and Shashua, 2016), and from an empirical perspective, ConvACs work well in many practical settings (Sharir et al., 2016; Cohen et al., 2016a). Analogously, we examine a class of recurrent networks referred to as recurrent arithmetic circuits (RACs) (Levine et al., 2017; Khrulkov et al., 2018). These (a) 1x1 Conv 2x2 Pooling 2 H(in) H(in) H(out) H(out)= H(in) 2 2 y = W (L)x Block 0 1x1 Conv representa3on input M r0 2x2 Pooling H H Block L-1 1x1 Conv output r0 rL-1 rL-1 rl rl previous layer H(in) H(in) rl-1 x y = P(x(1), . . . , x(4)) y = W (l)x 1 1 rl rl-1 2x2 Pooling x(4) x(3) x(2) x(1) P:Rrlâ‡¥. . .â‡¥Rrl!Rrl ConvNet ConvAC & RAC yiâŒ˜maxj max n x(j) i , 0 o yiâŒ˜ Y j x(j) i RNN yiâŒ˜tanh â‡£X j x(j) i âŒ˜ ConvAC â‡â‡’ W (3) Î´ W (2) Î´ W (1) Î´ W (0) d1 f(x1) M r0 W (0) d2 f(x2) M r0 r0 r1 W (1) Î´ W (0) d3 f(x3) M r0 W (0) d4 f(x4) M r0 r0 r1 r1 r2 W (2) Î´ W (1) Î´ W (0) d5 f(x5) M r0 W (0) d6 f(x6) M r0 r0 r1 W (1) Î´ W (0) d7 f(x7) M r0 W (0) d8 f(x8) M r0 r0 r1 r1 r2 r2 Aconv (b) f(xt) W Ia P(a, b) W Ha W Oa yt â‡â‡’ h0 f(x1) W Ia P(a, b) W Ha f(x2) W Ia P(a, b) W Ha f(x3) W Ia P(a, b) W Ha W Oa y3 RAC â‡â‡’ h0 f(x1) d1 W I Î´ W H M R R f(x2) d2 W I Î´ W H M R R f(x3) d3 W I Î´ W H M R R R R R W O R Arec Figure 11.4 (a) The ConvAC (Cohen et al., 2016b), which has convolution kernels of size 1 Ã— 1, linear activations, and product pooling operations (P is an element-wise multiplication operation), is equivalent to a tree TN (presented for the case of N = 8 in its 1D form for clarity). The triangular Î´-tensors impose the same-channel pooling trait as the ConvAC. The matrices in the tree TN host the ConvACâ€™s convolutional weights, and the bond dimensions at each tree level i âˆˆ[L] are equal to the number of channels in the corresponding layer of the ConvAC, also referred to as its width, denoted ri. This fact allows us to translate a min-cut result on the TN into practical conclusions regarding pooling geometry and layer widths in convolutional networks. (b) The shallow RAC (Levine et al., 2017), which merges the hidden state of the previous time step with new incoming data via the multiplicative integration operation, is equivalent to an MPS TN (presented for the case of N = 3). The matrices W I, W H, and W O, are the RACâ€™s input, hidden, and output weight matrices, respectively. The Î´-tensors correspond to the multiplicative integration property of RACs. In (11.5), we write down the output of the RAC after N time steps. https://doi.org/10.1017/9781009025096.012 Published online by Cambridge University Press 452 Levine et al: Quantum Physics and Deep Learning via TNs networks employ linear activations but otherwise share the architectural features of standard recurrent networks, where information from previous time-steps is mixed with current incoming data via a multiplicative integration operation (Wu et al., 2016; Sutskever et al., 2011) (see Figure 11.4b). It has been experimentally demonstrated that the conclusions reached by analyses of the above arithmetic circuits extend to commonly used networks (Cohen and Shashua, 2017; Levine et al., 2018; Sharir and Shashua, 2018; Khrulkov et al., 2018). In the convolutional case, we consider tasks in which the network is given an N-pixel input image, X = (x[0] 1 ,. . .,x[0] N ) (e.g., image classiï¬cation). In the recurrent case, we focus on tasks in which the network is given a sequential input {x[0] t }N t=1 (e.g. speech recognition). The output of a ConvAC/single-layered RAC was shown to obey: y(x[0] 1 ,. . .,x[0] N ) = M Ã• d1,...,dN =1 Aconv/rec d1,...,dN N Ã– j=1 fdj(x[0] j ), (11.5) where { fd}M d=1 are linearly independent representation functions, which form an initial mapping of each input x[0] j to a vector ( f1(x[0] j ),. . ., fM(x[0] j ))T âˆˆRM. The tensors Aconv and Arec that deï¬ne the computation of the ConvAC and RAC, have been analyzed to date via the hierarchical tucker (Hackbusch and KÃ¼hn, 2009) and tensor train (Oseledets, 2011) decompositions, respectively. Their entries are polynomials in the appropriate networkâ€™s convolutional weights (Cohen et al., 2016b) or recurrent weights (Levine et al., 2017; Khrulkov et al., 2018), and their N indices respectively correspond to the N spatial or temporal inputs. Considering N-particle quantum states with a local Hilbert space Hj of dimen- sion M for j âˆˆ[N], (11.5) is equivalent to the inner product, y(x[0] 1 ,. . .,x[0] N ) = âŸ¨Ïˆ product(x[0] 1 ,. . .,x[0] N )|Ïˆ conv/recâŸ©, (11.6) with the ket state corresponding to the convolutional/recurrent networkâ€™s operation (see Â§11.2.1 for a deï¬ntion of the many-body quantum wave function), |Ïˆ conv/recâŸ©= M Ã• d1,...,dN =1 Aconv/rec d1,...,dN Ë†Ïˆd1,...,dN , (11.7) and the product state (which exhibits no entanglement with respect to any partition) corresponding to the inputs, |Ïˆ product(x[0] 1 ,. . .,x[0] N )âŸ©= M Ã• d1,...,dN =1 N Ã– j=1 fdj(x[0] j ) Ë†Ïˆd1,...,dN , (11.8) where Ë†Ïˆd1,...,dN is some orthonormal basis of the many-body Hilbert space H = 11.4 Deep Learning Architecture Design via Entanglement Measures 453 âŠ—N j=1Hj . In this structural equivalence, the N inputs to the deep learning architecture (e.g., pixels in an input image or syllables in an input sentence) are analogous to the N particles. Since the unentangled product state6 |Ïˆ product(x[0] 1 ,. . .,x[0] N )âŸ©can be associated with some local preprocessing of the inputs, all the information regarding input dependencies that the network is able to model is eï¬€ectively encapsulated in Aconv/rec, which by deï¬nition also holds the entanglement structure of the state |Ïˆconv/recâŸ©(see Â§11.2.2). 11.3.2 Tensor Network Representations of Convolutional and Recurrent Networks In order to investigate the properties of the convolutional or recurrent weights tensors, and in light of their above connection with quantum wave functions, we examine their form in TN language (see Â§11.2.3 for an introduction to TNs). In Figure 11.4a, we present the TN corresponding to Aconv. The depth of this tree TN is equal to the depth of the convolutional network, L, and the circular 2-legged tensors represent matrices holding the convolutional weights of each layer i âˆˆ[L], denoted W(i). Accordingly, the bond dimension of the TN edges comprising each tree level i âˆˆ[L] is equal to the number of channels in the corresponding layer of the convolutional network, ri, referred to as the layerâ€™s width. The 3-legged triangles in the tree TN represent Î´jkl tensors (equal to 1 if j = k = l and 0 otherwise), which correspond to the prevalent â€˜same-channel poolingâ€™ decimation procedure. In Figure 11.4b we present the TN corresponding to Arec. In this MPS-shaped TN, the circular 2-legged tensors represent matrices holding the input, hidden, and output weights, respectively denoted WI, WH, and WO. The Î´-tensors correspond to the multiplicative integration trait of the RAC. 11.4 Deep Learning Architecture Design via Entanglement Measures The structural connection between many-body wave functions and functions real- ized by convolutional and recurrent networks, (11.5) and (11.6) above, creates an opportunity to employ well-established tools and insights from many-body physics for the design of these deep learning architectures. We begin this section by showing that quantum entanglement measures extend the means previously used for quan- tifying dependencies modeled by deep learning architectures. Then, inspired by common condensed matter physics practice, we rely on the TN descriptions of the above architectures and propose a novel methodology for principled deep network design. 6 The underlying tensor of this state, given entry-wise by A product d1,...,dN := ÃN j=1 fdj (x[0] j ), is of rank 1. Therefore, any entanglement measure deï¬ned on it (see Â§11.2.2) would be minimal. 454 Levine et al: Quantum Physics and Deep Learning via TNs 11.4.1 Dependencies via Entanglement Measures In Cohen and Shashua (2017), the algebraic notion of the separation rank is used as a tool for measuring dependencies modeled between two disjoint parts of a deep convolutional networkâ€™s input. Let (A, B) be a partition of [N]. The separation rank of y(x[0] 1 ,. . .,x[0] N ) with respect to (A, B) is deï¬ned as the minimal number of multiplicatively separable (with respect to (A, B)) summands that together give y. For example, if y is separable with respect to (A, B) then its separation rank is 1 and it models there being no dependency between the inputs of A and those of B.7 The higher the separation rank, the stronger the dependency modeled between sides of the partition (Beylkin and Mohlenkamp, 2002). Remarkably, due to the equivalence in (11.6), the separation rank of the function realized by a ConvAC or a single-layered RAC, (11.5), with respect to a partition (A, B) is equal to the Schmidt entanglement measure of Ïˆconv/rec with respect to (A, B).8 As per their deï¬nitions (see Â§11.2.2), the logarithm of the Schmidt entanglement measure upper-bounds the stateâ€™s entanglement entropy with respect to (A, B). The analysis of separation ranks, now extended to entanglement measures, brings forth a principle for designing a deep learning architecture intended to perform a task speciï¬ed by certain dependencies â€“ the network should be designed such that these dependencies can be modeled, i.e., partitions that split dependent regions should have a high entanglement entropy. This connection places the analysis of dependencies as a key ingredient in the proper harnessing of the inductive bias when constructing a deep network architecture. For example, in a natural image, pixels which are closer to each other are more correlated than far away pixels. Therefore, the relevant partition to favor when the inputs are natural images is the interleaved partition, presented in Figure 11.5a, which splits the image in a checkerboard manner such that A is composed of the pixels located in blue positions and B is composed of the pixels located in yellow positions. This partition will split many pixels which are correlated to one another. Intuitively, this dependency manifests itself in the following manner: given an image composed only of the pixels in the blue positions, one would have a good idea of how to complete the missing parts. Thus, for natural images, constructing the network so that it supports exponentially high entanglement measures for the interleaved partition is preferable. Similarly, if the input is composed of symmetric images, such as human face images, one expects pixels positioned symmetrically around the middle to be highly correlated. Accordingly, constructing the network such that it supports exponentially high entanglement measures for the leftâ€“right partition, shown in Figure 11.5b, 7 In a statistical setting, where f (Â·) is a probability density function, separability with respect to (A, B) corresponds to statistical independence between the inputs from A and B. 8 The equivalence of the Schmidt entanglement measure and the separation rank follows from the linear inde- pendence of the representation functions. 11.4 Deep Learning Architecture Design via Entanglement Measures 455 Figure 11.5 For convolutional networks, an illustration of (a) an interleaved partition and (b) a leftâ€“right partition. The network should support high entanglement measures with respect to the left-right partition for it to be able to model intricate dependencies between the two sides of the image (e.g., for face images), and with respect to the interleaved partition if one wishes to do so for neighboring pixels (e.g., for natural images). In Â§11.4 we show how this control over the inductive bias of the convolutional network can be achieved by adequately tailoring the number of channels in each of its layers. (c) For recurrent networks, high entanglement with respect to the Startâ€“End partition implies that the network is able to represent functions which model long-term memory, i.e., intricate dependencies between the beginning and end of the input sequence. In the depicted â€˜next-word prediction taskâ€™ example, in order to correctly predict the word rug the network must be able to represent some convoluted function involving the two subsets â€˜Startâ€™ and â€˜Endâ€™. Levine et al. (2017) employed the TN construction of recurrent networks presented above and proved that deep recurrent networks have a combinatorially large Startâ€“End entanglement relative to their shallow counterparts, thus providing theoretical grounds for an empirically widely observed phenomenon (Hermans and Schrauwen, 2013). is advisable in this case. Otherwise, the networkâ€™s expressive ability to capture required long-range dependencies is hindered. As for recurrent networks, they should be able to integrate data from diï¬€erent time steps. Speciï¬cally, in order to translate long sentences or answer elaborate questions, their expressive ability to model long-term dependencies between the beginning and ending of the input sequence is of interest â€“ see the toy example in Figure 11.5c. Therefore, the recurrent network should ideally support high entanglement with respect to the partition separating the ï¬rst inputs from the later inputs, referred to as the Startâ€“End partition (Levine et al., 2017). 456 Levine et al: Quantum Physics and Deep Learning via TNs 11.4.2 Quantum-Physics-Inspired Control of Inductive Bias When physicists choose a TN to represent the coeï¬ƒcient tensor of a certain wave function, the entanglement characterizing the wave function is taken into consider- ation and the network which can best model it is chosen (e.g., simulating a system with logarithmic corrections to area-law entanglement scaling with a MERA TN rather than with an MPS TN, which cannot model it). Thus, understanding the inter- particle dependency characteristics of the wave function serves as a prior knowledge that helps restrict the hypothesis space to a suitable TN architecture. In accordance with the analogies discussed above, we draw inspiration from this approach, as it represents a â€˜healthyâ€™ process of ï¬rst quantifying the key dependencies that the network is required to model, and then constructing the network appropriately. This is in eï¬€ect a control over the inductive bias of the network. The TN constructions of convolutional and recurrent networks in Figure 11.4 allow us to investigate means of matching the inductive bias to given input depen- dencies. In the following theorem, we translate a known result on the quantitative connection between quantum entanglement and TNs (Cui et al., 2016) into bounds on dependencies supported by the above deep learning architectures: Theorem 11.1 (Proof in Levine et al., 2018). Let y be the function computed by a ConvAC/single-layered RAC, (11.5), with Aconv/rec represented by the TNs in Figure 11.4. Let (A, B) be any partition of [N]. Assume that the channel numbers (equivalently, bond dimensions) are all powers of the same integer.9 Then, the maximal entanglement entropy with respect to (A, B) supported by the convolutional or recurrent network is equal to the weight of the minimal cut separating A from B in the TN representing Aconv/rec, where the weight of each edge is the logarithm of its bond dimension. Theorem 11.1 not only provides us with theoretical observations regarding the role that the number of channels in each layer fulï¬lls in the overall expressiveness of a deep network, but also entails practical implications for the construction of a deep network architecture when there is prior knowledge regarding the task at hand. If one wishes to construct a deep learning architecture that is expressive enough to model intricate dependencies according to some partition (A, B), it is advisable to design the network so that all the cuts separating A from B in the corresponding TN have high weights. To get a grasp of what can be understood from the theoretical results, consider 1D partitions similar in essence to the leftâ€“right partition and the interleaved par- tition depicted in Figure 11.5. For a TN representing a depth-L ConvAC network (Figure 11.4a) it is simple to see that the minimal weight of a cut with respect to 9 See (Levine et al., 2018) for a treatment of a general channel numbers setting. 11.4 Deep Learning Architecture Design via Entanglement Measures 457 the leftâ€“right partition, Wleft-right, obeys Wleft-right = min(rLâˆ’1,rLâˆ’2,. . .,r2(Lâˆ’2âˆ’l) i ,. . .,r N/4 0 , M N/2), (11.9) whereas the minimal weight of a cut with respect to the interleaved partition, Winterleaved, is guaranteed to be exponential in the number of pixels N: Winterleaved = min(r N/4 0 , M N/2). (11.10) It is worth noting that these minimal cut values are a direct consequence of the contiguous pooling scheme in the convolutional network. The exponential value of Winterleaved explains the success of commonly employed networks with contiguous pooling windows â€“ they are able to model short-range dependencies in natural data sets (this was established also in Cohen and Shashua (2017) by using a separation- rank-based approach). The above are two examples that bring forth indications for the following â€˜rule of thumbâ€™. If one is interested in modeling elaborate dependencies between pixels from opposite ends of an image, such those characterizing face images for example, we see from the expression in (11.9) that a small number of channels in deep layers can create an undesired â€˜shortcutâ€™ which harms the expressiveness of the network in a way that prevents it from modeling the required dependencies. In this case, it is advisable to keep more parameters in the deeper layers in order to obtain a higher entanglement measure for the required partition. However, if one is interested in modeling only short-range dependencies, and one knows that the typical input to the network will not exhibit relevant long-range dependencies, it is advisable to concentrate more parameters (in the form of more channels) in the lower levels, as it raises the entanglement measure with respect to the partition which corresponds to the short-range dependencies. The two partitions analyzed above represent two extreme cases of the shortest- and longest-ranged dependencies. However, the min-cut result in Theorem 11.1 applies to any partition of the inputs, so that implications regarding the channel numbers can be drawn for any intermediate length scale of dependencies. The relevant layers that contribute to the min-cut between partitions (A, B) for which both A and B have contiguous segments of a certain length Î¾ can be easily identiï¬ed â€“ the minimal cut with respect to such a partition (A, B) may only include the channel numbers M,r0,. . .,râŒˆlog2 Î¾âŒ‰. This is in fact a generalization of the treatment above with Î¾ = 1 for the interleaved partition and Î¾ = N/2 for the leftâ€“right partition. Any cut which includes edges in higher levels is guaranteed to have a higher weight than the minimal cut, as, in addition, it will have to include a cut of edges in the lower levels in order for a separation between A and B to actually take place. This can be understood by ï¬‚ow considerations in the graph underlying this tree TN â€“ a cut that 458 Levine et al: Quantum Physics and Deep Learning via TNs Figure 11.6 Samples of the randomly positioned MNIST digits to be classiï¬ed in the global task (upper panels) and the local task (lower panels). is located above a certain sub-branch can not assist in cutting the ï¬‚ow between the A and B vertices that reside in that sub-branch. For a dataset with features of characteristic size D (e.g., in a 2D digit classiï¬cation task it is the size of the digits that are to be classiï¬ed), such partitions of length scales Î¾ < D are guaranteed to separate diï¬€erent parts of a feature placed in any input location. However, in order to perform the classiï¬cation task of this feature correctly, an elaborate function modeling a strong dependence between diï¬€erent parts of it must be realized by the network. As discussed above, this means that a high measure of entanglement with respect to such a partition must be supported by the network, and now we are able to describe this measure of entanglement in terms of a min-cut in the TN graph. We accompanied this understanding by corroborating experiments exemplifying that the theoretical ï¬ndings, established above for the deep ConvAC, apply to a regular ConvNet architecture which involves the more common ReLU activations and average or max pooling. Two tasks were designed, one with a short characteristic length, to be referred to as the â€˜local taskâ€™, and the other with a long characteristic length, to be referred to as the â€˜global taskâ€™. Both tasks were based on the MNIST dataset (LeCun et al., 1998) and consisted of 64 Ã— 64 black background images on top of which resized binary MNIST images were placed in random positions. For the local task, the MNIST images were shrunk to small 8 Ã— 8 images while for the global task they were enlarged to size 32 Ã— 32. In both tasks the digit was to be identiï¬ed correctly with a label 0,. . .,9. See Figure 11.6 for a sample of images from each task. We designed two diï¬€erent networks that tackle these two tasks, with ReLU activation and max pooling, which diï¬€er in the channel ordering â€“ in the â€˜wide- baseâ€™ network they are wider in the beginning and narrow down in the deeper layers while in the â€˜wide-tipâ€™ network they follow the opposite trend. Speciï¬cally, we set a parameter r to determine each pair of such networks according to the following scheme: 11.4 Deep Learning Architecture Design via Entanglement Measures 459 Figure 11.7 Results of applying deep convolutional rectiï¬er networks with max pooling to the global and local classiï¬cation tasks. Two channel arrangement geometries were evaluated: â€˜wide-tipâ€™, which supports modeling dependencies between far away regions; and â€˜wide-baseâ€™, which puts the focus on dependencies between regions that are close to each other. Each channel arrangement geometry outperforms the other on a task which exhibits ï¬tting dependencies, demonstrating how prior knowledge regarding a task at hand may be used to tailor the inductive bias through appropriate channel arrangements. Furthermore, these results demonstrate that the theoretical conclusions following Theorem 11.1, obtained for a ConvAC, extend to the common ConvNet architecture which involves ReLU activations and max pooling. â€¢ wide-base: [10; 4r; 4r; 2r; 2r; r; r; 10] â€¢ wide-tip: [10; r; r; 2r; 2r; 4r; 4r; 10] The channel numbers from left to right go from input to output. The channel numbers were chosen to be gradually increased or decreased in iterations of two layers at a time as a tradeoï¬€â€“ we wanted the network to be reasonably deep but not to have too many diï¬€erent channel numbers, in order to resemble conventional channel choices. The parameter count for both conï¬gurations is identical: 10 Â· r + r Â· r + r Â· 2r + 2r Â· 2r + 2r Â· 4r + 4r Â· 4r + 4r Â· 10 = 31r2 + 50r. A result compliant with our theoretical expectation was achieved, as shown in Figure 11.7 â€“ the â€˜wide-baseâ€™ network outperforms the â€˜wide-tipâ€™ network in the local classiï¬cation task, and the converse occurs in the global classiï¬cation task. In natural images it may be hard to point out a single most important length scale D; however, the conclusions presented above can be viewed as an incentive to better characterize the input dependencies which are most relevant to the task at hand. In the above we have described practical insights regarding deep convolutional network design, in the form of architecture selection (the pooling scheme) and resource allocation (choosing layer widths). Before concluding this section, we mention two additional theoretical results regarding the expressive power of deep learning architectures that can be obtained by using the TN framework. First, Levine et al. (2018) showed that such TN-based analyses can straightforwardly 460 Levine et al: Quantum Physics and Deep Learning via TNs lead to reproduction of the exponential depth eï¬ƒciency in convolutional networks, shown originally in Cohen et al. (2016b). The second result relates to the recurrent network case. We recall the discussion regarding entanglement with respect to the Startâ€“End partition as a surrogate for long-term memory in recurrent networks (Figure 11.5c). Since the TN corresponding to a single-layered RAC has the form of an MPS (Figure 11.4b), the minimal weight of a cut with respect to the Startâ€“End partition is trivially equal to the logarithm of the number of channels in the hidden recurrent unit, R. We thus obtain that by adding parameters to the RAC one may only linearly increase its ability to model long-term dependencies, as reï¬‚ected by the Startâ€“End Schmidt entanglement measure. Levine et al. (2017) employed the TNs tool and demonstrated that the Startâ€“End Schmidt entanglement measure increases polynomially with the number of parameters for deep recurrent networks, thus providing a ï¬rst-of-its-kind theoretical assertion for the widely observed empirical phenomenon of depth-enhanced long-term memory in recurrent networks. In conclusion, inspired by entanglement-based TN architecture selection for many-body wave functions, novel deep learning outcomes can be found. 11.5 Power of Deep Learning for Wave Function Representations In this section we provide a novel quantitative analysis of the ability of deep learn- ing architectures to model highly entangled many-body systems, as detailed in Levine et al. (2019). We consider successful extensions to the above-presented architectures, in the form of deep recurrent networks and overlapping convolu- tional networks. These extensions, presented below (Figures 11.8 and 11.9), are seemingly â€˜innocentâ€™ â€“ they introduce a linear growth in the number of parameters and their computation remains tractable. Despite this, both are empirically known to enhance performance (Krizhevsky et al., 2012; Graves et al., 2013), and have been shown theoretically to introduce a boost in network expressivity (Sharir and Shashua, 2018; Levine et al., 2017). As demonstrated below, both deep recurrent and overlapping convolutional architectures inherently involve information re-use, where a single activation is duplicated and sent to several subsequent calculations along the network computation. We pinpoint this re-use of information along the network as a key element diï¬€er- entiating powerful deep learning architectures from the standard TN representations employed in many-body physics. Though data duplication is generally unachievable in the language of TNs, we circumvent this restriction and construct TN equivalents of the above networks, which may be viewed as deep-learning inspired enhance- ments of MPS and tree TNs. We are thus able to translate expressivity results on the above networks into super-area-law lower bounds on the entanglement scaling that can be supported by them (see Â§11.2.2 for the deï¬nition of entanglement scaling). 11.5 Power of Deep Learning for Wave Function Representations 461 f(xt) W I,1a P(a, b) W H,1a W I,2a P(a, b) W H,2a W Oa yt (a) â‡â‡’ h2 0 h1 0 f(x1) d1 W I,1 Î´ W H,1 M R R R W I,2 Î´ W H,2 R R R h1 0 f(x1) d1 W I,1 Î´ W H,1 M R R f(x2) d2 W I,1 Î´ W H,1 M R R R W I,2 Î´ W H,2 R R R h1 0 f(x1) d1 W I,1 Î´ W H,1 M R R f(x2) d2 W I,1 Î´ W H,1 M R R f(x3) d3 W I,1 Î´ W H,1 M R R R W I,2 Î´ W H,2 R R R R R R R R R W O Adeep-rec (b) Adeep-rec d1 Î´ d1 d1 d1 d2 Î´ d2 d2 d3 Î´ d3 DUP(Adeep-rec) (c) Figure 11.8 (a) A deep recurrent network is represented by a concise and tractable computation graph, which employs information re-use (two arrows emanating from a single node). (b) In TN language, deep RACs (Levine et al., 2017) are represented by a â€˜recursive MPSâ€™ TN structure (presented for the case of N = 3), which makes use of input duplication to circumvent the inherent inability of TNs to model information re-use. This novel tractable extension to an MPS TN supports logarithmic corrections to the area-law entanglement scaling in 1D (Theorem 11.2). (c) Given the high-order tensor Adeep-rec with duplicated external indices, presented in (b), the process of obtaining a dup-tensor DUP(Adeep-rec) that corresponds to |Ïˆdeep-recâŸ©, (11.13), involves a single Î´-tensor (operating here similarly to the copy-tensor in Biamonte et al., 2011) per unique external index. Our results indicate that these successful deep learning architectures are natural can- didates for joining the recent eï¬€orts regarding neural-network-based wave function representations, currently focused mainly on RBMs. As a by product, our construc- tion suggests new TN architectures, which enjoy an equivalence to tractable deep learning computation schemes and surpass the traditionally used expressive TNs in representable entanglement scaling. 11.5.1 Entanglement Scaling of Deep Recurrent Networks Beginning with recurrent networks, an architectural choice that has been shown empirically to yield enhanced performance in sequential tasks (Graves et al., 2013; Hermans and Schrauwen, 2013), and recently proven to bring forth a signiï¬cant advantage in network long-term memory capacity (Levine et al., 2017), is the addition of adding more layers, i.e., deepening (see Figure 11.8a). The construction of a TN which matches the calculations of a deep RAC is less trivial than that of the shallow case, since the output vector of each layer at every time-step is re-used and sent to two diï¬€erent calculations â€“ as an input to the next layer up and as a hidden vector for the next time step. This operation of duplicating data, which is easily achieved in any practical setting, is actually impossible to represent in the framework of TNs (see Claim 1 in Levine et al., 2017). However, the form of a TN representing a deep recurrent network may be attained by a simple â€˜trickâ€™ â€“ duplication of the input data itself, so that each instance of a duplicated intermediate vector is generated by a separate TN branch. This technique yields the â€˜recursive- MPSâ€™ TN construction of deep recurrent networks, depicted in Figure 11.8b. 462 Levine et al: Quantum Physics and Deep Learning via TNs It is noteworthy that owing to these external duplications, the tensor represented by a deep RAC TN, written Adeep-rec, does not immediately correspond to an N- particle wave function, as the TN has more than N external edges. However, when considering the operation of the deep recurrent network over inputs comprised solely of standard basis vectors, { Ë†e(ij)}N j=1 (ij âˆˆ[M]), with identity representation functions,10 we may write the function realized by the network in a form analogous to that of (11.6): y  Ë†e(i1),. . ., Ë†e(iN ) = âŸ¨Ïˆ product  Ë†e(i1),. . ., Ë†e(iN ) |Ïˆdeep-recâŸ©, (11.11) where in this case the product state simply satisï¬es |Ïˆ product  Ë†e(i1),. . ., Ë†e(iN ) âŸ©= Ë†Ïˆd1,...,dN , (11.12) i.e., some orthonormal basis element of the many-body Hilbert space H, and the state representing the deep recurrent networkâ€™s operation obeys |Ïˆdeep-recâŸ©:= M Ã• d1,...,dN =1 DUP(Adeep-rec)d1,...,dN Ë†Ïˆd1,...,dN , (11.13) where DUP(Adeep-rec) is the N-indexed sub-tensor of Adeep-rec that holds the values of the latter when the duplicated external indices are equal; it is referred to as the dup-tensor. Figure 11.8c shows the TN calculation of DUP(Adeep-rec), which makes use of Î´-tensors for external index duplication, similarly to the operation of the copy-tensors applied on boolean inputs in Biamonte et al. (2011). The resulting TN resembles other TNs that can be written using copy-tensors to duplicate their external indices, such as those representing string bond states (Schuch et al., 2008) and entangled plaquette states (Mezzacapo et al., 2009) (also referred to as correlator product states; see Changlani et al., 2009), both recently shown to generalize RBMs (Glasser et al., 2018; Clark, 2018). Eï¬€ectively, since (11.11â€“11.13) dictate that y  Ë†e(i1),. . ., Ë†e(iN ) = DUP(Adeep-rec)i1,...,iN, (11.14) under the above conditions the deep recurrent network represents the N-particle wave function |Ïˆdeep-recâŸ©. In the following theorem we show that the maximum entanglement entropy of a 1D state |Ïˆdeep-recâŸ©modeled by such a deep recurrent network increases logarithmically with sub-system size: Theorem 11.2 (Proof in Levine et al., 2019). Let y be the function computing the output after N time steps of an RAC with two layers and R hidden channels per 10 This scenario of representing inputs to an RNN as â€˜one-hotâ€™ vectors is actually quite common in sequential tasks; see e.g., Graves (2013). 11.5 Power of Deep Learning for Wave Function Representations 463 layer (Figure 11.8a), with â€˜one-hotâ€™ inputs and M identity representation functions, (11.11). Let Adeep-rec be the tensor represented by the TN corresponding to y (Fig- ure 11.8b) and DUP(Adeep-rec) the matching N-indexed dup-tensor (Figure 11.8c). Let (A, B) be a partition of [N] such that |A| â‰¤|B| and B = {1,. . ., |B|}.11 Then the maximal entanglement entropy with respect to (A, B) supported by DUP(Adeep-rec) is lower bounded by log min {R, M} + |A| âˆ’1 |A|  âˆlog {|A|} . (11.15) The above theorem implies that the entanglement scaling representable by a deep RAC is similar to that of the multiscale entanglement renormalization ansatz (MERA) TN (Vidal, 2007), which is also linear in log{|A|}. Indeed, in order to model wave functions with such logarithmic corrections to the area-law entangle- ment scaling in 1D, which cannot be eï¬ƒciently represented by an MPS TN (e.g., in the case of ground states of critical systems), a common approach is to employ the MERA TN. Theorem 11.2 demonstrates that the â€˜recursive MPSâ€™ in Figure 11.8b, which corresponds to the deep-recurrent networkâ€™s tractable computation graph in Figure 11.8a, constitutes a competing enhancement to the MPS TN that supports similar logarithmic corrections to the area-law entanglement scaling in 1D. We have thus established the motivation for the inclusion of deep recurrent networks in the recent eï¬€ort to achieve wave function representations in neural networks. Due to the tractability of the deep recurrent network, calculation of the represented wave function amplitude would be eï¬ƒcient, and stochastic sampling techniques for optimization such as in Carleo and Troyer (2017), which cannot be used for optimizing a MERA TN, become available. 11.5.2 Entanglement Scaling of Overlapping Convolutional Networks Moving to convolutional networks, state-of-the-art architectures make use of con- volution kernels of size K Ã—K, where K > 1 (Krizhevsky et al., 2012; Simonyan and Zisserman, 2014). This architectural trait, which implies that the kernels spatially overlap when slid across the feature maps during computation, has been shown to yield an exponential enhancement in network expressivity (Sharir and Shashua, 2018). An example of such an architecture is the overlapping ConvAC depicted in Figure 11.9a. It is important to notice that the overlap in convolution kernels automatically results in information re-use, since a single activation is being used for computing several neighboring activations in the following layer. As above, such 11 We focus on the case where A is located to the right of B for proof simplicity; numerical simulations of the network in Figure 11.8 with randomized weight matrices indicate that the lower bound in Theorem 11.2 holds for all other locations of A. (a) KxK Conv H(in) H(in) P:Rrlâ‡¥. . .â‡¥Rrl!Rrl ConvNet ConvAC yiâŒ˜max nX j x(j) i , 0 o yiâŒ˜ Y j x(j) i Block 0 KxK Conv representa.on input M r0 H H linear output rL-1 rL-1 rl previous layer H(in) H(in) rl-1 Global Pooling Block L-1 KxK Conv rl-1 K K x (1) x (K2) x (K) y=P(W C,l,1x(1), . . . , W C,l,K2x(K2)) y=W (L)x ConvAC â‡• (b) Î´ Î´ Î´ Î´ 12 Î´ 123 Î´ Î´ 12 Î´ 123 Î´ 234 Î´ Î´ Î´ 12 Î´ 123 Î´ Î´ 12 Î´ 123 Î´ 234 Î´ Î´ 123 Î´ 234 Î´ 34 Î´ Î´ Î´ 12 Î´ 123 Î´ 234 Î´ Î´ 123 Î´ 234 Î´ 34 Î´ Î´ 234 Î´ 34 Î´ Î´ Î´ 123 Î´ 234 Î´ 34 Î´ Î´ 234 Î´ 34 AK,L,d N = 4, K = 3, L = 3, d = 1 â‡’ AK,L,d d1 Î´ d1 Â· Â· Â· d1 d2 Î´ d2 Â· Â· Â· d2 d3 Î´ d3 Â· Â· Â· d3 d4 Î´ d4 Â· Â· Â· d4 DUP(AK,L,d) (c) Figure 11.9 (a) A deep overlapping convolutional network, in which the convolution kernels are of size K Ã— K, with K > 1. This results in information re-use since each activation in layer i âˆˆ[L] is part of the calculations of several adjacent activations in layer l + 1. (b) The TN corresponding to the calculation of the overlapping ConvAC (Sharir and Shashua, 2018) in the 1D case when the convolution kernel size is K = 3, the network depth is L = 3, and its spatial extent is N = 4. As in the case of deep recurrent networks, the inherent re-use of information in the overlapping convolutional network results in the duplication of external indices and a recursive TN structure. This novel tractable extension to a tree TN supports volume-law entanglement scaling until the linear dimension of the represented system exceeds the order LK (Theorem 11.3). For legibility, j âˆˆ[4] stands for dj in this ï¬gure. (c) The TN representing the calculation of the dup-tensor DUP(AK ,L,d), which corresponds to |Ïˆoverlap-convâŸ©, (11.17). https://doi.org/10.1017/9781009025096.012 Published online by Cambridge University Press 11.5 Power of Deep Learning for Wave Function Representations 465 s1 s2 s3 s4 s5 s6 s7 s8 Figure 11.10 A 1D MERA TN with open boundary conditions for the case N = 8. The loops in this TN render the computation of its inner product with a product state (analogous to its tensor entry computation, required for wave function amplitude sampling) intractable for large system sizes. re-use of information poses a challenge to the straightforward TN description of overlapping convolutional networks. We employ an input duplication technique sim- ilar to that used for deep recurrent networks, in which all instances of a duplicated vector are generated in separate TN branches. This results in a complex-looking TN representing the computation of the overlapping ConvAC, presented in Figure 11.9b (in 1D form, with j âˆˆ[4] representing dj for legibility). Comparing the ConvAC TN in Figure 11.9b and the MERA TN in Figure 11.10, it is evident that the many-body physics and deep learning communities have eï¬€ectively elected competing mechanisms in order to enhance the naive decima- tion/coarse graining scheme represented by a tree TN. The MERA TN introduces loops which entail the intractability of straightforward wave-function amplitude computation, yet facilitate properties of interest such as automatic normalization. In contrast, overlapping-convolutional networks employ information re-use, result- ing in a compact and tractable computation of the amplitude of the wave function in question. Owing to the input duplications, the tensor represented by the TN of the over- lapping ConvAC of depth L with kernel size Kd in d spatial dimensions, denoted AK,L,d, has more than N external edges and does not correspond to an N-particle wave function. When considering the operation of the overlapping ConvAC over standard basis inputs, { Ë†e(ij)}N j=1, with identity representation functions, we obtain a form similar to the above: y  Ë†e(i1),. . ., Ë†e(iN ) = âŸ¨Ïˆproduct  Ë†e(i1),. . ., Ë†e(iN ) |Ïˆoverlap-convâŸ©, (11.16) where the product state is a basis element of the many-body Hilbert space H, as in 466 Levine et al: Quantum Physics and Deep Learning via TNs (11.12), and |Ïˆ overlap-convâŸ©:= M Ã• d1,...,dN =1 DUP(AK,L,d)d1,...,dN Ë†Ïˆd1,...,dN . (11.17) The dup-tensor DUP(AK,L,d) is the N-indexed sub-tensor of AK,L,d that hold the values of the latter when the duplicated external indices are equal (see its TN calculation in Figure 11.9c). Equations (11.16) and (11.17) imply that, under the above conditions, the overlapping ConvAC represents the dup-tensor, which corre- sponds to the N-particle wave-function |Ïˆoverlap-convâŸ©. Relying on results regarding the expressiveness of overlapping-convolutional architectures (Sharir and Shashua, 2018), we examine the entanglement scaling of a state |Ïˆoverlap-convâŸ©modeled by such a network: Theorem 11.3 (Proof in Levine et al., 2019). Let y be the function computing the output of the depth-L, d-dimensional overlapping ConvAC with convolution kernels of size Kd (Figure 11.9a) for d = 1,2, with â€˜one-hotâ€™ inputs and identity representation functions, (11.16). Let AK,L,d be the tensor represented by the TN corresponding to y (Figure 11.9b) and DUP(AK,L,d) be the matching N-indexed dup-tensor (Figure 11.9c). Let (A, B) be a partition of [N] such that A is of size Alin in d = 1 and Alin Ã— Alin in d = 2 (Alin is the linear dimension of the subsystem), with |A| â‰¤|B|. Then, the maximal entanglement entropy with respect to (A, B) modeled by DUP(AK,L,d) obeys â„¦  min n (Alin)d , LK (Alin)dâˆ’1o . Thus, an overlapping convolutional network with L layers supports volume-law entanglement scaling: (Alin)d, for systems of linear size Alin < LK. Practically, overlapping-convolutional networks with common characteristics of e.g. kernel size K = 5 and depth L = 20, can support the entanglement of any 2D system of interest up to sizes 100 Ã— 100, which are unattainable by competing intractable approaches (Gull et al., 2013; Chen et al., 2013; Lubasch et al., 2014; Zheng and Chan, 2016; Liu et al., 2017; LeBlanc et al., 2015). Moreover, the result in Theorem 11.3 indicates a signiï¬cant advantage in model- ing the volume-law entanglement scaling of deep convolutional networks relative to the competing veteran neural-network-based approaches. These new approaches promised to grant tractable access to 2D systems that cannot be modeled by 2D TNs, since they render the computation of wave function amplitudes tractable and therefore stochastic-sampling-based optimization techniques can be employed, even for large 2D systems. However, in order to represent volume-law entanglement in a 2D system of N particles, fully connected networks require a number of network parameters that scales as O(N2) (Saito, 2017; Cai and Liu, 2018), while RBMs 11.6 Discussion 467 require O(N) parameters (Carleo and Troyer, 2017; Deng et al., 2017b). In con- trast, since the number of parameters in 2D overlapping convolutional networks is proportional to LK2, the above volume-law condition, Alin < LK, implies the following corollary of Theorem 11.3. Corollary 11.4. The number of overlapping convolutional network parameters re- quired for modeling volume-law entanglement scaling in a 2D system of N particles, scales as O( âˆš N). Therefore, these networks have a clear polynomial advantage in resource eï¬ƒ- ciency over the fully connected networks and RBMs used previously. Thus, deep convolutional networks have the potential to provide access to highly entangled 2D systems of sizes unattainable by the competing veteran neural-network-based approaches, and to shed light on currently unreachable quantum phenomena. Finally, the result in Theorem 11.3 applies to a network with no spatial decimation (pooling) in its ï¬rst L layers, such as that employed in, e.g., Oord et al. (2016) and van den Oord et al. (2016). In Levine et al. (2019), a result analogous to that of Theorem 11.3 is proven for overlapping networks that integrate pooling layers in between convolution layers. In this case the volume-law entanglement scaling is limited to system sizes under a cut-oï¬€equal to the convolution kernel size K, which is small in common convolutional network architectures. Practically, this suggests the use of overlapping convolutional networks without pooling operations for modeling highly entangled states, and the use of overlapping convolutional networks that include pooling for modeling states that obey area-law entanglement scaling. 11.6 Discussion The TN constructions of prominent deep learning architectures that we have pre- sented, serve as a bidirectional bridge for the transfer of concepts and results between the two domains. In one direction, it allows us to convert well-established tools and approaches from many-body physics into new deep learning insights. An identi- ï¬ed structural equivalence between many-body wave functions and the functions realized by non-overlapping convolutional and shallow recurrent networks brings forth the use of entanglement measures as well-deï¬ned quantiï¬ers of a networkâ€™s ability to model dependencies in the inputs. Via the TN construction of the above networks, in the form of tree and MPS TNs (Figure 11.4), we have made use of a quantum physics result which bounds the entanglement represented by a generic TN (Cui et al., 2016) in order to propose a novel deep learning architecture design scheme. We were thus able to suggest principles for parameter allocation along the network (the layer widths) and the choice of network connectivity (i.e., the pooling 468 Levine et al: Quantum Physics and Deep Learning via TNs geometry), which have been shown to correspond to a networkâ€™s ability to model de- pendencies of interest. Notably, Levine et al. (2020) used similar tools for analyzing the relation between layer width and network depth in self-attention architectures, which have recently become the standard in natural language processing. In the opposite direction, we constructed TNs corresponding to powerful en- hancements of the above architectures, in the form of deep recurrent networks and overlapping convolutional networks. These architectures, which stand at the forefront of recent deep learning achievements, inherently involve the re-use of information along network computation. In order to construct their TN equivalents, we employed a method of index duplications, resulting in recursive MPS (Fig- ure 11.8) and tree TNs (Figure 11.9). This method allowed us to demonstrate how a tensor corresponding to an N-particle wave function can be represented by the above architectures, and thus makes available the investigation of their entanglement scaling properties. Relying on a result which established that depth has a combinatorially enhancing eï¬€ect on long-term memory capacity in recurrent networks (Levine et al., 2017), we showed that deep recurrent networks support logarithmic corrections to the area-law entanglement scaling in 1D. Similarly, by translating a recent result which shows that overlapping convolutional networks are exponentially more expressive than their non-overlapping counterparts (Sharir and Shashua, 2018), we showed that such architectures support volume-law entanglement scaling for systems of linear sizes smaller than LK (where L is the network depth and K is the convolution kernel size) and area-law entanglement scaling for larger systems. Our results show that overlapping convolutional networks are polynomially more eï¬ƒcient for modeling highly entangled states in 2D than competing neural-network-based representations. Our treatment addressed the expressivity of deep learning architectures in the context of modeling many-body wave-functions, ensuring their a priori ability to support suï¬ƒcient entanglement scaling. Once this is established, one must be able to optimize them eï¬ƒciently, which means ï¬nding the correct parameters for de- scribing the wave function of interest. In this regard, our proposed architectures do not diï¬€er greatly from recently employed neural-network architectures, which are optimized by tractable computations of overlaps with product states, allowing for optimization techniques that use stochastic sampling (see, e.g., Carleo and Troyer (2017) or the more recent work of Sharir et al. (2020) that optimize convolu- tional networks for modeling ground states). Overlapping convolutional networks are advantageous relative to previously employed RBMs in this aspect as well. Equations (11.16) and (11.17) show that the deep network in Figure 11.9 computes a wave function amplitude per inserted spin conï¬guration. The computational cost of this operation is O(NL) versus O(N2) for computing wave function amplitudes using RBMs. Moreover, the demonstrated polynomial eï¬ƒciency of these deep net- References 469 works in representing 2D volume-law entanglement, together with the locality of the convolution computation relative to the long range of RBMs, can provide a further boost in optimization speed. When implemented on graphical processing units (GPUs), which have substantially speeded up deep networks in recent years, there are signiï¬cant runtime advantages to having low memory together with local operations. The view that we have established, of entanglement measures as quantiï¬ers of dependencies supported by deep networks, indicates that this connection may help shed light on the question of characteristic dependencies in machine learning datasets. Physicists often have a clear understanding of the entanglement properties of the many-body system they wish to represent, which assists them in choosing an adequate TN architecture to represent it. In the machine learning domain, de- pendencies in natural data sets are yet to be adequately characterized. Empirical evidence suggests that the mutual information in various natural datasets such as English Wikipedia, works of Bach, the human genome, etc., decays polynomially with a critical exponent similar in value to that of the critical Ising model (Lin and Tegmark, 2016). Our results show that deep learning architectures can support the entanglement scaling of such critical systems. A future quantiï¬cation of the â€˜characteristic entanglementâ€™ in natural data sets may shed light on the empirical success of deep learning architectures, and it may suggest further task-speciï¬c de- sign principles such as those brought forth in this work. Overall, we believe that the bidirectional bridge presented in this work can help bring quantum many-body physics research and state-of-the-art machine learning approaches one step closer together. References Amodei, Dario, Ananthanarayanan, Sundaram, Anubhai, Rishita, Bai, Jingliang, Battenberg, Eric, Case, Carl, et al. 2016. Deep speech 2: End-to-end speech recognition in english and mandarin. Pages 173â€“182 of: Proc. International Conference on Machine Learning. Bahdanau, Dzmitry, Cho, Kyunghyun, and Bengio, Yoshua. 2014. Neural ma- chine translation by jointly learning to align and translate. ArXiv preprint arXiv:1409.0473. Beylkin, Gregory, and Mohlenkamp, Martin J. 2002. Numerical operator calculus in higher dimensions. Proceedings of the National Academy of Sciences, 99(16), 10246â€“10251. Biamonte, Jacob D., Clark, Stephen R., and Jaksch, Dieter. 2011. Categorical tensor network states. AIP Advances, 1(4), 042172. Cai, Zi, and Liu, Jinguo. 2018. Approximating quantum many-body wave functions using artiï¬cial neural networks. Physical Review B, 97(3), 035116. 470 Levine et al: Quantum Physics and Deep Learning via TNs Carleo, Giuseppe, and Troyer, Matthias. 2017. Solving the quantum many-body problem with artiï¬cial neural networks. Science, 355(6325), 602â€“606. Carleo, Giuseppe, Nomura, Yusuke, and Imada, Masatoshi. 2018. Constructing ex- act representations of quantum many-body systems with deep neural networks. ArXiv preprint arXiv:1802.09558. Changlani, Hitesh J., Kinder, Jesse M., Umrigar, Cyrus J., and Chan, Garnet Kin- Lic. 2009. Approximating strongly correlated wave functions with correlator product states. Physical Review B, 80(24), 245116. Chen, K.-S., Meng, Zi Yang, Yang, S.-X., Pruschke, Thomas, Moreno, Juana, and Jarrell, Mark. 2013. Evolution of the superconductivity dome in the two- dimensional Hubbard model. Physical Review B, 88(24), 245110. Clark, Stephen R. 2018. Unifying neural-network quantum states and correlator product states via tensor networks. Journal of Physics A: Mathematical and Theoretical, 51(13), 135301. Cohen, Nadav, and Shashua, Amnon. 2016. Convolutional rectiï¬er networks as generalized tensor decompositions. In: Proc. International Conference on Machine Learning. Cohen, Nadav, and Shashua, Amnon. 2017. Inductive bias of deep convolutional networks through pooling geometry. In: Proc. 5th International Conference on Learning Representations. Cohen, Nadav, Sharir, Or, and Shashua, Amnon. 2016a. Deep SimNets. In: Proc. IEEE Conference on Computer Vision and Pattern Recognition. Cohen, Nadav, Sharir, Or, and Shashua, Amnon. 2016b. On the expressive power of deep learning: A tensor analysis. In: Proc. Conference On Learning Theory. Cui, Shawn X., Freedman, Michael H., Sattath, Or, Stong, Richard, and Minton, Greg. 2016. Quantum max-ï¬‚ow/min-cut. Journal of Mathematical Physics, 57(6), 062206. Deng, Dong-Ling, Li, Xiaopeng, and Das Sarma, S. 2017a. Machine learning topological states. Physical Review B, 96(Nov), 195145. Deng, Dong-Ling, Li, Xiaopeng, and Das Sarma, S. 2017b. Quantum entanglement in neural network states. Physical Review X, 7(May), 021021. Fannes, Mark, Nachtergaele, Bruno, and Werner, Reinhard F. 1992. Finitely cor- related states on quantum spin chains. Communications in Mathematical Physics, 144(3), 443â€“490. Gao, Xun, and Duan, Lu-Ming. 2017. Eï¬ƒcient representation of quantum many- body states with deep neural networks. Nature Communications, 8(1), 662. Glasser, Ivan, Pancotti, Nicola, August, Moritz, Rodriguez, Ivan D., and Cirac, J. Ignacio. 2018. Neural-network quantum states, string-bond states, and chiral topological states. Physical Review X, 8(Jan), 011006. Graves, Alex. 2013. Generating sequences with recurrent neural networks. ArXiv preprint arXiv:1308.0850. Graves, Alex, Mohamed, Abdel-Rahman, and Hinton, Geoï¬€rey. 2013. Speech recognition with deep recurrent neural networks. Pages 6645â€“6649 of: Proc. References 471 IEEE International Conference on Acoustics, Speech and Signal Processing. IEEE. Gull, Emanuel, Parcollet, Olivier, and Millis, Andrew J. 2013. Superconductivity and the pseudogap in the two-dimensional Hubbard model. Physical Review Letters, 110(21), 216405. Hackbusch, Wolfgang. 2012. Tensor Spaces and Numerical Tensor Calculus. Springer Science & Business Media. Hackbusch, Wolfgang, and KÃ¼hn, Stefan. 2009. A new scheme for the tensor representation. Journal of Fourier Analysis and Applications, 15(5), 706â€“722. Hall, Brian C. 2013. Quantum Theory for Mathematicians. Springer. Han, Zhao-Yu, Wang, Jun, Fan, Heng, Wang, Lei, and Zhang, Pan. 2017. Un- supervised generative modeling using matrix product states. ArXiv preprint arXiv:1709.01662. Hastings, Matthew B. 2007. An area law for one-dimensional quantum systems. Journal of Statistical Mechanics: Theory and Experiment, 2007(08), P08024. He, Kaiming, Zhang, Xiangyu, Ren, Shaoqing, and Sun, Jian. 2016. Deep residual learning for image recognition. Pages 770â€“778 of: Proc. IEEE Conference on Computer Vision and Pattern Recognition. Hermans, Michiel, and Schrauwen, Benjamin. 2013. Training and analysing deep recurrent neural networks. Pages 190â€“198 of: Advances in Neural Information Processing Systems. Khrulkov, Valentin, Novikov, Alexander, and Oseledets, Ivan. 2018. Expressive power of recurrent neural networks. In: Proc. 6th International Conference on Learning Representations. Kolda, Tamara G., and Bader, Brett W. 2009. Tensor decompositions and applica- tions. SIAM Review, 51(3), 455â€“500. Krizhevsky, Alex, Sutskever, Ilya, and Hinton, Geoï¬€rey E. 2012. ImageNet Clas- siï¬cation with Deep Convolutional Neural Networks. Pages 1097â€“1105 of: Advances in Neural Information Processing Systems, 25. LeBlanc, J. P. F., Antipov, Andrey E., Becca, Federico, Bulik, Ireneusz W., Chan, Garnet Kin-Lic, Chung, Chia-Min, Solutions of the two-dimensional Hubbard model: Benchmarks and results from a wide range of numerical algorithms. Physical Review X, 5(4), 041041. LeCun, Yann, Cortes, Corinna, and Burges, Christopher J. C. 1998. The MNIST Database of Handwritten Digits. Levine, Yoav, Sharir, Or, and Shashua, Amnon. 2017. On the long-term memory of deep recurrent networks. ArXiv preprint arXiv:1710.09431. Levine, Yoav, Yakira, David, Cohen, Nadav, and Shashua, Amnon. 2018. Deep learning and quantum entanglement: Fundamental connections with implica- tions to network design. In: Proc. 6th International Conference on Learning Representations. 472 Levine et al: Quantum Physics and Deep Learning via TNs Levine, Yoav, Sharir, Or, Cohen, Nadav, and Shashua, Amnon. 2019. Quantum entanglement in deep learning architectures. Physical Review Letters, 122(6), 065301. Levine, Yoav, Wies, Noam, Sharir, Or, Bata, Hoï¬t, and Shashua, Amnon. 2020. Limits to depth eï¬ƒciencies of self-attention. Pages 22640â€“22651 of: Advances in Neural Information Processing Systems, 33. Lin, Henry W., and Tegmark, Max. 2016. Critical behavior from deep dynamics: A hidden dimension in natural language. ArXiv preprint arXiv:1606.06737. Liu, Wen-Yuan, Dong, Shao-Jun, Han, Yong-Jian, Guo, Guang-Can, and He, Lixin. 2017. Gradient optimization of ï¬nite projected entangled pair states. Physical Review B, 95(19), 195154. Lubasch, Michael, Cirac, J. Ignacio, and Banuls, Mari-Carmen. 2014. Algorithms for ï¬nite projected entangled pair states. Physical Review B, 90(6), 064425. Mezzacapo, Fabio, Schuch, Norbert, Boninsegni, Massimo, and Cirac, J. Ignacio. 2009. Ground-state properties of quantum many-body systems: Entangled- plaquette states and variational Monte Carlo. New Journal of Physics, 11(8), 083026. Oord, Aaron van den, Kalchbrenner, Nal, and Kavukcuoglu, Koray. 2016. Pixel recurrent neural networks. ArXiv preprint arXiv:1601.06759. OrÃºs, RomÃ¡n. 2014. A practical introduction to tensor networks: Matrix product states and projected entangled pair states. Annals of Physics, 349, 117â€“158. Oseledets, Ivan V. 2011. Tensor-train decomposition. SIAM Journal on Scientiï¬c Computing, 33(5), 2295â€“2317. Perez-GarcÃ­a, David, Verstraete, Frank, Wolf, Michael M., and Cirac, J. Ignacio. 2007. Matrix product state representations. Quantum Information and Com- putation, 7(5-6), 401â€“430. Plenio, Martin B, and Virmani, Shashank. 2007. An introduction to entanglement measures. Quantum Information and Computation, 7(1), 001â€“051. Preskill, John. 1998. Lecture Notes for Physics 229: Quantum Information and Computation. California Institute of Technology. Saito, Hiroki. 2017. Solving the Boseâ€“Hubbard model with machine learning. Journal of the Physical Society of Japan, 86(9), 093001. SchrÃ¶dinger, Erwin. 1935. Discussion of probability relations between separated systems. Mathematical Proceedings of the Cambridge Philosophical Society, 31, 555â€“563. Schuch, Norbert, Wolf, Michael M., Verstraete, Frank, and Cirac, J. Ignacio. 2008. Simulation of quantum many-body systems with strings of operators and Monte Carlo tensor contractions. Physical Review Letters, 100(4), 040501. Sharir, Or, and Shashua, Amnon. 2018. On the expressive power of overlapping architectures of deep learning. In: Proc. 6th International Conference on Learning Representations. References 473 Sharir, Or, Tamari, Ronen, Cohen, Nadav, and Shashua, Amnon. 2016. Tractable generative convolutional arithmetic circuits. ArXiv preprint arXiv:1610.04167. Sharir, Or, Levine, Yoav, Wies, Noam, Carleo, Giuseppe, and Shashua, Amnon. 2020. Deep autoregressive models for the eï¬ƒcient variational simulation of many-body quantum systems. Physical Review Letters, 124(January), 020503. Simonyan, Karen, and Zisserman, Andrew. 2014. Very deep convolutional networks for large-scale image recognition. ArXiv preprint arXiv:1409.1556. Stoudenmire, E. Miles. 2017. Learning relevant features of data with multi-scale tensor networks. ArXiv preprint arXiv:1801.00315. Stoudenmire, Edwin, and Schwab, David J. 2016. Supervised learning with tensor networks. Pages 4799â€“4807 of: Advances in Neural Information Processing Systems, 29. Sutskever, Ilya, Martens, James, and Hinton, Geoï¬€rey E. 2011. Generating text with recurrent neural networks. Pages 1017â€“1024 of: Proc. 28th International Conference on Machine Learning. Szegedy, Christian, Liu, Wei, Jia, Yangqing, Sermanet, Pierre, Reed, Scott, Anguelov, Dragomir, Erhan, Dumitru, Vanhoucke, Vincent, and Rabinovich, Andrew. 2015. Going deeper with convolutions. In: Proc. IEEE Conference on Computer Vision and Pattern Recognition. van den Oord, Aaron, Kalchbrenner, Nal, Espeholt, Lasse, Vinyals, Oriol, Graves, Alex, et al. 2016. Conditional image generation with pixelcnn decoders. Pages 4790â€“4798 of: Advances in Neural Information Processing Systems. Vedral, Vlatko, and Plenio, Martin B. 1998. Entanglement measures and puriï¬ca- tion procedures. Physical Review A, 57(3), 1619. Verstraete, Frank, and Cirac, J. Ignacio. 2004. Renormalization algorithms for quantum-many body systems in two and higher dimensions. ArXiv preprint cond-mat/0407066. Vidal, Guifre. 2007. Entanglement renormalization. Physical Review Letters, 99(22), 220405. Wu, Yuhuai, Zhang, Saizheng, Zhang, Ying, Bengio, Yoshua, and Salakhutdinov, Ruslan R. 2016. On multiplicative integration with recurrent neural networks. Pages 2856â€“2864 of: Advances in Neural Information Processing Systems. Zheng, Bo-Xiao, and Chan, Garnet Kin-Lic. 2016. Ground-state phase diagram of the square lattice Hubbard model from density matrix embedding theory. Physical Review B, 93(3), 035126.